commit c5009e61286eb6bc68192b8c823a60a4b3d71934
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 5 07:53:33 2024 +0300

    py : switch to snake_case (#8305)
    
    * py : switch to snake_case
    
    ggml-ci
    
    * cont
    
    ggml-ci
    
    * cont
    
    ggml-ci
    
    * cont : fix link
    
    * gguf-py : use snake_case in scripts entrypoint export
    
    * py : rename requirements for convert_legacy_llama.py
    
    Needed for scripts/check-requirements.sh
    
    ---------
    
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>

README.md
ci/run.sh
convert_hf_to_gguf.py
convert_hf_to_gguf_update.py
docs/HOWTO-add-model.md
examples/convert_legacy_llama.py
examples/finetune/convert_finetune_checkpoint_to_gguf.py
examples/json_schema_pydantic_example.py
examples/llava/MobileVLM-README.md
examples/llava/README.md
examples/llava/convert_image_encoder_to_gguf.py
examples/llava/llava_surgery.py
examples/llava/llava_surgery_v2.py
examples/llava/requirements.txt
examples/pydantic_models_to_grammar_examples.py
examples/regex_to_grammar.py
examples/server_embd.py
examples/train-text-from-scratch/convert_train_checkpoint_to_gguf.py
gguf-py/README.md
gguf-py/scripts/__init__.py
gguf-py/scripts/gguf_convert_endian.py
gguf-py/scripts/gguf_dump.py
gguf-py/scripts/gguf_new_metadata.py
gguf-py/scripts/gguf_set_metadata.py
requirements.txt
requirements/requirements-convert_hf_to_gguf.txt
requirements/requirements-convert_hf_to_gguf_update.txt
requirements/requirements-convert_legacy_llama.txt
requirements/requirements-convert_llama_ggml_to_gguf.txt
scripts/check-requirements.sh
scripts/convert-gg.sh
scripts/pod-llama.sh

commit 6d6ecd3200669b0d1879e491f82dae623aba7757
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Jul 4 20:55:03 2024 +0200

    cli: add EOT when user hit Ctrl+C (#8296)
    
    * main: add need_insert_eot
    
    * do not format system prompt if it is empty

common/common.cpp
examples/main/main.cpp

commit cbfc8507935b4393270a5218ca803f8070b533ae
Author: Icecream95 <the.real.icecream95@gmail.com>
Date:   Fri Jul 5 05:14:21 2024 +1200

    llama : add OpenELM support (#7359)
    
    * Initial OpenELM support (270M only so far)
    
    * Fill out missing entries in llama_model_type_name
    
    * fixup! Initial OpenELM support (270M only so far)
    
    Fix formatting
    
    * llama : support all OpenELM models
    
    * llama : add variable GQA and variable FFN sizes
    
    Some metadata keys can now also be arrays to support setting
    their value per-layer for models like OpenELM.
    
    * llama : minor spacing changes
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * llama : use std::array for per-layer hparams
    
    * llama : fix save/load state
    
    * llama : do not print hparams for vocab-only models
    
    * llama : handle n_head == 0
    
    * llama : use const ref for print_f and fix division by zero
    
    * llama : fix t5 uses of n_head and n_ff
    
    * llama : minor comment
    
    ---------
    
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert_hf_to_gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/tensor_mapping.py
src/llama.cpp

commit 63c6e90eabc965027afe212e8cbb1343cb31ab2a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jul 4 18:38:58 2024 +0200

    tokenize : add --show-count (token) option (#8299)
    
    This commit adds a new option to the tokenize example, --show-count.
    When this is set the total number of tokens are printed to stdout.
    
    This was added as an option as I was concerned that there might be
    scripts that use the output from this program and it might be better to
    not print this information by default.
    
    The motivation for this is that can be useful to find out how many
    tokens a file contains, for example when trying to determine prompt
    input file sizes for testing.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/tokenize/tokenize.cpp

commit 498d561ab1fdbecad007413154e4888bd608a156
Author: ditsuke <ditsuke@protonmail.com>
Date:   Thu Jul 4 20:54:35 2024 +0530

    build: Export hf-to-gguf as snakecase

CMakeLists.txt

commit cb46165d9e7608ca002615803d3e3fb80e58f69d
Author: ditsuke <ditsuke@protonmail.com>
Date:   Wed Jul 3 01:02:56 2024 +0530

    doc: Add context for why we add an explicit pytorch source

pyproject.toml

commit ba8aea8457244c4f62956f0cf88adaeaefaf26d1
Author: ditsuke <ditsuke@protonmail.com>
Date:   Tue Jul 2 15:48:13 2024 +0530

    chore: Remove rebase artifacts

convert_hf_to_gguf_update.py
convert_lora_to_ggml.py
convert_persimmon_to_gguf.py
pyproject.toml
requirements.txt
requirements/requirements-convert_lora_to_ggml.txt
requirements/requirements-convert_persimmon_to_gguf.txt

commit 1d1fea0b6e15f4fc2b1060b1e727a5282e2825e6
Author: ditsuke <ditsuke@protonmail.com>
Date:   Tue Jul 2 15:35:43 2024 +0530

    chore: Fixup requirements and build

CMakeLists.txt
examples/llava/requirements.txt
pyproject.toml
requirements.txt
requirements/requirements-convert_lora_to_ggml.txt
requirements/requirements-convert_persimmon_to_gguf.txt

commit 1ee5d59f67b3ca867c91ba907e5c45de816918a1
Author: ditsuke <ditsuke@protonmail.com>
Date:   Tue Jul 2 15:18:13 2024 +0530

    chore: ignore all __pychache__

.gitignore

commit 3aefc742fee2b3c18e9d58c83c5a74ffa9c4aa79
Author: ditsuke <ditsuke@protonmail.com>
Date:   Sun Mar 10 23:21:46 2024 +0530

    fix: Update script paths in CI scripts

ci/run.sh
requirements.txt
requirements/requirements-convert_hf_to_gguf.txt
requirements/requirements-convert_hf_to_gguf_update.txt
requirements/requirements-convert_llama_ggml_to_gguf.txt
requirements/requirements-convert_lora_to_ggml.txt
requirements/requirements-convert_persimmon_to_gguf.txt
scripts/check-requirements.sh

commit 84f249c4e8f37f151e90b357eb48861d1f49d701
Author: ditsuke <ditsuke@protonmail.com>
Date:   Thu Feb 29 01:47:15 2024 +0530

    fix: Actually include scripts in build
    
    Not namespaced though :(

__init__.py
pyproject.toml

commit 2c753017aee615d7a80bc184b4fdcd9de1fe4902
Author: ditsuke <ditsuke@protonmail.com>
Date:   Tue Feb 27 12:01:02 2024 +0530

    build(python): Package scripts with pip-0517 compliance

.gitignore
__init__.py
convert_hf_to_gguf.py
convert_hf_to_gguf_update.py
convert_llama_ggml_to_gguf.py
convert_lora_to_ggml.py
convert_persimmon_to_gguf.py
poetry.lock
pyproject.toml

commit ff2ca9cfb70209da1e7b8a0e4b654337c7187895
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Thu Jul 4 15:46:11 2024 +0200

    Inference support for T5 and FLAN-T5 model families (#5763)
    
    * llama : add inference support and model types for T5 and FLAN-T5 model families
    
    * llama : add new API functions to support encoder-decoder models: llama_encode(), llama_model_has_encoder(), llama_model_decoder_start_token()
    
    * common, llama-cli, llama-batched : add support for encoder-decoder models
    
    * convert-hf : handle shared token embeddings tensors in T5Model
    
    * convert-hf : add support for SentencePiece BPE tokenizer in T5Model (for Pile-T5 models)
    
    * convert-hf : add MT5ForConditionalGeneration and UMT5ForConditionalGeneration to architectures supported by T5Model
    
    * convert : add t5 tokenizer tests, use "slow" HF tokenizer for t5
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
examples/batched/batched.cpp
examples/main/main.cpp
include/llama.h
models/ggml-vocab-bert-bge.gguf.inp
models/ggml-vocab-bert-bge.gguf.out
models/ggml-vocab-command-r.gguf.inp
models/ggml-vocab-command-r.gguf.out
models/ggml-vocab-deepseek-coder.gguf.inp
models/ggml-vocab-deepseek-coder.gguf.out
models/ggml-vocab-deepseek-llm.gguf.inp
models/ggml-vocab-deepseek-llm.gguf.out
models/ggml-vocab-falcon.gguf.inp
models/ggml-vocab-falcon.gguf.out
models/ggml-vocab-gpt-2.gguf.inp
models/ggml-vocab-gpt-2.gguf.out
models/ggml-vocab-llama-bpe.gguf.inp
models/ggml-vocab-llama-bpe.gguf.out
models/ggml-vocab-llama-spm.gguf.inp
models/ggml-vocab-llama-spm.gguf.out
models/ggml-vocab-mpt.gguf.inp
models/ggml-vocab-mpt.gguf.out
models/ggml-vocab-phi-3.gguf.inp
models/ggml-vocab-phi-3.gguf.out
models/ggml-vocab-qwen2.gguf.inp
models/ggml-vocab-qwen2.gguf.out
models/ggml-vocab-refact.gguf.inp
models/ggml-vocab-refact.gguf.out
models/ggml-vocab-starcoder.gguf.inp
models/ggml-vocab-starcoder.gguf.out
src/llama.cpp

commit 3a710b6aaf9af63a9289ed5d158a8b359fcc411b
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jul 4 12:53:42 2024 +0200

    tests : add _CRT_SECURE_NO_WARNINGS for WIN32 (#8231)
    
    This commit adds the compile definition `_CRT_SECURE_NO_WARNINGS`
    to the root cmake subproject.
    
    The motivation for this is that currently the following warnings are
    displayed when compiling the tests and common cmake subprojects:
    ```console
    test-llama-grammar.cpp
    C:\llama.cpp\src\.\llama.cpp(1406,77): warning C4996: 'strerror':
    This function or variable may be unsafe. Consider using strerror_s
    instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. See
    online help for details.
    [C:\llama.cpp\build\tests\test-llama-grammar.vcxproj]
    ...
    ```
    
    This compile definition is currently set for the `src` subproject
    and this change moves into the root cmake project so that it is applied
    to all cmake subprojects.

CMakeLists.txt
src/CMakeLists.txt

commit ef1600090f091ff791b0023375684fff185e8aef
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jul 4 12:50:57 2024 +0200

    llama : suppress unref var in Windows MSVC (#8150)
    
    * llama : suppress unref var in Windows MSVC
    
    This commit suppresses two warnings that are currently generated for
    src/llama.cpp when building on Windows MSVC
    
    ```console
    C:\llama.cpp\src\llama.cpp(14349,45): warning C4101: 'ex':
    unreferenced local variable [C:\llama.cpp\build\src\llama.vcxproj]
    C:\llama.cpp\src\llama.cpp(19285,44): warning C4101: 'e':
    unreferenced local variable [C:\llama.cpp\build\src\llama.vcxproj]
    ```
    
    * Update src/llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

src/llama.cpp

commit e9d503a5d7ed15364413cf5330df7ffde6ebb8a2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 4 10:41:03 2024 +0300

    convert : fix gemma v1 tokenizer convert (#8248)
    
    ggml-ci

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
models/ggml-vocab-bert-bge.gguf.inp
models/ggml-vocab-bert-bge.gguf.out
models/ggml-vocab-command-r.gguf.inp
models/ggml-vocab-command-r.gguf.out
models/ggml-vocab-deepseek-coder.gguf.inp
models/ggml-vocab-deepseek-coder.gguf.out
models/ggml-vocab-deepseek-llm.gguf.inp
models/ggml-vocab-deepseek-llm.gguf.out
models/ggml-vocab-falcon.gguf.inp
models/ggml-vocab-falcon.gguf.out
models/ggml-vocab-gpt-2.gguf.inp
models/ggml-vocab-gpt-2.gguf.out
models/ggml-vocab-llama-bpe.gguf.inp
models/ggml-vocab-llama-bpe.gguf.out
models/ggml-vocab-llama-spm.gguf.inp
models/ggml-vocab-llama-spm.gguf.out
models/ggml-vocab-mpt.gguf.inp
models/ggml-vocab-mpt.gguf.out
models/ggml-vocab-phi-3.gguf.inp
models/ggml-vocab-phi-3.gguf.out
models/ggml-vocab-qwen2.gguf.inp
models/ggml-vocab-qwen2.gguf.out
models/ggml-vocab-refact.gguf.inp
models/ggml-vocab-refact.gguf.out
models/ggml-vocab-starcoder.gguf.inp
models/ggml-vocab-starcoder.gguf.out

commit ab0e5dee198abf3c9e64dba5bc11a6f039415833
Author: Daniele <57776841+daniandtheweb@users.noreply.github.com>
Date:   Wed Jul 3 23:02:58 2024 +0000

    Define and optimize  RDNA1 (#8085)

ggml/src/ggml-cuda/common.cuh
ggml/src/ggml-cuda/mmq.cuh

commit 80ffd6e4977686bdb46f0282d2949acc080952a9
Author: slaren <slarengh@gmail.com>
Date:   Wed Jul 3 19:33:31 2024 +0200

    ppl : fix n_seq_max for perplexity (#8277)
    
    * ppl : fix n_seq_max for perplexity
    
    * use 1 seq for kl_divergence

examples/perplexity/perplexity.cpp

commit 40a2a1b936d91165a14428ade8563a38e6b34ab7
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Jul 3 16:01:54 2024 +0200

    fix phi 3 conversion (#8262)

convert-hf-to-gguf.py

commit fdef7d606ef4864ceb5f51a576c0ecddec2cce2a
Author: Neo Zhang <NA>
Date:   Thu Jul 4 11:55:23 2024 +0800

    replace get_work_group_size() by local buf

examples/sycl/build.sh
ggml/src/ggml-sycl.cpp
ggml/src/ggml-sycl/common.hpp
ggml/src/ggml-sycl/norm.cpp

commit 249347995825713ad03b9cda3a036851f2e95033
Author: Neo Zhang <NA>
Date:   Thu Jul 4 08:28:58 2024 +0800

    skip UT for BF16

ggml/src/ggml-sycl.cpp
ggml/src/ggml-sycl/convert.cpp

commit 96e3826f83a834abcd692f3410363e50c5d6eaba
Author: Neo Zhang <NA>
Date:   Wed Jul 3 12:59:34 2024 +0800

    update for title

README.md

commit 51be86243892833c9779ae903d5b54241ec2507a
Author: AidanBeltonS <aidan.belton@codeplay.com>
Date:   Wed Jul 3 02:55:34 2024 +0100

    Dequant improvements rebase (#8255)
    
    * Single load for half2
    
    * Store scales in local mem
    
    * Vec load quantized values

ggml/src/ggml-sycl/common.hpp
ggml/src/ggml-sycl/convert.cpp
ggml/src/ggml-sycl/dequantize.hpp

commit 85ec6c02c24f9e4f948694895b1aedf2528aab50
Author: MistApproach <98988043+MistApproach@users.noreply.github.com>
Date:   Tue Jul 2 22:56:46 2024 +0200

    fix: add missing short command line argument -mli for multiline-input (#8261)

common/common.cpp

commit 044995e2d1fbc21ca275be66a9cb580034124502
Author: Clint Herron <hanclinto@gmail.com>
Date:   Tue Jul 2 12:18:10 2024 -0400

    Removes multiple newlines at the end of files that is breaking the editorconfig step of CI. (#8258)

.github/ISSUE_TEMPLATE/config.yml
common/common.h
examples/embedding/README.md
examples/infill/infill.cpp
examples/lookup/README.md
examples/main-cmake-pkg/.gitignore
examples/main-cmake-pkg/CMakeLists.txt
examples/server-embd.py
examples/server/tests/features/passkey.feature
examples/server/themes/buttons-top/index.html
examples/server/themes/wild/index.html
examples/sycl/run-llama2.sh
examples/sycl/win-build-sycl.bat
examples/sycl/win-run-llama2.bat
ggml/include/ggml-metal.h
ggml/src/ggml-cuda/cpy.cu
ggml/src/ggml-metal.metal
ggml/src/ggml-quants.h
ggml/src/ggml-vulkan-shaders.hpp
scripts/pod-llama.sh
src/unicode-data.cpp
tests/test-rope.cpp

commit 6b695b5a2c7b6dd0cab57f22afda79fc4be5a892
Author: Faisal Zaghloul <faisal.zaghloul@gmail.com>
Date:   Tue Jul 2 10:36:00 2024 -0400

    Add `JAIS` model(s) (#8118)
    
    * Add `JAIS` model(s)
    
    * cleanup
    
    * address review comments
    
    * remove hack
    
    * un-hardcode max-alibi-bias
    
    * minor tweaks
    
    ---------
    
    Co-authored-by: fmz <quic_fzaghlou@quic.com>

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
include/llama.h
src/llama.cpp

commit 785f24b9548122a12526adb60666b94d83da451e
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jul 2 08:40:49 2024 +0200

    convert-hf : print output file name when completed (#8181)
    
    * convert-hf : print output file name when completed
    
    This commit adds the output file name to the log message when the
    conversion is completed.
    
    The motivation for this change is that when `--outfile` option is not
    specified it migth not be obvious where the output file is written.
    
    With this change the output of running the script will be something like
    the following:
    ```console
    INFO:hf-to-gguf:Model successfully exported to models/gemma-2-9b-it.gguf.
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! convert-hf : print output file name when completed
    
    Updates the output of to support printing the directory if the output is
    split into multiple files. Also the output file name is now retrieved
    from the model_instance object.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! convert-hf : print output file name when completed
    
    Use parent attribute of Path object and string interpolation.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! convert-hf : print output file name when completed
    
    Use os.sep instead of hardcoding the path separator.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

convert-hf-to-gguf.py

commit 726953cda51b9af3686592973bd85c1a7a97d405
Author: slaren <slarengh@gmail.com>
Date:   Tue Jul 2 08:39:38 2024 +0200

    cuda : update supports_op for matrix multiplication (#8245)

ggml/src/ggml-cuda.cu
tests/test-backend-ops.cpp

commit 9c593619f363fda422375b386674d558fa48d437
Author: Neo Zhang <NA>
Date:   Wed Jul 3 11:20:54 2024 +0800

    fix multiple gpu, add device choose mode, update the guide for usages

README-sycl.md
examples/sycl/CMakeLists.txt
examples/sycl/win-run-llama2.bat
ggml/include/ggml-sycl.h
ggml/src/ggml-sycl.cpp
ggml/src/ggml-sycl/common.cpp
ggml/src/ggml-sycl/common.hpp
ggml/src/ggml-sycl/dpct/helper.hpp
src/llama.cpp

commit de2763118fd5b6ea89702cc9981349e0556b0c3d
Author: Jianyu Zhang <jianyu.zhang@intel.com>
Date:   Wed Jun 19 22:54:15 2024 +0800

    fix to support multiple GPUs, fix set single device, unify id/device_id/device_index

ggml/include/ggml-sycl.h
ggml/src/ggml-sycl.cpp
ggml/src/ggml-sycl/common.hpp
src/llama.cpp

commit a9f3b102157ba992cfe058909b7f6e1906d2d647
Author: luoyu-intel <yu.luo@intel.com>
Date:   Tue Jul 2 04:50:07 2024 +0000

    [SYCL] Fix win build conflict of math library (#8230)
    
    * fix win build conflict of math library
    
    * fix the condition: !(win32 & SYCL)
    
    * revert warp_size=16

CMakePresets.json
ggml/src/CMakeLists.txt

commit d08c20eddedb24515a3212e2de66bdff41a26b8c
Author: luoyu-intel <yu.luo@intel.com>
Date:   Tue Jul 2 02:16:00 2024 +0000

    [SYCL] Fix the sub group size of Intel (#8106)
    
    * use warp_size macro for all sycl kernels
    
    * fix mask of permute_sub_group_by_xor
    
    * fix rms_norm with correct warp number
    
    * fix rms_norm_f32/group_norm_f32
    
    * move norm to norm.cpp file
    
    * fix quantize bug
    
    * fix mmvq's batch size

ggml/src/CMakeLists.txt
ggml/src/ggml-sycl.cpp
ggml/src/ggml-sycl/backend.hpp
ggml/src/ggml-sycl/common.hpp
ggml/src/ggml-sycl/dmmv.cpp
ggml/src/ggml-sycl/mmvq.cpp
ggml/src/ggml-sycl/norm.cpp
ggml/src/ggml-sycl/norm.hpp
ggml/src/ggml-sycl/presets.hpp

commit 5fac350b9cc49d0446fc291b9c4ad53666c77591
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Jul 2 01:07:23 2024 +0200

    Fix gemma2 tokenizer convert (#8244)
    
    * fix gemma2 tokenizer convert
    
    * remove scores
    
    * improve code, fix new line issue

convert-hf-to-gguf.py

commit cb5fad4c6c2cbef92e9b8b63449e1cb7664e4846
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 1 20:39:06 2024 +0200

    CUDA: refactor and optimize IQ MMVQ (#8215)
    
    * CUDA: refactor and optimize IQ MMVQ
    
    * uint -> uint32_t
    
    * __dp4a -> ggml_cuda_dp4a
    
    * remove MIN_CC_DP4A checks
    
    * change default
    
    * try CI fix

ggml/src/ggml-common.h
ggml/src/ggml-cuda.cu
ggml/src/ggml-cuda/common.cuh
ggml/src/ggml-cuda/fattn-common.cuh
ggml/src/ggml-cuda/mmvq.cu
ggml/src/ggml-cuda/vecdotq.cuh
ggml/src/ggml-sycl/mmvq.cpp
ggml/src/ggml-sycl/vecdotq.hpp

commit dae57a1ebc1c9bd5693ab999e19d77c5506ae559
Author: Mateusz Charytoniuk <mateusz.charytoniuk@protonmail.com>
Date:   Mon Jul 1 19:13:22 2024 +0200

    readme: add Paddler to the list of projects (#8239)

README.md

commit 49122a873f54615626d1b49a2a39013ed4be98d5
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Jul 1 18:48:34 2024 +0200

    gemma2: add sliding window mask (#8227)
    
    * gemma2: add sliding window mask
    
    * fix data_swa uninitialized
    
    * better naming
    
    * add co-author
    
    Co-authored-by: Arlo Phoenix <arlo-phoenix@users.noreply.github.com>
    
    * replace list with single tensor
    
    * update
    
    * llama : minor styling
    
    * convert : add sanity check for query_pre_attn_scalar
    
    * fix small typo in README
    
    ---------
    
    Co-authored-by: Arlo Phoenix <arlo-phoenix@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
src/llama.cpp

commit 0ddeff10230b88f1fa9866bbe5fe0d71ba2323a0
Author: Roni <sulpher@gmx.net>
Date:   Mon Jul 1 14:48:16 2024 +0200

    readme : update tool list (#8209)
    
    * Added gppm to Tool list in README
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md

commit 3840b6f593751a0ba636bfda73b630cd6c29d7b5
Author: Michael Francis <edude03@gmail.com>
Date:   Mon Jul 1 07:47:04 2024 -0400

    nix : enable curl (#8043)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.devops/nix/package.nix

commit 257f8e41e24b5bbfc27d9e907189a3e0cdb650d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 1 14:46:18 2024 +0300

    nix : remove OpenCL remnants (#8235)
    
    * nix : remove OpenCL remnants
    
    * minor : remove parentheses

.devops/nix/package.nix

commit 694c59cb42d1ebd6a7d912ca65d3d7363e0f14c9
Author: iacore <74560659+iacore@users.noreply.github.com>
Date:   Mon Jul 1 11:40:58 2024 +0000

    Document BERT support. (#8205)
    
    * Update README.md
    
    document BERT support
    
    * Update README.md

README.md

commit 197fe6c1d7bec6718ce901f0141b2725240f298c
Author: zhentaoyu <zhentao.yu@intel.com>
Date:   Mon Jul 1 19:39:06 2024 +0800

    [SYCL] Update SYCL-Rope op and Refactor (#8157)
    
    * align with rope.cu and move sycl-op to a single file

ggml/src/ggml-sycl.cpp
ggml/src/ggml-sycl/backend.hpp
ggml/src/ggml-sycl/rope.cpp
ggml/src/ggml-sycl/rope.hpp

commit d0a7145ba99ed3a8bc3145aa785b5c86ffe65020
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 1 02:09:34 2024 +0300

    flake.lock: Update (#8218)

flake.lock

commit 9ef07800622e4c371605f9419864d15667c3558f
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Jun 30 20:27:13 2024 +0200

    Fix new line issue with chat template, disable template when in-prefix/suffix is set (#8203)
    
    * preserve new line llama_chat_format_single
    
    * disable chat template if in-prefix/suffix is set
    
    * remove redundant change

common/common.cpp
common/common.h
examples/main/main.cpp
tests/test-chat-template.cpp

commit 1c5eba6f8e628fb0a98afb27d8aaeb3b0e136451
Author: Andrei <abetlen@gmail.com>
Date:   Sat Jun 29 20:44:08 2024 -0700

    llama: Add attention and final logit soft-capping, update scaling factor to Gemma2 (#8197)
    
    * Add attention and final logit softcapping.
    
    * fix
    
    * Add custom add_ functions
    
    * Disable flash attention for Gemma2
    
    * Update src/llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Add default value for attention and final logit softcap value
    
    * Add custom kq scaling from Gemma2Attention
    
    * Remove custom pre attention scaling and use computed value instead.
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
src/llama.cpp

commit 72272b83a3878e91251218c981b4c6ec16c33912
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Jun 29 00:14:20 2024 +0200

    fix code typo in llama-cli (#8198)

examples/main/main.cpp

commit 8748d8ac6f172b99826ab18f01d9a3a165987d54
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Jun 28 18:02:05 2024 +0100

    json: attempt to skip slow tests when running under emulator (#8189)

.github/workflows/build.yml
tests/test-json-schema-to-grammar.cpp

commit 26a39bbd6b0bbd66118bb68569f0276d7fe7df6c
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Jun 28 15:11:44 2024 +0200

    Add MiniCPM, Deepseek V2 chat template + clean up `llama_chat_apply_template_internal` (#8172)
    
    * tmp_contains
    
    * minicpm chat template
    
    * add DeepSeek Lite template
    
    * change deepseek-lite to deepseek2
    
    * correct code comment
    
    * correct code from master branch

src/llama.cpp
tests/test-chat-template.cpp

commit 38373cfbab5397cc2ab5c3694a3dee12a9e58f45
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Fri Jun 28 12:53:43 2024 +0200

    Add SPM infill support (#8016)
    
    * add --spm-infill option
    
    * support --spm-infill
    
    * support --spm-infill

common/common.cpp
common/common.h
examples/infill/README.md
examples/infill/infill.cpp
examples/server/README.md
examples/server/server.cpp

commit b851b3fba0a1b06a1129189bac300e07dd1648c8
Author: slaren <slarengh@gmail.com>
Date:   Fri Jun 28 12:37:45 2024 +0200

    cmake : allow user to override default options (#8178)

CMakeLists.txt

commit 139cc621e90b4f61830515c3c124cf35b3d7a6dc
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Jun 28 09:26:45 2024 +0100

    `json`: restore default additionalProperties to false, fix some pattern escapes (#8180)
    
    * json: expand ESCAPED_IN_REGEXPS_BUT_NOT_IN_LITERALS charset
    
    * json: revert default of additionalProperties to false
    
    * Update README.md

common/json-schema-to-grammar.cpp
examples/json_schema_to_grammar.py
examples/server/public/json-schema-to-grammar.mjs
grammars/README.md
tests/test-grammar-integration.cpp
tests/test-json-schema-to-grammar.cpp

commit e57dc62057d41211ac018056c19c02cd544694df
Author: pculliton <phillipculliton@gmail.com>
Date:   Fri Jun 28 00:00:43 2024 -0400

    llama: Add support for Gemma2ForCausalLM (#8156)
    
    * Inference support for Gemma 2 model family
    
    * Update convert-hf-to-gguf.py, constants, and tensor mappings
    
    * cleanup
    
    * format fix
    
    * Fix special token vocab bug
    
    * Don't add space prefix
    
    * fix deleted lines
    
    * Update src/llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Add model type names
    
    * Add control vector
    
    * Fix model type identification
    
    ---------
    
    Co-authored-by: Andrei Betlen <abetlen@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
src/llama.cpp

commit a27aa50ab7e07fe46aae619076b6e31d5663e914
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Jun 28 02:19:11 2024 +0200

    Add missing items in makefile (#8177)

Makefile

commit cb0b06a8a613f7a2ccb7253b2a3c00fdd397ba1c
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Jun 27 22:08:42 2024 +0100

    `json`: update grammars/README w/ examples & note about additionalProperties (#8132)
    
    * json: update grammars/README
    
    * mention broken prefixItems
    
    * add mention to llama-gbnf-validator
    
    * json: explicit type: object for nested items object in cli example

grammars/README.md

commit 558f44bf83d78242d4e5c4ab98d0be9125cb9780
Author: loonerin <132926317+loonerin@users.noreply.github.com>
Date:   Thu Jun 27 15:01:23 2024 -0400

    CI: fix release build (Ubuntu+Mac) (#8170)
    
    * CI: fix release build (Ubuntu)
    
    PR #8006 changes defaults to build shared libs. However, CI for releases
    expects static builds.
    
    * CI: fix release build (Mac)
    
    ---------
    
    Co-authored-by: loonerin <loonerin@users.noreply.github.com>

.github/workflows/build.yml

commit 8172ee9da9921ca53d698c7438c2d792b3f3f2c8
Author: slaren <slarengh@gmail.com>
Date:   Thu Jun 27 20:04:39 2024 +0200

    cmake : fix deprecated option names not working (#8171)
    
    * cmake : fix deprecated option names not working
    
    * remove LlAMA_OPENMP

CMakeLists.txt

commit 16791b8f0b4526aafbf5d0e5bbbd2e99c2253418
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Jun 27 18:14:19 2024 +0200

    Add chatml fallback for cpp `llama_chat_apply_template` (#8160)
    
    * add chatml fallback for cpp `llama_chat_apply_template`
    
    * remove redundant code

common/common.cpp
common/common.h

commit ab3679112d4c49a215a3d31550a7720b202e9015
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 27 18:37:29 2024 +0300

    flake.lock: Update (#8071)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/e9ee548d90ff586a6471b4ae80ae9cfcbceb3420?narHash=sha256-4Zu0RYRcAY/VWuu6awwq4opuiD//ahpc2aFHg2CWqFY%3D' (2024-06-13)
      → 'github:NixOS/nixpkgs/d603719ec6e294f034936c0d0dc06f689d91b6c3?narHash=sha256-k3JqJrkdoYwE3fHE6xGDY676AYmyh4U2Zw%2B0Bwe5DLU%3D' (2024-06-20)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Philip Taron <philip.taron@gmail.com>

flake.lock

commit 97877eb10bd8e7f8023420b5b5300bcbdadd62dc
Author: jukofyork <69222624+jukofyork@users.noreply.github.com>
Date:   Thu Jun 27 15:48:07 2024 +0100

    Control vector loading fixes (#8137)
    
    * Fixed leak in llama_control_vector_load_one() and allow llama_control_vector_load() to grow
    
    * refactored `llama_control_vector_load_one()`
    
    * allow multiple directions for same layer in same file
    
    * llama_control_vector_load_one() and llama_control_vector_load() now break on error
    
    * removed unnecessary ggml_free() call

common/common.cpp

commit 387952651a8fc493f8c85ea4c9774bd4a5694f87
Author: Raj Hammeer Singh Hada <hammeerraj@gmail.com>
Date:   Thu Jun 27 20:09:29 2024 +0530

    Delete examples/llama.android/llama/CMakeLists.txt (#8165)
    
    * Delete examples/llama.android/llama/CMakeLists.txt
    
    https://github.com/ggerganov/llama.cpp/pull/8145#issuecomment-2194534244
    
    This file is not being used for building on Android. `llama.cpp/examples/llama.android/llama/src/main/cpp/CMakeLists.txt` is being used instead.
    
    * Update CMakeLists.txt
    
    Pick local llama.cpp files instead of fetching content from git

examples/llama.android/llama/CMakeLists.txt
examples/llama.android/llama/src/main/cpp/CMakeLists.txt

commit 6030c61281c8a7eb94eceb7396a608fac8b71555
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Thu Jun 27 16:27:41 2024 +0200

    Add Qwen2MoE 57B-A14B model identifier (#8158)
    
    * Add Qwen2MoE 57B-A14B
    
    * Add Qwen2MoE 57B-A14B

src/llama.cpp

commit 85a267daaa1c6f8fd69160445bcb88717031d10c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 27 16:26:05 2024 +0200

    CUDA: fix MMQ stream-k for --split-mode row (#8167)

ggml/src/ggml-cuda/mmq.cuh

commit f675b20a3b7f878bf3be766b9a737e2c8321ff0d
Author: kustaaya <58045274+kustaaya@users.noreply.github.com>
Date:   Thu Jun 27 11:58:54 2024 +0300

    Added support for Viking pre-tokenizer (#8135)
    
    Co-authored-by: kustaaya <kustaaya@protonmail.com>

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
include/llama.h
src/llama.cpp

commit 911e35bb8bb2fd1c7d3f40f27e96ff432eae7e14
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Thu Jun 27 09:46:41 2024 +0200

    llama : fix CodeLlama FIM token checks (#8144)
    
    * account for space prefix character
    
    * use find instead

src/llama.cpp

commit ac146628e47451c531a3c7e62e6a973a2bb467a0
Author: Raj Hammeer Singh Hada <hammeerraj@gmail.com>
Date:   Thu Jun 27 07:27:57 2024 +0530

    Fix llama-android.cpp for error - "common/common.h not found" (#8145)
    
    - Path seems to be wrong for the common.h header file in llama-android.cpp file. Fixing the path so the Android Build doesn't fail with the error "There is no file common/common.h"

examples/llama.android/llama/src/main/cpp/llama-android.cpp

commit 9b31a40c6ddabe552875b811d7127aa039ca9703
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jun 27 01:50:09 2024 +0200

    clip : suppress unused variable warnings (#8105)
    
    * clip : suppress unused variable warnings
    
    This commit suppresses unused variable warnings for the variables e in
    the catch blocks.
    
    The motivation for this change is to suppress the warnings that are
    generated on Windows when using the MSVC compiler. The warnings are
    not displayed when using GCC because GCC will mark all catch parameters
    as used.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! clip : suppress unused variable warnings
    
    Remove e (/*e*/) instead instead of using GGML_UNUSED.
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/llava/clip.cpp

commit c70d117c37cc7876e775d1e2722208a50c52edb3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 23:25:22 2024 +0300

    scripts : fix filename sync

scripts/sync-ggml-am.sh

commit ae5d0f4b899ff2842bfca561370c945ad8d4368b
Author: slaren <slarengh@gmail.com>
Date:   Wed Jun 26 21:59:28 2024 +0200

    ci : publish new docker images only when the files change (#8142)

.github/workflows/build.yml
.github/workflows/docker.yml

commit 31ec3993f6e050322a249c07af79dbde66ea6ddc
Author: slaren <slarengh@gmail.com>
Date:   Wed Jun 26 21:34:14 2024 +0200

    ggml : add GGML_CUDA_USE_GRAPHS option, restore GGML_CUDA_FORCE_CUBLAS (cmake) (#8140)

CMakeLists.txt
ggml/CMakeLists.txt
ggml/src/CMakeLists.txt

commit c7ab7b612cbdce04499575e713076a026af4b9c5
Author: slaren <slarengh@gmail.com>
Date:   Wed Jun 26 20:20:22 2024 +0200

    make : fix missing -O3 (#8143)

Makefile

commit f2d48fffde76d959fdb0da37316bdc09e5518eb1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 19:39:19 2024 +0300

    sync : ggml

scripts/sync-ggml.last

commit 4713bf3093d58a3e12368ab2ab5fc3630f27803e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 19:36:44 2024 +0300

    authors : regen

AUTHORS

commit 0e814dfc42b4b57ad19598d239557b6a977ca16c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 19:32:07 2024 +0300

    devops : remove clblast + LLAMA_CUDA -> GGML_CUDA (#8139)
    
    ggml-ci

.devops/full-cuda.Dockerfile
.devops/full-rocm.Dockerfile
.devops/llama-cli-cuda.Dockerfile
.devops/llama-cli-intel.Dockerfile
.devops/llama-cli-rocm.Dockerfile
.devops/llama-cli-vulkan.Dockerfile
.devops/llama-cpp-clblast.srpm.spec
.devops/llama-cpp-cuda.srpm.spec
.devops/llama-server-cuda.Dockerfile
.devops/llama-server-intel.Dockerfile
.devops/llama-server-rocm.Dockerfile
.devops/llama-server-vulkan.Dockerfile

commit a95631ee97bb24861af6bdeec380270459631e8e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 19:26:13 2024 +0300

    readme : update API notes

README.md

commit f3f65429c44bb195a9195bfdc19a30a79709db7b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 26 18:33:02 2024 +0300

    llama : reorganize source code + improve CMake (#8006)
    
    * scripts : update sync [no ci]
    
    * files : relocate [no ci]
    
    * ci : disable kompute build [no ci]
    
    * cmake : fixes [no ci]
    
    * server : fix mingw build
    
    ggml-ci
    
    * cmake : minor [no ci]
    
    * cmake : link math library [no ci]
    
    * cmake : build normal ggml library (not object library) [no ci]
    
    * cmake : fix kompute build
    
    ggml-ci
    
    * make,cmake : fix LLAMA_CUDA + replace GGML_CDEF_PRIVATE
    
    ggml-ci
    
    * move public backend headers to the public include directory (#8122)
    
    * move public backend headers to the public include directory
    
    * nix test
    
    * spm : fix metal header
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * scripts : fix sync paths [no ci]
    
    * scripts : sync ggml-blas.h [no ci]
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

.devops/nix/package.nix
.github/labeler.yml
.github/workflows/bench.yml
.github/workflows/build.yml
.github/workflows/server.yml
.gitignore
.gitmodules
CMakeLists.txt
CMakePresets.json
Makefile
Package.swift
README-sycl.md
README.md
ci/run.sh
cmake/build-info.cmake
cmake/git-vars.cmake
cmake/llama-config.cmake.in
common/CMakeLists.txt
common/cmake/build-info-gen-cpp.cmake
docs/BLIS.md
examples/CMakeLists.txt
examples/imatrix/README.md
examples/llava/MobileVLM-README.md
examples/rpc/README.md
examples/server/CMakeLists.txt
examples/sycl/build.sh
examples/sycl/win-build-sycl.bat
ggml/CMakeLists.txt
ggml/cmake/FindSIMD.cmake
ggml/ggml_vk_generate_shaders.py
ggml/include/ggml-alloc.h
ggml/include/ggml-backend.h
ggml/include/ggml-blas.h
ggml/include/ggml-cuda.h
ggml/include/ggml-kompute.h
ggml/include/ggml-metal.h
ggml/include/ggml-rpc.h
ggml/include/ggml-sycl.h
ggml/include/ggml-vulkan.h
ggml/include/ggml.h
ggml/src/CMakeLists.txt
ggml/src/ggml-alloc.c
ggml/src/ggml-backend-impl.h
ggml/src/ggml-backend.c
ggml/src/ggml-blas.cpp
ggml/src/ggml-common.h
ggml/src/ggml-cuda.cu
ggml/src/ggml-cuda/acc.cu
ggml/src/ggml-cuda/acc.cuh
ggml/src/ggml-cuda/arange.cu
ggml/src/ggml-cuda/arange.cuh
ggml/src/ggml-cuda/argsort.cu
ggml/src/ggml-cuda/argsort.cuh
ggml/src/ggml-cuda/binbcast.cu
ggml/src/ggml-cuda/binbcast.cuh
ggml/src/ggml-cuda/clamp.cu
ggml/src/ggml-cuda/clamp.cuh
ggml/src/ggml-cuda/common.cuh
ggml/src/ggml-cuda/concat.cu
ggml/src/ggml-cuda/concat.cuh
ggml/src/ggml-cuda/convert.cu
ggml/src/ggml-cuda/convert.cuh
ggml/src/ggml-cuda/cpy.cu
ggml/src/ggml-cuda/cpy.cuh
ggml/src/ggml-cuda/dequantize.cuh
ggml/src/ggml-cuda/diagmask.cu
ggml/src/ggml-cuda/diagmask.cuh
ggml/src/ggml-cuda/dmmv.cu
ggml/src/ggml-cuda/dmmv.cuh
ggml/src/ggml-cuda/fattn-common.cuh
ggml/src/ggml-cuda/fattn-tile-f16.cu
ggml/src/ggml-cuda/fattn-tile-f16.cuh
ggml/src/ggml-cuda/fattn-tile-f32.cu
ggml/src/ggml-cuda/fattn-tile-f32.cuh
ggml/src/ggml-cuda/fattn-vec-f16.cuh
ggml/src/ggml-cuda/fattn-vec-f32.cuh
ggml/src/ggml-cuda/fattn-wmma-f16.cuh
ggml/src/ggml-cuda/fattn.cu
ggml/src/ggml-cuda/fattn.cuh
ggml/src/ggml-cuda/getrows.cu
ggml/src/ggml-cuda/getrows.cuh
ggml/src/ggml-cuda/im2col.cu
ggml/src/ggml-cuda/im2col.cuh
ggml/src/ggml-cuda/mma.cuh
ggml/src/ggml-cuda/mmq.cu
ggml/src/ggml-cuda/mmq.cuh
ggml/src/ggml-cuda/mmvq.cu
ggml/src/ggml-cuda/mmvq.cuh
ggml/src/ggml-cuda/norm.cu
ggml/src/ggml-cuda/norm.cuh
ggml/src/ggml-cuda/pad.cu
ggml/src/ggml-cuda/pad.cuh
ggml/src/ggml-cuda/pool2d.cu
ggml/src/ggml-cuda/pool2d.cuh
ggml/src/ggml-cuda/quantize.cu
ggml/src/ggml-cuda/quantize.cuh
ggml/src/ggml-cuda/rope.cu
ggml/src/ggml-cuda/rope.cuh
ggml/src/ggml-cuda/scale.cu
ggml/src/ggml-cuda/scale.cuh
ggml/src/ggml-cuda/softmax.cu
ggml/src/ggml-cuda/softmax.cuh
ggml/src/ggml-cuda/sumrows.cu
ggml/src/ggml-cuda/sumrows.cuh
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_0.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_1.cu
ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q8_0.cu
ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.cu
ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.cu
ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.cu
ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.cu
ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.cu
ggml/src/ggml-cuda/template-instances/generate_cu_files.py
ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu
ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu
ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu
ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu
ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu
ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu
ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu
ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu
ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu
ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu
ggml/src/ggml-cuda/tsembd.cu
ggml/src/ggml-cuda/tsembd.cuh
ggml/src/ggml-cuda/unary.cu
ggml/src/ggml-cuda/unary.cuh
ggml/src/ggml-cuda/upscale.cu
ggml/src/ggml-cuda/upscale.cuh
ggml/src/ggml-cuda/vecdotq.cuh
ggml/src/ggml-impl.h
ggml/src/ggml-kompute.cpp
ggml/src/ggml-metal.m
ggml/src/ggml-metal.metal
ggml/src/ggml-quants.c
ggml/src/ggml-quants.h
ggml/src/ggml-rpc.cpp
ggml/src/ggml-sycl.cpp
ggml/src/ggml-sycl/backend.hpp
ggml/src/ggml-sycl/common.cpp
ggml/src/ggml-sycl/common.hpp
ggml/src/ggml-sycl/convert.cpp
ggml/src/ggml-sycl/convert.hpp
ggml/src/ggml-sycl/dequantize.hpp
ggml/src/ggml-sycl/dmmv.cpp
ggml/src/ggml-sycl/dmmv.hpp
ggml/src/ggml-sycl/dpct/helper.hpp
ggml/src/ggml-sycl/mmq.cpp
ggml/src/ggml-sycl/mmq.hpp
ggml/src/ggml-sycl/mmvq.cpp
ggml/src/ggml-sycl/mmvq.hpp
ggml/src/ggml-sycl/presets.hpp
ggml/src/ggml-sycl/vecdotq.hpp
ggml/src/ggml-vulkan-shaders.hpp
ggml/src/ggml-vulkan.cpp
ggml/src/ggml.c
ggml/src/kompute
ggml/src/kompute-shaders/common.comp
ggml/src/kompute-shaders/op_add.comp
ggml/src/kompute-shaders/op_addrow.comp
ggml/src/kompute-shaders/op_cpy_f16_f16.comp
ggml/src/kompute-shaders/op_cpy_f16_f32.comp
ggml/src/kompute-shaders/op_cpy_f32_f16.comp
ggml/src/kompute-shaders/op_cpy_f32_f32.comp
ggml/src/kompute-shaders/op_diagmask.comp
ggml/src/kompute-shaders/op_gelu.comp
ggml/src/kompute-shaders/op_getrows.comp
ggml/src/kompute-shaders/op_getrows_f16.comp
ggml/src/kompute-shaders/op_getrows_f32.comp
ggml/src/kompute-shaders/op_getrows_q4_0.comp
ggml/src/kompute-shaders/op_getrows_q4_1.comp
ggml/src/kompute-shaders/op_getrows_q6_k.comp
ggml/src/kompute-shaders/op_mul.comp
ggml/src/kompute-shaders/op_mul_mat_f16.comp
ggml/src/kompute-shaders/op_mul_mat_mat_f32.comp
ggml/src/kompute-shaders/op_mul_mat_q4_0.comp
ggml/src/kompute-shaders/op_mul_mat_q4_1.comp
ggml/src/kompute-shaders/op_mul_mat_q6_k.comp
ggml/src/kompute-shaders/op_mul_mat_q8_0.comp
ggml/src/kompute-shaders/op_mul_mv_q_n.comp
ggml/src/kompute-shaders/op_mul_mv_q_n_pre.comp
ggml/src/kompute-shaders/op_norm.comp
ggml/src/kompute-shaders/op_relu.comp
ggml/src/kompute-shaders/op_rmsnorm.comp
ggml/src/kompute-shaders/op_rope_f16.comp
ggml/src/kompute-shaders/op_rope_f32.comp
ggml/src/kompute-shaders/op_scale.comp
ggml/src/kompute-shaders/op_scale_8.comp
ggml/src/kompute-shaders/op_silu.comp
ggml/src/kompute-shaders/op_softmax.comp
ggml/src/kompute-shaders/rope_common.comp
ggml/src/sgemm.cpp
ggml/src/sgemm.h
ggml/src/vulkan-shaders/add.comp
ggml/src/vulkan-shaders/argsort.comp
ggml/src/vulkan-shaders/clamp.comp
ggml/src/vulkan-shaders/copy.comp
ggml/src/vulkan-shaders/dequant_f32.comp
ggml/src/vulkan-shaders/dequant_funcs.comp
ggml/src/vulkan-shaders/dequant_head.comp
ggml/src/vulkan-shaders/dequant_q2_k.comp
ggml/src/vulkan-shaders/dequant_q3_k.comp
ggml/src/vulkan-shaders/dequant_q4_0.comp
ggml/src/vulkan-shaders/dequant_q4_1.comp
ggml/src/vulkan-shaders/dequant_q4_k.comp
ggml/src/vulkan-shaders/dequant_q5_0.comp
ggml/src/vulkan-shaders/dequant_q5_1.comp
ggml/src/vulkan-shaders/dequant_q5_k.comp
ggml/src/vulkan-shaders/dequant_q6_k.comp
ggml/src/vulkan-shaders/dequant_q8_0.comp
ggml/src/vulkan-shaders/diag_mask_inf.comp
ggml/src/vulkan-shaders/div.comp
ggml/src/vulkan-shaders/gelu.comp
ggml/src/vulkan-shaders/generic_binary_head.comp
ggml/src/vulkan-shaders/generic_head.comp
ggml/src/vulkan-shaders/generic_unary_head.comp
ggml/src/vulkan-shaders/get_rows.comp
ggml/src/vulkan-shaders/get_rows_quant.comp
ggml/src/vulkan-shaders/mul.comp
ggml/src/vulkan-shaders/mul_mat_split_k_reduce.comp
ggml/src/vulkan-shaders/mul_mat_vec.comp
ggml/src/vulkan-shaders/mul_mat_vec_base.comp
ggml/src/vulkan-shaders/mul_mat_vec_nc.comp
ggml/src/vulkan-shaders/mul_mat_vec_p021.comp
ggml/src/vulkan-shaders/mul_mat_vec_q2_k.comp
ggml/src/vulkan-shaders/mul_mat_vec_q3_k.comp
ggml/src/vulkan-shaders/mul_mat_vec_q4_k.comp
ggml/src/vulkan-shaders/mul_mat_vec_q5_k.comp
ggml/src/vulkan-shaders/mul_mat_vec_q6_k.comp
ggml/src/vulkan-shaders/mul_mm.comp
ggml/src/vulkan-shaders/norm.comp
ggml/src/vulkan-shaders/relu.comp
ggml/src/vulkan-shaders/rms_norm.comp
ggml/src/vulkan-shaders/rope_head.comp
ggml/src/vulkan-shaders/rope_neox.comp
ggml/src/vulkan-shaders/rope_norm.comp
ggml/src/vulkan-shaders/scale.comp
ggml/src/vulkan-shaders/silu.comp
ggml/src/vulkan-shaders/soft_max.comp
ggml/src/vulkan-shaders/square.comp
ggml/src/vulkan-shaders/sum_rows.comp
ggml/src/vulkan-shaders/types.comp
include/llama.h
scripts/build-info.sh
scripts/compare-commits.sh
scripts/debug-test.sh
scripts/pod-llama.sh
scripts/server-llm.sh
scripts/sync-ggml-am.sh
scripts/sync-ggml.sh
spm-headers/ggml-alloc.h
spm-headers/ggml-backend.h
spm-headers/ggml-metal.h
spm-headers/ggml.h
spm-headers/llama.h
src/CMakeLists.txt
src/llama.cpp
src/unicode-data.cpp
src/unicode-data.h
src/unicode.cpp
src/unicode.h
tests/test-backend-ops.cpp

commit 88540445615e77a0177fcca43aaa8e9d8eea6864
Author: Isaac McFadyen <isaac@imcf.me>
Date:   Wed Jun 26 02:29:28 2024 -0400

    Clarify default MMQ for CUDA and LLAMA_CUDA_FORCE_MMQ flag (#8115)
    
    * Add message about int8 support
    
    * Add suggestions from review
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

README.md

commit c8771ab5f89387cdd7d9a8a69280dac46b45e02f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 26 08:28:02 2024 +0200

    CUDA: fix misaligned shared memory read (#8123)

ggml-cuda/mma.cuh

commit 494165f3b6c4cbcd793123cb57fb3e1f5477f1db
Author: Eddie-Wang <wangjinheng1120@163.com>
Date:   Wed Jun 26 14:27:46 2024 +0800

    llama : extend llm_build_ffn() to support _scale tensors (#8103)

llama.cpp

commit 9b2f16f8055265c67e074025350736adc1ea0666
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Wed Jun 26 01:46:35 2024 +0100

    `json`: better support for "type" unions (e.g. nullable arrays w/ typed items) (#7863)
    
    * json: better suport for "type" arrays (e.g. `{"type": ["array", "null"], "items": {"type": "string"}}`)
    
    * json: add test for type: [array, null] fix
    
    * update tests

common/json-schema-to-grammar.cpp
examples/json_schema_to_grammar.py
examples/server/public/json-schema-to-grammar.mjs
tests/test-grammar-integration.cpp
tests/test-json-schema-to-grammar.cpp

commit 6777c544bdd8c5d9de3220d6e2557957bbbf2a4f
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Wed Jun 26 01:45:58 2024 +0100

    `json`: fix additionalProperties, allow space after enum/const (#7840)
    
    * json: default additionalProperty to true
    
    * json: don't force additional props after normal properties!
    
    * json: allow space after enum/const
    
    * json: update pydantic example to set additionalProperties: false
    
    * json: prevent additional props to redefine a typed prop
    
    * port not_strings to python, add trailing space
    
    * fix not_strings & port to js+py
    
    * Update json-schema-to-grammar.cpp
    
    * fix _not_strings for substring overlaps
    
    * json: fix additionalProperties default, uncomment tests
    
    * json: add integ. test case for additionalProperties
    
    * json: nit: simplify condition
    
    * reformat grammar integ tests w/ R"""()""" strings where there's escapes
    
    * update # tokens in server test: consts can now have trailing space

common/json-schema-to-grammar.cpp
examples/json-schema-pydantic-example.py
examples/json_schema_to_grammar.py
examples/server/public/json-schema-to-grammar.mjs
examples/server/tests/features/server.feature
tests/test-grammar-integration.cpp
tests/test-json-schema-to-grammar.cpp

commit 163d50adaf8897d8b734d701ff332de6be63d484
Author: jukofyork <69222624+jukofyork@users.noreply.github.com>
Date:   Tue Jun 25 21:47:40 2024 +0100

    fixes #7999 (adds control vectors to all `build_XXX()` functions in `llama.cpp` [needs testing] (#8060)
    
    * fixes #7999
    
    The `build_command_r` forgot to add the control vector.
    
    * Fixes qwen2 too
    
    * Fixed all models' control vectors
    
    * Removed double calls to `cb(cur, "l_out", il)`
    
    * Moved control vector logic to llama_control_vector:apply_to()

llama.cpp

commit 6fcbf6823553efabe52ed83e3c2a3329aa3387d1
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Tue Jun 25 21:14:35 2024 +0200

    llama : implement Unigram tokenizer needed by T5 and FLAN-T5 model families (#5763)
    
    * llama : add T5 model architecture, tensors and model header parameters
    
    * llama : add implementation of Unigram tokenizer with SentencePiece-like text normalization using precompiled charsmap
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

llama.cpp
llama.h
unicode.cpp
unicode.h

commit e6bf007744eb06336a231ef39cf08146dd16d2ce
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jun 25 21:07:28 2024 +0200

    llama : return nullptr from llama_grammar_init (#8093)
    
    * llama : return nullptr from llama_grammar_init
    
    This commit updates llama_grammar_init to return nullptr instead of
    throwing an exception.
    
    The motivation for this is that this function is declared inside an
    extern "C" block and is intended/may be used from C code which will not
    be able to handle exceptions thrown, and results in undefined behavior.
    
    On Windows and using MSVC the following warning is currently generated:
    ```console
    C:\llama.cpp\llama.cpp(13998,1): warning C4297: 'llama_grammar_init':
    function assumed not to throw an exception but does
    C:\llama.cpp\llama.cpp(13998,1): message :
    __declspec(nothrow), throw(), noexcept(true), or noexcept was specified
    on the function
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! llama : return nullptr from llama_grammar_init
    
    Add checks for nullptr when calling llama_grammar_init.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    Co-authored-by: Clint Herron <hanclinto@gmail.com>

common/sampling.cpp
examples/gbnf-validator/gbnf-validator.cpp
llama.cpp
llama.h
tests/test-grammar-integration.cpp
tests/test-llama-grammar.cpp

commit 84631fe1504de40427dc4b4cdac92fa7ebf65955
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue Jun 25 20:06:20 2024 +0100

    `json`: support integer minimum, maximum, exclusiveMinimum, exclusiveMaximum (#7797)
    
    * json: support minimum for positive integer values
    
    * json: fix min 0
    
    * json: min + max integer constraints
    
    * json: handle negative min / max integer bounds
    
    * json: fix missing paren min/max bug
    
    * json: proper paren fix
    
    * json: integration test for schemas
    
    * json: fix bounds tests
    
    * Update json-schema-to-grammar.cpp
    
    * json: fix negative max
    
    * json: fix negative min (w/ more than 1 digit)
    
    * Update test-grammar-integration.cpp
    
    * json: nit: move string rules together
    
    * json: port min/max integer support to Python & JS
    
    * nit: move + rename _build_min_max_int
    
    * fix min in [1, 9]
    
    * Update test-grammar-integration.cpp
    
    * add C++11-compatible replacement for std::string_view
    
    * add min/max constrained int field to pydantic json schema example
    
    * fix merge
    
    * json: add integration tests for min/max bounds
    
    * reshuffle/merge min/max integ test cases
    
    * nits / cleanups
    
    * defensive code against string out of bounds (apparently different behaviour of libstdc++ vs. clang's libc++, can't read final NULL char w/ former)

common/json-schema-to-grammar.cpp
examples/json-schema-pydantic-example.py
examples/json_schema_to_grammar.py
examples/server/public/json-schema-to-grammar.mjs
tests/test-grammar-integration.cpp
tests/test-json-schema-to-grammar.cpp

commit dd047b476c8b904e0c25e5dbc5bee6ffde2f6e17
Author: slaren <slarengh@gmail.com>
Date:   Tue Jun 25 19:20:06 2024 +0200

    disable docker CI on pull requests (#8110)

.github/workflows/docker.yml

commit 925c30956dd17723c3a25297bcd0a609aec60663
Author: joecryptotoo <80373433+joecryptotoo@users.noreply.github.com>
Date:   Tue Jun 25 08:13:27 2024 -0700

    Add healthchecks to llama-server containers (#8081)
    
    * added healthcheck
    
    * added healthcheck
    
    * added healthcheck
    
    * added healthcheck
    
    * added healthcheck
    
    * moved curl to base
    
    * moved curl to base

.devops/llama-server-cuda.Dockerfile
.devops/llama-server-intel.Dockerfile
.devops/llama-server-rocm.Dockerfile
.devops/llama-server-vulkan.Dockerfile
.devops/llama-server.Dockerfile

commit c8ad35955ad2c68db172dcd0e857423ab128518d
Author: Brian <mofosyne@gmail.com>
Date:   Tue Jun 25 22:03:25 2024 +1000

    Gguf dump start data offset via --data-offset and some extra refactor (#8054)
    
    * gguf-dump: add --data-offset
    
    * gguf-dump: add tensor data offset table
    
    * gguf-dump: refactor GGUFReader for clarity
    
    * gguf-dump: add --data-alignment
    
    * gguf-dump.py: Rename variables and adjust comments
    
    start_data_offset --> data_offset
    
    _build_tensors_info_fields --> _build_tensor_info

gguf-py/gguf/gguf_reader.py
gguf-py/scripts/gguf-dump.py

commit 49c03c79cda17913b72260acdc8157b742cee41c
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Jun 25 13:59:54 2024 +0200

    cvector: better prompt handling, add "mean vector" method (#8069)
    
    * remove completions file
    
    * fix inverted vector
    
    * add mean method
    
    * code style
    
    * remove inverted pca hotfix

common/common.cpp
common/common.h
examples/cvector-generator/README.md
examples/cvector-generator/cvector-generator.cpp
examples/cvector-generator/mean.hpp
examples/cvector-generator/negative.txt
examples/cvector-generator/pca.hpp
examples/cvector-generator/positive.txt

commit 48e6b92cc378c937e59719f2c0f482bf76c9ca81
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Jun 25 13:56:49 2024 +0200

    Add chat template support for llama-cli (#8068)
    
    * add chat template support for llama-cli
    
    * add help message
    
    * server: simplify format_chat
    
    * more consistent naming
    
    * improve
    
    * add llama_chat_format_example
    
    * fix server
    
    * code style
    
    * code style
    
    * Update examples/main/main.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
examples/main/main.cpp
examples/server/server.cpp
examples/server/utils.hpp
llama.cpp
tests/test-chat-template.cpp

commit 3791ad219323389106dc3fd80814eb5bbb7b80de
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Tue Jun 25 16:57:35 2024 +0530

    SimpleChat v3.1: Boolean chat request options in Settings UI, cache_prompt (#7950)
    
    * SimpleChat: Allow for chat req bool options to be user controlled
    
    * SimpleChat: Allow user to control cache_prompt flag in request
    
    * SimpleChat: Add sample GUI images to readme file
    
    Show the chat screen and the settings screen
    
    * SimpleChat:Readme: Add quickstart block, title to image, cleanup
    
    * SimpleChat: RePosition contents of the Info and Settings UI
    
    Make it more logically structured and flow through.
    
    * SimpleChat: Rename to apiRequestOptions from chatRequestOptions
    
    So that it is not wrongly assumed that these request options are
    used only for chat/completions endpoint. Rather these are used
    for both the end points, so rename to match semantic better.
    
    * SimpleChat: Update image included with readme wrt settings ui
    
    * SimpleChat:ReadMe: Switch to webp screen image to reduce size

examples/server/public_simplechat/readme.md
examples/server/public_simplechat/simplechat.js
examples/server/public_simplechat/simplechat_screens.webp

commit f702a90e245499283d6de0b287701c723cda2a87
Author: HatsuneMikuUwU33 <173229399+HatsuneMikuUwU33@users.noreply.github.com>
Date:   Tue Jun 25 10:44:48 2024 +0200

    Update control vector help (#8104)

common/common.cpp

commit 083bacce14c1aaf9976aa40e8266cdc25ac749d3
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Tue Jun 25 10:19:20 2024 +0800

    [SYCL] Re-enabled mul_mat_batched_sycl (#8095)

ggml-sycl.cpp

commit 2df373ac40ea581ccca8a58c713f03ad9d4b658d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jun 25 01:22:33 2024 +0200

    CUDA: fix matrix multiplication algorithm choice (#8102)

ggml-cuda.cu

commit 3b099bcd9cbf2434f90cbe40eba6fa2189ed1d02
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 24 22:15:33 2024 +0200

    CUDA: fix MMQ writeback for int8 tensor cores (#8100)

ggml-cuda/mmq.cuh

commit a818f3028d1497a51cb2b8eb7d993ad58784940e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 24 17:43:42 2024 +0200

    CUDA: use MMQ instead of cuBLAS by default (#8075)

CMakeLists.txt
Makefile
README.md
ggml-cuda.cu
ggml-cuda/common.cuh
ggml-cuda/mmq.cu
ggml-cuda/mmq.cuh
ggml-cuda/mmvq.cuh

commit d62e4aaa02540c89be8b59426340b909d02bbc9e
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Mon Jun 24 14:13:39 2024 +0200

    gguf-py : fix tensor groups for encoder-decoder models in gguf-dump.py (#8090)
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>
    Co-authored-by: Brian <mofosyne@gmail.com>

gguf-py/scripts/gguf-dump.py

commit 9a590c82262dd518137f85406e65e452fdf2aca3
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 24 12:41:23 2024 +0200

    CUDA: optimize MMQ int8 tensor core performance (#8062)
    
    * CUDA: optimize MMQ int8 tensor core performance
    
    * only a single get_mma_tile_x_k function
    
    * simplify code, make functions constexpr

ggml-cuda/common.cuh
ggml-cuda/mma.cuh
ggml-cuda/mmq.cuh

commit 52fc8705a0617452df08333e1161838726c322b4
Author: Christian Zhou-Zheng <59622928+christianazinn@users.noreply.github.com>
Date:   Mon Jun 24 05:42:03 2024 -0400

    Option to split during conversion (#6942)
    
    * support splits in convert.py
    
    * Support split by size and dry run to write estimated shards/filesizes
    
    * Move split functionality to new GGUFManager class
    
    * fix improper function signature
    
    * tentative push of convert-hf-to-gguf support
    
    * resolve merge + SplitArguments for easier parsing
    
    * Fix eager tensor memory leak and remove convert.py changes
    
    Removed a memory leak caused by unexpected reference retention to eager tensors.
    
    Also removed GGUFManager functionality in convert.py in favor of specializing for convert-hf-to-gguf.py.
    
    * refactor SplitStrategy to be a deque
    
    Instead of having SplitStrategy have a `data` field that is a deque, just have SplitStrategy be a subclass of deque itself.
    
    * fix Q8 quantization
    
    * remove unnecessary imports in gguf_manager
    
    * fix final? merge issue
    
    * fix gguf_writer placement and remove comments
    
    * oops, actually fix gguf_writer placement
    
    * reduce duplicated code from gguf_writer
    
    * further simplify GGUFManager
    
    * simplify even further and standardize with GGUFWriter
    
    * reduce diffs with master
    
    * form shards while adding tensors, SHA256 sums agree with master
    
    * re-add type hint
    
    Co-authored-by: compilade <git@compilade.net>
    
    * GGUFWriter compatibility fix
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Shard dataclass and un-negative dont_add_architecture
    
    * type consistency in format_n_bytes_to_str
    
    * move kv keys to constants.py
    
    * make pathlib explicit
    
    * base-1024 bytes to base-1000
    
    * rename GGUFManager to GGUFWriterSplit
    
    * Update gguf-py/gguf/constants.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * fix convert-hf-to-gguf.py permissions
    
    * fix line endings
    
    * Update gguf-py/gguf/gguf_writer_split.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * convert-hf : restore executable file permission
    
    * examples/convert-legacy-llama.py: restore executable file permission
    
    * reinstate original gguf package import and fix type annotation
    
    * attempt to appease the linter
    
    * attempt 2 to appease the linter
    
    * attempt 3 to appease the linter
    
    * comma consistency
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * edit cmd line args
    
    * use simplification from #7827
    
    * kv/ti data are still wrong
    
    * try to refactor kv data (still fails)
    
    * fix ti data messiness
    
    * tidy up
    
    * fix linting
    
    * actually make the linter happy
    
    * cleanup round 1
    
    * remove SplitStrategy, SplitArguments
    
    * appease linter
    
    * fix typing and clean up
    
    * fix linting
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * progress bar, fix split logic
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * catch oversights
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * swap bar orders
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * compatibility fix
    
    * Update gguf-py/gguf/gguf_writer.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: compilade <git@compilade.net>
    
    ---------
    
    Co-authored-by: Brian <mofosyne@gmail.com>
    Co-authored-by: compilade <git@compilade.net>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py

commit 8cb508d0d5c024e12692370d85237b45469a004b
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 24 07:36:11 2024 +0200

    disable publishing the full-rocm docker image (#8083)

.github/workflows/docker.yml

commit 646ef4a9cfb6f40236d3b9ef07ac03700b6efcc7
Author: Yann Follet <131855179+YannFollet@users.noreply.github.com>
Date:   Mon Jun 24 13:30:24 2024 +0800

    embedding : more cli arguments (#7458)
    
    * add parameters for embeddings
    --embd-normalize
    --embd-output-format
    --embd-separator
    description in the README.md
    
    * Update README.md
    
    fix tipo
    
    * Trailing whitespace
    
    * fix json generation, use " not '
    
    * fix merge master
    
    * fix code formating
    group of parameters // embedding
    print usage for embedding parameters
    
    ---------
    
    Co-authored-by: Brian <mofosyne@gmail.com>

common/common.cpp
common/common.h
examples/embedding/README.md
examples/embedding/embedding.cpp

commit de0d6a68ac99f307fe889c48e21124bc3b7ca29a
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Mon Jun 24 07:06:05 2024 +0200

    gguf-py, convert-hf : model conversion support for T5 and FLAN-T5 model variants (#5763)
    
    * gguf-py : add T5 model architecture
    
    * gguf-py : add separate tensors for encoder and decoder
    
    * gguf-py : add new model header parameters: decoder_start_token_id, attention.relative_buckets_count, tokenizer.ggml.remove_extra_whitespaces, tokenizer.ggml.precompiled_charsmap
    
    * convert-hf : add model conversion support for T5ForConditionalGeneration and T5WithLMHeadModel
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/tensor_mapping.py

commit 95f57bb5d5b18ef0beb2702a0d6c06e46804075c
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 24 03:07:59 2024 +0200

    ggml : remove ggml_task_type and GGML_PERF (#8017)
    
    * ggml : remove ggml_task_type and GGML_PERF
    
    * check abort_callback on main thread only
    
    * vulkan : remove usage of ggml_compute_params
    
    * remove LLAMA_PERF

CMakeLists.txt
Makefile
ggml-vulkan.cpp
ggml.c
ggml.h
llama.cpp
sgemm.cpp
sgemm.h

commit e112b610a1a75cb7fa8351e1a933e2e7a755a5ce
Author: Eddie-Wang <wangjinheng1120@163.com>
Date:   Mon Jun 24 02:27:57 2024 +0800

    llama : add support for BitnetForCausalLM (#7931)
    
    * hf bitnet v1
    
    * hf bitnet e2e v2
    
    * finish bitnet e2e
    
    * finish f16 hf bitnet e2e
    
    * remove unsed
    
    * finish bitnet i2 e2e
    
    * move i2s to quantize v1
    
    * move i2 to quantize
    
    * clean code
    
    * clean code 2
    
    * fix codestyle
    
    * fix code
    
    * fix
    
    * fix code
    
    * fix merge
    
    * remove unused
    
    * change table name
    
    * fix whitespace
    
    * delete redundant
    
    * i2_s to absmax
    
    * finish i2_s/i8_s vec_dot x86 simd
    
    * i2s->q22
    
    * fix code
    
    * remove block scale
    
    * add dequantize
    
    * fix seq
    
    * update avx2
    
    * remove q2_2
    
    * remove q22_grid
    
    * fix whitespace
    
    * reuse llm_build_kv
    
    * fix bo
    
    ---------
    
    Co-authored-by: root <root@wangjinheng>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 6a2f298bd784403c5c33eebb822217ec5d9b5590
Author: Aarni Koskela <akx@iki.fi>
Date:   Sun Jun 23 18:03:08 2024 +0300

    server : fix JSON-Scheme typo (#7975)

examples/server/public/index-new.html

commit 11318d9aa1f668aa10407a5cb9614371af32f3ce
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sun Jun 23 15:39:45 2024 +0200

    Fix typo in llama_set_embeddings comment (#8077)

llama.h

commit b6b9a8e606da7764bd3649c2ea574979c85abd43
Author: slaren <slarengh@gmail.com>
Date:   Sun Jun 23 13:14:45 2024 +0200

    fix CI failures (#8066)
    
    * test-backend-ops : increase cpy max nmse
    
    * server ci : disable thread sanitizer

.github/workflows/server.yml
tests/test-backend-ops.cpp

commit 45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc
Author: 0cc4m <picard12@live.de>
Date:   Sun Jun 23 10:21:25 2024 +0200

    Refactor Vulkan backend to allow multiple contexts (#7961)
    
    * Refactor Vulkan backend to allow multiple contexts
    
    * Fix too many shader groups called validation error in llama3 on AMD and Intel GPUs
    
    * Fix Vulkan debug build error

ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
vulkan-shaders/mul_mat_vec.comp
vulkan-shaders/mul_mat_vec_q2_k.comp
vulkan-shaders/mul_mat_vec_q3_k.comp
vulkan-shaders/mul_mat_vec_q4_k.comp
vulkan-shaders/mul_mat_vec_q5_k.comp
vulkan-shaders/mul_mat_vec_q6_k.comp

commit b5a5f34efadec8d8f0057b35cb04742abfeb2ef5
Author: Clint Herron <hanclinto@gmail.com>
Date:   Sat Jun 22 14:28:18 2024 -0400

    Removing extra blank lines that were breaking Lint. (#8067)

convert-hf-to-gguf.py

commit 3e58b0ee355f78be41cf5211b68426761339bc3c
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Jun 22 18:11:30 2024 +0200

    cvector: fix CI + correct help message (#8064)
    
    * cvector: fix CI + correct help message
    
    * also correct --pca-iter

.editorconfig
common/common.cpp
examples/cvector-generator/README.md
examples/cvector-generator/cvector-generator.cpp

commit adf480c3ab3abedc964c8ae381476257583ae134
Author: HatsuneMikuUwU33 <173229399+HatsuneMikuUwU33@users.noreply.github.com>
Date:   Sat Jun 22 17:19:37 2024 +0200

    cvector-generator: Moe Moe Fixie-Fixie for Lots of Formats~! ♡(ᐢ ᴥ ᐢ)♡ (#8052)
    
    * Update negative.txt
    
    * Update positive.txt
    
    * Update cvector-generator.cpp
    
    * Update cvector-generator.cpp

examples/cvector-generator/cvector-generator.cpp
examples/cvector-generator/negative.txt
examples/cvector-generator/positive.txt

commit 3aa184a8c7c553b5dfcc142d919f3db695df297a
Author: 0xspringtime <110655352+0xspringtime@users.noreply.github.com>
Date:   Sat Jun 22 09:37:41 2024 -0400

    convert-hf : change assert to exception (#8015)

convert-hf-to-gguf.py

commit 5b48cd53a87928db0c6447f0c9dac4db5802102d
Author: ddh0 <dylanhalladay02@icloud.com>
Date:   Sat Jun 22 07:16:10 2024 -0600

    Update llama-quantize ppl/file size output from LLaMA-v1 to Llama-3 values (#8058)
    
    Uses the values computed by @JohannesGaessler in PR #7413

examples/quantize/quantize.cpp

commit c5a8d4b749352645afd4c024f85d6eca2ca72c6d
Author: Clint Herron <hanclinto@gmail.com>
Date:   Fri Jun 21 23:18:36 2024 -0400

    JSON Schema to GBNF integration tests (#7790)
    
    * Adding simple bare-bones test for end-to-end integration test for json validation against auto-generated JSON-schema grammars.
    
    * Adding additional examples as documented in #7789 . Also adding the ability to automatically output improperly failing grammars to debug output files so they can more easily be examined in the gbnf-validator program.
    
    * Uncommenting formerly commented tests so that they fail for others who are attempting to reproduce the bugs.
    
    * Merging improved schema test methods added by @ochafik in #7797
    
    * Adding #define to temporarily remove failing tests so that this PR can pass CI, but still be useful for other PRs that want to leverage the framework.
    
    * Fixing nits from ochafik. Removing escape slashes, adding additional failing cases, fixing some other strings.
    
    * Fixing grammar indentation to be consistent throughout file.

Makefile
tests/test-grammar-integration.cpp

commit 557b653dc9ed91e8c313e87500e0050c775f81b6
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Fri Jun 21 16:28:20 2024 +0800

    vulkan: detect multiple devices by deviceUUID instead of deviceID (#8022)
    
    * vulkan: detect multiple devices by deviceUUID instead of deviceID
    
    * vulkan: remove unneeded variables
    
    * vulkan: fix id query

ggml-vulkan.cpp

commit 7d5e8777ae1d21af99d4f95be10db4870720da91
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Fri Jun 21 05:57:36 2024 +0000

    ggml : AVX IQ quants (#7845)
    
    * initial iq4_xs
    
    * fix ci
    
    * iq4_nl
    
    * iq1_m
    
    * iq1_s
    
    * iq2_xxs
    
    * iq3_xxs
    
    * iq2_s
    
    * iq2_xs
    
    * iq3_s before sllv
    
    * iq3_s
    
    * iq3_s small fix
    
    * iq3_s sllv can be safely replaced with sse multiply

ggml-quants.c

commit a927b0f3dd9a86ee042cd2bdcc8c9da4a855926b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jun 21 08:51:28 2024 +0300

    llama : optimize long word tokenization with WPM (#8034)
    
    ggml-ci

llama.cpp
unicode.cpp

commit 80ea089d771f0c2d97afa8bead80ded412f600d7
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Fri Jun 21 00:38:22 2024 -0500

    llama : allow pooled embeddings on any model (#7477)
    
    * create append_pooling operation; allow to specify attention_type; add last token pooling; update examples
    
    * find result_norm/result_embd tensors properly; update output allocation logic
    
    * only use embd output for pooling_type NONE
    
    * get rid of old causal_attn accessor
    
    * take out attention_type; add in llama_set_embeddings
    
    * bypass logits when doing non-NONE pooling

common/common.cpp
examples/embedding/embedding.cpp
examples/gritlm/gritlm.cpp
examples/retrieval/retrieval.cpp
llama.cpp
llama.h

commit 0e64591e8290037db6412665a56354b789a0597e
Author: Shuichi Tsutsumi <shuichi0526@gmail.com>
Date:   Fri Jun 21 14:30:58 2024 +0900

    swiftui : enable stream updating (#7754)

examples/llama.swiftui/llama.swiftui/Models/LlamaState.swift

commit b1ef562bc17fbf8ba436ddf2d89b78efd10d3e03
Author: Hamdoud Hakem <90524568+hamdoudhakem@users.noreply.github.com>
Date:   Thu Jun 20 21:01:15 2024 +0100

    requirements : Bump torch and numpy for python3.12 (#8041)

requirements/requirements-convert-hf-to-gguf-update.txt
requirements/requirements-convert-hf-to-gguf.txt
requirements/requirements-convert-legacy-llama.txt

commit 17b291a6a581c47f24f99bad926b42617894f99f
Author: Hamdoud Hakem <90524568+hamdoudhakem@users.noreply.github.com>
Date:   Thu Jun 20 20:59:59 2024 +0100

    convert-hf : Fix the encoding in the convert-hf-to-gguf-update.py (#8040)

convert-hf-to-gguf-update.py

commit abd894ad96a242043b8e197ec130d8649eead22e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 20 16:40:13 2024 +0200

    common: fix warning (#8036)
    
    * common: fix warning
    
    * Update common/common.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

common/common.cpp

commit de391e4c803383bbea054b6edd016e78c024a74d
Author: luoyu-intel <yu.luo@intel.com>
Date:   Thu Jun 20 13:19:05 2024 +0000

    [SYCL] Fix windows build and inference (#8003)
    
    * add sycl preset
    
    * fix debug link error. fix windows crash
    
    * update README

CMakeLists.txt
CMakePresets.json
README-sycl.md
examples/sycl/win-build-sycl.bat
ggml-sycl.cpp
ggml-sycl/dpct/helper.hpp
ggml.h

commit d50f8897a797a5a03f31228d1b5a7b8130ee1bc2
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 20 14:39:21 2024 +0200

    CUDA: stream-k decomposition for MMQ (#8018)
    
    * CUDA: stream-k decomposition for MMQ
    
    * fix undefined memory reads for small matrices

ggml-cuda.cu
ggml-cuda/common.cuh
ggml-cuda/mmq.cu
ggml-cuda/mmq.cuh

commit 2075a66a96cc1b04eabec7cf4b3051193d6f719e
Author: Michael de Gans <michael.john.degans@gmail.com>
Date:   Wed Jun 19 22:32:01 2024 -0700

    metal : fix `ggml_metal_supports_op` for BF16 (#8021)
    
    Currently the Metal backend does not support BF16. `ggml_metal_supports_op` was returning true in these cases, leading to a crash with models converted with `--leave-output-tensor`. This commit checks if the first few sources types are BF16 and returns false if that's the case.

ggml-metal.m

commit ba5899315283eb1df3902363daf79bdc5eefe426
Author: sasha0552 <admin@sasha0552.org>
Date:   Wed Jun 19 23:57:10 2024 +0000

    server : fix smart slot selection (#8020)

examples/server/server.cpp

commit a7854743c5e6216a6178824a603aa9479728ddd5
Author: Michael de Gans <michael.john.degans@gmail.com>
Date:   Wed Jun 19 13:10:42 2024 -0700

    un-ignore `build-info.cmake` and `build-info.sh` (#7996)
    
    * un-ignore `build-info.cmake` and `build-info.sh`
    
    I am assuming that ignoring them was unintentional. If they are ignored, some tools, like cargo, will consider the files inexistent, even if they're comitted, for the purpose of publishing. This leads to the build failing in such cases.
    
    * un-ignore `build-info.cpp.in`
    
    For the same reason as the previous two files.
    
    * Reorganize `.gitignore`
    
    * Add exceptions for files mentioned by @slaren
    
    I did leave .clang-tidy since it was explicitly ignored before.
    
    * Add comments for organization
    * Sort some lines for pretty
    * Test with `make` and `cmake` builds to ensure no build artifacts might be comitted
    
    * Remove `.clang-tidy` from `.gitignore`
    
    Per comment by @ggerganov
    
    * Remove `IDEWorkspaceChecks.plist` from root-level `.gitignore`

.gitignore

commit 9c77ec1d74874ee22bdef8f110e8e8d41389abf2
Author: slaren <slarengh@gmail.com>
Date:   Wed Jun 19 15:04:15 2024 +0200

    ggml : synchronize threads using barriers (#7993)

.github/workflows/server.yml
ggml.c

commit a04a953cab583f0229e7b4b506346e3e9a85c8d0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 19 13:04:36 2024 +0300

    codecov : remove (#8004)

.github/labeler.yml
.github/workflows/code-coverage.yml
codecov.yml

commit 623494a478134432fd2d7ee40135770a3340674f
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed Jun 19 09:11:51 2024 +0800

    [SYCL] refactor (#6408)
    
    * seperate lower precision GEMM from the main files
    
    * fix workgroup size hardcode

ggml-sycl.cpp
ggml-sycl/backend.hpp
ggml-sycl/convert.cpp
ggml-sycl/convert.hpp
ggml-sycl/dequantize.hpp
ggml-sycl/dmmv.cpp
ggml-sycl/dmmv.hpp
ggml-sycl/mmq.cpp
ggml-sycl/mmq.hpp
ggml-sycl/mmvq.cpp
ggml-sycl/mmvq.hpp
ggml-sycl/presets.hpp
ggml-sycl/vecdotq.hpp

commit 37bef8943312d91183ff06d8f1214082a17344a5
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Tue Jun 18 18:40:52 2024 +0200

    tokenizer : BPE fixes (#7530)
    
    * Random test: add_bos_token, add_eos_token
    * Random test: add BPE models for testing
    * Custom regex split fails with codepoint 0
    * Fix falcon punctuation regex
    * Refactor llm_tokenizer_bpe: move code to constructor
    * Move 'add_special_bos/eos' logic to llm_tokenizer_bpe
    * Move tokenizer flags to vocab structure.
    * Default values for special_add_bos/eos
    * Build vocab.special_tokens_cache using vocab token types
    * Generalize 'jina-v2' per token attributes
    * Fix unicode whitespaces (deepseek-coder, deepseek-llm)
    * Skip missing byte tokens (falcon)
    * Better unicode data generation
    * Replace char32_t with uint32_t

llama.cpp
scripts/gen-unicode-data.py
tests/test-tokenizer-random.py
unicode-data.cpp
unicode.cpp

commit 91c188d6c296bd3384f2a02a83b71187aa3d18b3
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Tue Jun 18 14:19:45 2024 +0200

    Only use FIM middle token if it exists (#7648)
    
    * Only use FIM middle if it exists
    
    * Only use FIM middle if it exists

examples/infill/infill.cpp
examples/server/server.cpp

commit 84f6de17f6a8602e7ff7f7c7bda36a73f510a2dd
Author: jojorne <jojorne@users.noreply.github.com>
Date:   Tue Jun 18 09:18:32 2024 -0300

    Fix no gcc pragma on Windows (#7751)

sgemm.cpp

commit 61665277afde2add00c0d387acb94ed5feb95917
Author: Ulrich Drepper <drepper@gmail.com>
Date:   Tue Jun 18 14:00:14 2024 +0200

    Allow compiling with CUDA without CUDA runtime installed (#7989)
    
    On hosts which are not prepared/dedicated to execute code using CUDA
    it is still possible to compile llama.cpp with CUDA support by just
    installing the development packages.  Missing are the runtime
    libraries like /usr/lib64/libcuda.so* and currently the link step
    will fail.
    
    The development environment is prepared for such situations.  There
    are stub libraries for all the CUDA libraries available in the
    $(CUDA_PATH)/lib64/stubs directory.  Adding this directory to the end
    of the search path will not change anything for environments which
    currently work fine but will enable compiling llama.cpp also in case
    the runtime code is not available.

Makefile

commit b96f9afb0d58b003ac8d1d0c94cd99393a3bc437
Author: Frank Mai <thxcode0824@gmail.com>
Date:   Tue Jun 18 15:11:40 2024 +0800

    chore: clean useless beam search param (#7985)
    
    Signed-off-by: thxCode <thxcode0824@gmail.com>

common/common.h

commit 1193778105c9a81bd38f72c61aaafbaf85dc9c04
Author: Abheek Gulati <abheekg@hotmail.com>
Date:   Mon Jun 17 23:57:41 2024 -0700

    readme : update UI list (#7943)

README.md

commit 5326bcceeb7dd34f16d0fe61b134d1e074a8e65d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 18 09:50:45 2024 +0300

    ggml : sync

scripts/sync-ggml.last

commit e6ecc2be470e3c5c6c5c9d8b90aa83a1f7725084
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 18 09:37:20 2024 +0300

    whisper : use ggml_backend_sched (whisper/2239)
    
    * whisper : use ggml_backend_sched (wip)
    
    * use sched in whisper_allocr
    
    * whisper : single backend in whisper_context
    
    * whisper : remove whisper_state->backends_used
    
    * whisper : remove whisper_context->backend
    
    * whisper : reset scheduler after init
    
    * whisper : fix external encoder (e.g. CoreML)
    
    * whisper : cleanup
    
    * whisper : handle null GPU buffer types + fix sycl
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-backend.c
ggml-backend.h

commit a94e6ff8774b7c9f950d9545baf0ce35e8d1ed2f
Author: Ștefan-Gabriel Muscalu <legraphista@users.noreply.github.com>
Date:   Mon Jun 17 22:08:46 2024 +0300

    update: support Qwen2-57B-A14B (#7835)
    
    * update: convert-hf-to-gguf.py to support Qwen2-57B-A14B
    
    * fix: QWEN2MOE support for expert_feed_forward_length
    
    previously, expert ff was taken from n_ff (intermediate size) but it is now properly taken from LLM_KV_EXPERT_FEED_FORWARD_LENGTH
    
    n_ff_exp and n_ff_shared_exp are now properly calculated
    
    * update: convert-hf-to-gguf.py cleanup for Qwen2MoeForCausalLM
    
    * fix: QWEN2MOE support for expert_feed_forward_length
    
    previously, expert ff was taken from n_ff (intermediate size) but it is now properly taken from LLM_KV_EXPERT_FEED_FORWARD_LENGTH
    
    n_ff_exp and n_ff_shexp are now properly calculated

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
llama.cpp

commit 5b6da187508f49a9fa9d95fa22ae804a0780d256
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Mon Jun 17 23:53:17 2024 +0530

    Make updates to type cast based on compiler instead of OS (#7851)

ggml-impl.h

commit 7c26775adb579e92b59c82e8084c07a1d0f75e9c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 17 19:40:01 2024 +0300

    llama : disable FA if KV head size do not match (#7982)

llama.cpp

commit b473e95084c286780165568cf0f385f21141d68d
Author: Bryan Honof <bryanhonof@gmail.com>
Date:   Mon Jun 17 17:37:55 2024 +0200

    Add Nix and Flox install instructions (#7899)

README.md

commit 99052cd227c7182fcf53343d2e7d33bfa180a9cf
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 17 16:51:42 2024 +0200

    sched : offload_op also requires supports_op (#7977)

ggml-backend.c

commit c637fcd34d135a9ff4f97d3a53ad03a910a4a31f
Author: Frank Mai <thxcode0824@gmail.com>
Date:   Mon Jun 17 22:11:08 2024 +0800

    fix: divide 0 exception in mamba (#7932)
    
    Signed-off-by: thxCode <thxcode0824@gmail.com>

llama.cpp

commit 6a2f0b3474d479bda4ac2ee7cfd5dcdcf0be1f79
Author: Markus Tavenrath <mtavenrath@users.noreply.github.com>
Date:   Mon Jun 17 16:10:15 2024 +0200

    Implement non-mapped async IO for CUDA on Windows.  (#7896)
    
    * Implement non-mapped async IO for CUDA on Windows. On a fast Gen5 NVMe drive this change improves model load time by >3x while it should be the same (or slightly faster) on any other drive.
    
    * Free resources except for backend.
    
    * Change assertions to exceptions in llama_file, find correct cuda backend to create CUDA resources and respect the use_mmap flag again for CUDA.
    
    * Apply suggestions from code review
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Fix editorconfig and unused variable
    
    * Fix issues with Windows build
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

llama.cpp

commit 21be9cab94e0b5b53cb6edeeebf8c8c799baad03
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 17 11:09:20 2024 +0300

    rpc : fix load/store misaligned addresses (#7948)

ggml-rpc.cpp

commit 006167aaf6b6aaa4daa52961035f7460af19f469
Author: Brian <mofosyne@gmail.com>
Date:   Mon Jun 17 15:25:20 2024 +1000

    gguf-dump.py: add --markdown dump output (#7853)
    
    * gguf-dump.py: add --markdown dump output
    
    * gguf-dump.py: Add toc
    
    * gguf-dump.py: use standard tensor name lookup. Also add tensor ID field
    
    * gguf-dump.py: Add tensor overview count
    
    * gguf-dump.py: fix array preview
    
    * gguf-dump.py: markdownTableWithAlignmentSupport() added
    
    * Add type hints and spacing
    
    Co-authored-by: compilade <git@compilade.net>
    
    * gguf-dump.py: prettyfy dimention
    
    * gguf-dump: right align element count
    
    * gguf-dump.py: element count autosizing
    
    * Apply suggestions from code review
    
    Co-authored-by: compilade <git@compilade.net>
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

gguf-py/scripts/gguf-dump.py

commit df68d4fa5dc929217d3e64d673e099d7a417b206
Author: Neo Zhang <zhang.jianyu@outlook.com>
Date:   Mon Jun 17 11:17:07 2024 +0800

    [SYCL] Update README-sycl.md for Chapter "Recommended release" and "News" (#7946)
    
    * Update README-sycl.md
    
    * Update README-sycl.md
    
    * Update README-sycl.md
    
    * Update README-sycl.md

README-sycl.md

commit 43b35e38ba371f9a7faa6dca4c5d1e8f698ffd87
Author: Calvin Laurenson <calvin@laurenson.dev>
Date:   Sun Jun 16 15:23:04 2024 -0700

    Add support for sqrt on CUDA (#7953)
    
    * cuda sqrt support
    
    * enable cuda in pca
    
    * fix comments in pca
    
    * add test
    
    * add sqrt to ggml_backend_cuda_supports_op
    
    * fix test
    
    * new line
    
    * Use F32 sqrtf instead of F64 sqrt
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

examples/cvector-generator/pca.hpp
ggml-cuda.cu
ggml-cuda/unary.cu
ggml-cuda/unary.cuh
tests/test-backend-ops.cpp

commit 19b7a836f6658e18e973af532a5cc6ad6b3a27f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 11 17:39:01 2024 +0300

    cuda : fix bounds check for src0 rows in MMVQ kernel (whisper/2231)
    
    * cuda : fix bounds check for src0 rows in MMVQ kernel
    
    * Update ggml-cuda/mmvq.cu
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

ggml-cuda/mmvq.cu

commit b5fcf8ef5c29df53cfff60e180b4992a3b2332a6
Author: Hong Bo PENG <penghb@cn.ibm.com>
Date:   Sun Jun 16 16:53:11 2024 +0800

    ggml : fix and optimize ppc64le (ggml/849)
    
    * fix compile issues introduced by loongarch_asx
    
    * restore quant changes to merge
    
    * fix compile issues introduced by loongarch_asx
    
    * further optimize by using vec_msum & vec_sum4s on ppc64le

ggml-quants.c

commit 398105ff4373eea385ea8e8625cb417b2ae51134
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sun Jun 16 10:51:18 2024 +0200

    ggml : remove duplicate include of ggml-common.h (ggml/853)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

ggml-quants.c

commit bc6c457fa35f6791e9a2bb61108e7d49e8fc98bd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 16 19:16:21 2024 +0300

    flake.lock: Update (#7951)

flake.lock

commit 52399254b3bceda279b4ea9111a983e32310166e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 16 14:51:40 2024 +0300

    unicode : avoid char32_t (#7957)
    
    ggml-ci

llama.cpp
unicode.cpp
unicode.h

commit 6fe1c627413725ddc1f9e323f6b13fe388c53e0a
Author: hopkins385 <98618192+hopkins385@users.noreply.github.com>
Date:   Sun Jun 16 13:51:18 2024 +0200

    readme : update UI list [no ci] (#7958)

README.md

commit cddaf028adc738b5a7ecc60809cb78e0ba0f97c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 16 14:50:12 2024 +0300

    ggml : fix handling of zero blocks in IQ quants (#7955)
    
    ggml-ci

ggml-quants.c

commit c8a82194a888f68f259e0d0fa96f3332ac7c5cb6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 16 10:46:51 2024 +0300

    github : update pr template

.github/pull_request_template.md

commit 7c7836d9d4062d6858e3fb337b135c417ccee6ce
Author: 0cc4m <picard12@live.de>
Date:   Sun Jun 16 07:17:31 2024 +0200

    Vulkan Shader Refactor, Memory Debugging Option (#7947)
    
    * Refactor shaders, extract GLSL code from ggml_vk_generate_shaders.py into vulkan-shaders directory
    
    * Improve debug log code
    
    * Add memory debug output option
    
    * Fix flake8
    
    * Fix unnecessary high llama-3 VRAM use

CMakeLists.txt
Makefile
ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py
vulkan-shaders/add.comp
vulkan-shaders/argsort.comp
vulkan-shaders/clamp.comp
vulkan-shaders/copy.comp
vulkan-shaders/dequant_f32.comp
vulkan-shaders/dequant_funcs.comp
vulkan-shaders/dequant_head.comp
vulkan-shaders/dequant_q2_k.comp
vulkan-shaders/dequant_q3_k.comp
vulkan-shaders/dequant_q4_0.comp
vulkan-shaders/dequant_q4_1.comp
vulkan-shaders/dequant_q4_k.comp
vulkan-shaders/dequant_q5_0.comp
vulkan-shaders/dequant_q5_1.comp
vulkan-shaders/dequant_q5_k.comp
vulkan-shaders/dequant_q6_k.comp
vulkan-shaders/dequant_q8_0.comp
vulkan-shaders/diag_mask_inf.comp
vulkan-shaders/div.comp
vulkan-shaders/gelu.comp
vulkan-shaders/generic_binary_head.comp
vulkan-shaders/generic_head.comp
vulkan-shaders/generic_unary_head.comp
vulkan-shaders/get_rows.comp
vulkan-shaders/get_rows_quant.comp
vulkan-shaders/mul.comp
vulkan-shaders/mul_mat_split_k_reduce.comp
vulkan-shaders/mul_mat_vec.comp
vulkan-shaders/mul_mat_vec_base.comp
vulkan-shaders/mul_mat_vec_nc.comp
vulkan-shaders/mul_mat_vec_p021.comp
vulkan-shaders/mul_mat_vec_q2_k.comp
vulkan-shaders/mul_mat_vec_q3_k.comp
vulkan-shaders/mul_mat_vec_q4_k.comp
vulkan-shaders/mul_mat_vec_q5_k.comp
vulkan-shaders/mul_mat_vec_q6_k.comp
vulkan-shaders/mul_mm.comp
vulkan-shaders/norm.comp
vulkan-shaders/relu.comp
vulkan-shaders/rms_norm.comp
vulkan-shaders/rope_head.comp
vulkan-shaders/rope_neox.comp
vulkan-shaders/rope_norm.comp
vulkan-shaders/scale.comp
vulkan-shaders/silu.comp
vulkan-shaders/soft_max.comp
vulkan-shaders/square.comp
vulkan-shaders/sum_rows.comp
vulkan-shaders/types.comp

commit 0c7b3595b9e5ad2355818e259f06b0dc3f0065b3
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Jun 15 18:53:40 2024 +0200

    Add `cvector-generator` example (#7514)
    
    * add control-vector-generator
    
    * calc diff
    
    * add comments
    
    * proof-of-concept stdlib implementation
    
    Implements PCA and file writing using mostly standard libraries. The output is recognized as a functional control vector, but outputs gibberish.
    
    * param parsing, refactor, comments
    
    Added basic command-line parameters for outfile and one each positive/negative prompt.
    
    Refactored some messy code in PCA computation and GGUF exporting.
    
    Left a bunch of comments regarding further work needed.
    
    * example template completions
    
    Implements an example template set built from the positive/negative prompts like the control vector Python implementation.
    
    * add multi prompts, multi-thread for PCA
    
    * fix mem error
    
    * add debugs
    
    * fix matrix transpose multiplication
    
    you have got to be kidding me
    
    * preliminary template/multiprompt support
    
    model is running out of context and that ought to be fixed (segfaulting) but other than that it looks goodish
    
    * fix zero output & param parsing, functional templating
    
    fixed a bug where the output file had no tensor data/was all zero
    
    fixed a bug where single hyphen flags were not being correctly parsed
    
    implements creation of templated prompts from input (still need to adapt based on model)
    
    * fix square_diff matmul index range and CRLF->LF line endings
    
    fixed a logic error where square_diff would not multiply all rows
    
    fixed a formatting error where the provided completions.txt had CRLF line endings
    
    * add command-line args for num threads, num completions file lines, always reload model
    
    refactored a few things and did what the commit message says on the tin
    
    * code aestheticization
    
    * fix compiler warnings
    
    * in-series multithreading for prompt embedding?
    
    added commented-out code to attempt to start implementing mutlithreading for embedding in main
    
    * remove unnecessary multithreading
    
    * interim fix memory leak
    
    * translated everything but PCA (I think)
    
    * tentatively translate the rest
    
    * fix ggml errors and make new ones
    
    at least it compiles and runs
    
    * fix cb_eval
    
    * temporary commit while I move dev environments
    
    it finally outputs a functioning control vector - "functioning" in the sense that it can be loaded and it clearly has the right idea, but makes the model incoherent
    
    * update debug statements
    
    * pre-tokenize so we can allocate correct memory to ctx_diffs_wrapped
    
    * update comments
    
    * (wip) refactor
    
    * clean up PCA ggml implementation
    
    * fix shape of v_diff_original
    
    * add n_batch for pca
    
    * working version
    
    * remember to copy back the last_eigenvector
    
    * fix n_completions
    
    * bring back n_completions
    
    * default n_pca_batch to 20
    
    * fix macos build
    
    * add to makefile all targets
    
    * use ggml_format_name
    
    * add readme
    
    * fix .editorconfig
    
    * use ggml_backend_tensor_copy
    
    * attemp to fix compile problem on mac
    
    * fix compile warn
    
    * reuse allocr
    
    * move param parser to common
    
    * better error handling
    
    * clean up a bit
    
    * add print_usage
    
    * shorten help msg
    
    * beautify help msg
    
    * escape prompt by default
    
    * change compile target to llama-cvector-generator
    
    * typo
    
    * disable GPU for PCA
    
    * code style
    
    ---------
    
    Co-authored-by: Christian Zhou-Zheng <christianzhouzheng@gmail.com>

.editorconfig
Makefile
common/common.cpp
common/common.h
examples/CMakeLists.txt
examples/cvector-generator/CMakeLists.txt
examples/cvector-generator/README.md
examples/cvector-generator/completions.txt
examples/cvector-generator/cvector-generator.cpp
examples/cvector-generator/negative.txt
examples/cvector-generator/pca.hpp
examples/cvector-generator/positive.txt

commit 7b2f4a7d193ef2475259bbe7656fcccfab4b1217
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Sat Jun 15 14:05:10 2024 +0800

    [SYCL] remove global variables (#7710)
    
    * separate DPCT helpers outside
    
    * replace global variables with context
    
    * remove useless extra
    
    * update mul_mat condition
    
    * remove duplicate buft initialization
    
    * remove duplicate extra and global work group size
    
    * remove useless backend check
    
    * remove duplicated extras
    
    * use macro for group_size and remove cuda-related

CMakeLists.txt
ggml-sycl.cpp
ggml-sycl.h
ggml-sycl/backend.hpp
ggml-sycl/common.cpp
ggml-sycl/common.hpp
ggml-sycl/dpct/helper.hpp
ggml-sycl/presets.hpp
llama.cpp

commit f8ec8877b75774fc6c47559d529dac423877bcad
Author: olexiyb <olexiyb@gmail.com>
Date:   Fri Jun 14 20:28:34 2024 +0300

    ci : fix macos x86 build (#7940)
    
    In order to use old `macos-latest` we should use `macos-12`
    
    Potentially will fix: https://github.com/ggerganov/llama.cpp/issues/6975

.github/workflows/build.yml

commit 76d66ee0be91e2bec93206e821ee1db8d023cff5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jun 14 18:41:49 2024 +0200

    CUDA: faster q2_K, q3_K MMQ + int8 tensor cores (#7921)
    
    * CUDA: faster q2_K, q3_K MMQ + int8 tensor cores
    
    * try CI fix
    
    * try CI fix
    
    * try CI fix
    
    * fix data race
    
    * rever q2_K precision related changes

ggml-cuda.cu
ggml-cuda/argsort.cu
ggml-cuda/common.cuh
ggml-cuda/mmq.cuh
ggml-cuda/softmax.cu
ggml-cuda/vecdotq.cuh

commit 66ef1ceedf983773c8ceb4d925285d41d4e50e2a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jun 14 17:14:09 2024 +0300

    metal : utilize max shared memory for mul_mat_id (#7935)

ggml-metal.m

commit e65bbf606c61f49dc06c7ac060cd5ba7ae446025
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Fri Jun 14 16:47:41 2024 +0300

    llama-bench : fix RPC indication (#7936)
    
    Show "<backend_name>+RPC" when RPC offloading is used

examples/llama-bench/llama-bench.cpp

commit 6fcd1331efbfbb89c8c96eba2321bb7b4d0c40e4
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Fri Jun 14 12:20:04 2024 +0200

    llama : more checks before assuming FIM tokens (#7644)
    
    * More checks before assuming FIM tokens for Llama arch
    
    * extensive token check

llama.cpp

commit 41b9260f18eb7f325c952006ac46afc1d0d8ad2f
Author: Elaine <elaine.zosa@gmail.com>
Date:   Fri Jun 14 13:16:49 2024 +0300

    convert : add Poro-34B-chat tokenizer support (#7713)
    
    * support for Poro chat pre-tokenizer
    
    * add support for Poro pre-tokenizer
    
    * Update convert-hf-to-gguf-update.py
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Change Poro-34B-chat to poro-chat
    
    * Change Poro-34B-chat to poro-chat
    
    * Update convert-hf-to-gguf-update.py
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
llama.cpp
llama.h

commit 172c8256840ffd882ab9992ecedbb587d9b21f15
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Thu Jun 13 15:18:44 2024 +0300

    rpc : fix ggml_backend_rpc_supports_buft() (#7918)

ggml-rpc.cpp

commit a55eb1bf0fa2fd84147bdfd384391e029d988253
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu Jun 13 09:42:41 2024 +0200

    readme : Remove outdated instructions from README.md (#7914) [no ci]

README.md

commit f578b86b2123d0f92afbaa98a031df4d4464e582
Author: slaren <slarengh@gmail.com>
Date:   Thu Jun 13 03:11:35 2024 +0200

    move BLAS to a separate backend (#6210)
    
    * move BLAS to a separate backend
    
    * rename GGML_USE_OPENBLAS to GGML_USE_BLAS
    
    * alloc : reuse same buffer when the same buffer type if used multiple times
    
    * set number of threads automatically for openblas and blis
    
    * sched : print assignments when GGML_SCHED_DEBUG env variable is set
    
    * sched : allow ops with weights on an incompatible buffer type
    
    This will cause the weight to be copied to a backend that supports the
    op, which is very costly. The weight should have been stored in a buffer
    of a backend that can run the op, but llama.cpp cannot do this
    automatically at the moment.
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
Makefile
examples/llama-bench/llama-bench.cpp
ggml-alloc.c
ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-blas.cpp
ggml-blas.h
ggml-cuda.cu
ggml-kompute.cpp
ggml-metal.m
ggml-rpc.cpp
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
llama.cpp

commit 1c641e6aac5c18b964e7b32d9dbbb4bf5301d0d7
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Jun 13 00:41:52 2024 +0100

    `build`: rename main → llama-cli, server → llama-server, llava-cli → llama-llava-cli, etc... (#7809)
    
    * `main`/`server`: rename to `llama` / `llama-server` for consistency w/ homebrew
    
    * server: update refs -> llama-server
    
    gitignore llama-server
    
    * server: simplify nix package
    
    * main: update refs -> llama
    
    fix examples/main ref
    
    * main/server: fix targets
    
    * update more names
    
    * Update build.yml
    
    * rm accidentally checked in bins
    
    * update straggling refs
    
    * Update .gitignore
    
    * Update server-llm.sh
    
    * main: target name -> llama-cli
    
    * Prefix all example bins w/ llama-
    
    * fix main refs
    
    * rename {main->llama}-cmake-pkg binary
    
    * prefix more cmake targets w/ llama-
    
    * add/fix gbnf-validator subfolder to cmake
    
    * sort cmake example subdirs
    
    * rm bin files
    
    * fix llama-lookup-* Makefile rules
    
    * gitignore /llama-*
    
    * rename Dockerfiles
    
    * rename llama|main -> llama-cli; consistent RPM bin prefixes
    
    * fix some missing -cli suffixes
    
    * rename dockerfile w/ llama-cli
    
    * rename(make): llama-baby-llama
    
    * update dockerfile refs
    
    * more llama-cli(.exe)
    
    * fix test-eval-callback
    
    * rename: llama-cli-cmake-pkg(.exe)
    
    * address gbnf-validator unused fread warning (switched to C++ / ifstream)
    
    * add two missing llama- prefixes
    
    * Updating docs for eval-callback binary to use new `llama-` prefix.
    
    * Updating a few lingering doc references for rename of main to llama-cli
    
    * Updating `run-with-preset.py` to use new binary names.
    Updating docs around `perplexity` binary rename.
    
    * Updating documentation references for lookup-merge and export-lora
    
    * Updating two small `main` references missed earlier in the finetune docs.
    
    * Update apps.nix
    
    * update grammar/README.md w/ new llama-* names
    
    * update llama-rpc-server bin name + doc
    
    * Revert "update llama-rpc-server bin name + doc"
    
    This reverts commit e474ef1df481fd8936cd7d098e3065d7de378930.
    
    * add hot topic notice to README.md
    
    * Update README.md
    
    * Update README.md
    
    * rename gguf-split & quantize bins refs in **/tests.sh
    
    ---------
    
    Co-authored-by: HanClinto <hanclinto@gmail.com>

.devops/cloud-v-pipeline
.devops/llama-cli-cuda.Dockerfile
.devops/llama-cli-intel.Dockerfile
.devops/llama-cli-rocm.Dockerfile
.devops/llama-cli-vulkan.Dockerfile
.devops/llama-cli.Dockerfile
.devops/llama-cpp-clblast.srpm.spec
.devops/llama-cpp-cuda.srpm.spec
.devops/llama-cpp.srpm.spec
.devops/llama-server-cuda.Dockerfile
.devops/llama-server-intel.Dockerfile
.devops/llama-server-rocm.Dockerfile
.devops/llama-server-vulkan.Dockerfile
.devops/llama-server.Dockerfile
.devops/nix/apps.nix
.devops/nix/package.nix
.devops/tools.sh
.dockerignore
.github/ISSUE_TEMPLATE/01-bug-low.yml
.github/ISSUE_TEMPLATE/02-bug-medium.yml
.github/ISSUE_TEMPLATE/03-bug-high.yml
.github/ISSUE_TEMPLATE/04-bug-critical.yml
.github/workflows/bench.yml
.github/workflows/build.yml
.github/workflows/docker.yml
.github/workflows/server.yml
.gitignore
Makefile
README-sycl.md
README.md
ci/run.sh
docs/HOWTO-add-model.md
docs/token_generation_performance_tips.md
examples/CMakeLists.txt
examples/Miku.sh
examples/baby-llama/CMakeLists.txt
examples/base-translate.sh
examples/batched-bench/CMakeLists.txt
examples/batched-bench/README.md
examples/batched.swift/Makefile
examples/batched.swift/Package.swift
examples/batched.swift/README.md
examples/batched/CMakeLists.txt
examples/batched/README.md
examples/benchmark/CMakeLists.txt
examples/chat-13B.sh
examples/chat-persistent.sh
examples/chat-vicuna.sh
examples/chat.sh
examples/convert-llama2c-to-ggml/CMakeLists.txt
examples/convert-llama2c-to-ggml/README.md
examples/embedding/CMakeLists.txt
examples/embedding/README.md
examples/eval-callback/CMakeLists.txt
examples/eval-callback/README.md
examples/export-lora/CMakeLists.txt
examples/export-lora/README.md
examples/finetune/CMakeLists.txt
examples/finetune/README.md
examples/finetune/finetune.sh
examples/gbnf-validator/CMakeLists.txt
examples/gbnf-validator/gbnf-validator.cpp
examples/gguf-split/CMakeLists.txt
examples/gguf-split/tests.sh
examples/gguf/CMakeLists.txt
examples/gritlm/CMakeLists.txt
examples/gritlm/README.md
examples/imatrix/CMakeLists.txt
examples/imatrix/README.md
examples/infill/CMakeLists.txt
examples/infill/README.md
examples/jeopardy/jeopardy.sh
examples/json-schema-pydantic-example.py
examples/json_schema_to_grammar.py
examples/llama-bench/README.md
examples/llava/CMakeLists.txt
examples/llava/MobileVLM-README.md
examples/llava/README.md
examples/llava/android/adb_run.sh
examples/lookahead/CMakeLists.txt
examples/lookup/CMakeLists.txt
examples/lookup/lookup-merge.cpp
examples/main-cmake-pkg/CMakeLists.txt
examples/main-cmake-pkg/README.md
examples/main/CMakeLists.txt
examples/main/README.md
examples/parallel/CMakeLists.txt
examples/passkey/CMakeLists.txt
examples/passkey/README.md
examples/perplexity/CMakeLists.txt
examples/perplexity/perplexity.cpp
examples/quantize-stats/CMakeLists.txt
examples/quantize/CMakeLists.txt
examples/quantize/tests.sh
examples/reason-act.sh
examples/retrieval/CMakeLists.txt
examples/retrieval/README.md
examples/rpc/README.md
examples/save-load-state/CMakeLists.txt
examples/server-llama2-13B.sh
examples/server/CMakeLists.txt
examples/server/README.md
examples/server/bench/README.md
examples/server/bench/bench.py
examples/server/public_simplechat/readme.md
examples/server/tests/README.md
examples/server/tests/features/steps/steps.py
examples/simple/CMakeLists.txt
examples/speculative/CMakeLists.txt
examples/sycl/CMakeLists.txt
examples/sycl/README.md
examples/sycl/run-llama2.sh
examples/tokenize/CMakeLists.txt
examples/train-text-from-scratch/CMakeLists.txt
examples/train-text-from-scratch/README.md
flake.nix
grammars/README.md
pocs/vdot/CMakeLists.txt
scripts/get-hellaswag.sh
scripts/get-wikitext-103.sh
scripts/get-wikitext-2.sh
scripts/get-winogrande.sh
scripts/hf.sh
scripts/pod-llama.sh
scripts/qnt-all.sh
scripts/run-all-ppl.sh
scripts/run-with-preset.py
scripts/server-llm.sh

commit 963552903f51043ee947a8deeaaa7ec00bc3f1a4
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 12 17:41:51 2024 +0200

    CUDA: fix broken oob check for FA vec f32 kernel (#7904)

ggml-cuda/fattn-vec-f32.cuh

commit a9cae48003dfc4fe95b8f5c81682fc6e63425235
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 12 16:00:22 2024 +0300

    tests : add non-cont unary tests (#7857)
    
    * tests : add non-cont unary tests
    
    * ggml : update unary asserts and "supports_op"
    
    ggml-ci

ggml-cuda.cu
ggml-cuda/unary.cu
ggml-kompute.cpp
ggml-metal.m
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
tests/test-backend-ops.cpp

commit bfaa676b0841617d4ef3596e63aca6be1a8eb1b5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 12 15:24:20 2024 +0300

    ggml : improve ggml_is_contiguous logic (#7856)
    
    * ggml : improve ggml_is_contiguous logic
    
    ggml-ci
    
    * ggml : support more contiguous cases
    
    ggml-ci

ggml.c

commit 704a35b183748954013bd875bbbfdd9eaca14e62
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 12 14:42:29 2024 +0300

    server : restore numeric prompts (#7883)

examples/server/server.cpp

commit dcf752707d96eb305f546526c7bc5d01f0831130
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed Jun 12 17:05:35 2024 +0800

    update intel docker oneapi-basekit to 2024.1.1-devel-ubuntu22.04 (#7894)
    
    In addition this reverts a workaround we had to do to workaround the upstream issue with expired intel GPG package keys in 2024.0.1-devel-ubuntu22.04

.devops/main-intel.Dockerfile
.devops/server-intel.Dockerfile

commit f2b5764beb35583295e2475479c18f249b139b58
Author: Patrice Ferlet <metal3d@gmail.com>
Date:   Wed Jun 12 03:18:16 2024 +0200

    Fix a typo and add Fedora 40 pacakge to install for Vulkan (#7794) [no ci]
    
    Fix "appropiate" to "appropriate" and add Fedora 40 packages to install to compile with Vulkan support

README.md

commit 73bac2b11d7d3e20982fc9ee607625836387db8b
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Wed Jun 12 03:26:05 2024 +0800

    vulkan: select only one device for single gpu with multiple drivers (#7582)

ggml-vulkan.cpp

commit ef52d1d16afc695d798396cdd13594ea5e45a9dd
Author: 0cc4m <picard12@live.de>
Date:   Tue Jun 11 21:20:29 2024 +0200

    Update Vulkan RoPE implementation (#7818)
    
    * Update Vulkan RoPE implementation
    
    * Return nullptr on alloc_buffer when allocation fails, instead of throwing an exception
    
    Minor fixes
    
    * Fix segfault when running out of VRAM
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-alloc.c
ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py

commit 14f83526cd27f638c856ea6eff08110b9860eb2a
Author: Deven Mistry <31466137+deven367@users.noreply.github.com>
Date:   Tue Jun 11 12:18:58 2024 -0400

    fix broken link in pr template (#7880) [no ci]
    
    * fix broken link in pr template
    
    * Update pull_request_template.md [no ci]
    
    ---------
    
    Co-authored-by: Brian <mofosyne@gmail.com>

.github/pull_request_template.md

commit 6fe42d073f0554eada93ac9d40574025aeedb703
Author: Brian <mofosyne@gmail.com>
Date:   Wed Jun 12 00:43:41 2024 +1000

    github: move PR template to .github/ root (#7868)

.github/pull_request_template.md

commit 148995e5e57b313cce2672f75610db58c6327a51
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jun 11 14:45:40 2024 +0200

    llama-bench: more compact markdown tables (#7879)

examples/llama-bench/llama-bench.cpp

commit 4bfe50f741479c1df1c377260c3ff5702586719e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 11 10:10:20 2024 +0300

    tests : check the Python version (#7872)
    
    ggml-ci

tests/test-json-schema-to-grammar.cpp

commit bdcb8f42221bc40c411150a009a3d3a30fa74722
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jun 11 08:26:07 2024 +0200

    CUDA: int8 tensor cores for MMQ (q4_K, q5_K, q6_K) (#7860)

ggml-cuda/mma.cuh
ggml-cuda/mmq.cuh

commit c2ce6c47e4f2d891bf29d8810832a3b310a8f205
Author: slaren <slarengh@gmail.com>
Date:   Tue Jun 11 07:59:20 2024 +0200

    fix CUDA CI by using a windows-2019 image (#7861)
    
    * try to fix CUDA ci with --allow-unsupported-compiler
    
    * trigger when build.yml changes
    
    * another test
    
    * try exllama/bdashore3 method
    
    * install vs build tools before cuda toolkit
    
    * try win-2019

.github/workflows/build.yml

commit b61eb9644d64e90123ac805436d95b94b3b4cc3f
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue Jun 11 02:22:57 2024 +0100

    json: refine constraint for whitespace to avoid runaways yet allow pretty print (#7866)

common/json-schema-to-grammar.cpp
examples/json_schema_to_grammar.py
examples/server/public/json-schema-to-grammar.mjs
grammars/json.gbnf
grammars/json_arr.gbnf
tests/test-json-schema-to-grammar.cpp

commit 396b18dfec2c56846e80362db70af09b9e1d70ba
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue Jun 11 01:00:30 2024 +0100

    `json`: document schema conversion in GBNF readme, align manual grammar examples & converters (#7841)
    
    * json: fix char pattern in grammar converters
    
    * json: prevent number precision & whitespace runaways in example grammars
    
    * json: add doc to grammar readme

common/json-schema-to-grammar.cpp
examples/json_schema_to_grammar.py
examples/server/public/json-schema-to-grammar.mjs
grammars/README.md
grammars/json.gbnf
grammars/json_arr.gbnf
tests/test-json-schema-to-grammar.cpp

commit 864a99e7a01d9422d2f55618dbe62c8099a2175c
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Jun 10 18:32:10 2024 -0400

    cmake : fix CMake requirement for CUDA (#7821)

CMakeLists.txt

commit fd5ea0f897ecb3659d6c269ef6f3d833e865ead7
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 10 14:18:41 2024 +0200

    ci : try win-2019 on server windows test (#7854)

.github/workflows/server.yml

commit c28a83902cf6ab6a9e085ad6d4cc2e95c4ccfe40
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 10 15:00:15 2024 +0300

    examples : remove --instruct remnants (#7846)

README.md
examples/alpaca.sh
examples/gpt4all.sh
examples/llama2-13b.sh
examples/llama2.sh

commit d9da0e4986f121c727bdd9579a6688097b11602c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 10 14:59:55 2024 +0300

    server : improve "prompt" handling (#7847)

examples/server/server.cpp

commit 1f0dabda8d5c131f9d4632aa41de74317cdd61fb
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 10 11:45:13 2024 +0200

    CUDA: use tensor cores for MMQ (#7676)
    
    * CUDA: int8 tensor cores for MMQ (legacy quants)
    
    * fix out-of-bounds writes
    
    * __builtin_assume -> GGML_CUDA_ASSUME
    
    * fix writeback returning too early

ggml-cuda/common.cuh
ggml-cuda/fattn-common.cuh
ggml-cuda/fattn-tile-f16.cu
ggml-cuda/fattn-vec-f16.cuh
ggml-cuda/fattn-wmma-f16.cuh
ggml-cuda/mma.cuh
ggml-cuda/mmq.cuh

commit af4ae502ddaeb03cd5861273ca2e9a5ae4551db7
Author: Ben Ashbaugh <ben.ashbaugh@intel.com>
Date:   Mon Jun 10 02:21:31 2024 -0700

    use the correct SYCL context for host USM allocations (#7777)
    
    Signed-off-by: Ben Ashbaugh <ben.ashbaugh@intel.com>

ggml-sycl.cpp

commit 10ceba354a3b152ff425e9fa97f9caaef99a46b1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 10 02:04:50 2024 +0300

    flake.lock: Update (#7838)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/ad57eef4ef0659193044870c731987a6df5cf56b?narHash=sha256-SzDKxseEcHR5KzPXLwsemyTR/kaM9whxeiJohbL04rs%3D' (2024-05-29)
      → 'github:NixOS/nixpkgs/051f920625ab5aabe37c920346e3e69d7d34400e?narHash=sha256-4q0s6m0GUcN7q%2BY2DqD27iLvbcd1G50T2lv08kKxkSI%3D' (2024-06-07)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit e95beeb1fc4621826ddd616776dbdf717366bf5c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 9 20:19:35 2024 +0300

    imatrix : handle partial entries (#7833)

examples/imatrix/imatrix.cpp

commit 57bf62ce7cb75cca589943e2050d29bff4026e76
Author: Nicolás Pérez <nicolas_perez@brown.edu>
Date:   Sun Jun 9 11:24:29 2024 -0400

    docs: Added initial PR template with directions for doc only changes and squash merges [no ci] (#7700)
    
    This commit adds pull_request_template.md and CONTRIBUTING.md . It focuses on explaining to contributors the need to rate PR complexity level, when to add [no ci] and how to format PR title and descriptions.
    
    Co-authored-by: Brian <mofosyne@gmail.com>
    Co-authored-by: compilade <git@compilade.net>

.github/PULL_REQUEST_TEMPLATE/pull_request_template.md
CONTRIBUTING.md

commit 3e2ee443159724e2d3a0741f6b167e599ec088aa
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Sun Jun 9 12:50:35 2024 +0200

    server: do not remove whitespace at the start of a completion chunk (#7830)

examples/server/public/index-new.html

commit 42b53d192f4e3abf1b7c8e424628424504ea5dc5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jun 9 09:42:25 2024 +0200

    CUDA: revise q8_1 data layout for mul_mat_q (#7824)

ggml-cuda.cu
ggml-cuda/mmq.cu
ggml-cuda/mmq.cuh
ggml-cuda/quantize.cu
ggml-cuda/quantize.cuh

commit 2decf57bc6e4a6b45176c3727d964a01161beecc
Author: sasha0552 <admin@sasha0552.org>
Date:   Sun Jun 9 06:39:25 2024 +0000

    convert-hf : set the model name based on cli arg, if present (#7693)
    
     `--model-name` argument was added a while ago but did not do anything.
    This commit fixes this issue and enables this feature.

convert-hf-to-gguf.py

commit 5795b941827fdec6c1662986de962badff456718
Author: compilade <git@compilade.net>
Date:   Sat Jun 8 22:47:25 2024 -0400

    convert-hf : match model part name prefix and suffix (#7687)
    
    In #7075, to fix the conversion of (some) models using model-00001-of-00001.safetensors instead of model.safetensors for a single model part we simply used the same logic as the part count to get the part names.
    
    But this doesn't always work correctly, like when unusual additional model files like consolidated.safetensors in https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 are present.
    
    This commit matching both the prefix and the suffix of the model part names should fix this problem without breaking any previously-supported upstream models. But according to report by @teleprint-me there is still some
    persistent problem, but shall do in the meantime.

convert-hf-to-gguf.py

commit ed9f2521185706481501a5e6d5315397b11802ff
Author: compilade <git@compilade.net>
Date:   Sat Jun 8 22:34:29 2024 -0400

    gguf-py : decouple adding metadata from writing in GGUFWriter (#7827)
    
    Main changes of this PR is to consolidate GGUFWriter.add_key and GGUFWriter.add_val into GGUFWriter.add_key_value.
    
    In addition use_temp_file is now opt-in instead of opt-out defaulting to False.
    
    Also GGUFWriter now does not require output file name until when actually writing to it.
    
    And GGUFWriter doesn't really need to eagerly prepare the data layout of the metadata

convert-hf-to-gguf.py
gguf-py/gguf/gguf_writer.py
gguf-py/scripts/gguf-new-metadata.py

commit fe1e3917cfa0f9397a765cfd0aef880674d938d5
Author: slaren <slarengh@gmail.com>
Date:   Sun Jun 9 01:43:39 2024 +0200

    Revert "[SYCL] Update rpc-server.cpp to include SYCL backend (#7682)" (#7808)
    
    This reverts commit 9422c5e34bbd302493b77a8f6d546154a1f4fe82.

examples/rpc/rpc-server.cpp

commit d4d915d351d1f1270d56184bdd46672893e8a5d8
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Sat Jun 8 20:21:08 2024 +0100

    url: save -mu downloads to new cache location (#7826)
    
    * url: save -mu download to new cache location
    
    * url: fs_get_cache_file_path util
    
    * url: tweak sig of fs_get_cache_file

common/common.cpp
common/common.h

commit 7a16ce7db2a74a223f0f3b9cee66d4539c5bce8f
Author: sasha0552 <admin@sasha0552.org>
Date:   Sat Jun 8 07:50:31 2024 +0000

    server : smart slot selection using Longest Common Prefix (#7728)
    
    * server : Smart selection of available slot using Longest Common Substring
    
    * add usage
    
    * remove trailing whitespaces
    
    * Use Longest Common Prefix (LCP) instead of LCS
    
    * Rename argument

common/common.cpp
common/common.h
examples/server/server.cpp
examples/server/utils.hpp

commit da799b41891e34aac86ce4e173f9c4c0afd4fab3
Author: slaren <slarengh@gmail.com>
Date:   Fri Jun 7 19:47:49 2024 +0200

    vulkan : reuse parent extra for views (#7806)
    
    * vulkan : reuse parent extra for views
    
    * Fix validation error when multiple compute contexts are used in a graph
    
    ---------
    
    Co-authored-by: 0cc4m <picard12@live.de>

ggml-vulkan.cpp

commit c00fad71e507ff386d42bd74846fe06d19dd63a4
Author: Christian Zhou-Zheng <59622928+christianazinn@users.noreply.github.com>
Date:   Fri Jun 7 08:56:01 2024 -0400

    gguf-split : change binary multi-byte units to decimal (#7803)

examples/gguf-split/gguf-split.cpp

commit 27615f5ab21060d96953c9c1e223051ab2188f57
Author: intelmatt <61025942+intelmatt@users.noreply.github.com>
Date:   Fri Jun 7 05:15:07 2024 -0700

    cmake : fix BUILD_SHARED_LIBS=ON build (#7784)
    
    common depends on pthreads in Linux

common/CMakeLists.txt

commit 7027b27d765db95d4ac6b569d976e387a8715881
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jun 7 11:15:49 2024 +0200

    server: update cache_prompt documentation [no ci] (#7745)

examples/server/README.md

commit a5cabd76491f07494c5b8267f921c73f5e2bbfb4
Author: woodx <124784234+woodx9@users.noreply.github.com>
Date:   Fri Jun 7 15:09:45 2024 +0800

    server : do not get prompt in infill mode (#7286)
    
    * avoid to get prompt in infill mode and embedding mode
    
    * remove embedding mode
    
    * refactor format
    
    ---------
    
    Co-authored-by: wudexiang <wudexiang@bytedance.com>

examples/server/server.cpp

commit d5c938cd7716b9a2ace49a43a469dfbffcff4d28
Author: pengxin99 <pengxin.yuan@intel.com>
Date:   Fri Jun 7 14:28:26 2024 +0800

    [SYCL] fix softmax r2r result wrong issue (#7811)

ggml-sycl.cpp

commit c9ee7118d5644dd3df70ea6878b36a9761616aab
Author: slaren <slarengh@gmail.com>
Date:   Fri Jun 7 08:01:29 2024 +0200

    check for nans in imatrix and quantize (#7807)
    
    * imatrix : detect nan/inf values
    
    * quantize : check imatrix for nan/inf values

examples/imatrix/imatrix.cpp
llama.cpp

commit ee459f40f65810a810151b24eba5b8bd174ceffe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 6 19:19:59 2024 +0300

    server : fix --threads-http arg (#7801)

common/common.cpp
common/common.h

commit f83351f9a62a6262f1fc3d08f320033089cddfb5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 6 16:30:58 2024 +0300

    imatrix : migrate to gpt_params (#7771)
    
    * imatrix : migrate to gpt_params
    
    ggml-ci
    
    * imatrix : add --save-frequency cli arg
    
    * common : fix --no-ppl

common/common.cpp
common/common.h
examples/imatrix/README.md
examples/imatrix/imatrix.cpp
examples/server/server.cpp

commit ad675e1c67a05b16e4e12abe30dbecfc808e7b7e
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Jun 6 06:08:52 2024 -0700

    Added support for . (any character) token in grammar engine. (#6467)
    
    * Added support for . (any characer) token in grammar engine.
    
    * Add integration tests for any-character symbol.

common/grammar-parser.cpp
llama.cpp
llama.h
tests/test-grammar-integration.cpp

commit a143c04375828b1f72eb1a326115791b63e79345
Author: Mattheus Chediak <shammcity00@gmail.com>
Date:   Thu Jun 6 09:17:54 2024 -0300

    README minor fixes (#7798) [no ci]
    
    derievatives --> derivatives

README.md

commit 55b2d0849d3ec9e45e4a4d9e480f5fa7977872a6
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Jun 6 10:07:06 2024 +0100

    grammars: x{min,max} repetition operator (#6640)
    
    * grammars: x{min,max} repetition operator + tweak +/*/? to avoid duplication of original over alternates
    
    * grammars: handle `x{n}` and fix `x{n,n}`
    
    * grammars: document new repetition operators
    
    * grammars: uniform use of int for min & max
    
    * grammars: refactor parser test
    
    * grammar: parsing tests w/ natural pretty print of updated expectations
    
    * grammars: much prettier print of expectations (+ TEST_GRAMMAR_PARSER_PRINT_ALL=1 to force all)
    
    * grammars: improve test pretty print again
    
    * grammars: pretty print rules and chars
    
    * grammars: fix copy rule skipping
    
    * grammars: disallow `a{,}` (not allowed in regexps)
    
    * Update common/grammar-parser.cpp
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * grammars: fix copy rule skipping (again) & display of expectations
    
    * grammars: more test cases
    
    * grammars: update reps parsing to bring ? / * / + closer to before
    
    * json: use new GBNF repetitions{m,n} syntax
    
    * grammars: update performance gotchas w/ repetition advice
    
    * Update examples/json_schema_to_grammar.py
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * Update examples/server/public/json-schema-to-grammar.mjs
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * grammars: comment on rule repetitions
    
    * grammars: ensure unambiguous number alternatives
    
    * grammar: nit typo switched error msgs
    
    * grammar: nit numbering in comment
    
    * json: update numeric rule to be unambiguous
    
    * Apply suggestions from code review
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * Update examples/server/public/json-schema-to-grammar.mjs
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>
    
    * json: fix integral-part
    
    * grammar: add repetition tests
    
    ---------
    
    Co-authored-by: Clint Herron <hanclinto@gmail.com>

common/grammar-parser.cpp
common/json-schema-to-grammar.cpp
examples/json_schema_to_grammar.py
examples/pydantic_models_to_grammar.py
examples/server/public/json-schema-to-grammar.mjs
grammars/README.md
tests/test-grammar-integration.cpp
tests/test-grammar-parser.cpp
tests/test-json-schema-to-grammar.cpp

commit f5d7b268ec4bf8628aa6ccc9f6631d0230dde76f
Author: Joan Fontanals <joan.fontanals.martinez@jina.ai>
Date:   Thu Jun 6 09:22:41 2024 +0200

    llama : add jina v2 base code (#7596)
    
    * feat: add changes to handle jina v2 base code
    
    * fix: do not complicate things
    
    * fix: fix the usage of the code model
    
    * fix: fix comments
    
    * fix: fix linting issues
    
    * fix: remove ollama patches
    
    * style : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 2d08b7fbb483c14bd2b173d4cd51ea3a4f862e8f
Author: slaren <slarengh@gmail.com>
Date:   Thu Jun 6 07:19:49 2024 +0200

    docker : build only main and server in their images (#7782)
    
    * add openmp lib to dockerfiles
    
    * build only main and server in their docker images

.devops/main-cuda.Dockerfile
.devops/main-rocm.Dockerfile
.devops/main.Dockerfile
.devops/server-cuda.Dockerfile
.devops/server.Dockerfile

commit d67caea0d6e6c303d31b01d0a010973e6c908dff
Author: slaren <slarengh@gmail.com>
Date:   Thu Jun 6 07:17:21 2024 +0200

    docker : add openmp lib (#7780)

.devops/full-cuda.Dockerfile
.devops/full.Dockerfile
.devops/main-cuda.Dockerfile
.devops/main-vulkan.Dockerfile
.devops/main.Dockerfile
.devops/server-cuda.Dockerfile
.devops/server.Dockerfile

commit 7672adeec7a79ea271058c63106c142ba84f951a
Author: Galunid <karolek1231456@gmail.com>
Date:   Wed Jun 5 19:07:24 2024 +0200

    Fix encoding in python scripts (#7733)

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py

commit 7d1a378b8fb266782d9248538a661405aad80768
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 5 16:53:00 2024 +0200

    CUDA: refactor mmq, dmmv, mmvq (#7716)
    
    * CUDA: refactor mmq, dmmv, mmvq
    
    * fix out-of-bounds write
    
    * struct for qk, qr, qi
    
    * fix cmake build
    
    * mmq_type_traits

CMakeLists.txt
Makefile
ggml-common.h
ggml-cuda.cu
ggml-cuda/common.cuh
ggml-cuda/dmmv.cu
ggml-cuda/mmq.cu
ggml-cuda/mmq.cuh
ggml-cuda/mmvq.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q8_0.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.cu
ggml-cuda/template-instances/generate_cu_files.py
ggml-cuda/template-instances/mmq-instance-q2_k.cu
ggml-cuda/template-instances/mmq-instance-q3_k.cu
ggml-cuda/template-instances/mmq-instance-q4_0.cu
ggml-cuda/template-instances/mmq-instance-q4_1.cu
ggml-cuda/template-instances/mmq-instance-q4_k.cu
ggml-cuda/template-instances/mmq-instance-q5_0.cu
ggml-cuda/template-instances/mmq-instance-q5_1.cu
ggml-cuda/template-instances/mmq-instance-q5_k.cu
ggml-cuda/template-instances/mmq-instance-q6_k.cu
ggml-cuda/template-instances/mmq-instance-q8_0.cu
ggml-cuda/vecdotq.cuh

commit 2b3389677a833cee0880226533a1768b1a9508d2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 5 11:29:20 2024 +0300

    ggml : refactor rope norm/neox (#7634)
    
    * ggml : unify rope norm/neox (CPU)
    
    * ggml : fix compile warning
    
    * ggml : remove GLM rope mode
    
    ggml-ci
    
    * metal : better rope implementation
    
    ggml-ci
    
    * cuda : better rope implementation
    
    ggml-ci
    
    * naming : n_orig_ctx -> n_ctx_orig
    
    ggml-ci
    
    * dev : add reminders to update backends
    
    ggml-ci
    
    * vulkan : fix ggml_rope_ext() usage
    
    * cuda : fix array size + indents
    
    ggml-ci

examples/baby-llama/baby-llama.cpp
examples/convert-legacy-llama.py
examples/finetune/finetune.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-cuda/rope.cu
ggml-kompute.cpp
ggml-metal.m
ggml-metal.metal
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
ggml.h
kompute-shaders/op_rope_f16.comp
kompute-shaders/op_rope_f32.comp
kompute-shaders/rope_common.comp
llama.cpp
tests/test-backend-ops.cpp
tests/test-grad0.cpp
tests/test-rope.cpp

commit 9973e81c5ccf4f31b3980f5aa73f5cfea8699860
Author: arch-btw <57669023+arch-btw@users.noreply.github.com>
Date:   Tue Jun 4 23:40:49 2024 -0700

    readme : remove -ins (#7759)
    
    -ins and --instruct were moved in https://github.com/ggerganov/llama.cpp/pull/7675
    
    I have adjusted the README accordingly.
    There was no trace of --chatml in the README.

examples/main/README.md

commit c90dbe026b456a233f8f0fbe752212e6a0503ca2
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Wed Jun 5 01:26:14 2024 +0200

    Fix per token atrributes bits (#7749)

llama.h

commit b90dc566c1c615289b05b50d61680f23744a21e7
Author: agray3 <agray3@users.noreply.github.com>
Date:   Tue Jun 4 21:06:49 2024 +0100

    Allow number of nodes in CUDA graph to change (#7738)
    
    Previously the code would have failed to cope in the case that the
    number of nodes changes in an existing CUDA graph. This fixes the
    issue by removing an unnecessary conditional.

ggml-cuda.cu

commit 1442677f92e45a475be7b4d056e3633d1d6f813b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 21:23:39 2024 +0300

    common : refactor cli arg parsing (#7675)
    
    * common : gpt_params_parse do not print usage
    
    * common : rework usage print (wip)
    
    * common : valign
    
    * common : rework print_usage
    
    * infill : remove cfg support
    
    * common : reorder args
    
    * server : deduplicate parameters
    
    ggml-ci
    
    * common : add missing header
    
    ggml-ci
    
    * common : remote --random-prompt usages
    
    ggml-ci
    
    * examples : migrate to gpt_params
    
    ggml-ci
    
    * batched-bench : migrate to gpt_params
    
    * retrieval : migrate to gpt_params
    
    * common : change defaults for escape and n_ctx
    
    * common : remove chatml and instruct params
    
    ggml-ci
    
    * common : passkey use gpt_params

common/common.cpp
common/common.h
examples/batched-bench/README.md
examples/batched-bench/batched-bench.cpp
examples/batched/README.md
examples/batched/batched.cpp
examples/embedding/embedding.cpp
examples/eval-callback/eval-callback.cpp
examples/gguf-split/tests.sh
examples/gritlm/gritlm.cpp
examples/imatrix/imatrix.cpp
examples/infill/infill.cpp
examples/llama-bench/llama-bench.cpp
examples/llava/llava-cli.cpp
examples/lookahead/lookahead.cpp
examples/lookup/lookup-create.cpp
examples/lookup/lookup-stats.cpp
examples/lookup/lookup.cpp
examples/main/README.md
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/passkey/README.md
examples/passkey/passkey.cpp
examples/perplexity/perplexity.cpp
examples/quantize/tests.sh
examples/retrieval/retrieval.cpp
examples/save-load-state/save-load-state.cpp
examples/server/server.cpp
examples/server/utils.hpp
examples/simple/README.md
examples/simple/simple.cpp
examples/speculative/speculative.cpp
llama.cpp
scripts/run-with-preset.py

commit 554c247caffed64465f372661f2826640cb10430
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 21:23:20 2024 +0300

    ggml : remove OpenCL (#7735)
    
    ggml-ci

.github/workflows/build.yml
CMakeLists.txt
Makefile
README-sycl.md
README.md
common/common.cpp
examples/llama-bench/README.md
examples/llama-bench/llama-bench.cpp
examples/main-cmake-pkg/README.md
flake.nix
ggml-metal.h
ggml-opencl.cpp
ggml-opencl.h
ggml.c
ggml.h
llama.cpp
scripts/LlamaConfig.cmake.in
scripts/compare-llama-bench.py
scripts/server-llm.sh
scripts/sync-ggml-am.sh
scripts/sync-ggml.sh

commit 0cd6bd3483fa66124b76a8a8ac794d9ee18c70c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 21:23:05 2024 +0300

    llama : remove beam search (#7736)

Makefile
examples/CMakeLists.txt
examples/beam-search/CMakeLists.txt
examples/beam-search/beam-search.cpp
llama.cpp
llama.h

commit 5ca0944a153b65724d51b2f484139aa25ccb7a8b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 19:43:01 2024 +0300

    readme : remove obsolete Zig instructions (#7471)

README.md

commit adc9ff384121f4d550d28638a646b336d051bf42
Author: slaren <slarengh@gmail.com>
Date:   Tue Jun 4 14:32:42 2024 +0200

    llama-bench : allow using a different printer for stderr with -oe (#7722)
    
    compare-commits.sh : hide stdout, use -oe to print markdown

examples/llama-bench/llama-bench.cpp
scripts/compare-commits.sh

commit 987d743d6bc4cee4bde6820733ea33a2abc0afac
Author: Daniele <57776841+daniandtheweb@users.noreply.github.com>
Date:   Tue Jun 4 12:09:15 2024 +0000

    Improve hipBLAS support in CMake (#7696)
    
    * Improve hipBLAS support in CMake
    
    This improves the detection of the correct CMAKE_PREFIX_PATH when using different distributions or a self-built ROCm SDK.
    
    * Set ROCM_PATH correctly

CMakeLists.txt

commit b226c1227bcf6412076ecf787421135fd2c42ef0
Author: zhouwg <zhouwg2000@gmail.com>
Date:   Tue Jun 4 19:21:26 2024 +0800

    refine .gitignore (#7688)
    
    This adds tags and android ndk into the git ignore list

.gitignore

commit 3b38d48609280aa5f8ab7ea135a4351b2a5ee240
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Tue Jun 4 09:17:17 2024 +0200

    Per token attributes (#7685)
    
    * Add per token attributes enum
    * Using phi-3 for testing 'rstrip'
    * Using jina-v2 for testing 'lstrip'
    * Brute force test for 'lstrip' and 'rstrip'
    * Implement 'rstrip' and 'lstrip'
    * Update phi-3 GGUF file (obsolete since 917dc8c)
    * Replace llama_token_type with llama_token_attribs

llama.cpp
llama.h
models/ggml-vocab-phi-3.gguf
tests/test-tokenizer-random.py

commit 6d1616944d9efd342ed2a4fd318722adfc9febcd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 4 10:01:09 2024 +0300

    ggml : prevent builds with -ffinite-math-only (#7726)
    
    This enforces a check that -fno-finite-math-only was set and that the operating
    compiling mode is not in finite maths mode. This is because during rewriting of
    silu and softmax for cpu #7154 there emerged an issue where the result that was
    observed when >1 slot was nondeterministic as found by @JohannesGaessler.
    
    @LostRuins narrowed the problem down to -ffinite-math-only which was theorised
    to be due to SiLU, instead of flushing small values to 0, returns NaN or some
    other garbage. @jart proposed a fix that @ggerganov then implemented in this fix
    
    ref https://github.com/ggerganov/llama.cpp/pull/7154#issuecomment-2145661825

cmake/arm64-windows-llvm.cmake
ggml.c

commit bde7cd3cd949c1a85d3a199498ac98e78039d46f
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon Jun 3 20:03:26 2024 +0300

    llama : offload to RPC in addition to other backends (#7640)
    
    * llama : offload to RPC in addition to other backends
    
    * - fix copy_tensor being called on the src buffer instead of the dst buffer
    
    - always initialize views in the view_src buffer
    
    - add RPC backend to Makefile build
    
    - add endpoint to all RPC object names
    
    * add rpc-server to Makefile
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

Makefile
ggml-alloc.c
ggml-backend.c
ggml-backend.h
ggml-rpc.cpp
llama.cpp

commit a5735e4426b19a3ebd0c653ad8ac01420458ee95
Author: Masaya, Kato <62578291+msy-kato@users.noreply.github.com>
Date:   Tue Jun 4 00:14:15 2024 +0900

    ggml : use OpenMP as a thread pool (#7606)
    
    * ggml: Added OpenMP for multi-threads processing
    
    * ggml : Limit the number of threads used to avoid deadlock
    
    * update shared state n_threads in parallel region
    
    * clear numa affinity for main thread even with openmp
    
    * enable openmp by default
    
    * fix msvc build
    
    * disable openmp on macos
    
    * ci : disable openmp with thread sanitizer
    
    * Update ggml.c
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
CMakeLists.txt
Makefile
ggml.c

commit 0b832d53ba0ffcc759c8d62ede3772dd62321f8e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 3 16:28:58 2024 +0200

    make: fix debug options not being applied to NVCC (#7714)

Makefile

commit 3d7ebf63123b8652fb7bbecef7ba731202309901
Author: 0cc4m <picard12@live.de>
Date:   Mon Jun 3 10:59:14 2024 +0200

    Vulkan Mixture of Experts (MoE) support (#7628)
    
    * Finish Vulkan mul_mat_id implementation
    
    * Add Vulkan sum_rows and div ops
    
    * Fix MUL_MAT_ID matrix matrix shader
    
    * Fix MUL_MAT_ID matrix vector shader dispatch size
    
    * Fix MUL_MAT_ID matrix vector shader and dispatch code
    
    * Update Vulkan CPU offload for MUL_MAT_ID
    
    * Fix crash when using split mode none and setting a main GPU

common/common.cpp
ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py
llama.cpp

commit a10cda58d3199cd85305e0f03a8c6056714ae2e8
Author: Andy Tai <andy-tai@users.noreply.github.com>
Date:   Mon Jun 3 01:06:24 2024 -0700

    cmake : add pkg-config spec file for llama.cpp (#7702)

CMakeLists.txt
cmake/llama.pc.in

commit 6f28a333c1e3fdfdc7b4f9d0367f2b41a9b7e9d4
Author: zhangkaihuo <zhangkaihuo@gmail.com>
Date:   Mon Jun 3 15:49:30 2024 +0800

    llama : MiniCPM support tied embeddings (#7664)
    
    * support lm_head
    
    * remove the code block
    
    ---------
    
    Co-authored-by: zhangkaihuo <zhangkaihuo@modelbest.cn>

gguf-py/gguf/constants.py
llama.cpp

commit 549279d8049d78620a2b081e26edb654f83c3bbd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 3 08:34:43 2024 +0300

    llama : avoid double token-to-piece cache (#7654)
    
    ggml-ci

llama.cpp

commit 9e405b6e2ecb888e860f7b92720b4809e21b3915
Author: woachk <24752637+woachk@users.noreply.github.com>
Date:   Mon Jun 3 07:32:16 2024 +0200

    kompute : implement op_getrows_f32 (#6403)
    
    op_getrows_f32 is required since https://github.com/ggerganov/llama.cpp/pull/6122
    for the Vulkan w/ Kompute backend to be functional.
    
    As such, implement this op to make this backend functional again.

CMakeLists.txt
ggml-kompute.cpp
kompute-shaders/op_getrows_f32.comp

commit 3413ae2193d0693f14bead02e5018f442cbf579b
Author: Dave Airlie <airlied@redhat.com>
Date:   Mon Jun 3 07:59:54 2024 +1000

    fix bug introduced in using calloc (#7701)
    
    compilade pointed this out on the previous MR

ggml-alloc.c

commit 1669810d7c2446af8425aa54ff6611bf6f14646c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 3 00:13:12 2024 +0300

    flake.lock: Update (#7686)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/8dc45382d5206bd292f9c2768b8058a8fd8311d9?narHash=sha256-/GJvTdTpuDjNn84j82cU6bXztE0MSkdnTWClUCRub78%3D' (2024-05-16)
      → 'github:hercules-ci/flake-parts/2a55567fcf15b1b1c7ed712a2c6fadaec7412ea8?narHash=sha256-iKzJcpdXih14qYVcZ9QC9XuZYnPc6T8YImb6dX166kw%3D' (2024-06-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'https://github.com/NixOS/nixpkgs/archive/50eb7ecf4cd0a5756d7275c8ba36790e5bd53e33.tar.gz?narHash=sha256-QBx10%2Bk6JWz6u7VsohfSw8g8hjdBZEf8CFzXH1/1Z94%3D' (2024-05-02)
      → 'https://github.com/NixOS/nixpkgs/archive/eb9ceca17df2ea50a250b6b27f7bf6ab0186f198.tar.gz?narHash=sha256-lIbdfCsf8LMFloheeE6N31%2BBMIeixqyQWbSr2vk79EQ%3D' (2024-06-01)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/bfb7a882678e518398ce9a31a881538679f6f092?narHash=sha256-4zSIhSRRIoEBwjbPm3YiGtbd8HDWzFxJjw5DYSDy1n8%3D' (2024-05-24)
      → 'github:NixOS/nixpkgs/ad57eef4ef0659193044870c731987a6df5cf56b?narHash=sha256-SzDKxseEcHR5KzPXLwsemyTR/kaM9whxeiJohbL04rs%3D' (2024-05-29)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit 7c4e5b7eae26581869e782015d9deca947c34997
Author: Austin <77757836+teleprint-me@users.noreply.github.com>
Date:   Sun Jun 2 13:39:08 2024 -0400

    chore : add ignore rule for generated server themes (#7689)

.gitignore

commit 9422c5e34bbd302493b77a8f6d546154a1f4fe82
Author: nickp27 <nb.porter@gmail.com>
Date:   Sun Jun 2 19:13:54 2024 +1000

    [SYCL] Update rpc-server.cpp to include SYCL backend (#7682)
    
    * Update rpc-server.cpp to include SYCL backend
    
    Draft PR to address inclusion of SYCL backend for RPC server
    
    * Update rpc-server.cpp

examples/rpc/rpc-server.cpp

commit e141ce624af57bdffbaf57014a044eb1d9689230
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jun 1 23:26:10 2024 +0200

    Fix FlashAttention debug test, FP32 assert (#7684)

ggml-cuda/fattn-vec-f32.cuh
tests/test-backend-ops.cpp

commit 2e666832e6ac78194edf030bd1c295e21bdb022c
Author: Yazan Agha-Schrader <mountaiin@icloud.com>
Date:   Sat Jun 1 21:31:48 2024 +0200

    server : new UI (#7633)
    
    * ic
    
    * migrate my eary work
    
    * add the belonging stuff: css,favicon etc
    
    * de prompts
    
    * chore: Update HTML meta tags in index.html file
    
    * add api-key css classes
    
    * some necessary fixes
    
    * Add API key CSS classes and update styling in style.css
    
    * clean the code
    
    * move API to the top, rearrange param sliders. update css
    
    * add tooltips to the parameters with comprehensible explanations
    
    * fix FloatField and BoolField tooltips
    
    * fix grammar field width
    
    * use template literales for promptFormats.js
    
    * update const ModelGenerationInfo
    
    * remove ms per token, since not relevant for most webui users and use cases
    
    * add phi-3 prompt template
    
    * add phi3 to dropdown
    
    * add css class
    
    * update forgotten css theme
    
    * add user message suffix
    
    * fix chatml & add llama3 format
    
    * fix llama3 prompt template
    
    * more prompt format fixes
    
    * add more comon stop tokens
    
    * add missing char
    
    * do not separate with new line or comma
    
    * move prompt style
    
    * add hacky llama2 prompt solution, reduce redundancy in promptFormats.js
    
    * fix toggle state localstorage
    
    * add cmd-r prompt et reduce redundancy
    
    * set default prompt to empty
    
    * move files, clean code
    
    * fix css path
    
    * add a button to the new ui
    
    * move new ui to "/public" due to otherwise problematic CORS behaviour
    
    * include new ui in cpp
    
    * fix wrong link to old ui
    
    * renaming to ensure consistency
    
    * fix typos "prompt-format" -> "prompt-formats"
    
    * use correct indent
    
    * add new ui files to makefile
    
    * fix typo

Makefile
examples/server/CMakeLists.txt
examples/server/public/colorthemes.css
examples/server/public/index-new.html
examples/server/public/index.html
examples/server/public/prompt-formats.js
examples/server/public/style.css
examples/server/public/system-prompts.js
examples/server/public/theme-beeninorder.css
examples/server/public/theme-ketivah.css
examples/server/public/theme-mangotango.css
examples/server/public/theme-playground.css
examples/server/public/theme-polarnight.css
examples/server/public/theme-snowstorm.css
examples/server/server.cpp

commit 2ac95c9d5678d05e253691fb1f26471675bff5ad
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Sat Jun 1 21:50:18 2024 +0530

    SimpleChat: Simple histogram/repeatMatching driven garbageTrimming, Settings UI, Streaming mode, OpenAi Compat (Model, Authorization Bearer), Save/Restore session, Auto Settings UI (#7548)
    
    * SimpleChat:DU:BringIn local helper js modules using importmap
    
    Use it to bring in a simple trim garbage at end logic, which is
    used to trim received response.
    
    Also given that importmap assumes esm / standard js modules, so
    also global variables arent implicitly available outside the
    modules. So add it has a member of document for now
    
    * SimpleChat:DU: Add trim garbage at end in loop helper
    
    * SimpleChat:DU:TrimGarbage if unable try skip char and retry
    
    * SimpleChat:DU: Try trim using histogram based info
    
    TODO: May have to add max number of uniq chars in histogram at
    end of learning phase.
    
    * SimpleChat:DU: Switch trim garbage hist based to maxUniq simple
    
    Instead of blindly building histogram for specified substring
    length, and then checking if any new char within specified min
    garbage length limit, NOW exit learn state when specified maxUniq
    chars are found. Inturn there should be no new chars with in
    the specified min garbage length required limit.
    
    TODO: Need to track char classes like alphabets, numerals and
    special/other chars.
    
    * SimpleChat:DU: Bring in maxType to the mix along with maxUniq
    
    Allow for more uniq chars, but then ensure that a given type of
    char ie numerals or alphabets or other types dont cross the
    specified maxType limit. This allows intermixed text garbage
    to be identified and trimmed.
    
    * SimpleChat:DU: Cleanup debug log messages
    
    * SimpleChat:UI: Move html ui base helpers into its own module
    
    * SimpleChat:DU:Avoid setting frequence/Presence penalty
    
    Some models like llama3 found to try to be over intelligent by
    repeating garbage still, but by tweaking the garbage a bit so that
    it is not exactly same. So avoid setting these penalties and let
    the model's default behaviour work out, as is.
    
    Also the simple minded histogram based garbage trimming from end,
    works to an extent, when the garbage is more predictable and
    repeatative.
    
    * SimpleChat:UI: Add and use a para-create-append helper
    
    Also update the config params dump to indicate that now one needs
    to use document to get hold of gMe global object, this is bcas of
    moving to module type js.
    
    Also add ui.mjs to importmap
    
    * SimpleChat:UI: Helper to create bool button and use it wrt settings
    
    * SimpleChat:UI: Add Select helper and use it wrt ChatHistoryInCtxt
    
    * SimpleChat:UI:Select: dict-name-value, value wrt default, change
    
    Take a dict/object of name-value pairs instead of just names.
    Inturn specify the actual value wrt default, rather than the
    string representing that value.
    
    Trap the needed change event rather than click wrt select.
    
    * SimpleChat:UI: Add Div wrapped label+element helpers
    
    Move settings related elements to use the new div wrapped ones.
    
    * SimpleChat:UI:Add settings button and bring in settings ui
    
    * SimpleChat:UI:Settings make boolean button text show meaning
    
    * SimpleChat: Update a bit wrt readme and notes in du
    
    * SimpleChat: GarbageTrim enable/disable, show trimmed part ifany
    
    * SimpleChat: highlight trim, garbage trimming bitmore aggressive
    
    Make it easy for end user to identified the trimmed text.
    
    Make garbage trimming logic, consider a longer repeat garbage
    substring.
    
    * SimpleChat: Cleanup a bit wrt Api end point related flow
    
    Consolidate many of the Api end point related basic meta data into
    ApiEP class.
    
    Remove the hardcoded ApiEP/Mode settings from html+js, instead use
    the generic select helper logic, inturn in the settings block.
    
    Move helper to generate the appropriate request json string based
    on ApiEP into SimpleChat class itself.
    
    * SimpleChat:Move extracting assistant response to SimpleChat class
    
    so also the trimming of garbage.
    
    * SimpleChat:DU: Bring in both trim garbage logics to try trim
    
    * SimpleChat: Cleanup readme a bit, add one more chathistory length
    
    * SimpleChat:Stream:Initial handshake skeleton
    
    Parse the got stream responses and try extract the data from it.
    
    It allows for a part read to get a single data line or multiple
    data line. Inturn extract the json body and inturn the delta
    content/message in it.
    
    * SimpleChat: Move handling oneshot mode server response
    
    Move handling of the oneshot mode server response into SimpleChat.
    
    Also add plumbing for moving multipart server response into same.
    
    * SimpleChat: Move multi part server response handling in
    
    * SimpleChat: Add MultiPart Response handling, common trimming
    
    Add logic to call into multipart/stream server response handling.
    
    Move trimming of garbage at the end into the common handle_response
    helper.
    
    Add new global flag to control between oneshot and multipart/stream
    mode of fetching response. Allow same to be controlled by user.
    
    If in multipart/stream mode, send the stream flag to the server.
    
    * SimpleChat: show streamed generative text as it becomes available
    
    Now that the extracting of streamed generated text is implemented,
    add logic to show the same on the screen.
    
    * SimpleChat:DU: Add NewLines helper class
    
    To work with an array of new lines. Allow adding, appending,
    shifting, ...
    
    * SimpleChat:DU: Make NewLines shift more robust and flexible
    
    * SimpleChat:HandleResponseMultiPart using NewLines helper
    
    Make handle_response_multipart logic better and cleaner. Now it
    allows for working with the situation, where the delta data line
    got from server in stream mode, could be split up when recving,
    but still the logic will handle it appropriately.
    
    ALERT: Rather except (for now) for last data line wrt a request's
    response.
    
    * SimpleChat: Disable console debug by default by making it dummy
    
    Parallely save a reference to the original func.
    
    * SimpleChat:MultiPart/Stream flow cleanup
    
    Dont try utf8-decode and newlines-add_append if no data to work on.
    
    If there is no more data to get (ie done is set), then let NewLines
    instance return line without newline at end, So that we dont miss
    out on any last-data-line without newline kind of scenario.
    
    Pass stream flag wrt utf-8 decode, so that if any multi-byte char
    is only partly present in the passed buffer, it can be accounted
    for along with subsequent buffer. At sametime, bcas of utf-8's
    characteristics there shouldnt be any unaccounted bytes at end,
    for valid block of utf8 data split across chunks, so not bothering
    calling with stream set to false at end. LATER: Look at TextDecoder's
    implementation, for any over intelligence, it may be doing..
    If needed, one can use done flag to account wrt both cases.
    
    * SimpleChat: Move baseUrl to Me and inturn gMe
    
    This should allow easy updating of the base url at runtime by the
    end user.
    
    * SimpleChat:UI: Add input element helper
    
    * SimpleChat: Add support for changing the base url
    
    This ensures that if the user is running the server with a
    different port or wants to try connect to server on a different
    machine, then this can be used.
    
    * SimpleChat: Move request headers into Me and gMe
    
    Inturn allow Authorization to be sent, if not empty.
    
    * SimpleChat: Rather need to use append to insert headers
    
    * SimpleChat: Allow Authorization header to be set by end user
    
    * SimpleChat:UI+: Return div and element wrt creatediv helpers
    
    use it to set placeholder wrt Authorization header.
    
    Also fix copy-paste oversight.
    
    * SimpleChat: readme wrt authorization, maybe minimal openai testing
    
    * SimpleChat: model request field for openai/equivalent compat
    
    May help testing with openai/equivalent web services, if they
    require this field.
    
    * SimpleChat: readme stream-utf-8 trim-english deps, exception2error
    
    * Readme: Add a entry for simplechat in the http server section
    
    * SimpleChat:WIP:Collate internally, Stream mode Trap exceptions
    
    This can help ensure that data fetched till that point, can be
    made use of, rather than losing it.
    
    On some platforms, the time taken wrt generating a long response,
    may lead to the network connection being broken when it enters
    some user-no-interaction related power saving mode.
    
    * SimpleChat:theResp-origMsg: Undo a prev change to fix non trim
    
    When the response handling was moved into SimpleChat, I had changed
    a flow bit unnecessarily and carelessly, which resulted in the non
    trim flow, missing out on retaining the ai assistant response.
    
    This has been fixed now.
    
    * SimpleChat: Save message internally in handle_response itself
    
    This ensures that throwing the caught exception again for higher
    up logic, doesnt lose the response collated till that time.
    
    Go through theResp.assistant in catch block, just to keep simple
    consistency wrt backtracing just in case.
    
    Update the readme file.
    
    * SimpleChat:Cleanup: Add spacing wrt shown req-options
    
    * SimpleChat:UI: CreateDiv Divs map to GridX2 class
    
    This allows the settings ui to be cleaner structured.
    
    * SimpleChat: Show Non SettingsUI config field by default
    
    * SimpleChat: Allow for multiline system prompt
    
    Convert SystemPrompt into a textarea with 2 rows. Reduce
    user-input-textarea to 2 rows from 3, so that overall
    vertical space usage remains same.
    
    Shorten usage messages a bit, cleanup to sync with settings ui.
    
    * SimpleChat: Add basic skeleton for saving and loading chat
    
    Inturn when ever a chat message (system/user/model) is added,
    the chat will be saved into browser's localStorage.
    
    * SimpleChat:ODS: Add a prefix to chatid wrt ondiskstorage key
    
    * SimpleChat:ODS:WIP:TMP: Add UI to load previously saved chat
    
    This is a temporary flow
    
    * SimpleChat:ODS:Move restore/load saved chat btn setup to Me
    
    This also allows being able to set the common system prompt
    ui element to loaded chat's system prompt.
    
    * SimpleChat:Readme updated wrt save and restore chat session info
    
    * SimpleChat:Show chat session restore button, only if saved session
    
    * SimpleChat: AutoCreate ChatRequestOptions settings to an extent
    
    * SimpleChat: Update main README wrt usage with server

README.md
examples/server/public_simplechat/datautils.mjs
examples/server/public_simplechat/index.html
examples/server/public_simplechat/readme.md
examples/server/public_simplechat/simplechat.css
examples/server/public_simplechat/simplechat.js
examples/server/public_simplechat/ui.mjs

commit 750f60c03e4d3f53fa51910551ce87a3d508d2d7
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jun 1 15:47:04 2024 +0200

    CUDA: fix Pascal FA, deq. KV to FP16 for batch > 8 (#7681)

ggml-cuda/fattn-common.cuh
ggml-cuda/fattn-tile-f16.cu
ggml-cuda/fattn-tile-f32.cu
ggml-cuda/fattn-vec-f16.cuh
ggml-cuda/fattn-vec-f32.cuh
ggml-cuda/fattn-wmma-f16.cuh
ggml-cuda/fattn.cu

commit 9b596417af11c9ac44fcae0fcfbc6f3665089083
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jun 1 08:44:14 2024 +0200

    CUDA: quantized KV support for FA vec (#7527)
    
    * CUDA: quantized KV support for FA vec
    
    * try CI fix
    
    * fix commented-out kernel variants
    
    * add q8_0 q4_0 tests
    
    * fix nwarps > batch size
    
    * split fattn compile via extern templates
    
    * fix flake8
    
    * fix metal tests
    
    * fix cmake
    
    * make generate_cu_files.py executable
    
    * add autogenerated .cu files
    
    * fix AMD
    
    * error if type_v != FP16 and not flash_attn
    
    * remove obsolete code

CMakeLists.txt
Makefile
README.md
ggml-cuda.cu
ggml-cuda/fattn-common.cuh
ggml-cuda/fattn-tile-f16.cu
ggml-cuda/fattn-tile-f32.cu
ggml-cuda/fattn-vec-f16.cu
ggml-cuda/fattn-vec-f16.cuh
ggml-cuda/fattn-vec-f32.cu
ggml-cuda/fattn-vec-f32.cuh
ggml-cuda/fattn-wmma-f16.cuh
ggml-cuda/fattn.cu
ggml-cuda/mmq.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_0.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_1.cu
ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q8_0.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.cu
ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.cu
ggml-cuda/template-instances/generate_cu_files.py
ggml-cuda/vecdotq.cuh
ggml-metal.m
llama.cpp
tests/test-backend-ops.cpp

commit a323ec60af14a33d560df98f2cc41b4112cb4f80
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 31 22:23:04 2024 +0300

    server : update js (#7670)

examples/server/public/index.js

commit 0515ad93f48df63bbff204eddb0cac75e8585c65
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri May 31 17:42:33 2024 +0200

    convert-hf : Handle NotImplementedError in convert-hf-to-gguf (#7660)

convert-hf-to-gguf.py

commit c8047d538f3addab40e3112be60bb92e70ce1a50
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri May 31 16:26:21 2024 +0200

    scripts: update compare_llama_bench.py [no ci] (#7673)

scripts/compare-llama-bench.py

commit 30e238b246f8002cc6eb7cb79afe242243f1f66d
Author: Daniele <57776841+daniandtheweb@users.noreply.github.com>
Date:   Fri May 31 14:00:29 2024 +0000

    Improve HIP compatibility (#7672)

Makefile

commit 16926dff92d6d0efa8cbc0f44d30d63349532b38
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 31 15:04:58 2024 +0300

    readme : link homebrew discussion

README.md

commit 0c27e6f62eea80140daf152d7b6c154466614e5c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 31 14:17:10 2024 +0300

    ggml : fix loongson compile warnings (#7537)
    
    * ggml : fix loongson compile warnings
    
    ggml-ci
    
    * Fix loongarch quantize test fail.
    
    Fix unexpected error introduced during rebase code.
    
    * tests : disable json test due to lack of python on the CI node
    
    ggml-ci
    
    ---------
    
    Co-authored-by: junchao-loongson <zhaojunchao@loongson.cn>

ggml-quants.c
ggml.c
tests/CMakeLists.txt

commit 2e32f874e675f7bc5307cb7b4470ddbe090bab8f
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri May 31 10:24:41 2024 +0200

    Somehow '**' got lost (#7663)

README.md

commit 1af511fc22cba4959dd8bced5501df9e8af6ddf9
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri May 31 10:09:20 2024 +0200

    Add convert.py removal to hot topics (#7662)

README.md

commit 0541f06296753dbc59a57379eb54cec865a4c9f9
Author: Sertaç Özercan <852750+sozercan@users.noreply.github.com>
Date:   Thu May 30 16:57:16 2024 -0700

    [no ci] docs: add aikit to readme (#7650)
    
    Signed-off-by: Sertac Ozercan <sozercan@gmail.com>

README.md

commit 9022c33646fbf78da35f40c3f98576cc08c40ddf
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Thu May 30 21:32:38 2024 +0100

    Fixed painfully slow single process builds. (#7326)
    
    * Fixed painfully slow single process builds.
    
    * Added nproc for systems that don't default to nproc

.devops/full-cuda.Dockerfile
.devops/full-rocm.Dockerfile
.devops/full.Dockerfile
.devops/main-cuda.Dockerfile
.devops/main-rocm.Dockerfile
.devops/main.Dockerfile
.devops/server-cuda.Dockerfile
.devops/server-rocm.Dockerfile
.devops/server.Dockerfile

commit 5921b8f089d3b7bda86aac5a66825df6a6c10603
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 30 19:01:41 2024 +0300

    llama : cache llama_token_to_piece (#7587)
    
    * llama : cache llama_token_to_piece
    
    ggml-ci
    
    * llama : use vectors and avoid has_cache
    
    ggml-ci
    
    * llama : throw on unknown tokenizer types
    
    ggml-ci
    
    * llama : print a log of the total cache size

llama.cpp
llama.h

commit 5dcdf946764fae49a8e2a90bf2f0960bde1c44e8
Author: Martin Delille <martin@delille.org>
Date:   Thu May 30 17:07:39 2024 +0200

    Fix conan badge display [no ci] (#7645)

README.md

commit 2e2340de1740b07f2418c04792607387d4dc1442
Author: Manuel <44313466+makuche@users.noreply.github.com>
Date:   Thu May 30 16:58:15 2024 +0200

    Add brew installation instruction to README [no ci] (#7616)

README.md

commit 7846540bd291e52cd4eee53882315760e05239be
Author: Martin Delille <martin@delille.org>
Date:   Thu May 30 14:52:50 2024 +0200

    readme : add Conan badge (#7638)

README.md

commit e6157f94c8f835f7f774b98409078867472a34fe
Author: Brian <mofosyne@gmail.com>
Date:   Thu May 30 21:55:36 2024 +1000

    github: add contact links to issues and convert question into research [no ci] (#7612)

.github/ISSUE_TEMPLATE/06-question.yml
.github/ISSUE_TEMPLATE/06-research.yml
.github/ISSUE_TEMPLATE/config.yml

commit 9c4c9cc83f7297a10bb3b2af54a22ac154fd5b20
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu May 30 13:40:00 2024 +0200

    Move convert.py to examples/convert-legacy-llama.py (#7430)
    
    * Move convert.py to examples/convert-no-torch.py
    
    * Fix CI, scripts, readme files
    
    * convert-no-torch -> convert-legacy-llama
    
    * Move vocab thing to vocab.py
    
    * Fix convert-no-torch -> convert-legacy-llama
    
    * Fix lost convert.py in ci/run.sh
    
    * Fix imports
    
    * Fix gguf not imported correctly
    
    * Fix flake8 complaints
    
    * Fix check-requirements.sh
    
    * Get rid of ADDED_TOKENS_FILE, FAST_TOKENIZER_FILE
    
    * Review fixes

.devops/tools.sh
CMakeLists.txt
README.md
ci/run.sh
convert-hf-to-gguf.py
docs/HOWTO-add-model.md
examples/convert-legacy-llama.py
examples/llava/MobileVLM-README.md
examples/llava/README.md
examples/llava/requirements.txt
examples/make-ggml.py
gguf-py/gguf/vocab.py
requirements.txt
requirements/requirements-convert-hf-to-gguf-update.txt
requirements/requirements-convert-hf-to-gguf.txt
requirements/requirements-convert-legacy-llama.txt
requirements/requirements-convert-llama-ggml-to-gguf.txt
scripts/check-requirements.sh
scripts/convert-gg.sh
scripts/pod-llama.sh

commit 59b0d077662fab430446b3119fa142f3291c45b2
Author: Chris Elrod <elrodc@gmail.com>
Date:   Thu May 30 07:32:55 2024 -0400

    faster avx512 exp implementation (#7551)
    
    * faster avx512 exp implementation
    
    * x->r
    
    * improve accuracy, handle special cases
    
    * remove `e`

ggml.c

commit d5c05821f3c3d6cabe8ac45776fe0ecb0da13eca
Author: junchao-loongson <68935141+junchao-loongson@users.noreply.github.com>
Date:   Thu May 30 17:30:10 2024 +0800

    ggml : fix loongarch build (O2 issue) (#7636)

ggml-quants.c
ggml.c

commit 972b555ab935705f3437abd5909a5c46852811f6
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu May 30 09:52:39 2024 +0200

    README: explain parallel build [no ci] (#7618)

README.md

commit 3854c9d07f67de7f8cd6d86117bfaef47549b05a
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Thu May 30 14:19:08 2024 +0800

    [SYCL] fix intel docker (#7630)
    
    * Update main-intel.Dockerfile
    
    * workaround for https://github.com/intel/oneapi-containers/issues/70
    
    * reset intel docker in CI
    
    * add missed in server

.devops/main-intel.Dockerfile
.devops/server-intel.Dockerfile
.github/workflows/docker.yml

commit eb57fee51f7b4d78039f003249873c2eb46f12f6
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu May 30 02:10:40 2024 +0200

    gguf-py : Add tokenizer.ggml.pre to gguf-new-metadata.py (#7627)

gguf-py/scripts/gguf-new-metadata.py

commit 55d62262a99cd8bc28a1492975791fe433c8cc0f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 22:20:40 2024 +0300

    metal : remove invalid asserts (#7617)

ggml-kompute.cpp
ggml-metal.m

commit 975ec63ff26cdf96156d1126d86f75a395fdc43a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 20:45:25 2024 +0300

    metal : add missing asserts (#7617)

ggml-kompute.cpp
ggml-metal.m

commit fb76ec31a9914b7761c1727303ab30380fd4f05c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 20:17:31 2024 +0300

    ggml : fix YARN + add tests + add asserts (#7617)
    
    * tests : add rope tests
    
    ggml-ci
    
    * ggml : fixes (hopefully)
    
    ggml-ci
    
    * tests : add non-cont tests
    
    ggml-ci
    
    * cuda : add asserts for rope/norm + fix DS2
    
    ggml-ci
    
    * ggml : assert contiguousness
    
    * tests : reduce RoPE tests
    
    ggml-ci

ggml-cuda.cu
ggml-cuda/norm.cu
ggml-cuda/rope.cu
ggml-kompute.cpp
ggml-metal.m
ggml-metal.metal
ggml-sycl.cpp
ggml.c
ggml.h
ggml_vk_generate_shaders.py
llama.cpp
tests/test-backend-ops.cpp

commit cce3dcffc5695bd24835f04e6080070a2a119873
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 15:38:26 2024 +0300

    cuda : non-cont concat support (#7610)
    
    * tests : add non-cont concat tests
    
    * cuda : non-cont concat support
    
    ggml-ci

ggml-cuda/concat.cu
tests/test-backend-ops.cpp

commit 210d99173dc82aafb48f6e39d787c387951fe3a9
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Wed May 29 14:45:44 2024 +0300

    llama-bench : add support for the RPC backend (#7435)

examples/llama-bench/llama-bench.cpp
ggml.c
ggml.h

commit 87bdf2a199acd62e19814d7a4d0500a04a7f09f3
Author: slaren <slarengh@gmail.com>
Date:   Wed May 29 13:36:39 2024 +0200

    ggml : use atomic_flag for critical section (#7598)
    
    * ggml : use atomic_flag for critical section
    
    * add windows shims

ggml.c

commit 00281b7be32462754618c42ed93f95743af46627
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 14:31:18 2024 +0300

    scripts : remove mpi remnants

scripts/sync-ggml-am.sh
scripts/sync-ggml.sh

commit 2ab977282b02ccd6783fbbaec393c96886cf33b1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 29 14:29:52 2024 +0300

    sync : ggml

scripts/sync-ggml.last

commit 72de268bec49f67e2883880f573c55cea32de736
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 26 18:35:23 2024 +0300

    ggml : restore ggml_rope_xpos_inplace (ggml/0)
    
    ggml-ci

ggml.c
ggml.h

commit 0e8d8bfd6caf1d0a8cbdf9d3d5c06fbbb9dfced8
Author: Akarshan Biswas <akarshanbiswas@fedoraproject.org>
Date:   Wed May 29 12:23:47 2024 +0530

    Add Arc A750 and Arch linux to readme-sycl.md as verified GPU model and Linux distro (#7605)

README-sycl.md

commit 504f0c340f6b5e04de682f6ddefdd3b81208df5d
Author: zhouwg <zhouwg2000@gmail.com>
Date:   Wed May 29 10:09:31 2024 +0800

    ggml : fix typo in ggml.c (#7603)

ggml.c

commit b864b50ce5e2beefc8c2fd31733e4e1a978b7754
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed May 29 07:00:24 2024 +0800

    [SYCL] Align GEMM dispatch (#7566)
    
    * align GEMM dispatch

CMakeLists.txt
README.md
ggml-sycl.cpp

commit 02c1ecad07f0e2d2febe8196271bcc64bdc9c006
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Tue May 28 21:46:34 2024 +0200

    Tokenizer WPM fixes (#7500)
    
    * Update random test: add_bos_token.
    * Update random test: add WPM models for testing.
    * Build vocab.special_tokens_cache using vocab token types.
    * Fix and improve WPM preprocessing.
      - Fix unicode edge case combinations.
      - Split by whitspace in the same pass.
    * Discard all tokens when no matching found.

llama.cpp
tests/test-tokenizer-random.py

commit 6bd12ce409f949012935b7d1b15a21ffa473a565
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 28 22:22:50 2024 +0300

    sycl : fix assert (#7563)

ggml-sycl.cpp

commit 5442939fcc5e6ae41abf40612a95fd71377e487e
Author: Giuseppe Scrivano <giuseppe@scrivano.org>
Date:   Tue May 28 20:49:49 2024 +0200

    llama : support small Granite models (#7481)
    
    * Add optional MLP bias for Granite models
    
    Add optional MLP bias for ARCH_LLAMA to support Granite models.
    Partially addresses ggerganov/llama.cpp/issues/7116
    Still needs some more changes to properly support Granite.
    
    * llama: honor add_space_prefix from the model configuration
    
    propagate the add_space_prefix configuration from the HF model
    configuration to the gguf file and honor it with the gpt2 tokenizer.
    
    Signed-off-by: Giuseppe Scrivano <gscrivan@redhat.com>
    
    * llama: add support for small granite models
    
    it works only for the small models 3b and 8b.
    
    The convert-hf-to-gguf.py script uses the vocabulary size of the
    granite models to detect granite and set the correct configuration.
    
    Signed-off-by: Giuseppe Scrivano <gscrivan@redhat.com>
    
    ---------
    
    Signed-off-by: Giuseppe Scrivano <gscrivan@redhat.com>
    Co-authored-by: Steffen Roecker <sroecker@redhat.com>

convert-hf-to-gguf.py
llama.cpp

commit 56411a950f255b523a9edd684fd1632752474399
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Wed May 29 01:25:08 2024 +0800

    vulkan: properly initialize vulkan devices for LLAMA_SPLIT_MODE_NONE (#7552)

ggml-vulkan.cpp

commit 2b737caae100cf0ac963206984332e422058f2b9
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Tue May 28 18:13:36 2024 +0300

    rpc : resource management rework (#7562)
    
    * rpc : resource management rework
    
    * address review comments

ggml-rpc.cpp

commit ee3dff6b8e39bb8c1cdea1782a7b95ef0118f970
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Tue May 28 17:07:05 2024 +0200

    Add support for DeepseekV2ForCausalLM (#7519)
    
    * common : increase max number of experts to 160
    
    * common : add tensors ATTN_Q_A, ATTN_Q_A_NORM, ATTN_Q_B, ATTN_KV_A_MQA, ATTN_KV_A_NORM, ATTN_KV_B needed by DeepSeek-V2 MLA (multi-head latent attention) architecture
    
    * common : add model header parameters: leading_dense_block_count, expert_feed_forward_length, expert_shared_count, expert_weights_scale, attention.q_lora_rank, attention.kv_lora_rank, rope.scaling.yarn_log_multiplier
    
    * convert-hf : add model conversion support for DeepseekV2ForCausalLM
    
    * llama : add model types for DeepSeek-V2 and DeepSeek-V2-Lite models
    
    * llama : add two new llm_build_moe_ffn() arguments: scale_w (whether to scale weights of selected MoE experts) and w_scale (numerical value of the scaling factor)
    
    * llama : add inference support for LLM_ARCH_DEEPSEEK2
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit edc29433fa08b4e5aeb67649a29fc7713af13d04
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 28 15:04:09 2024 +0300

    tests : fix test-tokenizer-0.sh

tests/test-tokenizer-0.sh

commit 8b99e2aa66ba39e4e1114effea6ef7430881eca4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 28 13:55:35 2024 +0300

    llama : handle unknown utf8 bytes (#7588)

llama.cpp

commit 271ff3fc44a6ecfcea3ebc192e67567d578b7772
Author: Brian <mofosyne@gmail.com>
Date:   Tue May 28 20:27:27 2024 +1000

    github: add refactor to issue template (#7561)
    
    * github: add refactor issue template [no ci]
    
    * Update 07-refactor.yml

.github/ISSUE_TEMPLATE/05-enhancement.yml
.github/ISSUE_TEMPLATE/06-question.yml
.github/ISSUE_TEMPLATE/07-refactor.yml

commit e2b065071c5fc8ac5697d12ca343551faee465cc
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Tue May 28 17:53:37 2024 +0800

    [SYCL]fix ggml_sycl_mul_mat_id() to match the change of api (#7436)
    
    * fix mul_mat_id to match the change of api
    
    * rm comment
    
    * rm unused or duplicated code, rename as review comment

ggml-sycl.cpp

commit 0548a4187f2e53b8fc6d9ff0f4c71988f708ff42
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 28 11:04:19 2024 +0300

    ggml : generalize GGML_OP_CONCAT (#7563)
    
    * ggml : generalize GGML_OP_CONCAT (WIP)
    
    ggml-ci
    
    * tests : add dim != 2 tests
    
    * metal : generalize concat kernel
    
    * tests : naming
    
    * cuda : generalize concat kernel
    
    ggml-ci
    
    * sycl : add warning and assert
    
    * ggml : fix op params handling
    
    * metal : bugfix kernel
    
    ggml-ci
    
    * ggml : reimplement CPU and Metal
    
    * cuda : add asserts
    
    ggml-ci
    
    * ggml : fix ptrs
    
    ggml-ci

ggml-cuda/concat.cu
ggml-metal.m
ggml-metal.metal
ggml-sycl.cpp
ggml.c
ggml.h
tests/test-backend-ops.cpp

commit 9335b969e86a222e247adacedf814d8abfff8847
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Tue May 28 06:55:51 2024 +0200

    server: do not remove whitespace at the start of a completion chunk (#7524)

examples/server/public/index.html

commit c41767154eb82aa3fe7568fc816c3402b78eae94
Author: Nathan Epstein <nate2@umbc.edu>
Date:   Tue May 28 00:41:14 2024 -0400

    Markdownish code block fix (#7571)
    
    * markdownish codeblock fix
    
    * updating regexes

examples/server/public/index.html

commit 74b239b3d5f067470d7ef5e26e2e059720572e32
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Tue May 28 11:48:16 2024 +0900

    llava : update clip.h (#7580)
    
    overriden -> overridden

examples/llava/clip.h

commit 852aafb163d32d5bad63c10bc323a02c28fec59d
Author: Djip007 <djip.perois@free.fr>
Date:   Tue May 28 01:40:47 2024 +0200

    update HIP_UMA #7399 (#7414)
    
    * update HIP_UMA #7399
    
    add use of hipMemAdviseSetCoarseGrain when LLAMA_HIP_UMA is enable.
    - get x2 on prompte eval and x1.5 on token gen with rocm6.0 on ryzen 7940HX iGPU (780M/gfx1103)
    
    * simplify code, more consistent style
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu
ggml-cuda/common.cuh

commit 0136966dafb452601c23f30395878d5a65ddc559
Author: kunnis <kunnis@users.noreply.github.com>
Date:   Mon May 27 18:40:12 2024 -0500

    adding in x64 targets to cmake presets (#7574)

CMakePresets.json

commit 10b1e4587670feba2c7730a645accf8234873113
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon May 27 19:34:40 2024 +0200

    make: add --device-debug to NVCC debug flags (#7542)

Makefile

commit 197c00681b80f9dea17d11a4436b6b8ef1be0ce8
Author: agray3 <agray3@users.noreply.github.com>
Date:   Mon May 27 18:33:42 2024 +0100

    Allow multiple copy function pointers for CUDA graph kernel param updates (#7565)
    
    CUDA graphs require parameter updates to kernels associated with
    GGML_OP_CPY nodes. Previously the implementation only checked for a
    single CUDA kernel in such nodes, but this caused a bug in cases where
    2 such kernels exist. This fixes the issue by using a vector to allow
    multiple function pointers to be stored and checked against.
    
    Fixes #7942

ggml-cuda.cu

commit 95f84d5ce8b449a9b16009434aca800df504a02e
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon May 27 17:34:51 2024 +0100

    Fix q_xxs using mul_mat_q (#7459)

ggml-sycl.cpp

commit 5487593bc7ee0b65b9d2e2985b4b61dc77043101
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon May 27 13:34:09 2024 +0100

    Add freq factors (#7495)

ggml-sycl.cpp

commit 1d8fca72ae9154eec0e1c0a75cfaac3c50f08e4a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 27 12:10:19 2024 +0300

    metal : add GGML_OP_REPEAT kernels (#7557)
    
    ggml-ci

ggml-metal.m
ggml-metal.metal

commit 62bfef5194d5582486d62da3db59bf44981b7912
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 27 10:38:39 2024 +0300

    metal : disable FA kernel for HS=256 (#7556)
    
    ggml-ci

ggml-metal.m
ggml-metal.metal

commit eaf6e031741ca2d3aafeff3e0f4dd7557a974d2b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 27 09:24:13 2024 +0300

    llama : add comments about experimental flags (#7544)

llama.h

commit d6ef0e77dd25f54fb5856af47e3926cf6f36c281
Author: Brian <mofosyne@gmail.com>
Date:   Mon May 27 10:54:30 2024 +1000

    github: add self sorted issue ticket forms (#7543)
    
    * github: add self sorted issue ticket forms [no ci]
    
    * github: consolidate BSD in bug issue ticket
    
    * github: remove contact from bug ticket template [no ci]
    
    * github: remove bios from os dropdown in bug report [no ci]

.github/ISSUE_TEMPLATE/01-bug-low.yml
.github/ISSUE_TEMPLATE/02-bug-medium.yml
.github/ISSUE_TEMPLATE/03-bug-high.yml
.github/ISSUE_TEMPLATE/04-bug-critical.yml
.github/ISSUE_TEMPLATE/05-enhancement.yml
.github/ISSUE_TEMPLATE/06-question.yml
.github/ISSUE_TEMPLATE/bug.md
.github/ISSUE_TEMPLATE/enhancement.md

commit dff451cfa1f297348751ce6b538670e1ae9a7d5b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 26 18:54:56 2024 +0300

    flake.lock: Update (#7540)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/4a6b83b05df1a8bd7d99095ec4b4d271f2956b64?narHash=sha256-%2BNpbZRCRisUHKQJZF3CT%2Bxn14ZZQO%2BKjxIIanH3Pvn4%3D' (2024-05-17)
      → 'github:NixOS/nixpkgs/bfb7a882678e518398ce9a31a881538679f6f092?narHash=sha256-4zSIhSRRIoEBwjbPm3YiGtbd8HDWzFxJjw5DYSDy1n8%3D' (2024-05-24)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit d298382ad977ec89c8de7b57459b9d7965d2c272
Author: Brian <mofosyne@gmail.com>
Date:   Mon May 27 00:10:17 2024 +1000

    main: replace --no-special with --special (#7534)
    
    This also flips the default behavior of the output to not include control token by default.

common/common.cpp
common/common.h
examples/main/main.cpp

commit 32a28217f475119926c603341e8273b26932b56a
Author: Galunid <karolek1231456@gmail.com>
Date:   Sun May 26 16:02:34 2024 +0200

    Fix aya-23 conversion scripts (#7539)

convert-hf-to-gguf.py

commit c429b33beb35f13934a4dfbe0c138d30b45e5d54
Author: Bartowski <ckealty1182@gmail.com>
Date:   Sun May 26 08:28:35 2024 -0400

    llama : add Smaug 70B support (#7402)

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
llama.cpp
llama.h

commit 9146d36fe7e3e911a07438c07efc1bae082f6390
Author: Aarni Koskela <akx@iki.fi>
Date:   Sun May 26 15:09:42 2024 +0300

    Readme: add akx/ggify to tools (#1484)

README.md

commit b9adcbbf92fc7096bee23fe61496d25652ebf765
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Sun May 26 06:26:34 2024 +0530

    SimpleChat Completion Mode flexibility and cleanup, Settings gMe, Optional sliding window (#7480)
    
    * SimpleChat: A placeholder system prompt, Use usage msg in code
    
    Just have a alert msg wrt needing javascript enabled in html. And
    have usage message from js file. Update the usage message a bit.
    So also enable switch session wrt setup_ui call.
    
    Add a possible system prompt as a placeholder for the system-input.
    
    * SimpleChat:CompletionMode: Allow control of Role: prefix
    
    * SimpleChat:Completion: Avoid Role: prefix; Newline only in between
    
    In completion mode
    
    * avoid inserting Role: prefix before each role's message
    
    * avoid inserting newline at the begin and end of the prompt
      message. However if there are multiple role messages, then
      insert newline when going from one role's message to the
      next role's message.
    
    * SimpleChat:CompletionMode: Update readme/usage, trim textarea newline
    
    Readme update wrt completion mode behavior.
    
    Usage help updated wrt completion mode behavior.
    
    When changing from input to textarea elment wrt user input, the last
    newline at the end of the user input wrt textarea, was forgotten to be
    filtered, this is fixed now. However if user wants to have a explicit
    newline they can using shift+enter to insert a newline, that wont be
    removed. The extra newline removal logic uses substring and keyup to
    keep things simple and avoid some previously noted bugs wrt other
    events in the key path as well as IME composition etal.
    
    * SimpleChat:SC: Ensure proper clearing/reseting
    
    previous logic would have cleared/reset the xchat, without doing
    the same wrt iLastSys, thus leading to it pointing to a now non
    existent role-content entry.
    
    So if a user set a system prompt and used completion mode, it would
    have done the half stupid clear, after the model response was got.
    Inturn when user tries to send a new completion query, it would
    inturn lead to handle_user_submit trying to add/update system prompt
    if any, which will fail, bcas iLastSys will be still pointing to a
    non existant entry.
    
    This is fixed now, by having a proper clear helper wrt SC class.
    
    * SimpleChat: Update usage note and readme a bit
    
    * SimpleChat:Completion: clear any prev chat history at begining
    
    Previously any chat history including model response to a completion
    query would have got cleared, after showing the same to the user,
    at the end of handle_user_submit, rather than at the begining.
    
    This gave the flexibility that user could switch from chat mode
    to completion mode and have the chat history till then sent to
    the ai model, as part of the completion query. However this flow
    also had the issue that, if user switches between different chat
    sessions, after getting a completion response, they can no longer
    see the completion query and its response that they had just got.
    
    The new flow changes the clearing of chat history wrt completion
    mode to the begining of handle_user_submit, so that user doesnt
    lose the last completion mode query and response, till a new
    completion mode query is sent to the model, even if they were to
    switch between the chat sessions. At the same time the loss of
    flexibility wrt converting previous chat history into being part
    of the completion query implicitly doesnt matter, because now
    the end user can enter multiline queries.
    
    * SimpleChat:Try read json early, if available
    
    For later
    
    the server flow doesnt seem to be sending back data early, atleast
    for the request (inc options) that is currently sent.
    
    if able to read json data early on in future, as and when ai model
    is generating data, then this helper needs to indirectly update
    the chat div with the recieved data, without waiting for the
    overall data to be available.
    
    * SimpleChat: Rename the half asleep mis-spelled global var
    
    * SimpleChat: Common chat request options from a global object
    
    * SimpleChat: Update title, usage and readme a bit
    
    Keep the title simple so that print file name doesnt have chars
    that need to be removed.
    
    Update readme wrt some of the new helpers and options.
    
    Change Usage list to a list of lists, add few items and style it
    to reduce the margin wrt lists.
    
    * SimpleChat:ChatRequestOptions: max_tokens
    
    As some times based on the query from the user, the ai model may get
    into a run away kind of generation with repeatations etal, so adding
    max_tokens to try and limit this run away behaviour, if possible.
    
    * SimpleChat: Reduce max_tokens to be small but still sufficient
    
    * SimpleChat: Consolidate global vars into gMe, Display to user
    
    This allows the end user to see the settings used by the logic,
    as well as allows users to change/update the settings if they
    want to by using devel-tools/console
    
    * SimpleChat:SlidingWindow: iRecentUserMsgCnt to limit context load
    
    This is disabled by default. However if enabled, then in addition
    to latest system message, only the last N user messages, after the
    latest system message and its reponses from the ai model will be sent
    to the ai-model, when querying for a new response.
    
    This specified N also includes the latest user query.
    
    * SimpleChat: placeholder based usage hint for user-in textarea
    
    * SimpleChat: Try make user experience better, if possible
    
    Reduce chat history context sent to the server/ai-model to be
    just the system-prompt, prev-user-request-and-ai-response and
    cur-user-request, instead of the previous full chat history.
    This way if there is any response with garbage/repeatation, it
    doesnt mess with things beyond the next question, in some ways.
    
    Increase max_tokens to 1024, so that a relatively large previous
    reponse doesnt eat up the space available wrt next query-response.
    However dont forget that the server when started should also
    be started with a model context size of 1k or more, to be on
    safe side.
    
    Add frequency and presence penalty fields set to 1.2 to the set
    of fields sent to server along with the user query. So that
    the model is partly set to try avoid repeating text in its
    response.
    
    * SimpleChat:Add n_predict (equiv max_tokens) for llamacpp server
    
    The /completions endpoint of examples/server doesnt take max_tokens,
    instead it takes the internal n_predict, for now add the same on
    the client side, maybe later add max_tokens to /completions endpoint
    handling.
    
    * SimpleChat: Note about trying to keep things simple yet flexible

examples/server/public_simplechat/index.html
examples/server/public_simplechat/readme.md
examples/server/public_simplechat/simplechat.css
examples/server/public_simplechat/simplechat.js

commit 9588f196b1d7b21bdff013fcf958c249576b2619
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 25 15:21:30 2024 +0300

    train : change default FA argument (#7528)

common/train.cpp
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp

commit 3cbd23ed88c03a27e1eb6090ac4a8186ca9ac29a
Author: Brian <mofosyne@gmail.com>
Date:   Sat May 25 19:30:42 2024 +1000

    labeler: added Apple Metal detector (+Kompute) (#7529)
    
    * labeler: added Apple Metal detector [no ci]
    
    * labeler: add Kompute to detector [no ci]

.github/labeler.yml

commit 00c63907931bb08a0ed2b7e38cf44dd290143cb9
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Sat May 25 05:04:03 2024 -0400

    main : don't print special tokens with --grammar (#6923)
    
    * main : don't print special tokens with --grammar
    
    The CLI interface was recently changed to print special control tokens
    like the </s> stop message one. This token shouldn't be printed if the
    grammar flag was passed, unless the grammar specifies it, because that
    breaks shell-scriptability.
    
    * main: use seperate stream for control characters
    
    * main: use dprintf and add --ctrl-token-no-out and --ctrl-token-fd-out
    
    * main: dprintf isn't part of the IEEE POSIX standard. Just use write().
    
    * main: remove --ctrl-token-fd-out in favor for fcntl() based detection
    
    * common.cpp: accidentally removed --interactive-first
    
    * main: only merge stdout and control token if not in conversation or grammar mode
    
    * main: rejig control token descriptor handling
    
    * main: must check pipe status on very top of program
    
    * main: renamed --no-special from  --ctrl-token-no-out and other refactoring
    
    * main: refactor ctrl_token_no_out --> no_special
    
    * llama: rename llama_token_is_control_token() to llama_token_is_control()
    
    * main: remove special token file descriptor feature (#5)
    
    ---------
    
    Co-authored-by: Brian <mofosyne@gmail.com>

common/common.cpp
common/common.h
examples/main/main.cpp
llama.cpp
llama.h

commit faa0e6979a11dcb731e9d778ad42ceaa0302015e
Author: Masaya, Kato <62578291+msy-kato@users.noreply.github.com>
Date:   Sat May 25 17:42:31 2024 +0900

    ggml: aarch64: SVE kernels for q8_0_q8_0, q4_0_q8_0 vector dot (#7433)
    
    * Add SVE support for q4_0_q8_0 q8_0_q8_0
    
    * remove ifdef

CMakeLists.txt
common/common.cpp
ggml-impl.h
ggml-quants.c
ggml.c
ggml.h
llama.cpp

commit 9791f402580838d7f8543ae7bc633ef265e436f0
Author: Elton Kola <eltonkola@gmail.com>
Date:   Sat May 25 04:11:33 2024 -0400

    android : module (#7502)
    
    * move ndk code to a new library
    
    * add gradle file

examples/llama.android/app/build.gradle.kts
examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt
examples/llama.android/build.gradle.kts
examples/llama.android/llama/.gitignore
examples/llama.android/llama/CMakeLists.txt
examples/llama.android/llama/build.gradle.kts
examples/llama.android/llama/consumer-rules.pro
examples/llama.android/llama/proguard-rules.pro
examples/llama.android/llama/src/androidTest/java/android/llama/cpp/ExampleInstrumentedTest.kt
examples/llama.android/llama/src/main/AndroidManifest.xml
examples/llama.android/llama/src/main/cpp/CMakeLists.txt
examples/llama.android/llama/src/main/cpp/llama-android.cpp
examples/llama.android/llama/src/main/java/android/llama/cpp/LLamaAndroid.kt
examples/llama.android/llama/src/test/java/android/llama/cpp/ExampleUnitTest.kt
examples/llama.android/settings.gradle.kts

commit 902184dd3a9d6685e752b19027a48423742531db
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat May 25 05:30:59 2024 +0200

    fix missing slash in `fs_get_cache_directory()` (#7503)
    
    * fix missing slash in fs_get_cache_directory()
    
    * use LOCALAPPDATA for fs_get_cache_directory()
    
    * better code style

common/common.cpp

commit 57684331fc2d685f7d1f5775af0b9e47d1829833
Author: Mikko Juola <mikjuo@gmail.com>
Date:   Fri May 24 18:14:42 2024 -0700

    Make tokenize CLI tool have nicer command line arguments. (#6188)
    
    * Make tokenizer.cpp CLI tool nicer.
    
    Before this commit, tokenize was a simple CLI tool like this:
    
      tokenize MODEL_FILENAME PROMPT [--ids]
    
    This simple tool loads the model, takes the prompt, and shows the tokens
    llama.cpp is interpreting.
    
    This changeset makes the tokenize more sophisticated, and more useful
    for debugging and troubleshooting:
    
      tokenize [-m, --model MODEL_FILENAME]
               [--ids]
               [--stdin]
               [--prompt]
               [-f, --file]
               [--no-bos]
               [--log-disable]
    
    It also behaves nicer on Windows now, interpreting and rendering Unicode
    from command line arguments and pipes no matter what code page the user
    has set on their terminal.
    
    * style fix: strlen(str) == 0 --> *str == 0
    
    * Simplify tokenize.cpp; by getting rid of handling positional style arguments.
    
    It must now be invoked with long --model, --prompt etc. arguments only.
    Shortens the code.
    
    * tokenize.cpp: iostream header no longer required
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: brian khuu <mofosyne@gmail.com>

examples/tokenize/tokenize.cpp

commit b83bab15a5d2a1e7807d09613a9b34309d86cfaa
Author: compilade <git@compilade.net>
Date:   Fri May 24 21:11:48 2024 -0400

    gguf-py : fix and simplify quantized shape round-trip (#7483)
    
    * gguf-py : fix and simplify quantized shape round-trip
    
    * gguf-py : remove unused import

convert-hf-to-gguf.py
gguf-py/gguf/gguf_reader.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/quants.py
gguf-py/scripts/gguf-new-metadata.py

commit d041d2ceaaf50e058622d92921b3e680ffa4e9e7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 24 18:59:06 2024 +0300

    flake.lock: Update (#7232)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/e5d10a24b66c3ea8f150e47dfdb0416ab7c3390e?narHash=sha256-yzcRNDoyVP7%2BSCNX0wmuDju1NUCt8Dz9%2BlyUXEI0dbI%3D' (2024-05-02)
      → 'github:hercules-ci/flake-parts/8dc45382d5206bd292f9c2768b8058a8fd8311d9?narHash=sha256-/GJvTdTpuDjNn84j82cU6bXztE0MSkdnTWClUCRub78%3D' (2024-05-16)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/63c3a29ca82437c87573e4c6919b09a24ea61b0f?narHash=sha256-4cPymbty65RvF1DWQfc%2BBc8B233A1BWxJnNULJKQ1EY%3D' (2024-05-02)
      → 'github:NixOS/nixpkgs/4a6b83b05df1a8bd7d99095ec4b4d271f2956b64?narHash=sha256-%2BNpbZRCRisUHKQJZF3CT%2Bxn14ZZQO%2BKjxIIanH3Pvn4%3D' (2024-05-17)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit 27891f6db03de6e3fd5941983838c29bef253352
Author: Brian <mofosyne@gmail.com>
Date:   Fri May 24 23:47:56 2024 +1000

    docker.yml: disable light-intel and server-intel test (#7515)
    
    * docker.yml: disable light-intel test
    
    * docker.yml: disable server-intel test

.github/workflows/docker.yml

commit fbca2f27fc7fa9aa4a8ad0357478fdb908472908
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Fri May 24 14:31:13 2024 +0200

    Add support for ArcticForCausalLM (#7020)
    
    * common : increase max number of experts to 128
    
    * common : add tensor LLM_TENSOR_FFN_NORM_EXPS for normalization before MoE that runs in parallel to attention + ffn
    
    * gguf-py : add architecture-specific block mappings that override selected general block mappings
    
    * convert-hf : add model conversion support for ArcticForCausalLM
    
    * convert-hf : use added_tokens_decoder from tokenizer_config.json to redefine tokens from SentencePiece model (only for ArcticForCausalLM)
    
    * llama : add inference support for LLM_ARCH_ARCTIC
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 0df0aa8e43c3378975269a51f9b876c8692e70da
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Fri May 24 10:06:56 2024 +0800

    add build shared lib in win release package (#7438)

examples/sycl/win-build-sycl.bat

commit 74f33adf5f8b20b08fc5a6aa17ce081abe86ef2f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 17:43:18 2024 +0300

    readme : remove trailing space (#7469)

README.md

commit 1debe72737ea131cb52975da3d53ed3a835df3a6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 17:17:43 2024 +0300

    ggml : silence UB sanitizer error during iq2_xxs quantization (#0)

ggml-quants.c

commit 007489e895bad02e4e54758bf0bdf2d6a4cdb7c1
Author: Tristan Druyen <tristan@vault81.mozmail.com>
Date:   Thu May 23 16:15:15 2024 +0200

    Fix phi3 chat template confusion with zephyr (#7449)
    
    * Fix phi3 template matching vs zephyr
    
    * Add regression test for new phi3 chat template
    
    * Implement review suggestions
    
    * Fix phi3 jinja test templates & match by <|end|>
    
    * Apply suggestion
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * Add all phi3 template variants in tests
    
    * Remove unneeded message trimming
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * Fix tests to not expect trimmed messages
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

llama.cpp
tests/test-chat-template.cpp

commit 8b94e799dfa482adf63419df4905dc79b37e179f
Author: Raj Hammeer Singh Hada <hammeerraj@gmail.com>
Date:   Thu May 23 18:00:13 2024 +0530

    readme : add Bunny in supported models [no ci] (#7469)

README.md

commit 3015851c5ac7334fb544a23a70a284c117b87044
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu May 23 14:29:26 2024 +0200

    llama : add getters for n_threads/n_threads_batch (#7464)
    
    * llama : add getters for n_threads/n_threads_batch
    
    This commit adds two new functions to the llama API. The functions
    can be used to get the number of threads used for generating a single
    token and the number of threads used for prompt and batch processing
    (multiple tokens).
    
    The motivation for this is that we want to be able to get the number of
    threads that the a context is using. The main use case is for a
    testing/verification that the number of threads is set correctly.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! llama : add getters for n_threads/n_threads_batch
    
    Rename the getters to llama_n_threads and llama_n_threads_batch.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

llama.cpp
llama.h

commit 55ac3b7aeaf52f19786ed96e885d89521fc0f6c8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 15:28:14 2024 +0300

    ci : use Pythia models instead of OpenLlama (#7470)
    
    * ci : start using Pythia models over OpenLlama
    
    ggml-ci
    
    * ci : disable q2_k ppl tests
    
    * ci : use convert-hf-to-gguf.py
    
    * ci : update gg_get_model
    
    * ci : fix convert outfile name
    
    ggml-ci
    
    * llama : gptneox arch use F32 attn prec
    
    ggml-ci

ci/run.sh
llama.cpp

commit dacfcebd6022175848e978f82811a244f1033038
Author: Victor Nogueira <felladrin@gmail.com>
Date:   Thu May 23 15:12:43 2024 +0300

    readme : add GPT-NeoX + Pythia to the list of supported models (#7491)

README.md

commit 9b82476ee9e73065a759f8bcc4cf27ec7ab2ed8c
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Thu May 23 11:49:53 2024 +0200

    Add missing inference support for GPTNeoXForCausalLM (Pythia and GPT-NeoX base models) (#7461)
    
    * convert-hf : add conversion of bloom-style qkv tensor to gpt-style qkv (code borrowed from BloomModel)
    
    * llama : add inference support for LLM_ARCH_GPTNEOX
    
    * llama : add model types for every Pythia variant and GPT-NeoX
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

convert-hf-to-gguf.py
llama.cpp

commit a61a94e543e3c6877c087e80fca27a0313ce5fd5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 12:38:18 2024 +0300

    llama : rename n_ctx -> cache.size, less confusing (#0)

llama.cpp

commit 152da28ae54139e3754189b9e6e1c28e11277502
Author: Brian <mofosyne@gmail.com>
Date:   Thu May 23 17:40:43 2024 +1000

    labeler.yml: add embedding label detector [no ci] (#7482)

.github/labeler.yml

commit d48c88cbd563b6cf0ce972e2f56796896e240736
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 10:00:44 2024 +0300

    ggml : remove ggml_flash_attn and ggml_flash_ff (#7463)
    
    ggml-ci

examples/finetune/finetune.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml.c
ggml.h
tests/test-grad0.cpp

commit e84b71c2c6da6e69c8f815168ea836f9716a325e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 10:00:21 2024 +0300

    ggml : drop support for QK_K=64 (#7473)
    
    * ggml : drop support for QK_K=64
    
    ggml-ci
    
    * opencl : restore QK_K=256 define

CMakeLists.txt
Makefile
ci/run.sh
ggml-common.h
ggml-cuda/convert.cu
ggml-cuda/dmmv.cu
ggml-cuda/mmq.cu
ggml-cuda/vecdotq.cuh
ggml-metal.m
ggml-metal.metal
ggml-opencl.cpp
ggml-quants.c
ggml-sycl.cpp
ggml.c
gguf-py/gguf/constants.py
llama.cpp

commit 1b1e27cb49158123ef4902aa41eb368c9e76e6a1
Author: 0cc4m <picard12@live.de>
Date:   Thu May 23 08:59:59 2024 +0200

    Update vulkan rope implementation to support frequency factors (#7475)

ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py

commit fbf777d2b9c30e7569e3d1c149501c1e31d9b5b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 23 09:43:24 2024 +0300

    main : minor (#7462)

examples/main/main.cpp

commit cd93a28cb1446319af5e2f4b416174c3a8e43546
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu May 23 00:31:20 2024 +0200

    CUDA: fix FA out-of-bounds reads (#7479)

ggml-cuda/fattn-tile-f16.cu
ggml-cuda/fattn-tile-f32.cu
ggml-cuda/fattn-vec-f16.cu
ggml-cuda/fattn-vec-f32.cu

commit 1e374365d170b7f692fd7753c145e21bc14486c8
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Wed May 22 23:23:21 2024 +0530

    SimpleChat: a simple and dumb web front end for testing /chat/completions and /completions end points and try chat (#7350)
    
    * SimpleChat: Add a skeletal html page
    
    Contains a div placeholder for showing chat messages till now
    
    a text-input for allowing user to enter next chat message/query
    to the model.
    
    a submit button to allow sending of the user entered message and
    chat till now to the model.
    
    * SimpleChat: A js skeleton with SimpleChat class
    
    Allows maintaining an array of chat message.
    
    Allows adding chat message (from any of the roles be it system,
    user, assistant, ...)
    
    Allows showing chat messages till now, in a given div element.
    
    * SimpleChat: request_json, globals, startme
    
    * SimpleChatJS: Roles Class, submitClick
    
    Define Role class with static members corresponding to the roles.
    
    Update startme to
    
    * Get hold of the ui elements.
    
    * Attach a click handler to submit button, which adds the user input
      to xchats array and shows the chat messages till now in chat div
      element.
    
    Trap DOMContentLoaded to trigger startme
    
    * SimpleChat:HTML: Bring in the js file
    
    * SimpleChat: Rather value wrt input text element
    
    * SimpleChat: Also add completions related prompt
    
    * SimpleChat: Use common helper logic wrt json data
    
    * SimpleChat: Move handling of submit request into its own func
    
    * SimpleChat: Try handshake with llm over its web service endpoint
    
    * SimpleChat:JS: Extract model response and show to user
    
    * SimpleChat:JS: Messages/Prompt, indicate working to end user
    
    * SimpleChat: Try keep input element in view
    
    * SimpleChat: Diff user/assistant msgs, Make input wider
    
    Also show a default message to user
    
    Also add some metas
    
    * SimpleChat: Move into its own sub directory to avoid confusion
    
    * SimpleChat:sh: Add simple shell script to run python3 http.server
    
    So one needs to run the llm server locally
    then run this script and access it using a local browser
    
    * SimpleChat:JS: Try trap enter key press wrt input text field
    
    So user can either press submit button or press enter key
    
    * SimpleChat: Allow user to select chat or completion mode
    
    * SimpleChat: Dont submit if already submitted and waiting
    
    Also make chat the default selection wrt mode
    
    * SimpleChat:JS: Handle difference in response
    
    Try read the assistance response from appropriate field in the
    response got.
    
    Also examples/server seems to return the response in a slightly
    different field, so try account for that also.
    
    * SimpleChat:JS: Force completion mode be single message by default
    
    * SimpleChat: Add a simple readme file
    
    * SimpleChat:HTML: Cleanup/structure UI a bit, Add input for system
    
    * SimpleChat:Allow system prompt to be set, if provided before user
    
    * SimpleChat: Ignore empty user input, without trimming
    
    * SimpleChat:Alert user if they provide sysprompt late or change it
    
    * SimpleChat: Move handling systemprompt into its own func
    
    * SimpleChat:HTML: Add a style for system role message
    
    * SimpleChat: Update the readme file
    
    * SimpleChat:CSS: Move style info into its own css file
    
    To keep it simple, clean and seperate so that things are not
    unnecessarily cluttered.
    
    * SimpleChat:CSS: Allow for chat div to be scrollable
    
    * SimpleChat:JS: Try ensure the last entry in chat is visible
    
    Needed because now only the chat div is scrollable and not the full
    page.
    
    In last commit the chat div size was fixed to 75% vertical height,
    so the full page no longer scrolls, so the old bring user-input
    element to view wont work, instead now the last element in the
    chat div should be brought into view.
    
    * SimpleChat:JS: bottom of element visible, Set focus to user input
    
    As the generated text could be multiple lines and occupy more space
    that the full scrollable div's vertical space, make the bottom of
    the last element (which can be such a generated text) in the div
    visible by scrolling.
    
    Ensure that the user input box has focus
    
    * SimpleChat: Update notes a bit. Try keep browser happy
    
    Avoid browser quirk mode with DOCTYPE.
    
    Help with accessibility a bit by specifying the language explicitly.
    
    Specify the char encoding explicitly, inturn utf-8 is a safe bet,
    even with intermixing of languages if reqd in future.
    
    Add a cache-control http-equiv meta tag, which in all probability
    will be ignored.
    
    Defer js loading and execution, just for fun and future, not that
    critical here as it stands now.
    
    * SimpleChat:HTML:Group user input+btn together; Note about multichat
    
    * SimpleChat:JS: Allow for changing system prompt anytime for future
    
    * SimpleChat:Readme: Note about handle_systemprompt begin/anytime
    
    * SimpleChat:HTML: Add viewport meta for better mobile friendliness
    
    Without this the page content may look too small.
    
    * SimpleChat:HtmlCss: Cleanup UI flow
    
    set margin wrt vmin rather than vw or vh so portrait/landscape ok.
    
    Use flex and flex-grow to put things on the same line as well as
    distribute available space as needed. Given two main elements/line
    so it remains simple.
    
    In each line have one element with grows and one sits with a basic
    comfortably fixed size.
    
    * SimpleChat: textarea for multiline user chat, inturn shift+enter 4 enter
    
    * SimpleChat: Make vertical layout better responsive (flex based)
    
    Also needed to make things cleaner and properly usable whether
    landscape or portrait, after changing to multiline textarea rather
    than single line user input.
    
    Avoid hardcoding the chat-till-now display area height, instead
    make it a flex-growable within a flex column of ui elements within
    a fixed vertical area.
    
    * SimpleChat: Rename simplechat.html to index.html, update readme
    
    Instead of providing a seperate shell script, update the readme wrt
    how to run/use this web front end.
    
    * SimpleChat: Screen fixed view and scrolling, Printing full
    
    * SimpleChat:JS:CI: Avoid space at end of jsdoc param line
    
    * SimpleChat:JS: MultiChat initial skeleton
    
    Will help maintain multiple independent chats in future
    
    * SimpleChat:JS: Move system prompt begin/anytime into SimpleChat
    
    * SimpleChat:JS:Keep MultiChatUI simple for now
    
    Worry about different chats with different servers for later.
    
    * SimpleChat:JS: Move handle submit into MultiChat, build on same
    
    Create an instance of MultiChatUI and inturn a instance of chat
    session, which is what the UI will inturn work on.
    
    * SimpleChat:JS: Move to dictionary of SimpleChat, instead of array
    
    * SimpleChat: Move ui elements into MultiChatUI, Update el IDs
    
    Move ui elements into MultiChatUI, so that current handleUserSubmit
    doesnt need to take the element arguments. Also in future, when
    user is allowed to switch between different chat sessions, the
    UI can be updated as needed by using the elements in UI already
    known to MultiChatUI instance.
    
    Rename the element ids' so that they follow a common convention,
    as well as one can identify what the element represents in a more
    consistant manner.
    
    * SimpleChat:MCUI:Show available chat sessions, try switch btw them
    
    Previous commits brought in / consolidated existing logic into
    MultiChatUI class.
    
    Now start adding logic towards multichat support
    
    * show buttons indicating available chat sessions
    
    * on sessin button click, try switch to that session
    
    * SimpleChat:MCUI: Store and use current chat session id
    
    Also
    
    allow to switch chat session optionally, wrt some of the related
    helpers.
    
    setup for two chat sessions by default.
    
    * SimpleChat:MCUI: Delay enabling user-input to avoid race
    
    Re-enable user-input, only after response to a user query has been
    updated to the chat-div. This ensures that if user tries to switch
    chat session, it wont be allowed till chat-request-response flow is
    done.
    
    * SimpleChat: Take care of system prompt
    
    Helper to get the latest system prompt and inturn use same to
    set the system prompt ui, when switching.
    
    Ensure that system prompt is set if and when enter key is pressed.
    
    * SimpleChat:GetSystemLatest, fix a oversight.
    
    * SimpleChat:MCUI: Allow selected chat-session btn to be highlighted
    
    Also have a general helper for setting class of children.
    
    * SimpleChat:Cleanup corners
    
    Show system prompt in chat space, when it is set by pressing enter,
    as a feedback to user.
    
    Alert user, if they try to switch chat session in the middle of
    waiting for a response from the ai model.
    
    * SimpleChat:MCUI: Ensure req-resp failure doesnt lock up things
    
    * SimpleChat:MCUI: Support for new chat sessions
    
    Also a general create button helper.
    
    * SimpleChat:MCUI: CreateSessionBtn helper, use wrt NewChat
    
    Also fix a oversight wrt using stale data wrt the list of chat
    sessions.
    
    * SimpleChat:MCUI: NewChat btn first before existing chat sessions
    
    * SimpleChat:MCUI:CornerCases:Skip new chat, show only if current
    
    Skip NewChat if user cancels or if one waiting for response from
    the ai model.
    
    Dont show a chat with newly got ai model response, if current chat
    session has changed, some how. Chat session shouldnt be allowed to
    change, if there is a pending response, but still as a additional
    sanity check.
    
    * SimpleChat: Update readme, title, show usage if no chat to show
    
    * SimpleChat: Cleanup the log/dialog messages a bit

examples/server/public_simplechat/index.html
examples/server/public_simplechat/readme.md
examples/server/public_simplechat/simplechat.css
examples/server/public_simplechat/simplechat.js

commit 197ff91462dd05bb9a3be03578114abf0c355536
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 22 20:05:38 2024 +0300

    build : remove zig (#7471)

.github/workflows/zig-build.yml
build.zig

commit 6ff13987ad1a9519bee13dd98b6a21cd98979aab
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 22 20:04:20 2024 +0300

    common : normalize naming style (#7462)
    
    * common : normalize naming style
    
    ggml-ci
    
    * common : match declaration / definition order
    
    * zig : try to fix build

build.zig
common/common.cpp
common/common.h
common/sampling.cpp
common/sampling.h
common/train.cpp
examples/batched/batched.cpp
examples/embedding/embedding.cpp
examples/eval-callback/eval-callback.cpp
examples/imatrix/imatrix.cpp
examples/infill/infill.cpp
examples/llama-bench/llama-bench.cpp
examples/llava/llava-cli.cpp
examples/lookahead/lookahead.cpp
examples/lookup/lookup.cpp
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/perplexity/perplexity.cpp
examples/quantize/quantize.cpp
examples/retrieval/retrieval.cpp
examples/server/server.cpp

commit 38c03478a37e460ecd3a21155b338a83bfed7f90
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 22 17:58:25 2024 +0200

    CUDA: fix FA out-of-bounds writes (#7465)

ggml-cuda/fattn-tile-f16.cu
ggml-cuda/fattn-tile-f32.cu
ggml-cuda/fattn-vec-f16.cu
ggml-cuda/fattn-vec-f32.cu

commit b18532a4efeca8796fea8e36195c81cbfd596a4a
Author: slaren <slarengh@gmail.com>
Date:   Wed May 22 16:10:46 2024 +0200

    phi3 : duplicate rope factors in each layer (#7447)
    
    * phi3 : duplicate rope factors in each layer
    
    phi3 : set phi-3 model type as 14B
    
    model loader : simplify the process for duplicating model tensors
    
    llama-bench : remove default pg test
    
    * replace bool parameters in llama_model_loader with named flags

examples/llama-bench/llama-bench.cpp
llama.cpp

commit fcda1128bc5f8eb7e1811708fe9d9867b9aec815
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Wed May 22 20:53:21 2024 +0800

    vulkan: add workaround for iterator boundary check to fix clang-cl debug build (#7426)

CMakeLists.txt

commit 03d8900ebe062355e26a562379daee5f17ea099f
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Wed May 22 07:08:18 2024 -0400

    llama : add missing model type names (#7445)

llama.cpp

commit 9b3d83318931aa98c487baaa977626931d059e6a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 22 12:36:37 2024 +0300

    cuda : fix compile warning (#7454)

ggml-cuda/fattn-tile-f32.cu

commit 95fb0aefab568348da159efdd370e064d1b35f97
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 22 10:24:29 2024 +0200

    CUDA: remove incorrect precision check (#7454)

ggml-cuda/fattn-tile-f32.cu

commit 3e5faa85032ec3106a2ad831bf412be9ff139f47
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 22 11:01:35 2024 +0300

    cuda : fix rope + add tests (#7452)
    
    * cuda : fix rope pos data
    
    ggml-ci
    
    * ggml : drop mode & 1 == 1 support for ggml_rope
    
    ggml-ci
    
    * ggml : support freq_factors for f16 rope (CPU)
    
    ggml-ci
    
    * tests : add rope tests using frequency factors
    
    ggml-ci

ggml-cuda/rope.cu
ggml.c
ggml.h
tests/test-backend-ops.cpp

commit 201cc11afa0a1950e1f632390b2ac6c937a0d8f0
Author: liuwei-git <14815172+liuwei-git@users.noreply.github.com>
Date:   Wed May 22 04:28:32 2024 +0800

    llama : add phi3 128K model support (#7225)
    
    * add phi3 128k support in convert-hf-to-gguf
    
    * add phi3 128k support in cuda
    
    * address build warnings on llama.cpp
    
    * adjust index value in cuda long rope freq factors
    
    * add long rope support in ggml cpu backend
    
    * make freq factors only depend on ctx size
    
    * remove unused rope scaling type 'su' frin gguf converter
    
    * fix flint warnings on convert-hf-to-gguf.py
    
    * set to the short freq factor when context size is small than trained context size
    
    * add one line of comments
    
    * metal : support rope freq_factors
    
    * ggml : update ggml_rope_ext API to support freq. factors
    
    * backends : add dev messages to support rope freq. factors
    
    * minor : style
    
    * tests : update to use new rope API
    
    * backends : fix pragma semicolons
    
    * minor : cleanup
    
    * llama : move rope factors from KV header to tensors
    
    * llama : remove tmp assert
    
    * cuda : fix compile warning
    
    * convert : read/write n_head_kv
    
    * llama : fix uninitialized tensors
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
examples/finetune/finetune.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-cuda/rope.cu
ggml-kompute.cpp
ggml-metal.m
ggml-metal.metal
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
ggml.h
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
llama.cpp
tests/test-backend-ops.cpp

commit 6369bf04336ab60e5c892dd77a3246df91015147
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 21 23:03:42 2024 +0300

    metal : handle F16 inf values, fix FA partial offload (#7434)
    
    ggml-ci

ggml-metal.metal

commit e402de364b643cb89ea9f43057733b5d36298670
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue May 21 20:40:00 2024 +0100

    `grammars`: fix resampling logic regression (#7424)

common/sampling.cpp
examples/main/main.cpp

commit fcf6538ba6702c55eaec70da9a75c81d04900a72
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue May 21 19:27:12 2024 +0200

    CUDA: fix unused warning in mmq.cu (#7442)

ggml-cuda/mmq.cu

commit c3f8d583560b4f261fa21c976793e538c60cd66c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 21 19:53:48 2024 +0300

    tests : test-tokenizer-0.sh print more info (#7402)

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
tests/test-tokenizer-0.sh

commit 11474e756de3f56b760986e73086d40e787e52f8
Author: Amir <amir_zia@outlook.com>
Date:   Tue May 21 17:13:12 2024 +0300

    examples: cache hf model when --model not provided (#7353)
    
    * examples: cache hf model when --model not provided
    
    * examples: cache hf model when --model not provided
    
    * examples: cache hf model when --model not provided
    
    * examples: cache hf model when --model not provided
    
    * examples: cache hf model when --model not provided

common/common.cpp
common/common.h
examples/main/README.md

commit d8ee90222791afff2ab666ded4cb6195fd94cced
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue May 21 16:02:12 2024 +0200

    CUDA: deduplicate mmq code (#7397)

ggml-cuda/mmq.cu

commit d7e852c1bc8e85bf62a6f1aede08cd2de723404a
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Tue May 21 14:39:48 2024 +0200

    Tokenizer SPM fixes for phi-3 and llama-spm (bugfix) (#7425)
    
    * Update brute force test: add_special
    * Update brute force test: default values for add_bos_token and add_eos_token
    * Enable rtrim when pre-inserting BOS
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    * Revert "server : fix test regexes"

convert-hf-to-gguf.py
examples/server/tests/features/server.feature
examples/server/tests/features/slotsave.feature
llama.cpp
tests/test-tokenizer-random.py

commit 917dc8cfa67a72fb7c8bf7392270da3bf4833af4
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Mon May 20 20:15:57 2024 +0200

    Tokenizer SPM fixes for phi-3 and llama-spm (#7375)
    
    * Update brute force test: special tokens
    * Fix added tokens
      - Try to read 'added_tokens.json'.
      - Try to read 'tokenizer_config.json'.
      - Try to read 'tokenizer.json'.
    * Fix special tokens rtrim
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    * server : fix test regexes

convert-hf-to-gguf.py
examples/server/tests/features/server.feature
examples/server/tests/features/slotsave.feature
llama.cpp
tests/test-tokenizer-random.py

commit fabf30b4c4fca32e116009527180c252919ca922
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 19:35:28 2024 +0300

    llama : remove Persimmon (#7408)
    
    * llama : remove Persimmon
    
    * requirements : remove

README.md
convert-hf-to-gguf.py
convert-persimmon-to-gguf.py
gguf-py/gguf/constants.py
llama.cpp
requirements.txt
requirements/requirements-convert-persimmon-to-gguf.txt

commit 20385cebcc4bb3f6dd10f989573c11864d70d53d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon May 20 18:15:38 2024 +0200

    perplexity: update README FP16 results [no ci] (#7413)

examples/perplexity/README.md

commit db10f01310beea8a1ef7798651b9d692fd1149d0
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Mon May 20 16:36:55 2024 +0300

    rpc : track allocated buffers (#7411)
    
    * rpc : track allocated buffers
    
    ref: #7407
    
    * rpc : pack rpc_tensor tightly

ggml-rpc.cpp

commit 3bc10cb485dd7efa4da6c64e73ad0c9e2bfe0821
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 15:10:03 2024 +0300

    server : fix temperature + disable some tests (#7409)
    
    * server : fix temperature
    
    * server : disable tests relying on parallel determinism
    
    * ci : change server Debug -> RelWithDebInfo

.github/workflows/server.yml
examples/server/tests/features/results.feature

commit 6bf9b66fa3f263ca2175dcb5f6d0a658581e1dfb
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon May 20 12:08:23 2024 +0100

    [SYCL] Update SYCL upscale operation (#7321)
    
    * Update SYCL upscale operation
    
    * Formatting
    
    * Remove messages

ggml-sycl.cpp

commit 26cd4237bc499f7144d76f440aa775d749b170bb
Author: Bingan <70050083+binganao@users.noreply.github.com>
Date:   Mon May 20 17:55:34 2024 +0800

    Update README.md (#7410)

README.md

commit 213e90ed73f8ac3cd3026dc3f086beae0d414f96
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Mon May 20 07:33:21 2024 +0000

    ggml-opencl, llama: using reserve() if count already known (#7272)

ggml-opencl.cpp
llama.cpp

commit 65c58207ece92ad213f4bfd0f91dcb2dfb664f5b
Author: junchao-loongson <68935141+junchao-loongson@users.noreply.github.com>
Date:   Mon May 20 15:19:21 2024 +0800

    ggml : add loongarch lsx and lasx support (#6454)
    
    * add loongarch lsx and lasx optimize code
    
    * Add loongarch compilation support to makefile
    
    * revert stb_image.h
    
    * opt bytes_from_nibbles_32 and sum_i16_pairs_float
    
    * fix undeclared
    
    * format code
    
    * update
    
    * update 2
    
    ---------
    
    Co-authored-by: Jinyang He <hejinyang@loongson.cn>

CMakeLists.txt
Makefile
ggml-impl.h
ggml-quants.c
ggml.c

commit 1cc0155d04918cb3017afa472acea51b77483c4a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 10:16:41 2024 +0300

    server : tuning tests (#7388)
    
    * server : don't pass temperature as string
    
    * server : increase timeout
    
    * tests : fix the fix 0.8f -> 0.8
    
    ggml-ci
    
    * tests : set explicit temperature

examples/server/tests/features/results.feature
examples/server/tests/features/steps/steps.py

commit e932094d58f513d5996c3efc9f6fed8238894c57
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 08:56:05 2024 +0300

    server : return error on too large embedding input (#7389)

examples/server/server.cpp

commit 2789baf480ba7dc9281b65f601f0918e58920f54
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 20 08:55:09 2024 +0300

    tests : fix --keep_split -> --keep-split (#7374)

examples/quantize/tests.sh

commit 33c8d50accd6dca73c9c4af00a05e24209c160fe
Author: Srihari-mcw <96763064+Srihari-mcw@users.noreply.github.com>
Date:   Sun May 19 19:18:39 2024 -0700

    Add provisions for windows support for BF16 code including CMake provision for enabling AVX512_BF16 (#7258)

CMakeLists.txt
ggml-impl.h
ggml.c
ggml.h
llama.cpp

commit d359f30921a9f62a0fd299c412ff3f270286fea6
Author: slaren <slarengh@gmail.com>
Date:   Mon May 20 01:17:03 2024 +0200

    llama : remove MPI backend (#7395)

.devops/nix/package.nix
.github/workflows/build.yml
CMakeLists.txt
Makefile
README.md
ggml-mpi.c
ggml-mpi.h
llama.cpp
scripts/LlamaConfig.cmake.in

commit 1ea2a0036e88172d6c8bf7e1a1989a03894dc955
Author: Fred Douglas <43351173+fredlas@users.noreply.github.com>
Date:   Sun May 19 11:37:04 2024 -0500

    quantize : fix --keep-split check (#7374)

examples/quantize/quantize.cpp

commit f030ec1f7a72aa825b2104823946551b9ec5dfc1
Author: 0cc4m <picard12@live.de>
Date:   Sun May 19 17:19:53 2024 +0200

    Vulkan Embedding Fix (#7360)
    
    * Fix empty Vulkan host buffers
    
    Add fp32 fp16 matmul shader
    
    Fix matmul shader alignment
    
    * Remove deprecated tensor->backend uses
    
    * Fix Vulkan validation errors on embedding models with no offloaded layers
    
    * Fix Vulkan llava segfault when not offloading layers

ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py

commit e4e6f67be6a8a697f5f89a28c98934e53c99c359
Author: slaren <slarengh@gmail.com>
Date:   Sun May 19 17:08:46 2024 +0200

    ggml : fix another case of quants nans (#7387)

ggml-quants.c

commit 5ca49cbecda27ce0a7266658fc3b640bff3ed386
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 19 16:46:13 2024 +0200

    ggml: implement quantized KV cache for FA (#7372)

ggml.c

commit 1b01f06db0cff5f5f600bb754fc39fde565ed56a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 19 16:26:02 2024 +0200

    server: add test for token probs (#7347)

examples/server/README.md
examples/server/tests/features/results.feature
examples/server/tests/features/steps/steps.py

commit 41858392e17abead21735309bf17cb55183d8c31
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 19 16:06:33 2024 +0200

    server: fix seed being reported back (#7382)

examples/server/server.cpp

commit 6aade19ee74b896c59929676629340b36be3e22c
Author: Anas Ahouzi <112881240+aahouzi@users.noreply.github.com>
Date:   Sun May 19 14:46:46 2024 +0200

    Add StableLM2 pre-tokenizer (#7349)
    
    * Add StableLM pre-tokenizer
    
    * Fix space
    
    * Fix trailing whitespace

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
llama.cpp
llama.h

commit ab33f7a338593f6cf1ae98b10b6f8684f63bd72c
Author: slaren <slarengh@gmail.com>
Date:   Sun May 19 14:19:37 2024 +0200

    cuda : clear error after buffer allocation failure (#7376)

ggml-cuda.cu

commit e23b974f4cf9270d05062d446f406e3ff55d9451
Author: Brian <mofosyne@gmail.com>
Date:   Sun May 19 20:51:03 2024 +1000

    labeler.yml: Use settings from ggerganov/llama.cpp [no ci] (#7363)
    
    https://github.com/actions/labeler#using-configuration-path-input-together-with-the-actionscheckout-action
    Recommends the use of checkout action to use the correct repo context
    when applying settings for PR labels
    
    e.g.
    
        steps:
        - uses: actions/checkout@v4 # Uploads repository content to the runner
          with:
            repository: "owner/repositoryName" # The one of the available inputs, visit https://github.com/actions/checkout#readme to find more
        - uses: actions/labeler@v5
          with:
            configuration-path: 'path/to/the/uploaded/configuration/file'

.github/workflows/labeler.yml

commit 854d365abab7194b5013a523f72da19860c3c550
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 19 11:01:01 2024 +0300

    cmake : update android comments (#7341)

examples/llama.android/app/src/main/cpp/CMakeLists.txt

commit f5bf761747988ee1832766f7d1433739aff810da
Author: fraxy-v <65565042+fraxy-v@users.noreply.github.com>
Date:   Sun May 19 01:44:42 2024 +0300

    Capture CUDA logging output (#7298)
    
    * logging: output capture in cuda module
    
    * fix compile error
    
    * fix: vsnprintf terminates with 0, string use not correct
    
    * post review
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu
ggml-cuda.h
llama.cpp

commit 059031b8c40e1f4ba60586842c5b1ed3ddf61842
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 18 18:55:54 2024 +0300

    ci : re-enable sanitizer runs (#7358)
    
    * Revert "ci : temporary disable sanitizer builds (#6128)"
    
    This reverts commit 4f6d1337ca5a409dc74aca8c479b7c34408a69c0.
    
    * ci : trigger

.github/workflows/build.yml
.github/workflows/server.yml
CMakeLists.txt

commit 511182eabb36f6ec9776e2b3c4d7e16d93d0ac0d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 18 13:40:39 2024 +0300

    android : use "ci-android" branch for CI (#7341)
    
    * android : use "ci-android" branch for CI
    
    * ggml : disable SIMD exp and silu for 32-bit ARM
    
    ggml-ci
    
    * android : do not fetch, use add_subdirectory instead
    
    * cmake : provide binary dir

examples/llama.android/app/src/main/cpp/CMakeLists.txt
ggml.c

commit 133d99c59980139f5bb75922c8b5fca67d7ba9b8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 18 12:36:25 2024 +0200

    CUDA: deduplicate FlashAttention code (#7352)

ggml-cuda/common.cuh
ggml-cuda/fattn-common.cuh
ggml-cuda/fattn-tile-f16.cu
ggml-cuda/fattn-tile-f32.cu
ggml-cuda/fattn-vec-f16.cu
ggml-cuda/fattn-vec-f32.cu
ggml-cuda/fattn.cu
ggml-cuda/softmax.cu

commit cb42c294279bc4a0a4e926a7b5a5568049f12fa7
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 18 11:10:47 2024 +0200

    server: correct --threads documentation [no ci] (#7362)

examples/server/README.md

commit d233b507cd19fcc2d8d8963ecc6a3eb7a33f2ecc
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Sat May 18 02:05:17 2024 -0600

    cuda : add half2 __shfl_xor() for ROCm 5.5 (#7263)

ggml-cuda/common.cuh

commit 0f98acfac6cc561dc57586bfff778405e42b576b
Author: Steffen Röcker <sroecker@gmail.com>
Date:   Sat May 18 10:04:55 2024 +0200

    llama : add support for larger Granite Code Models (20B, 34B) (#7324)
    
    Tie the weights for ARCH_STARCODER to support the larger Granite code models.
    Partially addresses ggerganov/issues/7116
    
    There still remains to be a few things to fix.
    Currently requires `--override-kv tokenizer.ggml.add_bos_token=bool:false`

llama.cpp

commit ca57e0f35e33f714b9a6c2c4482b87bfe059c819
Author: strawberrymelonpanda <152940198+strawberrymelonpanda@users.noreply.github.com>
Date:   Sat May 18 00:57:08 2024 -0700

    perplexity : ndot progress and show stats with < 100 tasks (#7348)
    
    Fix floating point error with ndot printing, allow end stats on lower task numbers if multiple-choice tasks.

examples/perplexity/perplexity.cpp

commit c1b295eea5c49887a066559527a74e8b94fe9db0
Author: 0cc4m <picard12@live.de>
Date:   Sat May 18 08:10:58 2024 +0200

    Update and fix Vulkan soft_max and argsort implementations (#7237)
    
    * Update and fix Vulkan softmax implementation
    
    * Update and fix Vulkan argsort implementation

ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py

commit de731963441ff128248259e1b99573d75264d210
Author: Brian <mofosyne@gmail.com>
Date:   Sat May 18 16:04:23 2024 +1000

    github-actions-labeler: initial commit (#7330)
    
    * github-actions-labeler: initial commit [no ci]
    
    * github actions: remove priority auto labeling [no ci]

.github/labeler.yml
.github/workflows/labeler.yml

commit b49a13dd2fa9c94c2c19a8c248bb7fa45499f9a8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 18 08:46:20 2024 +0300

    convert : fix set_vocab_sentencepiece (#6866)
    
    * convert : fix set_vocab_sentencepiece
    
    * Update convert-hf-to-gguf.py

convert-hf-to-gguf.py

commit 05834841dcb4f922983ea976539c70472272df9a
Author: slaren <slarengh@gmail.com>
Date:   Sat May 18 02:39:54 2024 +0200

    ggml : fix quants nans when all the group weights are very close to zero (#7313)

ggml-quants.c
tests/test-backend-ops.cpp

commit ef277de2add255a08b2b909ebfbf70364d1f4dc4
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Fri May 17 18:39:25 2024 -0600

    cmake : fix typo in AMDGPU_TARGETS (#7356)

CMakeLists.txt

commit b43272afa29a64dcb8bcf26a96a05bac40792b92
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Sat May 18 01:09:13 2024 +0200

    Unicode codepoint flags for custom regexs (#7245)
    
    * Replace CODEPOINT_TYPE_* with codepoint_flags
    * Update and bugfix brute force random test
    * Deterministic brute force random test
    * Unicode normalization NFD
    * Get rid of BOM

llama.cpp
scripts/gen-unicode-data.py
tests/test-tokenizer-random.py
unicode-data.cpp
unicode-data.h
unicode.cpp
unicode.h

commit 0fc1e820a9900a3dd08ddd3c6abe6604c53b689b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri May 17 18:54:52 2024 +0200

    CUDA: faster large batch FA without tensor cores (#7314)

ggml-cuda/fattn-tile-f16.cu
ggml-cuda/fattn-tile-f16.cuh
ggml-cuda/fattn-tile-f32.cu
ggml-cuda/fattn-tile-f32.cuh
ggml-cuda/fattn-vec-f16.cu
ggml-cuda/fattn-vec-f32.cu
ggml-cuda/fattn.cu

commit 82ca83db3c8d45df559c03a4225b6eb34808a2db
Author: Gavin Zhao <gavinzhaojw@protonmail.com>
Date:   Fri May 17 11:03:03 2024 -0400

    ROCm: use native CMake HIP support (#5966)
    
    Supercedes #4024 and #4813.
    
    CMake's native HIP support has become the
    recommended way to add HIP code into a project (see
    [here](https://rocm.docs.amd.com/en/docs-6.0.0/conceptual/cmake-packages.html#using-hip-in-cmake)).
    This PR makes the following changes:
    
    1. The environment variable `HIPCXX` or CMake option
    `CMAKE_HIP_COMPILER` should be used to specify the HIP
    compiler. Notably this shouldn't be `hipcc`, but ROCm's clang,
    which usually resides in `$ROCM_PATH/llvm/bin/clang`. Previously
    this was control by `CMAKE_C_COMPILER` and `CMAKE_CXX_COMPILER`.
    Note that since native CMake HIP support is not yet available on
    Windows, on Windows we fall back to the old behavior.
    
    2. CMake option `CMAKE_HIP_ARCHITECTURES` is used to control the
    GPU architectures to build for. Previously this was controled by
    `GPU_TARGETS`.
    
    3. Updated the Nix recipe to account for these new changes.
    
    4. The GPU targets to build against in the Nix recipe is now
    consistent with the supported GPU targets in nixpkgs.
    
    5. Added CI checks for HIP on both Linux and Windows. On Linux, we test
    both the new and old behavior.
    
    The most important part about this PR is the separation of the
    HIP compiler and the C/C++ compiler. This allows users to choose
    a different C/C++ compiler if desired, compared to the current
    situation where when building for ROCm support, everything must be
    compiled with ROCm's clang.
    
    ~~Makefile is unchanged. Please let me know if we want to be
    consistent on variables' naming because Makefile still uses
    `GPU_TARGETS` to control architectures to build for, but I feel
    like setting `CMAKE_HIP_ARCHITECTURES` is a bit awkward when you're
    calling `make`.~~ Makefile used `GPU_TARGETS` but the README says
    to use `AMDGPU_TARGETS`. For consistency with CMake, all usage of
    `GPU_TARGETS` in Makefile has been updated to `AMDGPU_TARGETS`.
    
    Thanks to the suggestion of @jin-eld, to maintain backwards
    compatibility (and not break too many downstream users' builds), if
    `CMAKE_CXX_COMPILER` ends with `hipcc`, then we still compile using
    the original behavior and emit a warning that recommends switching
    to the new HIP support. Similarly, if `AMDGPU_TARGETS` is set but
    `CMAKE_HIP_ARCHITECTURES` is not, then we forward `AMDGPU_TARGETS`
    to `CMAKE_HIP_ARCHITECTURES` to ease the transition to the new
    HIP support.
    
    Signed-off-by: Gavin Zhao <git@gzgz.dev>

.devops/nix/package.nix
.github/workflows/build.yml
CMakeLists.txt
Makefile
README.md

commit f4bd8b3d260bb09491ba63c77ab7012b744362ef
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Fri May 17 17:25:44 2024 +0300

    rpc : set SO_REUSEADDR for the server socket (#7320)
    
    ref: #7293

examples/rpc/rpc-server.cpp
ggml-rpc.cpp

commit 51e9d02599336e62948d29f1d6c05addeb921ac2
Author: Brian <mofosyne@gmail.com>
Date:   Fri May 17 22:40:14 2024 +1000

    Added a single test function script and fix debug-test.sh to be more robust (#7279)
    
    * run-single-test.sh: added a single test function script and fix debug-test.sh to be more robust
    
    * debug-test.sh: combined execute and gdb test mode via -g flag
    
    * debug-test.sh: refactor
    
    * debug-test: refactor for clarity
    
    * debug-test.sh: comment style changes
    
    * debug-test.sh: fix gdb

docs/debugging-tests.md
scripts/debug-test.sh

commit d273c1402b25086fd91aef2467ac13f2e49fa0ea
Author: Aarni Koskela <akx@iki.fi>
Date:   Fri May 17 15:11:45 2024 +0300

    py : convert-hf-to-gguf-update improvements (#7340)
    
    * convert-hf-to-gguf-update: automate updating
    
    * convert-hf-to-gguf-update: improve download
    
    * share requests session for performance
    * create directories only when needed, don't skip downloads when empty directory encountered
    * be more graceful about errors

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py

commit 27b040691cbe45314147c2745e891a38e9c048d4
Author: fairydreaming <166155368+fairydreaming@users.noreply.github.com>
Date:   Fri May 17 13:24:38 2024 +0200

    llama : use n_embd_head_v when reshaping kqv (#7327)
    
    * llama : use n_embd_head_v instead of n_embd_head_k when reshaping kqv
    
    * llama : use n_embd_v_gqa and n_embd_head_v instead of n_embd_k_gqa and n_embd_head_k when making a view of cached value vectors.
    
    ---------
    
    Co-authored-by: Stanisław Szymczyk <sszymczy@gmail.com>

llama.cpp

commit 29c60d8cddcfd14fa8a6bf023a6c4eb8692c76ba
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri May 17 09:59:57 2024 +0200

    tokenization: add warning for double BOS (#7332)

llama.cpp

commit 359cbe3f46c90ce6f5151005e411b8fb74f8139e
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri May 17 07:08:49 2024 +0000

    ggml-quants, llama : removed excess checks (#7274)

common/common.cpp
ggml-quants.c
llama.cpp

commit e18bc6aaf3b547890609ed254ee5248e720e5840
Author: amd-lalithnc <lalithnc@amd.com>
Date:   Fri May 17 12:31:58 2024 +0530

    convert : fix Qwen/Qwen-7b conversion (#7308)

convert-hf-to-gguf.py

commit ee94172d33399d2e814ca05c8a3ff8c523ebb093
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Fri May 17 10:00:17 2024 +0300

    server : add support for the RPC backend (#7305)
    
    ref: #7292

examples/server/server.cpp

commit 934266c0e0b2aa9781fdba2deb112c161ff038a9
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Fri May 17 02:58:52 2024 -0400

    ggml : rewrite silu and softmax for cpu (#7154)
    
    This change upstreams llamafile's vectorized expf() functions. This lets
    us compute softmax and silu more accurately than the short[65536] lookup
    table that GGML previously used to make this operation go faster. We can
    support aarch64 and sse2+ with the worst case rounding error of 2ulp. It
    makes make -j8 tests && ./tests/test-backend-ops -o SOFT_MAX -b CPU perf
    go 1.5x faster for SSE2+FMA, 1.9x faster for AVX2+FMA and 2.1x on AVX512

ggml.c

commit 9c4fdcbec8c7fcc428e723b0d8a1cf1f351ba642
Author: Leon Knauer <git@leonknauer.com>
Date:   Fri May 17 02:11:03 2024 +0200

    [Server] Added --verbose option to README [no ci] (#7335)

examples/server/README.md

commit 24ecb58168dce81646c2ed425690a106591c8c6d
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu May 16 20:43:45 2024 +0200

    Revert "server bench: fix bench not waiting for model load (#7284)" (#7334)
    
    This reverts commit 583fd6b000ec9ad1b465b5c98524f4a0ae388077.

examples/server/bench/bench.py

commit 9afdffe70ebf3166d429b4434783bb0b7f97bdeb
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Wed May 15 16:04:40 2024 +0300

    rpc : get available mem for the CPU backend
    
    This can be overridden with the -m command line option
    
    ref: #7293

examples/rpc/rpc-server.cpp

commit 3b3963c55c8332e33533c44b2aa882b0e45f8292
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Wed May 15 15:29:07 2024 +0300

    rpc : add command line arg for specifying backend memory
    
    ref: #7293

examples/rpc/README.md
examples/rpc/rpc-server.cpp
ggml-rpc.cpp

commit dda64fc17c97820ea9489eb0cc9ae8b8fdce4926
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu May 16 02:15:23 2024 -0400

    convert : get general.name from model dir, not its parent (#5615)
    
    Co-authored-by: Brian <mofosyne@gmail.com>

convert.py

commit 0350f5815218c483fb3026a86adc44a115481625
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Thu May 16 06:14:24 2024 +0000

    grammar, json, llama: replace push on emplace if it possible (#7273)

common/grammar-parser.cpp
common/json-schema-to-grammar.cpp
llama.cpp

commit ad52d5c259344888b06fd5acd3344c663dd0621d
Author: Vaibhav Srivastav <vaibhavs10@gmail.com>
Date:   Thu May 16 07:38:43 2024 +0200

    doc: add references to hugging face GGUF-my-repo quantisation web tool. (#7288)
    
    * chore: add references to the quantisation space.
    
    * fix grammer lol.
    
    * Update README.md
    
    Co-authored-by: Julien Chaumond <julien@huggingface.co>
    
    * Update README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Julien Chaumond <julien@huggingface.co>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
examples/quantize/README.md

commit 172b78210aae0e54d3668c5de14200efab9fac23
Author: Max Krasnyansky <quic_maxk@quicinc.com>
Date:   Wed May 15 22:36:43 2024 -0700

    ci: fix bin/Release path for windows-arm64 builds (#7317)
    
    Switch to Ninja Multi-Config CMake generator to resurect bin/Release path
    that broke artifact packaging in CI.

.github/workflows/build.yml

commit 13ad16af1231ab2d245d35df3295bcfa23de1305
Author: Max Krasnyansky <max.krasnyansky@gmail.com>
Date:   Wed May 15 19:47:36 2024 -0700

    Add support for properly optimized Windows ARM64 builds with LLVM and MSVC (#7191)
    
    * logging: add proper checks for clang to avoid errors and warnings with VA_ARGS
    
    * build: add CMake Presets and toolchian files for Windows ARM64
    
    * matmul-int8: enable matmul-int8 with MSVC and fix Clang warnings
    
    * ci: add support for optimized Windows ARM64 builds with MSVC and LLVM
    
    * matmul-int8: fixed typos in q8_0_q8_0 matmuls
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * matmul-int8: remove unnecessary casts in q8_0_q8_0
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
CMakeLists.txt
CMakePresets.json
cmake/arm64-windows-llvm.cmake
cmake/arm64-windows-msvc.cmake
common/log.h
ggml-quants.c

commit 8f7080bf48828b538bc9387c3d150bbd4fb4cf2d
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed May 15 23:41:03 2024 +0200

    readme : remove stray double quote (#7310)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

README.md

commit e1b40ac3b94824d761b5e26ea1bc5692706029d9
Author: kunnis <kunnis@users.noreply.github.com>
Date:   Wed May 15 12:59:12 2024 -0500

    ggml : use dynamic thread scheduling for matrix multiplication (#6915)
    
    * Just reordering some structs.
    
    * Adding in the calls to mm_pause
    
    * Passing around the state
    
    * Renaming and moving a bunch of variables around.
    
    * Extracting the logic to it's own function.
    
    * Moving some variable definitions into the chunk function.
    
    * Moving some variables around
    
    * moving src1_cont inside
    
    * Moving row_size
    
    * adding the current_chunk
    
    * Reorg the code.
    
    * Formatting to match the orig patch
    
    * starting to setup the chunking variables
    
    * Starting the buildup of the loop
    
    * The yield shouldn't be necessary.
    
    * adding the looping structure based on the chunk configuration.
    
    * Add in the re-chunking code.
    
    * Making it much more likely to rechunk.
    
    * disable resizing if numa is enabled.
    
    * Updating comments with what we've learned.
    
    * Fix formatting
    
    * Couple more formatting fixes.
    
    * More style fixes.
    
    * Fix Warnings
    
    * Going with unused because there's conditional logic that needs it.
    
    * Update ggml.c
    
    * Update ggml.c
    
    ---------

ggml.c

commit dc020985b8755dd6aa93a2f002f43c3ede808cce
Author: agray3 <agray3@users.noreply.github.com>
Date:   Wed May 15 14:44:49 2024 +0100

    Avoid unnecessarily disabling CUDA graphs (#7302)
    
    As discussed in PR #6766, CUDA graphs were being disabled in the presence of long prompts.
    This fixes the issue by avoiding the consective update counter from incrementing unnecessarily
    for tokens in which cuda graphs are disabled due to batch size > 1.

ggml-cuda.cu

commit 344f9126cc0d15891fde9472fe40b8572628ad7d
Author: slaren <slarengh@gmail.com>
Date:   Wed May 15 15:08:48 2024 +0200

    ggml : tag ggml_tensor::backend as deprecated (#7290)

examples/llava/llava.cpp
ggml-backend.c
ggml.c
ggml.h

commit 9a17ab914b0aa7353389c656a3f2a0f086726868
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Wed May 15 13:26:30 2024 +0100

    Add missing " (#7303)

ggml-sycl.cpp

commit ea3b0590ee33d3573eb8ef76f88cc60f36d2a38d
Author: dm4 <sunrisedm4@gmail.com>
Date:   Wed May 15 20:01:12 2024 +0800

    embedding : free the batch after execution (#7297)

examples/embedding/embedding.cpp

commit 29499bb59383c2a8c5d557a90abb08b696cef7f6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 15 13:23:41 2024 +0300

    sync : ggml

scripts/sync-ggml.last

commit 48aa8fd1f213a69b41569f809cc954f24dbc4366
Author: John Balis <phobossystems@gmail.com>
Date:   Wed May 15 03:52:33 2024 -0500

    ggml : add `ggml_upscale_ext` (ggml/814)
    
    * initial commit with CPU implementation of upscale to shape and test, cuda implementation next
    
    * experimental commit to see if dst shape is correct
    
    * test version
    
    * test
    
    * removed unnecessary params
    
    * refactor
    
    * fixed tests
    
    * ggml : metal impl + cleanup + sycl dev warnings
    
    * patched ggml_upscale cuda op to handle non-contiguous tensors, added test for non-contiguous behavior
    
    * metal : fix upsacle op to support nb00 + style
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-cuda/upscale.cu
ggml-metal.m
ggml-metal.metal
ggml-sycl.cpp
ggml.c
ggml.h
tests/test-backend-ops.cpp

commit 583fd6b000ec9ad1b465b5c98524f4a0ae388077
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 15 08:44:16 2024 +0200

    server bench: fix bench not waiting for model load (#7284)

examples/server/bench/bench.py

commit 9f773486ab78d65f5cca3f7e31c862b7043bf721
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 14 19:14:38 2024 +0300

    script : sync ggml-rpc

scripts/sync-ggml-am.sh
scripts/sync-ggml.sh

commit e8a7fd4fb06d82f663850c21fcf86c0fb98ad9b4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 14 19:09:30 2024 +0300

    metal : support FA without mask + add asserts (#7278)
    
    * ggml : fa without mask + add asserts
    
    ggml-ci
    
    * metal : support non-contiguous KV
    
    ggml-ci

ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
tests/test-backend-ops.cpp

commit a5e3fde8578d54b98d941344a4da150669af200d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 14 15:33:16 2024 +0300

    sync : ggml
    
    ggml-ci

scripts/sync-ggml.last

commit f308ea705974dff62a1fe5367d776ad9d5109239
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 13 11:01:07 2024 +0300

    metal : tune soft_max number of threads (whisper/0)

ggml-metal.m

commit c3c88f296a72432edb697ac8026dbf2ec18f2b21
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 12 20:36:31 2024 +0300

    ggml : try fix ppc64 (whisper/0)

ggml-quants.c
ggml.c

commit 182adefcf36fc5f4263082ff032c0796fda65578
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Wed May 8 17:33:43 2024 +0200

    ggml : expose SSE3 and SSSE3 for MSVC when AVX is available (whisper/2128)

ggml-impl.h

commit 0d26d8ccd8caebab75af697c0275f599075fdacf
Author: Hong Bo PENG <penghb@cn.ibm.com>
Date:   Sun May 12 17:17:18 2024 +0800

    ggml : optimize for ppc64le using VSX intrinsics (ggml/784)
    
    * optimize for ppc64le using VSX intrinsics
    
    * 1. code clean up by removing comments about overflow concern.
    
    2. fix typo in suffix of scaling.
    
    * Continue to fix typo in suffix of scaling for QK_K <> 256
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-quants.c

commit 4f0263633b40e94e8b69fd6e7e4395cfedfd5c12
Author: Steve Grubb <ausearch.1@gmail.com>
Date:   Tue May 14 10:11:24 2024 -0400

    server: free sampling contexts on exit (#7264)
    
    * server: free sampling contexts on exit
    
    This cleans up last leak found by the address sanitizer.
    
    * fix whitespace
    
    * fix whitespace

examples/server/server.cpp

commit 1265c670fd8e41e1947352c96c5179adda97fb2c
Author: Brian <mofosyne@gmail.com>
Date:   Tue May 14 23:10:39 2024 +1000

    Revert "move ndk code to a new library (#6951)" (#7282)
    
    This reverts commit efc8f767c8c8c749a245dd96ad4e2f37c164b54c.

examples/llama.android/app/build.gradle.kts
examples/llama.android/app/src/main/cpp/CMakeLists.txt
examples/llama.android/app/src/main/cpp/llama-android.cpp
examples/llama.android/app/src/main/java/com/example/llama/Llm.kt
examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt
examples/llama.android/build.gradle.kts
examples/llama.android/llama/.gitignore
examples/llama.android/llama/consumer-rules.pro
examples/llama.android/llama/proguard-rules.pro
examples/llama.android/llama/src/androidTest/java/android/llama/cpp/ExampleInstrumentedTest.kt
examples/llama.android/llama/src/main/AndroidManifest.xml
examples/llama.android/llama/src/main/cpp/CMakeLists.txt
examples/llama.android/llama/src/test/java/android/llama/cpp/ExampleUnitTest.kt
examples/llama.android/settings.gradle.kts

commit 5e31828d3e35c76ecfee665bc23771a4bec1d130
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Tue May 14 14:27:19 2024 +0300

    ggml : add RPC backend (#6829)
    
    * ggml : add RPC backend
    
    The RPC backend proxies all operations to a remote server which runs a
    regular backend (CPU, CUDA, Metal, etc).
    
    * set TCP_NODELAY
    
    * add CI workflows
    
    * Address review comments
    
    * fix warning
    
    * implement llama_max_devices() for RPC
    
    * Address review comments
    
    * Address review comments
    
    * wrap sockfd into a struct
    
    * implement get_alignment and get_max_size
    
    * add get_device_memory
    
    * fix warning
    
    * win32 support
    
    * add README
    
    * readme : trim trailing whitespace
    
    * Address review comments
    
    * win32 fix
    
    * Address review comments
    
    * fix compile warnings on macos

.github/workflows/build.yml
CMakeLists.txt
common/common.cpp
common/common.h
examples/CMakeLists.txt
examples/rpc/CMakeLists.txt
examples/rpc/README.md
examples/rpc/rpc-server.cpp
ggml-rpc.cpp
ggml-rpc.h
llama.cpp
llama.h

commit 541600201e6480f54ae09e58d16b154d4b4b331d
Author: slaren <slarengh@gmail.com>
Date:   Tue May 14 09:33:42 2024 +0200

    llama : disable pipeline parallelism with nkvo (#7265)

llama.cpp

commit efc8f767c8c8c749a245dd96ad4e2f37c164b54c
Author: Elton Kola <eltonkola@gmail.com>
Date:   Tue May 14 03:30:30 2024 -0400

    move ndk code to a new library (#6951)

examples/llama.android/app/build.gradle.kts
examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt
examples/llama.android/build.gradle.kts
examples/llama.android/llama/.gitignore
examples/llama.android/llama/CMakeLists.txt
examples/llama.android/llama/consumer-rules.pro
examples/llama.android/llama/proguard-rules.pro
examples/llama.android/llama/src/androidTest/java/android/llama/cpp/ExampleInstrumentedTest.kt
examples/llama.android/llama/src/main/AndroidManifest.xml
examples/llama.android/llama/src/main/cpp/CMakeLists.txt
examples/llama.android/llama/src/main/cpp/llama-android.cpp
examples/llama.android/llama/src/main/java/android/llama/cpp/LLamaAndroid.kt
examples/llama.android/llama/src/test/java/android/llama/cpp/ExampleUnitTest.kt
examples/llama.android/settings.gradle.kts

commit e0f556186b6e1f2b7032a1479edf5e89e2b1bd86
Author: Haggai Nuchi <h.nuchi@gmail.com>
Date:   Mon May 13 22:25:56 2024 -0700

    Add left recursion check: quit early instead of going into an infinite loop (#7083)
    
    * Add left recursion check: quit early instead of going into an infinite loop
    
    * Remove custom enum, rename left recursion check and move to "grammar internal" section, add handling for edge case where a leftmost nonterminal may be empty
    
    * Remove unnecessary declaration

llama.cpp
tests/test-grammar-integration.cpp

commit 27f65d6267cf22a44c5ccefa7765d53a05bd1259
Author: Ryuei <louixs@users.noreply.github.com>
Date:   Tue May 14 14:20:47 2024 +0900

    docs: Fix typo and update description for --embeddings flag (#7026)
    
    - Change '--embedding' to '--embeddings' in the README
    - Update the description to match the latest --help output
    - Added a caution about defining physical batch size

examples/server/README.md

commit ee52225067622babc277371511b8124884e1c797
Author: compilade <git@compilade.net>
Date:   Mon May 13 14:10:51 2024 -0400

    convert-hf : support direct Q8_0 conversion (#7234)
    
    * convert-hf : support q8_0 conversion
    
    * convert-hf : add missing ftype
    
    This was messing with the checksums otherwise.
    
    * convert-hf : add missing ftype to Baichuan and Xverse
    
    I didn't notice these on my first pass.

convert-hf-to-gguf.py
gguf-py/gguf/__init__.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/lazy.py
gguf-py/gguf/quants.py

commit 614d3b914e1c3e02596f869649eb4f1d3b68614d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 13 17:15:15 2024 +0300

    llama : less KV padding when FA is off (#7257)
    
    ggml-ci

llama.cpp

commit 30e70334f71b3bd115024affcf98cac3d79aaa95
Author: k.h.lai <adrian.k.h.lai@outlook.com>
Date:   Mon May 13 22:02:36 2024 +0800

    llava-cli: fix base64 prompt (#7248)

examples/llava/llava-cli.cpp

commit 1c570d8beeebad95872dc738ea542a4a0022f78a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon May 13 13:03:27 2024 +0200

    perplexity: add BF16 vs. FP16 results (#7150)

examples/perplexity/README.md

commit 948f4ec7c5bff92b18e63303f2b2d1645bccd943
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Mon May 13 18:11:26 2024 +0800

    [SYCL] rm wait() (#7233)

ggml-sycl.cpp

commit 9aa672490c848e45eaa704a554e0f1f6df995fc8
Author: Joan Fontanals <joan.fontanals.martinez@jina.ai>
Date:   Mon May 13 10:35:14 2024 +0200

    llama : rename jina tokenizers to v2 (#7249)
    
    * refactor: rename jina tokenizers to v2
    
    * refactor: keep refactoring non-breaking

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
llama.cpp

commit b1f8af1886e8187db6bb2a9b87cfc1c0f175f629
Author: Brian <mofosyne@gmail.com>
Date:   Mon May 13 12:56:47 2024 +1000

    convert.py: Outfile default name change and additional metadata support (#4858)
    
    * convert.py: Outfile default name change and additional metadata support
    
    * convert.py: don't stringify Metadata load method output
    
    * convert.py: typo fix
    
    * convert.py: fix metadata format to sync with LLM_KV_NAMES in llama.cpp

convert.py

commit e586ee42595500c53938e937b6b6ad5353ad76dc
Author: Benjamin Findley <39356821+Kartoffelsaft@users.noreply.github.com>
Date:   Sun May 12 19:40:08 2024 -0700

    change default temperature of OAI compat API from 0 to 1 (#7226)
    
    * change default temperature of OAI compat API from 0 to 1
    
    * make tests explicitly send temperature to OAI API

examples/server/tests/features/steps/steps.py
examples/server/utils.hpp

commit cbf75894d256f1861f6409565db599365de3d4b8
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Mon May 13 08:04:29 2024 +0800

    [SYCL] Add oneapi runtime dll files to win release package (#7241)
    
    * add oneapi running time dlls to release package
    
    * fix path
    
    * fix path
    
    * fix path
    
    * fix path
    
    * fix path
    
    ---------
    
    Co-authored-by: Zhang <jianyu.zhang@intel.com>

.github/workflows/build.yml

commit 0d5cef78aeafae4d4e6d56e2d4bcda771af58cc9
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Mon May 13 08:02:55 2024 +0800

    [SYCL] update CI with oneapi 2024.1 (#7235)
    
    Co-authored-by: Zhang <jianyu.zhang@intel.com>

.github/workflows/build.yml

commit dc685be46622a8fabfd57cfa804237c8f15679b8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 12 19:40:45 2024 +0200

    CUDA: add FP32 FlashAttention vector kernel (#7188)
    
    * CUDA: add FP32 FlashAttention vector kernel
    
    * fixup! CUDA: add FP32 FlashAttention vector kernel
    
    * fixup! fixup! CUDA: add FP32 FlashAttention vector kernel
    
    * fixup! fixup! fixup! CUDA: add FP32 FlashAttention vector kernel

ggml-cuda.cu
ggml-cuda/common.cuh
ggml-cuda/fattn-common.cuh
ggml-cuda/fattn-vec-f16.cu
ggml-cuda/fattn-vec-f16.cuh
ggml-cuda/fattn-vec-f32.cu
ggml-cuda/fattn-vec-f32.cuh
ggml-cuda/fattn.cu
tests/test-backend-ops.cpp

commit 6f1b63606fc68a09d62d1d74dbd156c35219026d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 12 18:30:23 2024 +0300

    cmake : fix version cmp (#7227)

CMakeLists.txt

commit b228aba91ac2cd9eb90e9d423ba1d0d20e0117e2
Author: slaren <slarengh@gmail.com>
Date:   Sun May 12 02:29:33 2024 +0200

    remove convert-lora-to-ggml.py (#7204)

CMakeLists.txt
ci/run.sh
convert-lora-to-ggml.py
requirements.txt
requirements/requirements-convert-lora-to-ggml.txt

commit 7bd4ffb78062587e4012a1c24186223f09b1bc70
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 21:36:20 2024 +0300

    metal : fix warnings (skipme) (#0)

ggml-metal.metal
scripts/sync-ggml.last

commit 1622ac023f42e5e01c163321cd98c6596aa9402d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 21:35:05 2024 +0300

    sync : ggml

scripts/sync-ggml.last

commit 6aeff24f8b91e145e92d17ec7ce3adc4ef60b8e9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 16:57:53 2024 +0300

    metal : fix indent (ggml/0)

ggml-metal.m

commit 325756d28df7d018a7bac424e1b3bc8acb4ecf07
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 16:25:50 2024 +0300

    ggml : resolve merge (ggml/0)
    
    ggml-ci

ggml.c

commit fed0108491a3a3cbec6c6480dc8667ffff9d7659
Author: Josh Ramer <josh.ramer@icloud.com>
Date:   Sat May 11 12:26:35 2024 -0500

    Scripting & documenting debugging one test without anything else in the loop. (#7096)
    
    * A little documentation that shares my quick tips for working in the repository.
    
    * Update startup-testing-debugging.md
    
    * script that shows a menu of tests to pick from & run the debugger on
    
    * debug-test.sh: Refactor CLI help message
    
    * debug-test.sh: documentation update
    
    * debug-test.sh: CLI Help output corrections
    
    * debug-test.sh: minor doc fix
    
    ---------
    
    authored-by: Josh Ramer <ubuntu@ip-172-31-32-53.ec2.internal>
    Assisted-by: brian khuu <mofosyne@gmail.com>

docs/debugging-tests.md
scripts/debug-test.sh

commit 72c177c1f6c16693eee319d4ebd4eaab5e630dd2
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat May 11 17:28:10 2024 +0200

    fix system prompt handling (#7153)

examples/server/server.cpp

commit 5a419926b0c4efab0531401aea91522aaea9fd07
Author: compilade <git@compilade.net>
Date:   Sat May 11 11:06:26 2024 -0400

    convert-hf : support bfloat16 conversion (#7158)
    
    * convert-hf : support bfloat16 conversion
    
    * gguf-py : flake8 fixes
    
    * convert-hf : add missing space after comma
    
    * convert-hf : get bit-exact same output as ./quantize
    
    The quantization version was missing.
    
    * convert-hf : don't round bf16 NANs
    
    * convert-hf : save some memory with np.int16 intermediate bf16 weights
    
    * convert-hf : more closely match llama.cpp with which weights to keep in f32
    
    * convert-hf : add --outtype auto-f16
    
    A reason for this to exist is for model quantizers who want an initial
    GGUF with the most fidelity to the original model while still using
    a 16-bit float type instead of 32-bit floats.
    
    * convert-hf : remove a semicolon because flake8 doesn't like it
    
    It's a reflex from when programming in C/C++, I guess.
    
    * convert-hf : support outtype templating in outfile name
    
    * convert-hf : rename --outtype auto-f16 to --outtype auto

convert-hf-to-gguf.py
gguf-py/gguf/__init__.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/lazy.py

commit fae9d234b6606693704eca62fe4aefbb6c6abb45
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 12:02:39 2024 +0300

    sync : ggml
    
    ggml-ci

scripts/sync-ggml.last

commit f5ef34e428f3886544590ecb2d532e4d333c114c
Author: Justina Cho <justcho5@gmail.com>
Date:   Wed May 1 14:44:26 2024 -0700

    feat: implemented sigmoid function (ggml/806)
    
    * added sigmoid function
    
    * implemented metal kernel for sigmoid
    
    * implemented cuda kernel for sigmoid
    
    * added sigmoid unary op and incremented count

ggml-cuda.cu
ggml-cuda/unary.cu
ggml-cuda/unary.cuh
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h

commit ef0d5e3ec9f99003af3ff326384816c02850ea3f
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Thu Apr 25 17:24:07 2024 +0300

    build: fix and ignore msvc warnings (ggml/805)

ggml-backend.c
ggml-quants.c

commit 3292733f95d4632a956890a438af5192e7031c12
Author: CrispStrobe <154636388+CrispStrobe@users.noreply.github.com>
Date:   Sat May 11 10:18:35 2024 +0200

    convert : skip unaccessible HF repos (#7210)

convert-hf-to-gguf-update.py

commit 988631335a20d06497f58be0b8ba13adb4323a22
Author: Steve Grubb <ausearch.1@gmail.com>
Date:   Sat May 11 04:13:02 2024 -0400

    server : free llama_batch on exit (#7212)
    
    * [server] Cleanup a memory leak on exit
    
    There are a couple memory leaks on exit of the server. This hides others.
    After cleaning this up, you can see leaks on slots. But that is another
    patch to be sent after this.
    
    * make tab into spaces

examples/server/server.cpp

commit f99e1e456eaf69cc38c1982a2693ce41c0f897ef
Author: Haoxiang Fei <tonyfettes@tonyfettes.com>
Date:   Sat May 11 16:12:06 2024 +0800

    llama : lookup word in vocab before doing BPE merges (#7193)
    
    * fix: llama-3 ignore_merges
    
    * test: add test for llama-3 bpe ignore_merges
    
    * fix: set ignore_merges only for llama-3
    
    * fix: test-tokenizer-1-bpe --ingore-merges detection
    
    * fix: copy to fix fallthrough
    
    * fix: change ignore_merges to bool
    
    * fix: add ignore merges tests to cmake
    
    * llama : alternative merge ignore logic
    
    ---------
    
    Co-authored-by: Haoxiang Fei <feihaoxiang@idea.edu.cn>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp
models/ggml-vocab-llama-bpe.gguf.inp
models/ggml-vocab-llama-bpe.gguf.out
tests/CMakeLists.txt
tests/test-tokenizer-1-bpe.cpp

commit 5ae3426b0b64672991563d4c28b2018b9f961467
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 11 10:11:28 2024 +0200

    server: fix reported top tokens for temperature 0 (#7203)

common/sampling.cpp
common/sampling.h
examples/server/server.cpp

commit b83cc3f5b303ff30c52874b2d5864dc6385ebf9f
Author: Joan Fontanals <jfontanalsmartinez@gmail.com>
Date:   Sat May 11 09:46:09 2024 +0200

    llama : add Jina Embeddings architecture (#6826)
    
    * feat: first things to do
    
    * feat: create tensors for Jina architecture
    
    * fix: use other tensors
    
    * feat: embedding gets results
    
    * fix: fix usage of ALIBI
    
    * fix: clean prints
    
    * fix: do some cleanup unused vars
    
    * fix: revert changes to Makefile and CMakeLists
    
    * fix: revert some changes
    
    * fix: fix small detail
    
    * fix: fix convert formatting
    
    * fix: fix linting and editor
    
    * feat: set proper vocab settings
    
    * fix: JinaBertForMaskedLM registration
    
    * feat: support q_normalization and k_normalization in Jina arch
    
    * feat: handle gpt2 tokenizer with Jina architecture
    
    * feat: example comments in embedding
    
    * feat: rename Jina Bert to Jina Bert V2
    
    * fix: add some changes as per review
    
    * feat: proper KQ_pos for Jina embeddings
    
    * feat: add capacity to load models ES and DE for Spanish
    
    * llama : fix pre-tokenizers
    
    * ggml : full ALiBi support
    
    * ggml : update ggml_soft_max_ext() CUDA, SYCL
    
    * ggml : ggml_flash_attn_ext() support ALiBi (CPU)
    
    * ggml : ggml_flash_attn_ext() support ALiBi (Metal)
    
    * ggml : fix warning
    
    * ggml : ggml_flash_attn_ext() support ALiBi (CUDA)
    
    ggml-ci
    
    * minor : clean-up
    
    * embedding : add warning about missing SEP
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
examples/embedding/embedding.cpp
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 9cb317f77e53067f7a138cc89ef7657148eae8e6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 11 10:32:41 2024 +0300

    ggml : full ALiBi support (#7192)
    
    * ggml : full ALiBi support
    
    * ggml : update ggml_soft_max_ext() CUDA, SYCL
    
    * ggml : ggml_flash_attn_ext() support ALiBi (CPU)
    
    * ggml : ggml_flash_attn_ext() support ALiBi (Metal)
    
    * ggml : fix warning
    
    * ggml : ggml_flash_attn_ext() support ALiBi (CUDA)
    
    ggml-ci
    
    * ggml : fix assert message
    
    * vulkan : add dev notes
    
    * ggml : require mask when using ALiBi
    
    ggml-ci
    
    * convert : fix convert for refact models

convert-hf-to-gguf.py
ggml-cuda.cu
ggml-cuda/alibi.cu
ggml-cuda/alibi.cuh
ggml-cuda/fattn.cu
ggml-cuda/softmax.cu
ggml-kompute.cpp
ggml-metal.m
ggml-metal.metal
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
ggml.h
gguf-py/gguf/tensor_mapping.py
llama.cpp
tests/test-backend-ops.cpp

commit e849648888a11de13aaaa4cb2eda3f5a9c7b444d
Author: slaren <slarengh@gmail.com>
Date:   Fri May 10 18:03:54 2024 +0200

    llama-bench : add pp+tg test type (#7199)

examples/llama-bench/README.md
examples/llama-bench/llama-bench.cpp
scripts/compare-llama-bench.py

commit 18e437665ce626dddbd79119aa7498493e7cb13b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 10 18:20:10 2024 +0300

    metal : fix flash attention kernel requirements (#7169)
    
    * metal : fix flash attention kernel requirements
    
    ggml-ci
    
    * metal : fix ggml_metal_supports_op
    
    ggml-ci

ggml-metal.m

commit 8c660242d708d3913a2adc2b6e4a9ee9cf5e4ce7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 10 17:53:04 2024 +0300

    convert : print "ignore_merges" field

convert-hf-to-gguf-update.py

commit 25c6e82e7a1ad25a42b0894e87d9b5c557409516
Author: slaren <slarengh@gmail.com>
Date:   Fri May 10 14:28:01 2024 +0200

    llama : use n_vocab to differentiate between mistral 7B and llama3 8B (#7200)

llama.cpp

commit 4e3880978f8b1bf546dd4e6f3b524d6b8739c49c
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Fri May 10 07:01:08 2024 -0400

    Fix memory bug in grammar parser (#7194)
    
    The llama.cpp grammar parser had a bug where forgetting to add a closing
    quotation mark to strings would cause parsing to crash. Anyone running a
    server on a public endpoint is advised to upgrade. To reproduce this bug
    
        ./llamafile -m foo.gguf -p bar --grammar 'root::="'
    
    Credit for discovering and reporting this issue goes to Eclypsium
    Security Researcher Richard Johnson <Richard.johnson@eclypsium.com>.

common/common.cpp
common/grammar-parser.cpp
examples/llava/llava-cli.cpp
examples/main/main.cpp

commit f89fe2732c5709f6e86d5f4aee2e6d2a561f2eb2
Author: HanishKVC <hanishkvc@gmail.com>
Date:   Fri May 10 15:51:58 2024 +0530

    Main+: optionally allow special tokens from user in interactive mode (#7097)
    
    @hanishkvc added a new `--interactive-specials` flag which would allow for inserting special tokens from user side into the embedding stream.

common/common.cpp
common/common.h
examples/main/main.cpp

commit d11afd665241c1b3910ab5f040d0216403019d87
Author: Andrei <abetlen@gmail.com>
Date:   Fri May 10 02:41:10 2024 -0400

    llava : fix moondream support (#7163)
    
    * Revert "Revert "llava : add support for moondream vision language model (#6899)""
    
    This reverts commit 9da243b36ac0b9d609adfaaa4c8f1cc8c592f737.
    
    * Fix num_positions and embeddings initialization

README.md
examples/llava/clip.cpp

commit 8c570c9496212073079476651c7517c02581101f
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Fri May 10 01:32:15 2024 +0100

    Minor arithmetic improvement to mmvq wrapper kernel (#7172)

ggml-sycl.cpp

commit eaf4bd8b399a6ed4b834f91d22409d2a05c4d266
Author: slaren <slarengh@gmail.com>
Date:   Fri May 10 01:04:12 2024 +0200

    eval-callback : fix conversion to float (#7184)

examples/eval-callback/eval-callback.cpp

commit befddd0f15de6efb15d7e7f5b527dfb671f4196f
Author: 0cc4m <picard12@live.de>
Date:   Thu May 9 20:39:54 2024 +0200

    Vulkan Bugfixes and Improvements (#7084)
    
    * Modify mat mat mul shader for mul_mat_id, modify mat vec mul shaders for single call batch operation
    
    * Further work towards MoE, disabled for now
    
    * Disable MoE code (not ready yet), fix a number of bugs in shaders and Vulkan code
    
    * Add softmax with f16 mask and pos buffer support
    
    * Disable mul_mat_id shaders for now
    
    * Fix flake8
    
    * Fix validation errors caused by empty buffers on larger batch sizes

ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py

commit d46dbc76f8770caec0175f1e57777173c70556a0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu May 9 16:40:42 2024 +0300

    readme : add scheduled server workflow status badge

README.md

commit 0961d866044143eefec5b525e991611f73fd4f6e
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Thu May 9 22:32:40 2024 +0900

    readme : add app (#6371)
    
    * added Layla to supported UIs
    
    * Update README.md

README.md

commit 43248e559472556f368988575d9fba906b3eb139
Author: jaime-m-p <167997752+jaime-m-p@users.noreply.github.com>
Date:   Thu May 9 15:30:44 2024 +0200

    llama3 custom regex split (#6965)
    
    * merged the changes from deepseeker models to main branch
    
    * Moved regex patterns to unicode.cpp and updated unicode.h
    
    * Moved header files
    
    * Resolved issues
    
    * added and refactored unicode_regex_split and related functions
    
    * Updated/merged the deepseek coder pr
    
    * Refactored code
    
    * Adding unicode regex mappings
    
    * Adding unicode regex function
    
    * Added needed functionality, testing remains
    
    * Fixed issues
    
    * Fixed issue with gpt2 regex custom preprocessor
    
    * unicode : fix? unicode_wstring_to_utf8
    
    * lint : fix whitespaces
    
    * tests : add tokenizer tests for numbers
    
    * unicode : remove redundant headers
    
    * tests : remove and rename tokenizer test scripts
    
    * tests : add sample usage
    
    * gguf-py : reader prints warnings on duplicate keys
    
    * llama : towards llama3 tokenization support (wip)
    
    * unicode : shot in the dark to fix tests on Windows
    
    * unicode : first try custom implementations
    
    * convert : add "tokenizer.ggml.pre" GGUF KV (wip)
    
    * llama : use new pre-tokenizer type
    
    * convert : fix pre-tokenizer type writing
    
    * lint : fix
    
    * make : add test-tokenizer-0-llama-v3
    
    * wip
    
    * models : add llama v3 vocab file
    
    * llama : adapt punctuation regex + add llama 3 regex
    
    * minor
    
    * unicode : set bomb
    
    * unicode : set bomb
    
    * unicode : always use std::wregex
    
    * unicode : support \p{N}, \p{L} and \p{P} natively
    
    * unicode : try fix windows
    
    * unicode : category support via std::regex
    
    * unicode : clean-up
    
    * unicode : simplify
    
    * llama3 custom regex split
    
    * convert : add convert-hf-to-gguf-update.py
    
    ggml-ci
    
    * lint : update
    
    * convert : add falcon
    
    ggml-ci
    
    * unicode : normalize signatures
    
    * lint : fix
    
    * lint : fix
    
    * convert : remove unused functions
    
    * convert : add comments
    
    * convert : exercise contractions
    
    ggml-ci
    
    * Using char32_t for codepoints
    
    * lint : fix
    
    * already exists unicode_tolower()
    
    * Typing
    
    * Restore BOM
    
    * cmake : refactor test targets
    
    * tests : refactor vocab tests
    
    ggml-ci
    
    * tests : add more vocabs and tests
    
    ggml-ci
    
    * unicode : cleanup
    
    * scripts : ignore new update script in check-requirements.sh
    
    * Fix merge
    
    * models : add phi-3, mpt, gpt-2, starcoder
    
    * tests : disable obsolete
    
    ggml-ci
    
    * tests : use faster bpe test
    
    ggml-ci
    
    * llama : more prominent warning for old BPE models
    
    * tests : disable test-tokenizer-1-bpe due to slowness
    
    ggml-ci
    
    * Move unused variable value
    
    * GPT2 custom regex split
    
    * Add alternative regex for custom aplit llama3
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Style
    
    * Add bruteforce random tests for token encoding
    
    * wip: fixing unicode codepoint ranges
    
    * Fix merge
    
    * Unicode tables: separator, lowercase, uppercase and whitespace
    
    * llama3 custom regex split: fix \s
    
    * Restore BOM
    
    * Style
    
    * wip: generate NDF table
    
    * Ignore special tokens for testing
    
    * Clean gen-unicode-data.py
    
    * Refactor random tokenizer test
    
    * lint : fix
    
    * tests : add fail test for llama-bpe
    
    ---------
    
    Co-authored-by: Jaggzh <jaggz.h@gmail.com>
    Co-authored-by: Kazim Abrar Mahi <kazimabrarmahi135@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: jaime-m-p <>

convert-hf-to-gguf-update.py
llama.cpp
scripts/gen-unicode-data.py
tests/test-tokenizer-random.py
unicode-data.cpp
unicode-data.h
unicode.cpp
unicode.h

commit a743d76a01f23038b2c85af1e9048ee836767b44
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu May 9 14:32:02 2024 +0200

    CUDA: generalize FP16 fattn vec kernel (#7061)
    
    * CUDA: generalize FP16 fattn vec kernel
    
    * disable unsupported head sizes for AMD in test
    
    * try AMD fix
    
    * fix batch size 2-8
    
    * partially revert changes

ggml-cuda/common.cuh
ggml-cuda/fattn.cu
llama.cpp
tests/test-backend-ops.cpp

commit f31ec120bc36c6270e4948e6a065a7c4cfa0c404
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu May 9 14:13:05 2024 +0200

    Add warning if token is invalid (#7173)

convert-hf-to-gguf-update.py

commit fd9f92b154850014146f61717cd292a59a5cee5a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu May 9 13:03:29 2024 +0200

    llama : update llama_timings.n_p_eval setting (#7160)
    
    This commit changes the value assigned to llama_timings.n_p_eval when
    ctx->n_p_eval is 0 to be 1 instead of 1 which is the current value.
    
    The motivation for this change is that if session caching is enabled,
    for example using the `--prompt-cache main-session.txt` command line
    argument for the main example, and if the same prompt is used then on
    subsequent runs, the prompt tokens will not actually be passed to
    llama_decode, and n_p_eval will not be updated by llama_synchoronize.
    
    But the value of n_p_eval will be set 1 by llama_get_timings because
    ctx->n_p_eval will be 0. This could be interpreted as 1 token was
    evaluated for the prompt which could be misleading for applications
    using this value.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

llama.cpp

commit 22842164bcae3251b81ad9e497a16ef66833cb9e
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Thu May 9 12:56:00 2024 +0200

    gguf-py : add special token modification capability (#7166)
    
    * Add special token modification capability
    
    To be able to fix/amend special tokens in a GGUF let's add two new arguments:
    * `--special-token <name> <value>` where `<name>` can be bos, eos, prefix, middle, etc. while `<value>` is the token value, f.ex. `"<｜fim▁begin｜>"`
    * `--special-token-by-id <name> <id>` where `<id>` is the ID of the token, f.ex. 32006
    
    So, in order to f.ex. add fill-in-middle tokens to a GGUF you would do the following:
    ```bash
    python3 gguf-new-metadata.py input.gguf output.gguf --special-token prefix "<｜fim▁begin｜>" --special-token middle "<｜fim▁hole｜>" --special-token suffix "<｜fim▁end｜>"
    ```
    
    * improve help text
    
    * flake--
    
    * fix multiple tokens warning
    
    * make script executable
    
    * switch to namedtuple, no need to dataclass
    
    * typing++
    
    * add progress bar
    
    * Add special token modification capability
    
    To be able to fix/amend special tokens in a GGUF let's add two new arguments:
    * `--special-token <name> <value>` where `<name>` can be bos, eos, prefix, middle, etc. while `<value>` is the token value, f.ex. `"<｜fim▁begin｜>"`
    * `--special-token-by-id <name> <id>` where `<id>` is the ID of the token, f.ex. 32006
    
    So, in order to f.ex. add fill-in-middle tokens to a GGUF you would do the following:
    ```bash
    gguf-new-metadata.py input.gguf output.gguf --special-token prefix "<｜fim▁begin｜>" --special-token middle "<｜fim▁end｜>" --special-token suffix "<｜fim▁hole｜>"
    ```
    (yes, fim_end is the `middle` token, because completion is a `prefix`/`suffix`/`middle` sequence (where `middle` is unfilled))
    or
    ```bash
    gguf-new-metadata.py input.gguf output.gguf --special-token prefix "<fim_prefix>" --special-token middle "<fim_middle>" --special-token suffix "<fim_suffix>"
    ```
    etc...
    
    NB: The tokens have to exist already, trying to add non-existent token name/IDs will be ignored (with a warning), while non-existent values will fail (with an error).
    
    * improve help text
    
    * flake--
    
    * fix multiple tokens warning
    
    * make script executable
    
    * switch to namedtuple, no need to dataclass
    
    * typing++
    
    * add progress bar
    
    * fail on invalid token id

gguf-py/scripts/gguf-new-metadata.py

commit 47345248827f426038098d016ed11975021b4919
Author: Albert Jin <albert.jin@gmail.com>
Date:   Thu May 9 17:34:37 2024 +0800

    opencl : alignment size converted from bits to bytes (#7090)
    
    * opencl alignment size should be converted from bits to bytes
    
    Reference: https://registry.khronos.org/OpenCL/specs/3.0-unified/html/OpenCL_API.html#CL_DEVICE_MEM_BASE_ADDR_ALIGN
    
    > Alignment requirement (in bits) for sub-buffer offsets.
    
    * Update ggml-opencl.cpp for readability using division instead of shift
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

ggml-opencl.cpp

commit 07cd41d0965829463eff73eda3348aedbfd3a444
Author: Ahmet Zeer <ahmed.zeer@std.yildiz.edu.tr>
Date:   Thu May 9 11:16:45 2024 +0300

    TypoFix (#7162)

examples/convert-llama2c-to-ggml/README.md

commit 4426e2987b566f09c7aa96ada9706cc778637620
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed May 8 19:55:32 2024 -0400

    cmake : fix typo (#7151)

CMakeLists.txt

commit f98eb31c517c95960df1d0abc48002787f145f3b
Author: compilade <git@compilade.net>
Date:   Wed May 8 18:16:38 2024 -0400

    convert-hf : save memory with lazy evaluation (#7075)
    
    * convert-hf : begin refactoring write_tensor
    
    * convert : upgrade to sentencepiece v0.2.0
    
    * convert-hf : remove unused n_dims in extra_*_tensors
    
    * convert-hf : simplify MoE weights stacking
    
    * convert-hf : flake8 linter doesn't like semicolons
    
    * convert-hf : allow unusual model part names
    
    For example, loading `model-00001-of-00001.safetensors` now works.
    
    * convert-hf : fix stacking MoE expert tensors
    
    `torch.stack` and `torch.cat` don't do the same thing.
    
    * convert-hf : fix Mamba conversion
    
    Tested to work even with a SentencePiece-based tokenizer.
    
    * convert : use a string for the SentencePiece tokenizer path
    
    * convert-hf : display tensor shape
    
    * convert-hf : convert norms to f32 by default
    
    * convert-hf : sort model part names
    
    `os.listdir` is said to list files in arbitrary order.
    Sorting the file names should let "model-00009-of-00042.safetensors"
    be loaded before "model-00010-of-00042.safetensors".
    
    * convert-hf : use an ABC for Model again
    
    It seems Protocol can't be used as a statically type-checked ABC,
    because its subclasses also can't be instantiated. (why did it seem to work?)
    
    At least there's still a way to throw an error when forgetting to define
    the `model_arch` property of any registered Model subclasses.
    
    * convert-hf : use a plain class for Model, and forbid direct instantiation
    
    There are no abstract methods used anyway,
    so using ABC isn't really necessary.
    
    * convert-hf : more consistent formatting of cmdline args
    
    * convert-hf : align the message logged for converted tensors
    
    * convert-hf : fix Refact conversion
    
    * convert-hf : save memory with lazy evaluation
    
    * convert-hf : flake8 doesn't like lowercase L as a variable name
    
    * convert-hf : remove einops requirement for InternLM2
    
    * convert-hf : faster model parts loading
    
    Instead of pre-loading them all into a dict, iterate on the tensors
    in the model parts progressively as needed in Model.write_tensors
    
    Conversion for some architectures relies on checking for the presence
    of specific tensor names, so for multi-part models, the weight map is read
    from the relevant json file to quickly get these names up-front.
    
    * convert-hf : minor changes for consistency
    
    * gguf-py : add tqdm as a dependency
    
    It's small, and used for a progress bar
    in GGUFWriter.write_tensors_to_file

convert-hf-to-gguf.py
convert.py
examples/server/tests/features/steps/steps.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_reader.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/vocab.py
gguf-py/pyproject.toml
gguf-py/scripts/gguf-dump.py
gguf-py/scripts/gguf-new-metadata.py
pyrightconfig.json
requirements/requirements-convert-hf-to-gguf-update.txt
requirements/requirements-convert-hf-to-gguf.txt
requirements/requirements-convert.txt

commit bc4bba364fb96d908f2698e908648df5e6f55e02
Author: agray3 <agray3@users.noreply.github.com>
Date:   Wed May 8 21:55:49 2024 +0100

    Introduction of CUDA Graphs to LLama.cpp (#6766)
    
    * DRAFT: Introduction of CUDA Graphs to LLama.cpp
    
    * FIx issues raised in comments
    
    * Tidied to now only use CUDA runtime (not mixed with driver calls)
    
    * disable for multi-gpu and batch size > 1
    
    * Disable CUDA graphs for old GPU arch and with env var
    
    * added missing CUDA_CHECKs
    
    * Addressed comments
    
    * further addressed comments
    
    * limit to GGML_ALLOW_CUDA_GRAPHS defined in llama.cpp cmake
    
    * Added more comprehensive graph node checking
    
    * With mechanism to fall back if graph capture fails
    
    * Revert "With mechanism to fall back if graph capture fails"
    
    This reverts commit eb9f15fb6fcb81384f732c4601a5b25c016a5143.
    
    * Fall back if graph capture fails and address other comments
    
    * - renamed GGML_ALLOW_CUDA_GRAPHS to GGML_CUDA_USE_GRAPHS
    
    - rename env variable to disable CUDA graphs to GGML_CUDA_DISABLE_GRAPHS
    
    - updated Makefile build to enable CUDA graphs
    
    - removed graph capture failure checking in ggml_cuda_error
      using a global variable to track this is not thread safe, but I am also not safistied with checking an error by string
      if this is necessary to workaround some issues with graph capture with eg. cuBLAS, we can pass the ggml_backend_cuda_context to the error checking macro and store the result in the context
    
    - fixed several resource leaks
    
    - fixed issue with zero node graphs
    
    - changed fixed size arrays to vectors
    
    - removed the count of number of evaluations before start capturing, and instead changed the capture mode to relaxed
    
    - removed the check for multiple devices so that it is still possible to use a single device, instead checks for split buffers to disable cuda graphs with -sm row
    
    - changed the op for checking batch size to GGML_OP_ADD, should be more reliable than GGML_OP_SOFT_MAX
    
    - code style fixes
    
    - things to look into
      - VRAM usage of the cudaGraphExec_t, if it is significant we may need to make it optional
      - possibility of using cudaStreamBeginCaptureToGraph to keep track of which ggml graph nodes correspond to which cuda graph nodes
    
    * fix build without cuda graphs
    
    * remove outdated comment
    
    * replace minimum cc value with a constant
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

CMakeLists.txt
Makefile
ggml-cuda.cu
ggml-cuda/clamp.cu
ggml-cuda/common.cuh
ggml-cuda/convert.cu
ggml-cuda/cpy.cu
ggml-cuda/cpy.cuh
ggml-cuda/mmq.cu
ggml-cuda/mmvq.cu
ggml-cuda/scale.cu

commit c12452c7aec8a02264afc00196a13caa591a13ac
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 8 21:53:08 2024 +0200

    JSON: [key] -> .at(key), assert() -> GGML_ASSERT (#7143)

common/common.cpp
common/json-schema-to-grammar.h
examples/server/server.cpp
examples/server/utils.hpp
tests/test-json-schema-to-grammar.cpp

commit 9da243b36ac0b9d609adfaaa4c8f1cc8c592f737
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 8 22:14:39 2024 +0300

    Revert "llava : add support for moondream vision language model (#6899)"
    
    This reverts commit 46e12c4692a37bdd31a0432fc5153d7d22bc7f72.

README.md
examples/llava/clip.cpp

commit bd1871fa2b1bf8a081b43ba9bc85f8ffd46fac46
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Wed May 8 20:12:06 2024 +0100

    server : add themes + favicon (#6848)
    
    * Added themes support with two sample themes and a favicon.
    
    * Newline
    
    * Newline
    
    * Newline
    
    * Trailing whitespace
    
    * Increased opacity for contrast
    
    * Increase opacity.
    
    Check actions cancelled for some other priority job and I can't seem to manually re-run them, so MOAR OPACITY
    
    * Opacity action trigger.
    
    Trying to re-trigger the cancelled action.
    
    * One more opacity adjustment
    
    This Actions pipeline is failing for random issues.
    
    * Delete examples/server/themes/buttons_top/completion.js
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/buttons_top/index.js
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/wild/completion.js
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/buttons_top/json-schema-to-grammar.mjs
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/wild/index.js
    
    This will be served from the static string built-in to server.
    
    * Delete examples/server/themes/wild/json-schema-to-grammar.mjs
    
    This will be served from the static string built-in to server.
    
    * Replaced underscore.

examples/server/public/favicon.ico
examples/server/themes/README.md
examples/server/themes/buttons-top/README.md
examples/server/themes/buttons-top/buttons_top.png
examples/server/themes/buttons-top/favicon.ico
examples/server/themes/buttons-top/index.html
examples/server/themes/wild/README.md
examples/server/themes/wild/favicon.ico
examples/server/themes/wild/index.html
examples/server/themes/wild/llama_cpp.png
examples/server/themes/wild/llamapattern.png
examples/server/themes/wild/wild.png

commit 26458af1d63c85195cd96cd1673051e332d06d30
Author: Gilad S <giladgd@users.noreply.github.com>
Date:   Wed May 8 22:08:10 2024 +0300

    metal : use `vm_allocate` instead of `posix_memalign` on macOS (#7078)
    
    * fix: use `malloc` instead of `posix_memalign` in `ggml-metal.m` to make it not crash Electron proccesses
    
    * fix: typo
    
    * fix: use `vm_allocate` instead of `posix_memalign`
    
    * fix: don't call `newBufferWithBytesNoCopy` with `NULL` when `ggml_metal_host_malloc` returns `NULL`
    
    * fix: use `vm_allocate` only on macOS

ggml-metal.m

commit 83330d8cd6491e53e1aca4c5dfc47e039b3c04ff
Author: Dawid Potocki <github@dawidpotocki.com>
Date:   Thu May 9 02:32:32 2024 +1200

    main : add --conversation / -cnv flag (#7108)

common/common.cpp
common/common.h
examples/main/main.cpp

commit 465263d0cf1e8f8bc41948332dbd009d27a68590
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed May 8 14:29:23 2024 +0000

    sgemm : AVX Q4_0 and Q8_0 (#6891)
    
    * basic avx implementation
    
    * style
    
    * combine denibble with load
    
    * reduce 256 to 128 (and back!) conversions
    
    * sse load
    
    * Update sgemm.cpp
    
    * oops
    
    oops

sgemm.cpp

commit 911b3900dded9a1cfe0f0e41b82c7a29baf3a217
Author: Johan <JohanAR@users.noreply.github.com>
Date:   Wed May 8 14:27:58 2024 +0200

    server : add_special option for tokenize endpoint (#7059)

examples/server/README.md
examples/server/server.cpp
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py

commit ad211edef5db1f1fb955874b7ca6a67bd0c88708
Author: 20kdc <asdd2808@gmail.com>
Date:   Wed May 8 13:22:32 2024 +0100

    convert.py : --vocab-only generates false but valid params (#7027)
    
    An example of how this might be used in the style of baby-llama will be attached with this PR.

convert.py

commit 229ffff872f8ad0d21c997d18ee7a23692ae60a0
Author: Ren Xuancheng <jklj077@users.noreply.github.com>
Date:   Wed May 8 20:06:43 2024 +0800

    llama : add BPE pre-tokenization for Qwen2 (#7114)
    
    * Add BPE pre-tokenization for Qwen2.
    
    * minor : fixes
    
    ---------
    
    Co-authored-by: Ren Xuancheng <17811943+jklj077@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
llama.cpp
llama.h
models/ggml-vocab-qwen2.gguf
models/ggml-vocab-qwen2.gguf.inp
models/ggml-vocab-qwen2.gguf.out
tests/CMakeLists.txt

commit 1fd9c1741d864d01cd7ec6d67227b92d7bfabf22
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed May 8 13:24:14 2024 +0200

    clean up json_value & server_log (#7142)

examples/server/utils.hpp

commit 4cd621c26de2095cd7c4464bdec5fe2e696ef3f3
Author: DAN™ <dranger003@gmail.com>
Date:   Wed May 8 06:43:23 2024 -0400

    convert : add BPE pre-tokenization for DBRX (#7132)
    
    * Add BPE pre-tokenization for DBRX.
    
    * Add vocab GGUFs.
    
    * Remove test.
    
    * Remove GGUFs.

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
llama.cpp
llama.h

commit 7e0b6a7b3ba94ff624dc27c1e0e735fded8819b8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 8 12:47:07 2024 +0300

    py : also print the normalizers

convert-hf-to-gguf-update.py

commit acdce3cdef6fc2f0b7b5623231fd7762c0884d1c
Author: Brian <mofosyne@gmail.com>
Date:   Wed May 8 18:54:39 2024 +1000

    compare-llama-bench.py: add missing basicConfig (#7138)
    
    * compare-llama-bench.py: add missing basicConfig
    
    * compare-llama-bench.py: Add line break between error message and print_help()
    
    * Add regular print() markdown table

scripts/compare-llama-bench.py

commit 3855416027cb25d9a708ffa5581cf503a87856a6
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Wed May 8 02:30:09 2024 -0400

    ggml : introduce bfloat16 support (#6412)
    
    * Introduce bfloat16 support
    
    Many models on Hugging Face (e.g. Mistral, TinyLLaMA) use bfloat16 as
    their canonical floating point format.
    
          ┌sign
          │
          │   ┌exponent
          │   │
          │   │      ┌mantissa
          │   │      │
          │┌──┴───┐┌─┴───┐
        0b0000000000000000 brain16
    
    This encoding has the same number of exponent bits as float32. That
    makes conversion relatively straightforward, even in the absence of
    hardware support. For example, converting brain16 to binary32 means
    simply shifting 16 bits to the left.
    
          ┌sign
          │
          │   ┌exponent
          │   │
          │   │      ┌mantissa
          │   │      │
          │┌──┴───┐┌─┴───────────────────┐
        0b00000000000000000000000000000000 IEEE binary32
    
    The issue is that converting bf16 to fp16 can result in information
    loss. Only 13% of bf16 numbers can be precisely represented in fp16
    which in practice ends up being 99.71% of Mistral 7b v0.2's weights
    however there is currently no way other than fp32 to get the others
    
          ┌sign
          │
          │  ┌exponent
          │  │
          │  │    ┌mantissa
          │  │    │
          │┌─┴─┐┌─┴──────┐
        0b0000000000000000 IEEE binary16
    
    This change fixes that, by adding a bf16 data type to GGML. Support
    for CPU inference has been implemented along with optimizations for
    the AVX2, AVX512, and AVX512BF16 ISAs. Perplexity on Mistral 7b 0.2
    improves somewhere around -0.0024 to -0.0046 compared to using fp16
    
    * Remove GGML code that's not needed
    
    * Minimize the GGML API surface area for BF16
    
    * Remove bf16 luts
    
    * Make the GGML header look nicer
    
    * Fix documentation
    
    * Apply ggerganov's fixes for test-backend-ops
    
    * Add BF16 code for new ggml_validate_row_data() function

examples/finetune/finetune.cpp
examples/quantize/quantize.cpp
ggml-impl.h
ggml-metal.m
ggml-quants.c
ggml.c
ggml.h
gguf-py/gguf/constants.py
llama.cpp
llama.h
tests/test-backend-ops.cpp

commit c0e6fbf8c380718102bd25fcb8d2e55f8f9480d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 8 09:14:50 2024 +0300

    metal : fix unused warning

ggml-metal.metal

commit c780e75305dba1f67691a8dc0e8bc8425838a452
Author: Jeximo <jeximo@gmail.com>
Date:   Tue May 7 21:26:43 2024 -0300

    Further tidy on Android instructions README.md (#7077)
    
    * Further tidy on Android instructions README.md
    
    Fixed some logic when following readme direction
    
    * Clean up redundent information
    
    A new user arriving will see simple directions on llama.cpp homepage
    
    * corrected puncuation
    
    Period after cmake, colon after termux
    
    * re-word for clarity
    
    method seems to be more correct, instead of alternative in this context
    
    * Organized required packages per build type
    
    building llama.cpp with NDK on a pc doesn't require installing clang, cmake, git, or wget in termux.
    
    * README.md
    
    corrected title
    
    * fix trailing whitespace

README.md

commit 48b2f9c1fc71ab7df5432be2ed9fa7cdf5e8405e
Author: jukofyork <69222624+jukofyork@users.noreply.github.com>
Date:   Wed May 8 01:24:16 2024 +0100

    Fixed save_imatrix to match old behaviour for MoE (#7099)
    
    * Fixed save_imatrix to match old behaviour for MoE
    
    This fix is simple and clear, but unnecessarily doubles the memory overhead..
    
    * Fixed missing idx variable
    
    * Unconditionally increment ncall
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Fixed 2 bugs in save_imatrix()
    
    - Fixed segfault bug because the counts vector needed to be created.
    - Fixed pre-existing bug didn't actually add to the counts for "--combine" option.
    
    * ncall needs summing too
    
    * Trailing whitespace
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

examples/imatrix/imatrix.cpp

commit af0a5b616359809ce886ea433acedebb39b12969
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue May 7 23:07:58 2024 +0200

    server: fix incorrectly reported token probabilities (#7125)
    
    * server: normalize token probabilities
    
    * fix temperature == 0.0f

common/sampling.cpp
common/sampling.h
examples/server/README.md
examples/server/server.cpp

commit b6aa6702030320a3d5fbc2508307af0d7c947e40
Author: nopperl <54780682+nopperl@users.noreply.github.com>
Date:   Tue May 7 19:39:43 2024 +0000

    Fix OLMo HF to GGUF conversion (#6910)

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
llama.cpp
llama.h

commit 260b7c65296fba0568eeb1ff05244ea0be206b54
Author: Kyle Mistele <kyle@mistele.com>
Date:   Tue May 7 13:44:29 2024 -0500

    server : update readme with undocumented options (#7013)

examples/server/README.md

commit 53d6c52e227dedef347b21e28febcfb9caeecdad
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 7 21:43:13 2024 +0300

    readme : update hot topics

README.md

commit 3af34c1d1b0da47f85b95f60922abeded1cb5d33
Author: RhinoDevel <RhinoDevel@users.noreply.github.com>
Date:   Tue May 7 19:51:31 2024 +0200

    main : update log text (EOS to EOG) (#7104)
    
    * Update log text (EOS to EOG)
    
    The log text "found EOS" is no longer always correct, here, because there is now an is-EOG check that also returns true for EOT.
    
    * Improve log msg. further by using "an" instead of "some".
    
    As suggested, to avoid misunderstanding (no multiple EOG tokens found, just one).

examples/main/main.cpp

commit 04976db7a819fcf8bfefbfc09a3344210b79dd27
Author: omahs <73983677+omahs@users.noreply.github.com>
Date:   Tue May 7 17:20:33 2024 +0200

    docs: fix typos (#7124)
    
    * fix typo
    
    * fix typos
    
    * fix typo
    
    * fix typos
    
    * fix typo
    
    * fix typos

docs/BLIS.md
docs/HOWTO-add-model.md
examples/llava/README.md
examples/main/README.md
examples/sycl/README.md
grammars/README.md

commit 947d3ad27d94f1addef76b5d64c314618f063933
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 7 11:08:49 2024 +0300

    ci : add GG_BUILD_EXTRA_TESTS_0 env (#7098)
    
    * ci : add GG_BUILD_EXTRA_TESTS_0 env
    
    ggml-ci
    
    * Update run.sh
    
    ggml-ci

ci/run.sh

commit 858f6b73f6e57a62523d16a955d565254be889b4
Author: William Tambellini <william.tambellini@gmail.com>
Date:   Mon May 6 11:12:14 2024 -0700

    Add an option to build without CUDA VMM (#7067)
    
    Add an option to build ggml cuda without CUDA VMM
    resolves
    https://github.com/ggerganov/llama.cpp/issues/6889
    https://forums.developer.nvidia.com/t/potential-nvshmem-allocated-memory-performance-issue/275416/4

CMakeLists.txt
ggml-cuda.cu

commit b3a995b416e13ae3123a117a743e11d0ede0ca4c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 6 18:36:06 2024 +0300

    flake.lock: Update (#7079)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/9126214d0a59633752a136528f5f3b9aa8565b7d?narHash=sha256-sB4SWl2lX95bExY2gMFG5HIzvva5AVMJd4Igm%2BGpZNw%3D' (2024-04-01)
      → 'github:hercules-ci/flake-parts/e5d10a24b66c3ea8f150e47dfdb0416ab7c3390e?narHash=sha256-yzcRNDoyVP7%2BSCNX0wmuDju1NUCt8Dz9%2BlyUXEI0dbI%3D' (2024-05-02)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/d8fe5e6c92d0d190646fb9f1056741a229980089?dir=lib&narHash=sha256-iMUFArF0WCatKK6RzfUJknjem0H9m4KgorO/p3Dopkk%3D' (2024-03-29)
      → 'https://github.com/NixOS/nixpkgs/archive/50eb7ecf4cd0a5756d7275c8ba36790e5bd53e33.tar.gz?narHash=sha256-QBx10%2Bk6JWz6u7VsohfSw8g8hjdBZEf8CFzXH1/1Z94%3D' (2024-05-02)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/7bb2ccd8cdc44c91edba16c48d2c8f331fb3d856?narHash=sha256-Drmja/f5MRHZCskS6mvzFqxEaZMeciScCTFxWVLqWEY%3D' (2024-04-25)
      → 'github:NixOS/nixpkgs/63c3a29ca82437c87573e4c6919b09a24ea61b0f?narHash=sha256-4cPymbty65RvF1DWQfc%2BBc8B233A1BWxJnNULJKQ1EY%3D' (2024-05-02)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit bcdee0daa7c5e8e086b719e5eb4073b00df70e01
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 6 09:31:30 2024 +0300

    minor : fix trailing whitespace

README.md

commit 628b299106d1e9476fdecb3cbe546bf5c60f1b89
Author: kunnis <kunnis@users.noreply.github.com>
Date:   Sun May 5 07:17:47 2024 -0500

    Adding support for the --numa argument for llama-bench. (#7080)

examples/llama-bench/llama-bench.cpp

commit 8f8acc8683a00ce40fa5a81161a079d2167126e6
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Sun May 5 13:38:55 2024 +0200

    Disable benchmark on forked repo (#7034)
    
    * Disable benchmark on forked repo
    
    * only check owner on schedule event
    
    * check owner on push also
    
    * more readable as multi-line
    
    * ternary won't work
    
    * style++
    
    * test++
    
    * enable actions debug
    
    * test--
    
    * remove debug
    
    * test++
    
    * do debug where we can get logs
    
    * test--
    
    * this is driving me crazy
    
    * correct github.event usage
    
    * remove test condition
    
    * correct github.event usage
    
    * test++
    
    * test--
    
    * event_name is pull_request_target
    
    * test++
    
    * test--
    
    * update ref checks

.github/workflows/bench.yml

commit ca3632602091e959ed2ad4c09c67a7c790b10d31
Author: Lyle Dean <dean@lyle.dev>
Date:   Sun May 5 06:21:46 2024 +0100

    readme : add note that LLaMA 3 is not supported with convert.py (#7065)

README.md

commit 889bdd76866ea31a7625ec2dcea63ff469f3e981
Author: DAN™ <dranger003@gmail.com>
Date:   Sun May 5 01:19:30 2024 -0400

    command-r : add BPE pre-tokenization (#7063)
    
    * Add BPE pre-tokenization for Command-R/R+.
    
    * Bump transformers convert requirement.
    
    * command-r : add individual digits regex
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
llama.cpp
llama.h
models/ggml-vocab-command-r.gguf
models/ggml-vocab-command-r.gguf.inp
models/ggml-vocab-command-r.gguf.out
requirements/requirements-convert.txt
tests/CMakeLists.txt

commit 6fbd43221167bf96112f899daf22c127b282cbcf
Author: Brian <mofosyne@gmail.com>
Date:   Sun May 5 15:07:48 2024 +1000

    py : logging and flake8 suppression refactoring (#7081)
    
    Set one as executable and add basicConfig()
    to another. Also added noqa tag to test scripts.

.flake8
convert-hf-to-gguf-update.py
convert-lora-to-ggml.py
scripts/gen-unicode-data.py
tests/test-tokenizer-0.py

commit 842500144ee02c8b0a46d2cc43880f8d80998fa5
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat May 4 18:56:22 2024 +0200

    gguf-split: add --no-tensor-first-split (#7072)

examples/gguf-split/gguf-split.cpp
examples/gguf-split/tests.sh
ggml.c

commit cf768b7e71cbcc9886c753ae963c2b68893d02e4
Author: Jeximo <jeximo@gmail.com>
Date:   Sat May 4 13:10:15 2024 -0300

    Tidy Android Instructions README.md (#7016)
    
    * Tidy Android Instructions README.md
    
    Remove CLBlast instructions(outdated), added OpenBlas.
    
    * don't assume git is installed
    
    Added apt install git, so that git clone works
    
    * removed OpenBlas
    
    Linked to Linux build instructions
    
    * fix typo
    
    Remove word "run"
    
    * correct style
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * correct grammar
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * delete reference to Android API
    
    * remove Fdroid reference, link directly to Termux
    
    Fdroid is not required
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update README.md
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

README.md

commit fcd84a0f5a584ab5271745d7ffef21c8a6bc7b0c
Author: viric <viric@viric.name>
Date:   Sat May 4 15:26:53 2024 +0200

    Fix Linux /sys cpu path to guess number of cores (#7064)

common/common.cpp

commit 03fb8a002df2e96104f9e06de9c78d2a8ed91e92
Author: maor-ps <154728172+maor-ps@users.noreply.github.com>
Date:   Sat May 4 12:06:40 2024 +0300

    If first token generated from the server is the stop word the server will crash (#7038)
    
    This will reproduce the issue in llama13b
    {
    'prompt': 'Q: hello world \nA: ',
     'stop': ['\n'],
     'temperature': 0.0,
     'n_predict': 10,
     'cache_prompt': True,
     'n_probs': 10
    }

examples/server/server.cpp

commit 92139b90af4841d7fd060b526bdd443b621770ff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 4 08:32:32 2024 +0300

    tests : add test-tokenizer-0.sh + fix some tokenizers (#7036)
    
    * tests : add test-tokenizer-0.sh
    
    * unicode : add all unicode number ranges
    
    * starcoder : fix pre-tokenizer
    
    * tests : add test that fails with DeepSeek tokenizers
    
    * falcon : fix regex
    
    * unicode : regenerate unicode tables
    
    * refact : add tokenizer model
    
    * lint : fix
    
    * tests : disable failing tests
    
    ggml-ci
    
    * refact : add tests files
    
    ggml-ci
    
    * convert : print -> logging
    
    ggml-ci
    
    * lint : fix
    
    * unicode : digit -> number
    
    * phi-3 : update

.flake8
Makefile
convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
llama.cpp
llama.h
models/ggml-vocab-bert-bge.gguf.inp
models/ggml-vocab-bert-bge.gguf.out
models/ggml-vocab-deepseek-coder.gguf.inp
models/ggml-vocab-deepseek-coder.gguf.out
models/ggml-vocab-deepseek-llm.gguf.inp
models/ggml-vocab-deepseek-llm.gguf.out
models/ggml-vocab-falcon.gguf.inp
models/ggml-vocab-falcon.gguf.out
models/ggml-vocab-gpt-2.gguf.inp
models/ggml-vocab-gpt-2.gguf.out
models/ggml-vocab-llama-bpe.gguf.inp
models/ggml-vocab-llama-bpe.gguf.out
models/ggml-vocab-llama-spm.gguf.inp
models/ggml-vocab-llama-spm.gguf.out
models/ggml-vocab-mpt.gguf.inp
models/ggml-vocab-mpt.gguf.out
models/ggml-vocab-phi-3.gguf
models/ggml-vocab-phi-3.gguf.inp
models/ggml-vocab-phi-3.gguf.out
models/ggml-vocab-refact.gguf
models/ggml-vocab-refact.gguf.inp
models/ggml-vocab-refact.gguf.out
models/ggml-vocab-starcoder.gguf.inp
models/ggml-vocab-starcoder.gguf.out
scripts/gen-unicode-data.py
tests/CMakeLists.txt
tests/test-tokenizer-0-bpe.py
tests/test-tokenizer-0-spm.py
tests/test-tokenizer-0.cpp
tests/test-tokenizer-0.py
tests/test-tokenizer-0.sh
unicode-data.cpp
unicode-data.h
unicode.cpp
unicode.h

commit a2ac89d6efb41b535778bfeaecaae8fe295b6ed3
Author: Brian <mofosyne@gmail.com>
Date:   Sat May 4 05:36:41 2024 +1000

    convert.py : add python logging instead of print() (#6511)
    
    * convert.py: add python logging instead of print()
    
    * convert.py: verbose flag takes priority over dump flag log suppression
    
    * convert.py: named instance logging
    
    * convert.py: use explicit logger id string
    
    * convert.py: convert extra print() to named logger
    
    * convert.py: sys.stderr.write --> logger.error
    
    * *.py: Convert all python scripts to use logging module
    
    * requirements.txt: remove extra line
    
    * flake8: update flake8 ignore and exclude to match ci settings
    
    * gh-actions: add flake8-no-print to flake8 lint step
    
    * pre-commit: add flake8-no-print to flake8 and also update pre-commit version
    
    * convert-hf-to-gguf.py: print() to logger conversion
    
    * *.py: logging basiconfig refactor to use conditional expression
    
    * *.py: removed commented out logging
    
    * fixup! *.py: logging basiconfig refactor to use conditional expression
    
    * constant.py: logger.error then exit should be a raise exception instead
    
    * *.py: Convert logger error and sys.exit() into a raise exception (for atypical error)
    
    * gguf-convert-endian.py: refactor convert_byteorder() to use tqdm progressbar
    
    * verify-checksum-model.py: This is the result of the program, it should be printed to stdout.
    
    * compare-llama-bench.py: add blank line for readability during missing repo response
    
    * reader.py: read_gguf_file() use print() over logging
    
    * convert.py: warning goes to stderr and won't hurt the dump output
    
    * gguf-dump.py: dump_metadata() should print to stdout
    
    * convert-hf-to-gguf.py: print --> logger.debug or ValueError()
    
    * verify-checksum-models.py: use print() for printing table
    
    * *.py: refactor logging.basicConfig()
    
    * gguf-py/gguf/*.py: use __name__ as logger name
    
    Since they will be imported and not run directly.
    
    * python-lint.yml: use .flake8 file instead
    
    * constants.py: logger no longer required
    
    * convert-hf-to-gguf.py: add additional logging
    
    * convert-hf-to-gguf.py: print() --> logger
    
    * *.py: fix flake8 warnings
    
    * revert changes to convert-hf-to-gguf.py for get_name()
    
    * convert-hf-to-gguf-update.py: use triple quoted f-string instead
    
    * *.py: accidentally corrected the wrong line
    
    * *.py: add compilade warning suggestions and style fixes

.flake8
.github/workflows/python-lint.yml
.pre-commit-config.yaml
convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
convert-llama-ggml-to-gguf.py
convert-lora-to-ggml.py
convert-persimmon-to-gguf.py
convert.py
ggml_vk_generate_shaders.py
gguf-py/examples/reader.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_reader.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/vocab.py
gguf-py/scripts/gguf-convert-endian.py
gguf-py/scripts/gguf-dump.py
gguf-py/scripts/gguf-set-metadata.py
scripts/compare-llama-bench.py
scripts/run-with-preset.py
scripts/verify-checksum-models.py
tests/test-tokenizer-0-bpe.py
tests/test-tokenizer-0-spm.py

commit 433def286e98751bf17db75dce53847d075c0be5
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri May 3 15:24:30 2024 +0200

    llama : rename ctx to user_data in progress_callback (#7045)
    
    * llama : rename ctx to user_data in progress_callback
    
    This commit renames the `ctx` parameter to `user_data` in the
    `llama_progress_callback` typedef.
    
    The motivation for this is that other callbacks use `user_data` or
    `data`, and using `ctx` in this case might be confusing as it could be
    confused with `llama_context`.
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

llama.h

commit 60325fa56f61c228464c9f065db3aa6a61f2156e
Author: Bartowski <ckealty1182@gmail.com>
Date:   Thu May 2 19:49:09 2024 -0400

    Remove .attention from skipped tensors to match more accurately (#7051)

convert-hf-to-gguf.py

commit 6ecf3189e00a1e8e737a78b6d10e1d7006e050a2
Author: alwqx <kenan3015@gmail.com>
Date:   Thu May 2 23:56:41 2024 +0800

    chore: fix typo in llama.cpp (#7032)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

llama.cpp

commit b0d943de179ad5dbd83d51f327fb566066f4ccda
Author: Andrew Downing <andrew2085@gmail.com>
Date:   Wed May 1 17:31:30 2024 -0400

    Update LOG_IMPL and LOG_TEE_IMPL (#7029)
    
    ROCm clang defines _MSC_VER which results in the wrong implementation of LOG_IMPL and LOG_TEE_IMPL being compiled.
    
    This fixes https://github.com/ggerganov/llama.cpp/issues/6972

common/log.h

commit 8d608a81b7bd170f700648f8214e6f3279d4d715
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Thu May 2 04:27:41 2024 +0900

    main : fix off by one error for context shift (#6921)

examples/main/main.cpp

commit 3ea0d36000e2314baba7e9fc6a97f08670a6f7e4
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 1 17:52:55 2024 +0200

    Server: add tests for batch size, different seeds (#6950)

examples/server/tests/features/results.feature
examples/server/tests/features/steps/steps.py

commit 1613ef8d8eb2479ba55c4d598e08c8f3f18a0fed
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed May 1 14:46:37 2024 +0200

    CUDA: CUDART < 11.7 workaround for __hmax, __hmax2 (#7019)

ggml-cuda/common.cuh
ggml-cuda/fattn.cu

commit c4ec9c0d3d67e6b33638e6dad86419e6fd5ffe01
Author: slaren <slarengh@gmail.com>
Date:   Wed May 1 07:13:59 2024 +0200

    ci : exempt confirmed bugs from being tagged as stale (#7014)

.github/workflows/close-issue.yml

commit a8f9b076316e16aadd0791015b3bfd446fe1e904
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Apr 30 23:36:27 2024 +0200

    perplexity: more statistics, added documentation (#6936)
    
    * perplexity: more statistics, added documentation
    
    * add LLaMA 3 8b scoreboard

common/common.h
examples/perplexity/README.md
examples/perplexity/perplexity.cpp

commit f364eb6fb5d46118a76fa045f487318de4c24961
Author: Kevin Gibbons <bakkot@gmail.com>
Date:   Tue Apr 30 08:14:02 2024 -0700

    switch to using localizedDescription (#7010)

ggml-metal.m

commit 77e15bec6217a39be59b9cc83d6b9afb6b0d8167
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 30 15:52:21 2024 +0300

    metal : remove deprecated error code (#7008)

ggml-metal.m

commit a68a1e7ed060ee5af2d638585d19e3510ddbf16c
Author: Kevin Gibbons <bakkot@gmail.com>
Date:   Tue Apr 30 02:34:50 2024 -0700

    metal : log more info on error (#6987)

ggml-metal.m

commit 9c67c2773d4b706cf71d70ecf4aa180b62501960
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 30 12:16:08 2024 +0300

    ggml : add Flash Attention (#5021)
    
    * ggml : add ggml_flash_attn_ext API
    
    * ggml : fix GQA support in ggml_flash_attn_ext
    
    * ggml : online attention (CPU)
    
    * metal : initial implementation
    
    * metal : f16 precision
    
    * metal : reduce branches
    
    * metal : specialize for head size
    
    * wip : 8 rows per simd group
    
    * wip : 4 rows per simd group
    
    * wip : template for rows per warp
    
    * metal : parallelize across KV size
    
    * metal : parallel reduce across heads
    
    * metal : efficient flash_attn_f16 implementation
    
    * metal : avoid redundant loads of the attention
    
    * metal : scale and mask in matrix form
    
    * metal : fix comment
    
    * llama : avoid ggml_cast, use F32 query
    
    * metal : add parallel reduce version (disabled)
    
    * metal : move output into local memory + optimize
    
    - the result from each simdgroup now stays in the registers
    - significantly reduced SRAM usage
    - more efficient skipping of -INF blocks
    - avoid simdgroup barrier in hot loop
    - add comments
    
    * metal : add tests, fix scaling, support C > 32
    
    * metal : improve precision
    
    * ggml : fix f16 mad
    
    * metal : minor
    
    * metal : support Q > 8
    
    * tests : add ATTN tests
    
    * metal : disable buffer allocation logs
    
    * tests : more
    
    * metal : faster inner loop for C == 32
    
    * metal : fix array initialization
    
    * tests : ifdef
    
    * ggml : switch to padded F16 mask for ggml_soft_max, ggml_flash_attn_ext
    
    * ggml : fix ggml_soft_max mask requirement
    
    * cuda : fix soft_max to use correct mask size
    
    * cuda : add flash_attn kernel (wip)
    
    * metal : optimize softmax for C > 32
    
    * metal : optimize softmax
    
    * tests : minor fix
    
    * cuda : avoid zeroing fragments
    
    * tests : update dims
    
    * cuda : fix __hisinf() result check
    
    * cuda : avoid warp_reduce for smax
    
    * cuda : use int instead of int64_t
    
    Noticeably improves performance (thanks to Johannes)
    
    * cuda : make loops use the same loop values
    
    Thanks Johannes again for the tip
    
    * cuda : unroll some of the loops
    
    * cuda : avoid __hisinf branches
    
    * cuda : use half2 in softmax
    
    * cuda : switch to 1 warp for bs > 16
    
    * cuda : speed-up reduce part of the kernel
    
    * cuda : unroll Q*K^T loop
    
    * cuda : fix -INF block check
    
    * cuda : simplify softmax
    
    * cuda : fix matrix names
    
    * cuda : minor
    
    * llama : adapt to F16 KQ_pos
    
    * llama : adapt new models to F16 KQ_mask
    
    * ggml : fix F16 store (ARM NEON)
    
    * llama : fix type of KQ_mask and KQ_pos
    
    * ggml : fix CPU soft_max
    
    * tests : add hs=256
    
    * cuda : fix build
    
    * metal : improve perf via smaller int registers
    
    * cuda : adapt soft_max to F16 mask and pos
    
    * CUDA: faster FlashAttention, kernel for bs == 1
    
    * 16 cols for Phi-2
    
    * no vec for hs, no hs==256 ncols==32 for Volta
    
    * adjust kernel selection logic
    
    * 4 warps, 256 stride for all D
    
    * no ncols == 64
    
    * Multiple parallel blocks for batch size 1
    
    * fix compile warnings
    
    * fix excessive KQ_b loads
    
    * fix cmake build
    
    * fix KV cache padding, NaN from INFINITY (#6438)
    
    * llama : flash_attn cparam + fix defrag
    
    * server: support flash_attn param
    
    * server: bench: enable flash_attn param
    
    * CUDA: refactor host code, dyn. par. blocks
    
    * fix flash_attn_vec_f16 race condition
    
    * flush softmax exp below threshold to 0
    
    * store temp KQ in registers
    
    * Calculate KQ as FP32 if KQV has GGML_PREC_F32
    
    * Add __hgt2_mask implementation for CUDA 11
    
    * fix KQ FP32 precision fpr parallel_blocks > 1
    
    * llama-bench : add -fa,--flash-attn arg
    
    * metal : add BS=1 kernel for flash attention (#6508)
    
    * metal : add BS=1 kernel for flash attention (wip)
    
    * metal : support more than 1 warps
    
    * metal : opts
    
    * metal : opt
    
    * metal : switch to parallel reduce
    
    * metal : reduce registers
    
    * metal : simplify
    
    * metal : initial FA vec kernel
    
    * metal : use F32 attention accumulators
    
    * batched-bench : add fattn arg
    
    * llama : simplify llama_build_kv_store
    
    ggml-ci
    
    * llama : adapt build_olmo to changes
    
    * ggml : fix arm fp16 store on windows
    
    * metal : clean-up
    
    * metal : clean-up kernel code
    
    * metal : minor
    
    * tests : remove benchmarks
    
    ggml-ci
    
    * ggml : fix avx512 const correctness
    
    ggml-ci
    
    * ggml : fix soft_max with bias on CPU
    
    ggml-ci
    
    * common : print --flash-attn in help
    
    * ggml : fix num dimensions in ggml_flash_attn_ext
    
    * llama : force disable flash attention for incompatible models
    
    * ggml : ggml_soft_max support F16/F32 mask/pos
    
    ggml-ci
    
    * cuda : uint -> uint32_t
    
    * cuda : "constexpr dim3" -> "const dim3"
    
    ggml-ci
    
    * cuda : try to fix __hgt2_mask
    
    ggml-ci
    
    * ggml : add TODO's for F16/F32 mask/pos support in other backends
    
    * llama : replace bool need_kq_pos with use_alibi
    
    * llama : prep ALiBi support for BERT models
    
    ggml-ci
    
    * llama : fix n_batch requirements
    
    ggml-ci
    
    * cont
    
    * server : add help for --flash-attn arg
    
    * llama : disable FA for AMD
    
    * tests : remove TMP_ATTN_BENCH
    
    ggml-ci
    
    * llama : support save/load state with FA enabled
    
    ggml-ci
    
    * ci : add CUDA save-load-state tests
    
    ggml-ci
    
    * llama : llama_kv_cache_clear zeroes data + fix save-load seq
    
    ggml-ci
    
    * llama : fix copy-paste errors, add TODO
    
    * llama : disallow incompatible states
    
    * llama : update llama_state_get_size after v_trans field
    
    * metal : remove tmp log
    
    * llama : add static reminder for llama_state_get_size
    
    * metal : fix max nsg
    
    ggml-ci
    
    * ci : fix arg order
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    Co-authored-by: Pierrick HYMBERT <pierrick.hymbert@gmail.com>

ci/run.sh
common/common.cpp
common/common.h
examples/batched-bench/batched-bench.cpp
examples/llama-bench/llama-bench.cpp
examples/server/bench/bench.py
examples/server/server.cpp
ggml-cuda.cu
ggml-cuda/common.cuh
ggml-cuda/fattn.cu
ggml-cuda/fattn.cuh
ggml-cuda/softmax.cu
ggml-kompute.cpp
ggml-metal.m
ggml-metal.metal
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp

commit 952d03dbead16e4dbdd1d3458486340673cc2465
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 30 11:05:25 2024 +0300

    convert : use utf8 encoding (#7000)
    
    * convert : use utf8 encoding
    
    * convert : update instructions and warning message

convert-hf-to-gguf-update.py
convert-hf-to-gguf.py

commit 8843a98c2ba97a25e93319a104f9ddfaf83ce4c4
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Tue Apr 30 00:52:50 2024 +0100

    Improve usability of --model-url & related flags (#6930)
    
    * args: default --model to models/ + filename from --model-url or --hf-file (or else legacy models/7B/ggml-model-f16.gguf)
    
    * args: main & server now call gpt_params_handle_model_default
    
    * args: define DEFAULT_MODEL_PATH + update cli docs
    
    * curl: check url of previous download (.json metadata w/ url, etag & lastModified)
    
    * args: fix update to quantize-stats.cpp
    
    * curl: support legacy .etag / .lastModified companion files
    
    * curl: rm legacy .etag file support
    
    * curl: reuse regex across headers callback calls
    
    * curl: unique_ptr to manage lifecycle of curl & outfile
    
    * curl: nit: no need for multiline regex flag
    
    * curl: update failed test (model file collision) + gitignore *.gguf.json

.gitignore
common/common.cpp
common/common.h
examples/main/README.md
examples/quantize-stats/quantize-stats.cpp
examples/server/server.cpp
examples/server/tests/features/embeddings.feature

commit b8c1476e44cc1f3a1811613f65251cf779067636
Author: Clint Herron <hanclinto@gmail.com>
Date:   Mon Apr 29 14:40:14 2024 -0400

    Extending grammar integration tests (#6644)
    
    * Cleaning up integration tests to share code between tests and make it simpler to add new tests.
    
    * Add tests around quantifiers to ensure both matching and non-matching compliance.
    
    * Add slightly more complex grammar with quantifiers to test references with quantifiers.
    
    * Fixing build when C++17 is not present.
    
    * Separating test calls to give more helpful stack traces on failure. Adding verbose messages to give visibility for what is being tested.
    
    * Adding quotes around strings to explicitly show whitespace
    
    * Removing trailing whitespace.
    
    * Implementing suggestions from @ochafik -- grammars and test strings now print and flush before tests to aid in debugging segfaults and whatnot.
    
    * Cleaning up forgotten symbols. Modifying simple test to use test harness. Added comments for more verbose descriptions of what each test is accomplishing.
    
    * Unicode symbol modifications to hopefully make log easier to parse visually.

tests/test-grammar-integration.cpp

commit 5539e6fdd1b97e0c37c54d34df67f334fc344a26
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Apr 29 19:56:59 2024 +0200

    main : fix typo in comment in main.cpp (#6985)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/main/main.cpp

commit b8a7a5a90fd3187175d84227dad705ade395ba46
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Mon Apr 29 17:02:45 2024 +0100

    build(cmake): simplify instructions (`cmake -B build && cmake --build build ...`) (#6964)
    
    * readme: cmake . -B build && cmake --build build
    
    * build: fix typo
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * build: drop implicit . from cmake config command
    
    * build: remove another superfluous .
    
    * build: update MinGW cmake commands
    
    * Update README-sycl.md
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * build: reinstate --config Release as not the default w/ some generators + document how to build Debug
    
    * build: revert more --config Release
    
    * build: nit / remove -H from cmake example
    
    * build: reword debug instructions around single/multi config split
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>

.devops/main-intel.Dockerfile
.devops/main-vulkan.Dockerfile
.devops/server-intel.Dockerfile
.devops/server-vulkan.Dockerfile
.github/workflows/bench.yml
.github/workflows/server.yml
README-sycl.md
README.md
examples/main-cmake-pkg/README.md
examples/server/README.md

commit d2c898f746a527f09effb061829e68b2e1812a28
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 29 18:36:39 2024 +0300

    ci : tmp disable gguf-split (#6983)
    
    ggml-ci

ci/run.sh

commit 544f1f10adeec3d5ed91c4d3bd3f903904ecea63
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 29 17:55:02 2024 +0300

    ggml : fix __MSC_VER -> _MSC_VER (#6977)
    
    ggml-ci

ggml-impl.h

commit ffe666572f98a686b17a2cd1dbf4c0a982e5ac0a
Author: cpumaxx <163466046+cpumaxx@users.noreply.github.com>
Date:   Mon Apr 29 07:34:24 2024 -0700

    llava-cli : multiple images (#6969)
    
    Co-authored-by: root <root@nenya.lothlorien.ca>

common/common.cpp
common/common.h
examples/llava/llava-cli.cpp

commit 24affa7db3c9db148854b0ab4fd63de8bca7d898
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 29 17:06:19 2024 +0300

    readme : update hot topics

README.md

commit f4ab2a41476600a98067a9474ea8f9e6db41bcfa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 29 16:58:41 2024 +0300

    llama : fix BPE pre-tokenization (#6920)
    
    * merged the changes from deepseeker models to main branch
    
    * Moved regex patterns to unicode.cpp and updated unicode.h
    
    * Moved header files
    
    * Resolved issues
    
    * added and refactored unicode_regex_split and related functions
    
    * Updated/merged the deepseek coder pr
    
    * Refactored code
    
    * Adding unicode regex mappings
    
    * Adding unicode regex function
    
    * Added needed functionality, testing remains
    
    * Fixed issues
    
    * Fixed issue with gpt2 regex custom preprocessor
    
    * unicode : fix? unicode_wstring_to_utf8
    
    * lint : fix whitespaces
    
    * tests : add tokenizer tests for numbers
    
    * unicode : remove redundant headers
    
    * tests : remove and rename tokenizer test scripts
    
    * tests : add sample usage
    
    * gguf-py : reader prints warnings on duplicate keys
    
    * llama : towards llama3 tokenization support (wip)
    
    * unicode : shot in the dark to fix tests on Windows
    
    * unicode : first try custom implementations
    
    * convert : add "tokenizer.ggml.pre" GGUF KV (wip)
    
    * llama : use new pre-tokenizer type
    
    * convert : fix pre-tokenizer type writing
    
    * lint : fix
    
    * make : add test-tokenizer-0-llama-v3
    
    * wip
    
    * models : add llama v3 vocab file
    
    * llama : adapt punctuation regex + add llama 3 regex
    
    * minor
    
    * unicode : set bomb
    
    * unicode : set bomb
    
    * unicode : always use std::wregex
    
    * unicode : support \p{N}, \p{L} and \p{P} natively
    
    * unicode : try fix windows
    
    * unicode : category support via std::regex
    
    * unicode : clean-up
    
    * unicode : simplify
    
    * convert : add convert-hf-to-gguf-update.py
    
    ggml-ci
    
    * lint : update
    
    * convert : add falcon
    
    ggml-ci
    
    * unicode : normalize signatures
    
    * lint : fix
    
    * lint : fix
    
    * convert : remove unused functions
    
    * convert : add comments
    
    * convert : exercise contractions
    
    ggml-ci
    
    * lint : fix
    
    * cmake : refactor test targets
    
    * tests : refactor vocab tests
    
    ggml-ci
    
    * tests : add more vocabs and tests
    
    ggml-ci
    
    * unicode : cleanup
    
    * scripts : ignore new update script in check-requirements.sh
    
    * models : add phi-3, mpt, gpt-2, starcoder
    
    * tests : disable obsolete
    
    ggml-ci
    
    * tests : use faster bpe test
    
    ggml-ci
    
    * llama : more prominent warning for old BPE models
    
    * tests : disable test-tokenizer-1-bpe due to slowness
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Jaggzh <jaggz.h@gmail.com>
    Co-authored-by: Kazim Abrar Mahi <kazimabrarmahi135@gmail.com>

.github/workflows/python-lint.yml
.gitignore
Makefile
common/common.cpp
common/common.h
convert-hf-to-gguf-update.py
convert-hf-to-gguf.py
convert-llama-ggml-to-gguf.py
convert-persimmon-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_reader.py
gguf-py/gguf/gguf_writer.py
llama.cpp
llama.h
models/ggml-vocab-bert-bge.gguf
models/ggml-vocab-bert-bge.gguf.inp
models/ggml-vocab-bert-bge.gguf.out
models/ggml-vocab-deepseek-coder.gguf
models/ggml-vocab-deepseek-coder.gguf.inp
models/ggml-vocab-deepseek-coder.gguf.out
models/ggml-vocab-deepseek-llm.gguf
models/ggml-vocab-deepseek-llm.gguf.inp
models/ggml-vocab-deepseek-llm.gguf.out
models/ggml-vocab-falcon.gguf
models/ggml-vocab-falcon.gguf.inp
models/ggml-vocab-falcon.gguf.out
models/ggml-vocab-gpt-2.gguf
models/ggml-vocab-gpt-2.gguf.inp
models/ggml-vocab-gpt-2.gguf.out
models/ggml-vocab-llama-bpe.gguf
models/ggml-vocab-llama-bpe.gguf.inp
models/ggml-vocab-llama-bpe.gguf.out
models/ggml-vocab-llama-spm.gguf
models/ggml-vocab-llama-spm.gguf.inp
models/ggml-vocab-llama-spm.gguf.out
models/ggml-vocab-mpt.gguf
models/ggml-vocab-mpt.gguf.inp
models/ggml-vocab-mpt.gguf.out
models/ggml-vocab-phi-3.gguf
models/ggml-vocab-phi-3.gguf.inp
models/ggml-vocab-phi-3.gguf.out
models/ggml-vocab-stablelm.gguf
models/ggml-vocab-starcoder.gguf
models/ggml-vocab-starcoder.gguf.inp
models/ggml-vocab-starcoder.gguf.out
requirements.txt
requirements/requirements-convert-hf-to-gguf-update.txt
scripts/check-requirements.sh
tests/CMakeLists.txt
tests/test-tokenizer-0-bpe.py
tests/test-tokenizer-0-falcon.cpp
tests/test-tokenizer-0-llama.cpp
tests/test-tokenizer-0-spm.py
tests/test-tokenizer-0.cpp
tests/test-tokenizer-1-spm.cpp
unicode-data.cpp
unicode-data.h
unicode.cpp
unicode.h

commit 3f167476b11efa7ab08f6cacdeb8cab0935c1249
Author: David Renshaw <dwrenshaw@gmail.com>
Date:   Mon Apr 29 09:35:45 2024 -0400

    sampling : use std::random_device{}() for default random seed (#6962)

common/sampling.cpp

commit 3055a4180557c6cbe29eacc8284c9e070ac10eab
Author: Christian Zhou-Zheng <59622928+christianazinn@users.noreply.github.com>
Date:   Mon Apr 29 09:34:41 2024 -0400

    convert : fix conversion of some BERT embedding models (#6937)

convert-hf-to-gguf.py

commit 577277ffd203b190c3dc2ab3e737946dc432132c
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Mon Apr 29 15:08:20 2024 +0200

    make : change GNU make default CXX from g++ to c++ (#6966)

Makefile

commit ca7f29f568803bee4c92d1b3e41c7d721b0dc570
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Mon Apr 29 14:59:47 2024 +0200

    ci : add building in MSYS2 environments (Windows) (#6967)

.github/workflows/build.yml

commit c4f708a93f1df5e35167f9313e000d381298be7f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Apr 29 14:36:22 2024 +0200

    llama : fix typo LAMMAFILE -> LLAMAFILE (#6974)

llama.cpp

commit e00b4a8f816ebc45b98a46e5f5231359b9a017e0
Author: DAN™ <dranger003@gmail.com>
Date:   Sun Apr 28 18:38:44 2024 -0400

    Fix more int overflow during quant (PPL/CUDA). (#6563)
    
    * Fix more int overflow during quant.
    
    * Fix some more int overflow in softmax.
    
    * Revert back to int64_t.

ggml-cuda/convert.cu
ggml-cuda/softmax.cu

commit 7bb36ccf91b8a2e92b182dd75624f1fd7cb205ac
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Apr 28 17:36:18 2024 +0200

    gguf : enforce that tensor names are unique (#6905)
    
    * not allow adding duplicated tensor name
    
    * no duplicated tensor while reading gguf
    
    * typo
    
    * throw exception inside llama_model_loader
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml.c
gguf-py/gguf/gguf_reader.py
gguf-py/gguf/gguf_writer.py
llama.cpp

commit ce023f6f2ff34fbe840e32e65d443d2fed7393de
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Sun Apr 28 22:40:31 2024 +0800

    add device version in device list (#6959)
    
    Co-authored-by: arthw <>

ggml-sycl.cpp

commit 6e472f58e40cd4acf6023e15c75a2700535c5f0b
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Apr 28 00:18:27 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/5c24cf2f0a12ad855f444c30b2421d044120c66f?narHash=sha256-XtTSSIB2DA6tOv%2Bl0FhvfDMiyCmhoRbNB%2B0SeInZkbk%3D' (2024-04-19)
      → 'github:NixOS/nixpkgs/7bb2ccd8cdc44c91edba16c48d2c8f331fb3d856?narHash=sha256-Drmja/f5MRHZCskS6mvzFqxEaZMeciScCTFxWVLqWEY%3D' (2024-04-25)

flake.lock

commit 4dba7e8114d84241c842b986e008af8b88d1a019
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Sat Apr 27 21:02:06 2024 +0200

    Replace "alternative" boolean operator in conditional compilation directive (#6949)

common/log.h

commit b7368332e24c5b2c8038bf8267f43632783fcc35
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Apr 27 17:50:48 2024 +0200

    ci: server: tests python env on github container ubuntu latest / fix n_predict (#6935)
    
    * ci: server: fix python env
    
    * ci: server: fix server tests after #6638
    
    * ci: server: fix windows is not building PR branch

.github/workflows/server.yml
examples/server/server.cpp

commit 928e0b7013c862cf10701957b3d654aa70f11bd8
Author: agray3 <agray3@users.noreply.github.com>
Date:   Fri Apr 26 19:08:30 2024 +0100

    Reset schedule earlier to allow overlap with ggml graph computation on device (#6933)
    
    * Reset schedule earlier to allow overlap with graph computation on device

ggml-backend.c
llama.cpp

commit 0c4d489e29e53589bf13a801fe7c94b7b546d8f6
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 20:06:33 2024 +0200

    quantize: add imatrix and dataset metadata in GGUF (#6658)
    
    * imatrix: save the dataset file used in the output file
    
    * llama: support kv overrides type string string
    
    * common: factorize KV Overrides parsing between common and server
    
    * quantize: add imatrix n entries and dataset KV metadata
    quantize: factorize KV Overrides parsing between common
    #6656
    
    * llama: remove kv override str_value initialization as it does not compile on some toolchain
    
    * quantize: add imatrix m_last_call as `quantize.imatrix.chunks_count`
    
    * quantize: add imatrix filename in KV
    
    * llama: add llama_model_kv_override_free
    
    * common: add llama_model_kv_override_free
    common: free kv override if used after model loading
    
    * llama: finally move the string KV override value to the stack
    
    * llama : minor
    
    * no need to add a NUL to the std::vector, std::string can be initialized from a pair of iterators.
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * kv override: ensure string termination
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

Makefile
common/common.cpp
common/common.h
examples/imatrix/imatrix.cpp
examples/quantize/CMakeLists.txt
examples/quantize/quantize.cpp
examples/server/server.cpp
llama.cpp
llama.h

commit 017e6999b5184234370b22a2f868e1be911e8d88
Author: slaren <slarengh@gmail.com>
Date:   Fri Apr 26 18:39:58 2024 +0200

    add basic tensor data validation function (#6884)
    
    * add basic tensor data validation function
    
    * add --check-tensors command line argument
    
    tensor validation is disabled by default and can be enabled by adding
    `--check-tensors` to the command line arguments.
    
    quantize always validates tensors.

common/common.cpp
common/common.h
ggml-quants.c
ggml.h
llama.cpp
llama.h

commit e2764cd7ca1112d9303eba9e81c9935ee67352ff
Author: slaren <slarengh@gmail.com>
Date:   Fri Apr 26 17:07:42 2024 +0200

    gguf : fix mismatch between alloc and free functions (#6929)

ggml.c

commit 4b1c3c98b442a4c84a788fff6323f6fa7e678a5b
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Fri Apr 26 10:05:33 2024 -0400

    llamafile : use 64-bit integers in sgemm (#6928)

sgemm.cpp
sgemm.h

commit bbe3c6e76157a5d806fdc155451f0ca8936248ee
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 12:27:25 2024 +0200

    ci: server: fix python installation (#6925)

.github/workflows/server.yml

commit 7f5ff558eed0f732af8f25c2ab0645610bdec80c
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 12:15:30 2024 +0200

    server: stop generation at `n_ctx_train` if `n_predict` is not set (#6638)
    
    * server: cap n_predict if not set to n_ctx_train
    
    * server: fix infinite loop
    
    * server: infinite loop, move in process_token
    server: infinite loop: set stop limit to true
    
    * minor: spaces
    
    * minor: spaces
    
    * server: include prompt tokens in the EOS limit

examples/server/server.cpp

commit 9e4e077ec50fde6049b128662c72d37a3c28e34b
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 11:11:51 2024 +0200

    ci: server: fix python installation (#6922)

.github/workflows/server.yml

commit 83b72cb086ce46a33dececc86bfe4648b6120aa8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 26 10:41:53 2024 +0300

    Merge pull request from GHSA-p5mv-gjc5-mwqv
    
    * always use calloc
    
    clamp n_kv on failure to read a kv
    
    * ggml : alternative ctx->header.n_kv update
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml.c

commit d4a9afc1009f0da88d04b2c5f672d81d5ae94675
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 09:27:49 2024 +0200

    ci: server: fix python installation (#6918)

.github/workflows/server.yml

commit 7d641c26ac73956a54b204cabefe85c764823678
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 09:26:59 2024 +0200

    ci: fix concurrency for pull_request_target (#6917)

.github/workflows/bench.yml
.github/workflows/server.yml

commit 5790c8dac1a4f0aa80b4efee3b962d8c04c829e8
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 26 09:26:16 2024 +0200

    bench: server add stop word for PHI-2 (#6916)

examples/server/bench/script.js

commit 46e12c4692a37bdd31a0432fc5153d7d22bc7f72
Author: vik <vikhyatk@gmail.com>
Date:   Thu Apr 25 12:38:31 2024 -0700

    llava : add support for moondream vision language model (#6899)
    
    * add support for moondream vision language model
    
    This required making the following changes to the CLIP model:
    
    1. Support for patch embedding bias.
    2. Make class embedding and pre-layernorm optional.
    3. Add support for post-layernorm.
    
    * Update examples/llava/clip.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
examples/llava/clip.cpp

commit dba497e0c1eff27cf0e85d1d73c86fa08e1b5557
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 21:31:17 2024 +0300

    cmake : restore LLAMA_LLAMAFILE_DEFAULT

CMakeLists.txt
llama.cpp

commit fa0b4ad25208d5ec2df6b998ccb29b49b249e82c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 18:59:51 2024 +0300

    cmake : remove obsolete ANDROID check

CMakeLists.txt

commit d6e1d44f16e5580cf47f13325ff49960bf13ca37
Author: slaren <slarengh@gmail.com>
Date:   Thu Apr 25 17:59:03 2024 +0200

    llama : synchronize before get/set session data (#6911)

llama.cpp

commit 853d06ffe26621176d60909a6e3f7c4cd067b305
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 17:06:27 2024 +0300

    ci : tmp disable slow tests

ci/run.sh

commit 3fe0596c1817a6114ffffb6dbfd6c36ca7815dc7
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Thu Apr 25 09:52:28 2024 -0400

    readme : update model list (#6908)
    
    * Update README.md
    
    * missing space
    
    * llama3 !

README.md

commit 0ead1f1072fdc70720f92e008a294dc74a826b1d
Author: slaren <slarengh@gmail.com>
Date:   Thu Apr 25 15:23:47 2024 +0200

    llama : check that all the tensor data is in the model file (#6885)
    
    * llama : check that all the tensor data is in the model file
    
    * also check for unsigned overflow

llama.cpp

commit 51543729ff5b1361ebfb164875c93dff0850d5fe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 15:48:25 2024 +0300

    ggml : fix redefinition of vaddvq_f32 for 32-bit ARM (#6906)

ggml.c

commit 4ab99d8d4762869506bb675b36501fd7c2af00b2
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Apr 25 14:38:14 2024 +0200

    clip : rename lerp function to avoid conflict (#6894)
    
    This commit renamesthe lerp (linear interpolation) function in clip.cpp
    to avoid a conflict with the lerp function in the <cmath> standard C++
    library when using c++20.
    
    The motivation for this change is to enable projects that use c++20 to
    be able to compile clip.cpp without having to resort to patching it. The
    lerp function was added to cmath in version C++20 (202002L) and is why
    this is not causing any issue at the moment as C++11/C++17 is currently
    used by llama.cpp.
    
    I realize that llama.cpp uses either C++11 (or C++17 in the case for
    SYCL) but wanted to ask if this would be an acceptable change just the
    same.
    
    Refs: https://en.cppreference.com/w/cpp/numeric/lerp
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/llava/clip.cpp

commit 54770413c484660d021dd51b5dbacab7880b8827
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 15:12:28 2024 +0300

    ggml : fix MIN / MAX macros (#6904)
    
    ggml-ci

ggml-impl.h
ggml-quants.c

commit aa750c1ede6232c91de890a14a7731d6daa2bc8e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 25 14:27:20 2024 +0300

    tests : minor bash stuff (#6902)
    
    * tests : minor bash stuff
    
    ggml-ci
    
    * llama : fix build
    
    ggml-ci
    
    * tests : fix CUR_DIR -> ROOT_DIR
    
    ggml-ci
    
    * tests : fix fname
    
    ggml-ci

ci/run.sh
examples/gguf-split/tests.sh
examples/quantize/tests.sh
examples/server/tests/tests.sh
llama.cpp

commit 1966eb2615242f224bf9ca939db8905ab6a174a0
Author: jiez <373447296@qq.com>
Date:   Thu Apr 25 18:29:35 2024 +0800

    quantize : add '--keep-split' to quantize model into shards (#6688)
    
    * Implement '--keep-split' to quantize model into several shards
    
    * Add test script
    
    * Update examples/quantize/quantize.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Split model correctly even if tensor id is out-of-order
    
    * Update llama_model_quantize_params
    
    * Fix preci failures
    
    ---------
    
    Co-authored-by: z5269887 <z5269887@unsw.edu.au>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/quantize/quantize.cpp
examples/quantize/test.sh
llama.cpp
llama.h

commit 784e11dea1f5ce9638851b2b0dddb107e2a609c8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Apr 24 21:29:13 2024 +0200

    README: add graphic for matrix multiplication (#6881)

README.md
media/matmul.png
media/matmul.svg

commit b4e4b8a9351d918a56831c73cf9f25c1837b80d1
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Wed Apr 24 08:10:07 2024 -0500

    llama : add llama_get_pooling_type function (#6862)
    
    * add llama_get_pooling_type function
    
    * fix argument name, move with ctx funcs

common/common.h
llama.cpp
llama.h

commit 3fe847b5747676ee1bf90371c46ed0bc66b57240
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Wed Apr 24 12:54:24 2024 +0200

    server : do not apply Markdown formatting in code sections (#6850)

examples/server/public/index.html

commit 37246b1031b1680c0dcaf20aef736d6b446203fa
Author: Kyle Mistele <kyle@mistele.com>
Date:   Wed Apr 24 05:15:29 2024 -0500

    common : revert showing control tokens by default for server (#6860)
    
    * fix: revert showing control tokens by default
    
    * feat: revert changes to default behavior of llama_token_to_piece; provide overridden declaration to receive "bool special" param to toggle showing control tokens
    
    * feat: use the overridden declaration of llama_token_to_piece from common/common.cpp to specify "false" so that control tokens are not shown in chat completion responses"
    
    * common : simplify
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
examples/server/server.cpp

commit 28103f4832e301a9c84d44ff0df9d75d46ab6c76
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Apr 24 11:08:36 2024 +0200

    Server: fix seed for multiple slots (#6835)
    
    * Server: add tests for consistent results
    
    * sampling: separate rng per sampling context

common/common.cpp
common/sampling.cpp
common/sampling.h
examples/lookup/lookup-stats.cpp
examples/lookup/lookup.cpp
examples/main/main.cpp
examples/server/server.cpp
examples/server/tests/features/results.feature
examples/server/tests/features/steps/steps.py
llama.cpp
llama.h

commit c0d1b3e03e27634ac2871761f5033cf9324d472d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 24 12:00:07 2024 +0300

    ggml : move 32-bit arm compat in ggml-impl.h (#6865)
    
    ggml-ci

ggml-impl.h
ggml-quants.c

commit abd3314064cd3c513f9eef34c3ba6c23a107442c
Author: Tristan Druyen <tristan@vault81.mozmail.com>
Date:   Wed Apr 24 10:52:37 2024 +0200

    llama : add phi 3 chat template (#6857)
    
    * Add phi 3 chat template & tests
    
    * test : fix chat template result
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp
tests/test-chat-template.cpp

commit 3fec68be4e9577fc53158366d3b3af039c17bb1f
Author: Junyang Lin <justinlin930319@hotmail.com>
Date:   Wed Apr 24 15:16:21 2024 +0800

    convert : add support of codeqwen due to tokenizer (#6707)
    
    * add support of codeqwen due to tokenizer
    
    * override load_hparams
    
    * fix typo
    
    * fix load_params
    
    * convert : fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py

commit c8297c6af5693555652c40b95974b95d49d2674d
Author: liuwei-git <14815172+liuwei-git@users.noreply.github.com>
Date:   Wed Apr 24 15:00:37 2024 +0800

    llama : add phi3 support (#6852)
    
    * add explicit phi3 support
    
    * add explicit phi3 support
    
    * remove unused code
    
    * convert : add BOS token
    
    * llama : match EOT token <|end|>
    
    * llama : minor / style
    
    * llama : tabs -> spaces
    
    * convert : fix lint checks
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 4e96a812b3ce7322a29a3008db2ed73d9087b176
Author: Anas Ahouzi <112881240+aahouzi@users.noreply.github.com>
Date:   Tue Apr 23 02:53:18 2024 +0200

    [SYCL] Windows default build instructions without -DLLAMA_SYCL_F16 flag activated (#6767)
    
    * Fix FP32/FP16 build instructions
    
    * Fix typo
    
    * Recommended build instruction
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * Recommended build instruction
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * Recommended build instruction
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * Add comments in Intel GPU linux
    
    ---------
    
    Co-authored-by: Anas Ahouzi <112881240+aahouzi-intel@users.noreply.github.com>
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>

README-sycl.md

commit 192090bae47960f0d38d4967abe398a5d190057e
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Mon Apr 22 15:00:36 2024 -0400

    llamafile : improve sgemm.cpp (#6796)
    
    * llamafile : improve sgemm.cpp
    
    - Re-enable by default
    - Fix issue described in #6716
    - Make code more abstract, elegant, and maintainable
    - Faster handling of weirdly shaped `m` an `n` edge cases
    
    * Address review comments
    
    * Help clang produce fma instructions
    
    * Address review comments

CMakeLists.txt
Makefile
ggml.c
sgemm.cpp

commit e931888d5024de814ce7119a18d6a959bfff3821
Author: Dave Airlie <airlied@gmail.com>
Date:   Tue Apr 23 00:05:06 2024 +1000

    ggml : fix calloc argument ordering. (#6820)
    
    Latest gcc complains here:
    /home/airlied/devel/llama.cpp/ggml-alloc.c: In function ‘ggml_gallocr_new_n’:
    /home/airlied/devel/llama.cpp/ggml-alloc.c:374:59: warning: ‘calloc’ sizes specified with ‘sizeof’ in the earlier argument and not in the later argument [-Wcalloc-transposed-args]
      374 |     ggml_gallocr_t galloc = (ggml_gallocr_t)calloc(sizeof(struct ggml_gallocr), 1);
          |                                                           ^~~~~~
    /home/airlied/devel/llama.cpp/ggml-alloc.c:374:59: note: earlier argument should specify number of elements, later size of each element
    
    and a bunch more.
    
    calloc is specified to take nmemb first then size, so realign the code.
    
    In a couple of places there was a * x, 1 so I fixed those to use calloc properly.

ggml-alloc.c
ggml-backend.c

commit 8960fe86ae075c846c5df8848230d1904ba8877f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 22 15:41:11 2024 +0300

    llama : fix typo in <|im_end|> token text (#6745)

llama.cpp

commit c0956b09ba845a7cd787d5580d7c8b96e80f40f5
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Apr 22 13:22:54 2024 +0200

    ci: fix job are cancelling each other (#6781)

.github/workflows/bench.yml
.github/workflows/server.yml

commit e9b4a1bf68c18beff4e33f23ea62c1245b296915
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Apr 21 00:17:47 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/1042fd8b148a9105f3c0aca3a6177fd1d9360ba5?narHash=sha256-3sbWO1mbpWsLepZGbWaMovSO7ndZeFqDSdX0hZ9nVyw%3D' (2024-04-10)
      → 'github:NixOS/nixpkgs/5c24cf2f0a12ad855f444c30b2421d044120c66f?narHash=sha256-XtTSSIB2DA6tOv%2Bl0FhvfDMiyCmhoRbNB%2B0SeInZkbk%3D' (2024-04-19)

flake.lock

commit 5cf5e7d490dfdd2e70bface2d35dfd14aa44b4fb
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Sun Apr 21 18:48:53 2024 +0100

    `build`: generate hex dump of server assets during build (#6661)
    
    * `build`: generate hex dumps of server assets on the fly
    
    * build: workaround lack of -n on gnu xxd
    
    * build: don't use xxd in cmake
    
    * build: don't call xxd from build.zig
    
    * build: more idiomatic hexing
    
    * build: don't use xxd in Makefile (od hackery instead)
    
    * build: avoid exceeding max cmd line limit in makefile hex dump
    
    * build: hex dump assets at cmake build time (not config time)

.gitignore
Makefile
build.zig
examples/server/CMakeLists.txt
examples/server/completion.js.hpp
examples/server/deps.sh
examples/server/index.html.hpp
examples/server/index.js.hpp
examples/server/json-schema-to-grammar.mjs.hpp
scripts/xxd.cmake

commit 40f74e4d739e9250431cf339ae7588b28d8d0663
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 21 18:36:45 2024 +0300

    llama : add option to render special/control tokens (#6807)
    
    * make : fix common dep on llama.h
    
    * llama : add option to render special tokens
    
    * readme : add API change notice
    
    ggml-ci
    
    * swift : fix build

Makefile
README.md
common/common.cpp
examples/batched.swift/Sources/main.swift
examples/llama.swiftui/llama.cpp.swift/LibLlama.swift
llama.cpp
llama.h

commit b9cc76d87e3d7ae5900f19d4fe8f8976d0a35888
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 21 16:47:57 2024 +0300

    ggml : fix ggml_backend_cpu_supports_op() for CPY (#0)

ggml-backend.c

commit 7dbdba5690ca61b3ee8c92cfac8e7e251042e787
Author: Wouter <9594229+DifferentialityDevelopment@users.noreply.github.com>
Date:   Sun Apr 21 15:03:39 2024 +0200

    llama : add llama-3 chat template (#6751)
    
    * Added llama-3 chat template
    
    * Update llama.cpp
    
    Co-authored-by: Samuel Tallet <36248671+SamuelTallet@users.noreply.github.com>
    
    * Update llama.cpp
    
    Co-authored-by: Samuel Tallet <36248671+SamuelTallet@users.noreply.github.com>
    
    * Update tests/test-chat-template.cpp
    
    Co-authored-by: Samuel Tallet <36248671+SamuelTallet@users.noreply.github.com>
    
    * Added EOS stop sequence according to https://github.com/ggerganov/llama.cpp/pull/6751#issuecomment-2065602862
    
    * Removed adding of BOS token before first message
    
    * Removed bos token from expected output from llama-3
    
    * Update tests/test-chat-template.cpp
    
    Co-authored-by: Rene Leonhardt <65483435+reneleonhardt@users.noreply.github.com>
    
    * Update tests/test-chat-template.cpp
    
    Co-authored-by: Rene Leonhardt <65483435+reneleonhardt@users.noreply.github.com>
    
    * Added <|end_of_text|> as another stop token
    
    * Reverted last change of adding the end_of_text stop word for llama 3
    
    ---------
    
    Co-authored-by: Wouter Tichelaar <tichelaarw@spar.net>
    Co-authored-by: Samuel Tallet <36248671+SamuelTallet@users.noreply.github.com>
    Co-authored-by: Rene Leonhardt <65483435+reneleonhardt@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp
tests/test-chat-template.cpp

commit c1386c936e9fbc38eb2816c711ab28f13355708e
Author: pmysl <piotr.myslinski@outlook.com>
Date:   Sun Apr 21 14:49:30 2024 +0200

    gguf-py : add IQ1_M to GGML_QUANT_SIZES (#6761)

gguf-py/gguf/constants.py

commit e8d35f47cb8cb4002fca02e18aaa1cb9fa21d6f1
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Sun Apr 21 20:35:40 2024 +0800

    doc : add link to falcon (#6789)

README.md

commit 2cca09d509c0e114acc16cb347c3cdd86e5f1b40
Author: Mohammadreza Hendiani <mohammad.r.hendiani@gmail.com>
Date:   Sun Apr 21 16:02:05 2024 +0330

    readme : add Fedora instructions (#6783)
    
    * added fedora to list of distros that may need the package (the packages have the same name on Fedora)
    
    * how to add clblast that is avalible in the fedora repos

README.md

commit 89b0bf0d5dc123cc1053708f5f84b89e52cdc80b
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Sun Apr 21 08:19:04 2024 -0400

    llava : use logger in llava-cli (#6797)
    
    This change removes printf() logging so llava-cli is shell scriptable.

examples/llava/clip.cpp
examples/llava/llava-cli.cpp
examples/llava/llava.cpp

commit b97bc3966e852adb626c90be64fd48282800f504
Author: Pedro Cuenca <pedro@huggingface.co>
Date:   Sun Apr 21 13:50:41 2024 +0200

    llama : support Llama 3 HF conversion (#6745)
    
    * Support Llama 3 conversion
    
    The tokenizer is BPE.
    
    * style
    
    * Accept suggestion
    
    Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
    
    * llama : add llama_token_is_eog()
    
    ggml-ci
    
    * llama : auto-detect more EOT tokens when missing in KV data
    
    * convert : replacing EOS token is a hack
    
    * llama : fix codegemma EOT token + add TODOs
    
    * llama : fix model type string for 8B model
    
    ---------
    
    Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
convert.py
examples/batched.swift/Sources/main.swift
examples/batched/batched.cpp
examples/beam-search/beam-search.cpp
examples/infill/infill.cpp
examples/llama.android/app/src/main/cpp/llama-android.cpp
examples/llama.swiftui/llama.cpp.swift/LibLlama.swift
examples/llava/llava-cli.cpp
examples/lookahead/lookahead.cpp
examples/lookup/lookup.cpp
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/passkey/passkey.cpp
examples/server/server.cpp
examples/server/utils.hpp
examples/simple/simple.cpp
examples/speculative/speculative.cpp
llama.cpp
llama.h

commit b8109bc0139f15a5b321909f47510b89dca47ffc
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Sun Apr 21 00:29:50 2024 +0800

    doc : server tests require llama to be built with curl enabled (#6788)

examples/server/tests/README.md

commit aed82f6837a3ea515f4d50201cfc77effc7d41b4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 20 13:27:12 2024 +0300

    common : try to fix Android CI (#6780)
    
    * common : disable get_math_cpu_count() until Android CI gets fixed
    
    * common : another try

common/common.cpp

commit 0e4802b2ecbaab04b4f829fde4a3096ca19c84b5
Author: loonerin <132926317+loonerin@users.noreply.github.com>
Date:   Fri Apr 19 13:03:35 2024 -0400

    ci: add ubuntu latest release and fix missing build number (mac & ubuntu) (#6748)

.github/workflows/build.yml

commit 637e9a86c220718d008b54842dfd294aa96d3b7a
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 19 13:19:01 2024 +0200

    server: static: upstream upgrade (#6765)

examples/server/index.js.hpp
examples/server/public/index.js

commit 9958c81b798a5872087b30b360e4674871f2479e
Author: nopperl <54780682+nopperl@users.noreply.github.com>
Date:   Fri Apr 19 09:35:54 2024 +0000

    Implement the OLMo architecture (#6741)
    
    * implement olmo architecture
    
    * remove unused variable
    
    * remove unused moe branch
    
    * remove check for weight
    
    * remove superfluous moe, bias and rope tensors
    
    * clarified comment
    
    * fix clamp_kqv setting
    
    * remove obsolete parameter name filter

README.md
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
llama.cpp

commit 8b1b1f4982d3e9b994308d05a1c8b9e45c23edb5
Author: Austin <77757836+teleprint-me@users.noreply.github.com>
Date:   Fri Apr 19 03:16:45 2024 -0400

    train : add general name (#6752)
    
    * llama : make general.name optional
    
    * train: Add 'general.name' to model metadata
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    
    ---------
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/train-text-from-scratch/train-text-from-scratch.cpp

commit bca40e98149c7b673558ddd7a3ebeffef789349d
Author: Neo Zhang <14088817+arthw@users.noreply.github.com>
Date:   Fri Apr 19 09:16:31 2024 +0800

    fix wrong parameter in cmd in readme-sycl.md (#6755)
    
    Co-authored-by: jianyuzh <jianyu.zhang@intel.com>

README-sycl.md

commit 0d56246f4b9764158525d894b96606f6163c53a8
Author: slaren <slarengh@gmail.com>
Date:   Thu Apr 18 15:18:48 2024 +0200

    ggml : group all experts in a single ggml_mul_mat_id (#6505)
    
    * ggml : group all experts in a single ggml_mul_mat_id
    cuda : improve mmid row copy
    
    * cuda : fix bin bcast with non-cont src0
    
    * test-backend-ops : only run all mul mat tests for base types
    
    * llama : disable moe offloading with SYCL
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/imatrix/imatrix.cpp
ggml-cuda.cu
ggml-cuda/binbcast.cu
ggml-cuda/convert.cu
ggml-metal.m
ggml-metal.metal
ggml-sycl.cpp
ggml.c
ggml.h
llama.cpp
scripts/compare-commits.sh
tests/test-backend-ops.cpp

commit 03c0946d73c63ea73e1d85015b7088298443d438
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Thu Apr 18 13:49:01 2024 +0200

    convert : support models with multiple chat templates (#6588)
    
    * Support converting models with multiple chat templates
    
    Adds the following metadata:
    * tokenizer.chat_templates
    * tokenizer.chat_template.<name1>
    * tokenizer.chat_template.<name2>
    * tokenizer.chat_template.<...>
    
    Where `tokenizer.chat_templates` is an array of the template names (except `default`), `default` is added to the regular `tokenizer.chat_template`.
    
    * replace filtered characters with underscore
    
    * New script to add/modify/remove metadata
    
    This scripts creates a copy of a GGUF file and allows you to add/modify/remove metadata in the process.
    
    Most importantly this allows you to update chat templates, either as a string or directly from an updated tokenizer_config.json file.
    
    * Add files via upload
    
    add new script to project/readme
    
    * flake--

gguf-py/README.md
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/vocab.py
gguf-py/pyproject.toml
gguf-py/scripts/__init__.py
gguf-py/scripts/gguf-new-metadata.py

commit e11b2e6e1e18522ca7cf129600875a0f6fb9307d
Author: Ren Xuancheng <jklj077@users.noreply.github.com>
Date:   Thu Apr 18 19:38:04 2024 +0800

    Qwen2 : assume tied weights if lm_head/output weights is missing (#6738)

llama.cpp

commit c71bfd736ee99a56e697697b39240f2ee06ed26d
Author: slaren <slarengh@gmail.com>
Date:   Thu Apr 18 09:04:47 2024 +0200

    llama : fix compatibility with old 2 expert models (#6735)

llama.cpp

commit 3b8f1ec4b18770531d0b1d792f3edf08254e4f0c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 17 23:58:26 2024 +0300

    llamafile : tmp disable + build sgemm.o when needed (#6716)
    
    * build : sgemm.o only when needed
    
    ggml-ci
    
    * llamafile : tmp disable due to MoE bug
    
    ggml-ci

CMakeLists.txt
Makefile

commit 8dd1ec8b3ffbfa2d26e82e672cea89f5eeb2f141
Author: Yaroslav <yaroslav.yashin@me.com>
Date:   Wed Apr 17 14:47:50 2024 +0200

    readme : add UI (#6724)
    
    * Update README.md
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md

commit facb8b56f8fd3bb10a693bf0943ae9d69d0828ef
Author: Zheng.Deng <32841220+dengzheng-cloud@users.noreply.github.com>
Date:   Wed Apr 17 04:51:07 2024 +0800

    convert : fix autoawq gemma (#6704)
    
    * fix autoawq quantized gemma model convert error
    
    using autoawq to quantize gemma model will include a lm_head.weight tensor in model-00001-of-00002.safetensors. it result in this situation that convert-hf-to-gguf.py can't map lm_head.weight. skip loading this tensor could prevent this error.
    
    * change code to full string match and print necessary message
    
    change code to full string match and print a short message to inform users that lm_head.weight has been skipped.
    
    ---------
    
    Co-authored-by: Zheng.Deng <32841220+CUGfred@users.noreply.github.com>

convert-hf-to-gguf.py

commit 532c1737a14bb4b99747e6f460874947df37e450
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 16 23:50:38 2024 +0300

    llama : make general.name optional (#6709)

llama.cpp

commit 666867b799ddd9da7dfdc905ece291ecf286effa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 16 23:50:22 2024 +0300

    ggml : fix llamafile sgemm wdata offsets (#6710)
    
    ggml-ci

CMakeLists.txt
Makefile
ggml.c

commit 8cc91dc63c0df397d644a581b2cbeea74eb51ae0
Author: Justine Tunney <jtunney@mozilla.com>
Date:   Tue Apr 16 14:55:30 2024 -0400

    ggml : add llamafile sgemm (#6414)
    
    This change upstreams llamafile's cpu matrix multiplication kernels
    which improve image and prompt evaluation speed. For starters, Q4_0
    and Q8_0 weights should go ~40% faster on CPU. The biggest benefits
    are with data types like f16 / f32, which process prompts 2x faster
    thus making them faster than quantized data types for prompt evals.
    
    This change also introduces bona fide AVX512 support since tinyBLAS
    is able to exploit the larger register file. For example, on my CPU
    llama.cpp llava-cli processes an image prompt at 305 tokens/second,
    using the Q4_K and Q4_0 types, which has always been faster than if
    we used f16 LLaVA weights, which at HEAD go 188 tokens/second. With
    this change, f16 LLaVA performance leap frogs to 464 tokens/second.
    
    On Intel Core i9-14900K this change improves F16 prompt perf by 5x.
    For example, using llama.cpp at HEAD with Mistral 7b f16 to process
    a 215 token prompt will go 13 tok/sec. This change has fixes making
    it go 52 tok/sec. It's mostly thanks to my vectorized outer product
    kernels but also because I added support for correctly counting the
    number of cores on Alderlake, so the default thread count discounts
    Intel's new efficiency cores. Only Linux right now can count cores.
    
    This work was sponsored by Mozilla who's given permission to change
    the license of this code from Apache 2.0 to MIT. To read more about
    what's improved, and how it works, see: https://justine.lol/matmul/

CMakeLists.txt
Makefile
Package.swift
build.zig
common/common.cpp
common/common.h
examples/llama-bench/llama-bench.cpp
ggml-impl.h
ggml-quants.c
ggml.c
sgemm.cpp
sgemm.h

commit dbceec87c0221ec952e69448df6a71f1372a7487
Author: Ashish <1856117+ashishdatta@users.noreply.github.com>
Date:   Tue Apr 16 08:48:35 2024 -0700

    llama : add StableLM2 12B (#6635)
    
    * StableLM2 12B support for huggingface -> GGUF
    
    * StableLM12 tensormapping and constants
    
    * StableLM-2-12b model support
    
    * fix
    
    * Added 12B support
    
    * Removed autoformatting; resolved bug where model_arch was not selecting StableLM2
    
    * Formatting
    
    * Do QK norm stacking in model conversion step
    
    * Converge StableLM and StableLM2 code to simplify graph construction
    
    * Fix accidental removal
    
    * Removed warnings
    
    * Revert formatter
    
    * Move QK norm stack to private function so it's easier to read
    
    * refactor stablelm graph builder to support 1.6, 3b and 12b more efficiently
    
    * Proper check for None type for new_name to avoid crash; formatting; revert change to base class `write_tensors()`
    
    * Format
    
    * Formatting
    
    * format
    
    Co-authored-by: compilade <git@compilade.net>
    
    * Fix incorrect check for K norm
    
    * space after commas; Keep indentation multiple of 4 spaces
    
    * Flake8 format
    
    * Removed unnecessary conditional branches
    
    * Removed unused comment
    
    * Fixed incorrect tensor passing
    
    * Format
    
    ---------
    
    Co-authored-by: compilade <git@compilade.net>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
llama.cpp

commit f4dea7da1841a92d2788b0535063abf2f0e28461
Author: Shijie <821898965@qq.com>
Date:   Tue Apr 16 23:40:48 2024 +0800

    llama : add qwen2moe (#6074)
    
    * support qwen2moe
    
    * fix-review
    
    * metal : support unary ops for nelements % 4 != 0
    
    * metal : require contiguousness for float4 unary kernels
    
    * metal : require contiguousness for float4 unary kernels (cont)
    
    * fix-review
    
    * names : for brevity "SHARED_EXP" -> "SHEXP"
    
    * llama : reuse build_moe_ffn()
    
    * llama : add model type name
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
ggml-metal.m
ggml-metal.metal
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp
tests/test-backend-ops.cpp

commit 8a56075b07a8b571bf95a912ffdce4c928c2b414
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Apr 16 08:34:06 2024 +0200

    gritlm : add --outdir option to hf.sh script (#6699)
    
    This commit updates the hf.sh script usage to include the --outdir option
    and specifies the models directory as the output directory.
    
    The motivation for this is to avoid cluttering the root directory with
    model files.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/gritlm/README.md

commit 58227ffdeb4fb89cacb0cffaadf76b1914324ad3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 16 09:28:33 2024 +0300

    perplexity : require positive --ctx-size arg (#6695)

examples/perplexity/perplexity.cpp

commit 4fbd8098e63670c6ae11a8adc350f5ba191cfda3
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Apr 16 08:13:13 2024 +0200

    gguf : add special tokens metadata for FIM/Infill (#6689)
    
    This commit adds special token metadata for Fill-In-the-Middle
    (FIM)/Infill to the GGUF model.
    
    The motivation for this is that currently there is support for CodeLlama
    but other models exist now like CodeGemma, but the different models use
    different token ids for the special tokens and this commit allows for
    supporting multiple models.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
llama.cpp

commit 7593639ce335e8d7f89aa9a54d616951f273af60
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Mon Apr 15 18:35:21 2024 +0100

    `main`: add --json-schema / -j flag (#6659)
    
    * main: add --json-schema / -j
    
    * json: move json-schema-to-grammar to common lib
    
    * json: fix zig build

Makefile
build.zig
common/CMakeLists.txt
common/common.cpp
examples/main/README.md
examples/server/CMakeLists.txt
tests/CMakeLists.txt

commit 132f55795e51094954f1b1f647f97648be724a3a
Author: compilade <git@compilade.net>
Date:   Mon Apr 15 08:56:55 2024 -0400

    llama : fix restoring the number of outputs from state files (#6687)

llama.cpp

commit 3272896d79465fd2befef3425dac8e83933b7ea4
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Apr 15 14:18:47 2024 +0200

    server : revert "minor layout improvements" (#6684)
    
    This reverts commit b3a96f27f065a828f08c5d89ff60aab5361188fe.

examples/server/index.html.hpp
examples/server/public/index.html

commit 7fc16a2c32650d46e79b382ecc6720f58c82f31b
Author: Steven Prichard <spprichard20@gmail.com>
Date:   Mon Apr 15 05:14:46 2024 -0500

    swift : linux support (#6590)
    
    - Package.swift now supports conditional compilation based on OS
    - Allows for package to be used by SPM on Non-Apple platforms
    
    Co-authored-by: Steven Prichard <steven.prichard@justeattakeaway.com>

Package.swift

commit 17e98d4c96a583d420f12046bc92102381dbd28e
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Mon Apr 15 17:12:26 2024 +0800

    fix mul_mat_id() for new input, make the ut pass (#6682)

ggml-sycl.cpp

commit 1958f7e06ca2d2e3ab5698cc67513ba359144d8e
Author: David Renshaw <dwrenshaw@gmail.com>
Date:   Sun Apr 14 15:24:15 2024 -0400

    llama : add missing kv clear in llama_beam_search (#6664)

llama.cpp

commit 04fbc5f23e9708136ff03ebe194c7a7e965a7ca4
Author: Chao Jiang <jc19chaoj@zoho.com>
Date:   Mon Apr 15 00:16:34 2024 +0800

    Add Command R chat template (#6650)
    
    * Add chat template for command-r model series
    
    * Fix indentation
    
    * Add chat template test for command-r models and update the implementation to trim whitespaces
    
    * Remove debug print

llama.cpp
tests/test-chat-template.cpp

commit f184dd920852d6d372b754f871ee06cfe6f977ad
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 14 16:55:30 2024 +0300

    flake.lock: Update (#6669)

flake.lock

commit 422c2aff1c9735853c9d8f5162104e41a364adc4
Author: Dave <dave-fl@users.noreply.github.com>
Date:   Sun Apr 14 07:14:19 2024 -0400

    Added support for GGML_OP_CLAMP in Metal (#6662)
    
    * Added support for GGML_OP_CLAMP in Metal
    
    * Corrected size
    
    ---------
    
    Co-authored-by: dave-fl <dave@Davids-MacBook-Pro.local>

ggml-metal.m
ggml-metal.metal

commit 8800226d65d5c98cd34eede6a6c05c78405c52da
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Sun Apr 14 13:12:59 2024 +0200

    Fix --split-max-size (#6655)
    
    * Fix --split-max-size
    
    Byte size calculation was done on int and overflowed.
    
    * add tests.sh
    
    * add examples test scripts to ci run
    
    Will autodiscover examples/*/tests.sh scripts and run them.
    
    * move WORK_PATH to a subdirectory
    
    * clean up before and after test
    
    * explicitly define which scripts to run
    
    * add --split-max-size to readme

ci/run.sh
examples/gguf-split/README.md
examples/gguf-split/gguf-split.cpp
examples/gguf-split/tests.sh

commit e689fc4e912feb19085be6894f475a873759cbfe
Author: Jaemin Son <woalsdnd@gmail.com>
Date:   Sun Apr 14 20:12:36 2024 +0900

    [bug fix] convert github repository_owner to lowercase (#6673)

.github/workflows/docker.yml

commit a4ec34e1cd68560e7ff16e21ddc6962b6abd3d1d
Author: James A Capozzoli <157492257+jac-jim@users.noreply.github.com>
Date:   Sun Apr 14 04:40:18 2024 -0400

    convert : enable the `--use-temp-file` cli flag (#6645)

convert-hf-to-gguf.py

commit de17e3f7455dc7fd298cc61d86798533b9ca7a29
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Sun Apr 14 10:42:29 2024 +0800

    fix memcpy() crash, add missed cmd in guide, fix softmax (#6622)
    
    * disable mmap to fix memcpy crash, add missed cmd in guide, fix softmax
    
    * refactor to disable mmap for SYCL backend
    
    * fix compile error in other os
    
    * refactor the solution, use host buf to fix it, instead of disable mmap
    
    * keep to support mmap()
    
    * use host buff to reduce malloc times
    
    * revert to malloc/free solution, for threaad safe

README-sycl.md
examples/sycl/build.sh
examples/sycl/run-llama2.sh
ggml-sycl.cpp

commit b5e7285baffb0da8a6619567b52d8e67de41291d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Apr 14 00:21:55 2024 +0200

    CUDA: fix matrix multiplication logic for tests (#6667)

ggml-cuda.cu

commit 4bd0f93e4ab4fe6682e7d0241c1bdec1397e954a
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Apr 13 11:33:52 2024 +0200

    model: support arch `DbrxForCausalLM` (#6515)
    
    * model: dbrx convert to gguf
    #6344
    
    * llama: support dbrx
    #6344
    
    * doc: dbrx: add the model as supported
    
    * scripts: get-wikitext-2 add unzip
    
    * llama: increase maximum experts allowed
    
    * llama: factorize moe graph implementation between grok, mixtral and dbrx
    
    
    ---------
    
    Co-authored-by: Megha Agarwal <16129366+megha95@users.noreply.github.com>

README.md
convert-hf-to-gguf.py
examples/eval-callback/eval-callback.cpp
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp
scripts/get-wikitext-2.sh

commit ab9a3240a9da941fdef5cd4a25f2b97c2f5a67aa
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Apr 12 19:43:38 2024 +0100

    JSON schema conversion: ⚡️ faster repetitions, min/maxLength for strings, cap number length (#6555)
    
    * json: rename python schema converter to make import easier
    
    * server: skip null json_schema / grammar fields
    
    * json: deps management for primitive rules (+ allow null values)
    
    * json: optimize repetitions for minItems/maxItems and regexps: `a{,3}` goes from `"a"? "a"? "a"?` (explosive combos) to `(a (a (a)?)?)?`
    
    * grammars: add troubleshooting section to readme
    
    * json: cap length of numbers to 15 digits before/after decimal point
    
    (avoids infinite gen, e.g. "one third" -> `0.333333333333...`)
    
    * json: unify all repetition code (w/ or w/o sep)
    
    * json: support string minLength/maxLength
    
    * server+json: update server/README w/ result_format
    
    * nits
    
    * json: fix type error w/ python 3.8
    
    * json: fix server/README (json_schema in /completion vs. result_format in /v1/chat/completions)
    
    * json: simplify DOT `{"type": "string", "pattern": "^.$"}`
    
    * json: remove recursion in opt_repetitions (avoids Python stack overflow)
    
    * json: rm dead code
    
    * json: rm useless assert & ggml.h import

common/json-schema-to-grammar.cpp
examples/json_schema_to_grammar.py
examples/regex-to-grammar.py
examples/server/README.md
examples/server/json-schema-to-grammar.mjs.hpp
examples/server/public/json-schema-to-grammar.mjs
examples/server/server.cpp
examples/ts-type-to-grammar.sh
grammars/README.md
tests/test-json-schema-to-grammar.cpp

commit fbbc030ba93561fac842af994c5c6c4c1147f13b
Author: slaren <slarengh@gmail.com>
Date:   Fri Apr 12 18:13:20 2024 +0200

    metal : unify mul_mv_id kernels (#6556)

ggml-metal.m
ggml-metal.metal
ggml.c
tests/test-backend-ops.cpp

commit 4cc120c7443cf9dab898736f3c3b45dc8f14672b
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Apr 12 14:11:46 2024 +0200

    infill : add download instructions for model (#6626)
    
    * infill : add download instructions for model
    
    This commit adds instructions on how to download a CodeLlama model
    using the `hf.sh` script. This will download the model and place it
    in the `models` directory which is the same model use later by the
    infill example.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! infill : add download instructions for model
    
    Clarify the reason for using CodeLlama.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/infill/README.md

commit 24ee66ed0d908d156bd0d1747b63a636a495cd7a
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 12 13:49:21 2024 +0200

    server : coherent log output for KV cache full (#6637)

examples/server/server.cpp

commit 91c736015b66ba1d0b82cbae6313b6d5eaa61b68
Author: jiez <373447296@qq.com>
Date:   Fri Apr 12 18:45:06 2024 +0800

    llama : add gguf_remove_key + remove split meta during quantize (#6591)
    
    * Remove split metadata when quantize model shards
    
    * Find metadata key by enum
    
    * Correct loop range for gguf_remove_key and code format
    
    * Free kv memory
    
    ---------
    
    Co-authored-by: z5269887 <z5269887@unsw.edu.au>

ggml.c
ggml.h
llama.cpp

commit 5c4d767ac028c0f9c31cba3fceaf765c6097abfc
Author: Rene Leonhardt <65483435+reneleonhardt@users.noreply.github.com>
Date:   Fri Apr 12 10:52:36 2024 +0200

    chore: Fix markdown warnings (#6625)

README-sycl.md
README.md
SECURITY.md
examples/llava/MobileVLM-README.md
examples/llava/README.md
examples/main/README.md
examples/perplexity/README.md
examples/quantize/README.md

commit ef21ce4ccb41164cb52997bd2210d92bc6a6c5d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 12 11:49:58 2024 +0300

    imatrix : remove invalid assert (#6632)

examples/imatrix/imatrix.cpp

commit dee7f8d6928cc680cc969f7d93f98c3e24dcad41
Author: MasterYi1024 <39848311+MasterYi1024@users.noreply.github.com>
Date:   Fri Apr 12 16:28:12 2024 +0800

    Correct free memory and total memory. (#6630)
    
    Co-authored-by: MasterYi <zouxiaoyi@kylinos.cn>

llama.cpp

commit 81da18e71ccfc196d4516fbea5dc3a6a1f92dccb
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Apr 12 10:26:47 2024 +0200

    eval-callback: use ggml_op_desc to pretty print unary operator name (#6631)

examples/eval-callback/eval-callback.cpp

commit 9ed2737acc233716374860e6b2ea7399c4aae29e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 12 11:15:05 2024 +0300

    ci : disable Metal for macOS-latest-cmake-x64 (#6628)

.github/workflows/build.yml

commit 04a5ac211ef40936295980b7cdf0ba6e97093146
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Apr 11 21:44:50 2024 -0400

    Optimization: eliminate addition of redundant stacks when advancing grammar. (#6616)

llama.cpp

commit f7001ccc5aa359fcf41bba19d1c99c3d25c9bcc7
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Apr 11 17:44:48 2024 -0400

    As suggested by @slaren, disabling Metal for test to fix CI build on OSX from #6576 (#6619)

examples/eval-callback/CMakeLists.txt

commit a474f50ebb3e10be3371562f75f3f573f1a86b5f
Author: Nikolas <127742645+nneubacher@users.noreply.github.com>
Date:   Thu Apr 11 21:56:29 2024 +0200

    Refactor Error Handling for CUDA (#6575)
    
    * Refactor Error Handling for CUDA
    
    Add guidance for setting CUDA_DOCKER_ARCH to match GPU compute capability for CUDA versions < 11.7. Include link to NVIDIA's CUDA GPUs documentation for compute capability reference.
    
    * Update Makefile
    
    Improved wording
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

Makefile

commit cbaadc92942c50aab599a9e4c163afc1f44f7c26
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Apr 11 19:47:34 2024 +0100

    grammars: 1.5x faster inference w/ complex grammars (vector reserves / reuses) (#6609)
    
    * grammars: reserve rejects & next candidates
    
    * grammars: reuse new_stacks
    
    * grammars: fix missing sig change in llama.h
    
    * grammars: fix test (api changed)
    
    * grammars: update gbnf-validator.cpp
    
    * grammars: simpler syntax (no swap)

examples/gbnf-validator/gbnf-validator.cpp
llama.cpp
llama.h
tests/test-grammar-integration.cpp

commit 1bbdaf6ecda6f0a360dfb307b256fcb6838c560b
Author: Hugo Roussel <hugo.rous@gmail.com>
Date:   Thu Apr 11 19:52:21 2024 +0200

    ci: download artifacts to release directory (#6612)
    
    When action download-artifact was updated to v4, the default download path changed.
    This fix binaries not being uploaded to releases.

.github/workflows/build.yml

commit f4183afe6a22f356ee222a710686ae7f83dbd949
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Apr 11 15:22:47 2024 +0200

    scripts : add --outdir option to hf.sh (#6600)
    
    * scripts : add --outdir option to hf.sh
    
    This commit adds an option to the hf.sh script that allows the user to
    specify an output directory for the downloaded file.
    
    The motivation for this changes is that examples that use the hf.sh
    script to download models from huggingface can now specify the output
    directory, perhaps to the `models` directory to keep them in one place
    and not clutter the root directory.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! scripts : add --outdir option to hf.sh
    
    Fix format of the --outdir option in the usage message.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

scripts/hf.sh

commit b804b1ef77351d2a11be945462c6c251710476cb
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Apr 11 14:51:07 2024 +0200

    eval-callback: Example how to use eval callback for debugging (#6576)
    
    * gguf-debug: Example how to use ggml callback for debugging
    
    * gguf-debug: no mutex, verify type, fix stride.
    
    * llama: cv eval: move cb eval field in common gpt_params
    
    * ggml_debug: use common gpt_params to pass cb eval.
    Fix get tensor SIGV random.
    
    * ggml_debug: ci: add tests
    
    * ggml_debug: EOL in CMakeLists.txt
    
    * ggml_debug: Remove unused param n_batch, no batching here
    
    * ggml_debug: fix trailing spaces
    
    * ggml_debug: fix trailing spaces
    
    * common: fix cb_eval and user data not initialized
    
    * ci: build revert label
    
    * ggml_debug: add main test label
    
    * doc: add a model: add a link to ggml-debug
    
    * ggml-debug: add to make toolchain
    
    * ggml-debug: tests add the main label
    
    * ggml-debug: ci add test curl label
    
    * common: allow the warmup to be disabled in llama_init_from_gpt_params
    
    * ci: add curl test
    
    * ggml-debug: better tensor type support
    
    * gitignore : ggml-debug
    
    * ggml-debug: printing also the sum of each tensor
    
    * ggml-debug: remove block size
    
    * eval-callback: renamed from ggml-debug
    
    * eval-callback: fix make toolchain
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
.gitignore
Makefile
common/common.cpp
common/common.h
docs/HOWTO-add-model.md
examples/CMakeLists.txt
examples/eval-callback/CMakeLists.txt
examples/eval-callback/README.md
examples/eval-callback/eval-callback.cpp
examples/imatrix/imatrix.cpp
llama.cpp

commit 8228b66dbc16290c5cbd70e80ab47c068e2569d8
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Apr 10 20:16:48 2024 +0200

    gguf : add option to not check tensor data (#6582)
    
    This commit adds an option to the gguf example to not check the tensor
    data.
    
    The motivation for this is that it can be nice to use the gguf tool to
    read other .gguf files that were not created by the gguf tool.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/gguf/gguf.cpp

commit b3a96f27f065a828f08c5d89ff60aab5361188fe
Author: Ralph Soika <ralph.soika@imixs.com>
Date:   Wed Apr 10 19:18:25 2024 +0200

    minor layout improvements (#6572)
    
    * minor layout improvements
    
    * added missing file, run deps.sh locally

examples/server/index.html.hpp
examples/server/public/index.html

commit 4f407a0a353dae4726c74cc33250b623a4911dd7
Author: slaren <slarengh@gmail.com>
Date:   Wed Apr 10 17:24:14 2024 +0200

    llama : add model types for mixtral (#6589)

llama.cpp

commit 65c64dc36f9bca5b3f100614cdd02bf12d6b3e49
Author: slaren <slarengh@gmail.com>
Date:   Wed Apr 10 15:23:12 2024 +0200

    convert.py : add consolidated.safetensors for mixtral 8x22b (#6587)

convert.py

commit 67fac4b95fcccfda8ab965e9ba4992a9ddf3a25f
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Wed Apr 10 08:58:48 2024 +0200

    docs : how to add a model (#6565)
    
    * docs: how to add a model
    
    * docs: model: typo and docs
    
    * docs: model: add prevision on RoPE
    
    * docs: model: rephrasing README.md
    
    * docs: model: rephrasing README.md
    
    * docs: model: README.md fix trailing spaces
    
    * docs : some fixes
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
docs/HOWTO-add-model.md

commit 29122d32ac8075ad27b7a8be05dcad2b7e7a5f9e
Author: Artem Zinnatullin <ceo@abstractny.gay>
Date:   Wed Apr 10 00:49:12 2024 -0600

    readme : fix ROCm link (#6579)

README.md

commit b231b37b095f172b26b1300ca442a147ea048b64
Author: sjxx <63994076+ylsdamxssjxxdd@users.noreply.github.com>
Date:   Wed Apr 10 14:34:00 2024 +0800

    readme : update UI list (#6560)

README.md

commit ba5e134e073ec6837078c874aba44a702944a676
Author: Jiří Sejkora <Sejseloid@gmail.com>
Date:   Wed Apr 10 00:23:02 2024 +0200

    readme: fix typo in amdgpu target name (#6573)

README.md

commit 1b67731e184e27a465b8c5476061294a4af668ea
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Apr 9 13:44:08 2024 -0400

    BERT tokenizer fixes (#6498)
    
    Key changes:
    * BERT conversion: fix abuse of LlamaHfVocab, do not set BOS or EOS
    * Nomic Embed conversion: pad vocab instead of slicing embedding tensor
    * llama_tokenize: handle added special tokens like HF does

common/common.cpp
common/common.h
convert-hf-to-gguf.py
convert-persimmon-to-gguf.py
convert.py
examples/embedding/embedding.cpp
examples/imatrix/imatrix.cpp
examples/infill/infill.cpp
examples/llava/llava-cli.cpp
examples/lookahead/lookahead.cpp
examples/lookup/lookup-create.cpp
examples/lookup/lookup-stats.cpp
examples/lookup/lookup.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/server/server.cpp
examples/speculative/speculative.cpp
examples/tokenize/tokenize.cpp
llama.cpp
llama.h

commit c4a3a4ff47d62d2503ddf9bd91b58c21f04fe3c3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 9 20:29:06 2024 +0300

    sync : ggml

scripts/sync-ggml.last

commit 400d5d722d7edf7de0cf24a18c42b183c65047d2
Author: Ed Lee <edilee@mozilla.com>
Date:   Tue Apr 9 01:31:47 2024 -0700

    server : detect search query to start webchat (#6554)

examples/server/index.html.hpp
examples/server/public/index.html

commit 5dc9dd7152dedc6046b646855585bd070c91e8c8
Author: Carolinabanana <140120812+Carolinabanana@users.noreply.github.com>
Date:   Tue Apr 9 09:16:13 2024 +0100

    llama : add Command R Plus support (#6491)
    
    * Add Command R Plus GGUF
    
    * Add Command R Plus GGUF
    
    * Loading works up to LayerNorm2D
    
    * Export new tensors in 1D so they are not quantized.
    
    * Fix embedding layer based on Noeda's example
    
    * Whitespace
    
    * Add line
    
    * Fix unexpected tokens on MPS. Re-add F16 fix. ((Noeda)
    
    * dranger003: Fix block index overflow in CUDA dequantizing.
    
    * Reverted blocked multiplication code as it still has issues and could affect other Llama arches
    
    * export norms as f32
    
    * fix overflow issues during quant and other cleanup
    
    * Type convention
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * dranger003: Fix more int overflow during quant.
    
    ---------
    
    Co-authored-by: S <seast@Ss-Mac-Studio.local>
    Co-authored-by: S <s@example.com>
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
ggml-cuda.cu
ggml-cuda/common.cuh
ggml-cuda/convert.cu
ggml-cuda/convert.cuh
ggml-cuda/dequantize.cuh
ggml-cuda/dmmv.cu
ggml-cuda/quantize.cu
ggml-cuda/quantize.cuh
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit e11a8999b5690f810c2c99c14347f0834e68c524
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 9 09:23:19 2024 +0300

    license : update copyright notice + add AUTHORS (#6405)
    
    * license : add AUTHORS
    
    * authors : update
    
    * scipts : add LICENSE and gen-authors.sh to sync

AUTHORS
LICENSE
scripts/gen-authors.sh
scripts/sync-ggml-am.sh
scripts/sync-ggml.sh

commit cc4a95426d17417d3c83f12bdb514fbe8abe2a88
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 8 22:25:49 2024 +0300

    llama : fix attention layer count sanity check (#6550)
    
    * llama : fix attention layer count sanity check
    
    * llama : fix parentheses in attention layer count sanity check
    
    There was otherwise a warning when compiling.
    
    ---------
    
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>

llama.cpp

commit cecd8d3c98b48f51aaa1d4c729e55bd319f6799c
Author: kunnis <kunnis@users.noreply.github.com>
Date:   Mon Apr 8 10:44:19 2024 -0500

    Comment explaining a decision (#6531)

convert.py

commit b73e564b16086845a8b4fffd26e22685d3e0c3db
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 8 16:23:01 2024 +0300

    quantize : fix precedence of cli args (#6541)

llama.cpp

commit e3c337d87ca650972105a51c6ce302dd236c07ad
Author: Rick G <26732651+TheFlipbook@users.noreply.github.com>
Date:   Mon Apr 8 06:02:30 2024 -0700

    llama : support negative ith in llama_get_ API (#6519)
    
    * llama_sampling_sample with default args is more naively usable
    
    * Batches populated by either llama_batch_get_one or llama_batch_add work with default args
      * Previously get_one could use the default argument
      * Previously add should usually have used the last index where logits[idx] == true
    * This hopefully encourages the use of llama_batch_add
      * By giving expected results when using default arguments.
    * Adds "negative indexing" feature to llama_get_logits_ith and llama_get_embeddings_ith
    * Believed to work with any currently well behaved program
      * Default arg now works for both cases (previously would give strange results for add case)
      * Any non-negative number is unaffected and behaves as previously
      * Negative arguments were previously invalid.
    * Implemented as a special case of indexing as suggested by @compilade in https://github.com/ggerganov/llama.cpp/pull/6519
    
    * Fixed mismatch type errors
    
    * cited in macOS CI tests
    * Missed in original updates based on PR feedback in https://github.com/ggerganov/llama.cpp/pull/6519

common/sampling.h
llama.cpp
llama.h

commit beea6e1b16e783a0886e78dec01002a8c00db24d
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Mon Apr 8 20:43:30 2024 +0800

    llama : save and restore kv cache for single seq id (#6341)
    
    * llama : save and restore kv cache for single seq id
    
    * remove trailing whitespace
    
    * respond error in case there's no space in the kv cache
    
    * add kv seq save restore to test case
    
    * add --slot-save-path arg to enable save restore and restrict save location
    
    * Returning 0 for some cases, instead of asserting.
    
    * cleanup error cases
    
    * rename sequence state functions
    
    * rename state get set functions
    
    * add previous function names back in with DEPRECATED notice
    
    * update doc
    
    * adjust endpoints to preferred style
    
    * fix restoring zero cell count
    
    * handle seq rm return value
    
    * unused param
    
    * keep in the size check
    
    * fix return types
    
    * add server test case for slot save restore
    
    * cleanup
    
    * add cake
    
    * cleanup style
    
    * add special
    
    * removing a whole sequence never fails
    
    * move sequence state file functionality from server to llama to match session api and add version tags
    
    * catch exceptions on save as well
    
    * error log messages
    
    * check types for stricter restore
    
    * update server doc
    
    * readme : update API changes date
    
    * strict filename validation
    
    * move include, reject bom as well
    
    * also reject empty filename
    
    * reject whitespace and trailing dot
    
    ---------
    
    Co-authored-by: Martin Evans <martindevans@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
common/common.cpp
common/common.h
examples/main/main.cpp
examples/save-load-state/save-load-state.cpp
examples/server/README.md
examples/server/server.cpp
examples/server/tests/features/slotsave.feature
examples/server/tests/features/steps/steps.py
llama.cpp
llama.h

commit 87fb5b4234d4b9c56ac94cf7aa229c8fd7defdb0
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Mon Apr 8 13:56:01 2024 +0530

    remove row=1 cond (#6532)

ggml-sycl.cpp

commit d752327c3338d5b9634121d651c0105f2c933f9b
Author: Firat <firatkiral@gmail.com>
Date:   Mon Apr 8 00:48:29 2024 -0700

    Adding KodiBot to UI list (#6535)
    
    KodiBot is free and open source ai chat app released under the GNU General Public License.

README.md

commit 855f54402e866ed19d8d675b56a81c844c64b325
Author: Mark Fairbairn <thebaron88@gmail.com>
Date:   Sun Apr 7 19:52:19 2024 +0100

    Change Windows AMD example to release build to make inference much faster. (#6525)

README.md

commit b909236c0bf0b6e872af95df9490492ecec310ac
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 7 21:25:30 2024 +0300

    flake.lock: Update (#6517)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/f7b3c975cf067e56e7cda6cb098ebe3fb4d74ca2' (2024-03-01)
      → 'github:hercules-ci/flake-parts/9126214d0a59633752a136528f5f3b9aa8565b7d' (2024-04-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8?dir=lib' (2024-02-29)
      → 'github:NixOS/nixpkgs/d8fe5e6c92d0d190646fb9f1056741a229980089?dir=lib' (2024-03-29)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/d8fe5e6c92d0d190646fb9f1056741a229980089' (2024-03-29)
      → 'github:NixOS/nixpkgs/fd281bd6b7d3e32ddfa399853946f782553163b5' (2024-04-03)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit e0717e751e12af13f4eedaae8bbbd608e40d7e54
Author: DAN™ <dranger003@gmail.com>
Date:   Sun Apr 7 13:33:59 2024 -0400

    Add GritLM as supported models. (#6513)

README.md

commit c37247796b4d45bdbbc8259afffb80208ad8fe55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 7 17:05:51 2024 +0300

    sync : ggml

scripts/sync-ggml.last

commit f77261a7c525fa1fa47b18a3d78cd308ae41cafc
Author: Slava Primenko <primenko.s@gmail.com>
Date:   Thu Apr 4 14:49:24 2024 +0200

    ggml: bypass code incompatible with CUDA < 11.1 (whisper/2020)
    
    `cudaHostRegisterReadOnly` parameter was only introduced in CUDA 11.1
    
    See this issue for more details:
    https://github.com/ggerganov/examples/whisper/whisper.cpp/issues/2007

ggml-cuda.cu

commit 43e8995e754b8e04642e92822055d193a3272b37
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 7 16:08:12 2024 +0300

    scripts : sync ggml-cuda folder

scripts/sync-ggml-am.sh

commit 9472bce30800a581071478a839bf93abf404c893
Author: limitedAtonement <limitedAtonement@users.noreply.github.com>
Date:   Sun Apr 7 07:05:40 2024 -0400

    Run make to build the project (#6457)

README-sycl.md

commit d4f220a5ccdc6308173c1a31fad21d7c3fbc96c1
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Sun Apr 7 10:55:59 2024 +0800

    support/fix OPs GGML_TYPE_IQ4_NL, GGML_TYPE_IQ4_XS, GGML_TYPE_IQ3_XXS, GGML_TYPE_IQ3_S, GGML_TYPE_IQ2_XXS, GGML_TYPE_IQ2_XS, GGML_TYPE_IQ2_S, GGML_TYPE_IQ1_S, GGML_TYPE_IQ1_M (#6521)

README-sycl.md
ggml-sycl.cpp

commit 54ea0698fbf87e36a5d68a98c95f6bdd0fb91557
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 6 17:43:15 2024 +0300

    sync : ggml

scripts/sync-ggml.last

commit b66aec675c1571a06b3570b858ae711246f96f84
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Apr 3 22:57:20 2024 +0200

    backend : fix typo in scheduler documentation (ggml/781)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

ggml-backend.h

commit 57dd02c44b2a0eb79e28f6c5eb8242a5d2d3174d
Author: Clint Herron <hanclinto@gmail.com>
Date:   Sat Apr 6 10:31:33 2024 -0400

    Tests: Added integration tests for GBNF parser  (#6472)
    
    * Added integration tests for GBNF parser to validate correctness of parsing, as well as correctness of string matching. Intended for use to pin behavior while working on performance improvements.
    
    * Fixing whitespace errors and cleaning error message alert to be clearer.
    
    * Removing hacky include to llama.cpp from grammar integration test now that needed functions are available via internal API.
    
    * Comment cleanup.
    
    * Reorganizing tests for readability.
    
    * Cleaning up debug message to make a bit more sense.

Makefile
tests/CMakeLists.txt
tests/test-grammar-integration.cpp

commit 75cd4c77292034ecec587ecb401366f57338f7c0
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Apr 6 05:40:47 2024 +0200

    ci: bench: support sse and fix prompt processing time / server: add tokens usage in stream OAI response (#6495)
    
    * ci: bench: support sse and fix prompt processing time
    server: add tokens usage in stream mode
    
    * ci: bench: README.md EOL
    
    * ci: bench: remove total pp and tg as it is not accurate
    
    * ci: bench: fix case when there is no token generated
    
    * ci: bench: change to the 95 percentile for pp and tg as it is closer to what the server exports in metrics
    
    * ci: bench: fix finish reason rate

.github/workflows/bench.yml
examples/server/bench/README.md
examples/server/bench/bench.py
examples/server/bench/script.js
examples/server/utils.hpp

commit a8bd14d55717754a1f48313a846a2b16fa998ad2
Author: Brian <mofosyne@gmail.com>
Date:   Sat Apr 6 05:41:38 2024 +1100

    gguf.py : add licence and version to gguf writer (#6504)

gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
llama.cpp

commit d0f5deebf898f8186a10148a03a56909ba05fc0b
Author: Hoang Nguyen <hugo53@users.noreply.github.com>
Date:   Fri Apr 5 11:39:43 2024 -0700

    readme : update UI list (#6503)
    
    * Add MindMac to UI list
    
    * Update proprietary description
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

README.md

commit 87e21bbacd830437ab653cf03b6f26d45c15395d
Author: Ting Sun <suntcrick@gmail.com>
Date:   Sat Apr 6 01:34:53 2024 +0700

    bench : make n_batch and n_ubatch configurable in Batched bench (#6500)
    
    * bench: make n_batch and n_ubatch configurable
    
    * bench: update doc for batched bench

examples/batched-bench/README.md
examples/batched-bench/batched-bench.cpp

commit 1b496a745c315022df2d919374052e6004ced8d3
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Fri Apr 5 14:35:06 2024 +0100

    [SYCL] Fixed minor bug when enabling FP16 for non intel targets (#6464)
    
    * moved INTEL_MKL guard from gemm_impl to gemm (wrapper)
    
    * Update ggml-sycl.cpp
    
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>

ggml-sycl.cpp

commit a307375c02cac45cff53cf2520330b43fecc7718
Author: alexpinel <93524949+alexpinel@users.noreply.github.com>
Date:   Thu Apr 4 18:22:50 2024 +0100

    readme : add Dot to UI list (#6487)

README.md

commit b660a5729e1e7508671d3d0515fd7efaeaeb85b9
Author: Jun Jie <71215065+junnjiee16@users.noreply.github.com>
Date:   Fri Apr 5 01:16:37 2024 +0800

    readme : fix typo (#6481)

README.md

commit 0a1d889e27d6aaa3293dd2c692b849a9bcf4b474
Author: Ed Lepedus <ed.lepedus@googlemail.com>
Date:   Thu Apr 4 17:31:22 2024 +0100

    server: add cURL support to server Dockerfiles (#6474)
    
    * server: add cURL support to `full.Dockerfile`
    
    * server: add cURL support to `full-cuda.Dockerfile` and `server-cuda.Dockerfile`
    
    * server: add cURL support to `full-rocm.Dockerfile` and `server-rocm.Dockerfile`
    
    * server: add cURL support to `server-intel.Dockerfile`
    
    * server: add cURL support to `server-vulkan.Dockerfile`
    
    * fix typo in `server-vulkan.Dockerfile`
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.devops/full-cuda.Dockerfile
.devops/full-rocm.Dockerfile
.devops/full.Dockerfile
.devops/server-cuda.Dockerfile
.devops/server-intel.Dockerfile
.devops/server-rocm.Dockerfile
.devops/server-vulkan.Dockerfile

commit 7dda1b727ef1730783a6077136d28b83e70dd397
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Fri Apr 5 01:30:53 2024 +0900

    ci: exempt master branch workflows from getting cancelled (#6486)
    
    * ci: exempt master branch workflows from getting cancelled
    
    * apply to bench.yml

.github/workflows/bench.yml
.github/workflows/build.yml
.github/workflows/code-coverage.yml
.github/workflows/docker.yml
.github/workflows/editorconfig.yml
.github/workflows/nix-ci-aarch64.yml
.github/workflows/nix-ci.yml
.github/workflows/python-check-requirements.yml
.github/workflows/python-lint.yml
.github/workflows/server.yml
.github/workflows/zig-build.yml

commit c666ba26c39d5c07e07b4e1e411332f408e309ad
Author: Ewout ter Hoeven <E.M.terHoeven@student.tudelft.nl>
Date:   Thu Apr 4 17:08:55 2024 +0200

    build CI: Name artifacts (#6482)
    
    Name the artifacts in the build CI, so that they get uploaded with separate names, instead of all put into the same `artifact` ZIP.
    
    It might be possible to further simplify the packing step (in future PRs).

.github/workflows/build.yml

commit 2e66913e5f56209f4c949f98e431925b78e7e84d
Author: Shakhar Dasgupta <shakhardasgupta@gmail.com>
Date:   Thu Apr 4 11:03:00 2024 -0400

    server: allow penalizing repetition of newlines on server webpage (#6431)

examples/server/index.html.hpp
examples/server/public/index.html

commit 8120efee1d9931b514aeb5a047209d576f23286c
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Apr 4 16:59:04 2024 +0200

    ci: bench fix concurrency for workflow trigger dispatch with sha1 (#6478)

.github/workflows/bench.yml

commit a74401f0e5ebb15fa4d8b6619d1baa6ea9179123
Author: limitedAtonement <limitedAtonement@users.noreply.github.com>
Date:   Thu Apr 4 10:30:02 2024 -0400

    Correct README link (#6458)
    
    README is called README.md.

README-sycl.md

commit 7a2c92637ae265654a68f62e6a7610b358255d3f
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Apr 4 11:57:58 2024 +0200

    ci: bench: add more ftype, fix triggers and bot comment (#6466)
    
    * ci: bench: change trigger path to not spawn on each PR
    
    * ci: bench: add more file type for phi-2: q8_0 and f16.
    - do not show the comment by default
    
    * ci: bench: add seed parameter in k6 script
    
    * ci: bench: artefact name perf job
    
    * Add iteration in the commit status, reduce again the autocomment
    
    * ci: bench: add per slot metric in the commit status
    
    * Fix trailing spaces

.github/workflows/bench.yml
examples/server/bench/bench.py
examples/server/bench/script.js

commit 4bcd6b959ca3991084ad1d8464caf2a734e29b1d
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Apr 4 09:49:21 2024 +0200

    common: remove duplicate check for curl (#6471)
    
    This commit removes one of the two identical checks for curl being NULL
    in llama_load_model_from_url.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

common/common.cpp

commit 9b84ae1806cded4d6683c7b810925da5ead40607
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Apr 4 03:44:28 2024 -0400

    examples : add GBNF validator program (#5948)
    
    * Revising GBNF validator program to be much simpler.
    
    * Changing from streams to using cstdio
    
    * Adding final newline character.

Makefile
examples/gbnf-validator/CMakeLists.txt
examples/gbnf-validator/gbnf-validator.cpp
llama.cpp
llama.h

commit 4399f13fb9462cd06f3f154d0aee738425000fea
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 4 09:34:58 2024 +0300

    server : remove obsolete --memory-f32 option

examples/server/README.md
examples/server/server.cpp

commit 1a43c7254ed5de91d09274b853db843c518f1b64
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Thu Apr 4 01:33:48 2024 -0500

    server : add option to disable KV offload (#6468)

examples/server/server.cpp

commit 72d73af65132792a52433952ca4729b01c36cde2
Author: Clint Herron <hanclinto@gmail.com>
Date:   Thu Apr 4 02:32:53 2024 -0400

    convert : fix for lint error complaining of bare except (#6470)

convert-hf-to-gguf.py

commit 5fb1574c8112c757fc202fdae279da883f34e610
Author: Fattire <528174+fat-tire@users.noreply.github.com>
Date:   Wed Apr 3 13:22:57 2024 -0700

    A few small fixes to server's README docs (#6428)
    
    * Typo fix to server's README.md
    
    Fix minor typo ("tonen") in server README.
    
    * server readme grammar/style fixes.
    
    Quickly went through this file to look for inconsistencies in
    presentation of defaults, flag options, and looked for typos
    and grammar issues.
    
    Not perfect, but hopefully improved.
    
    * Update README.md
    
    Remove an extra space before newline.

examples/server/README.md

commit 60cdf40cc32f0ad4cb11e0ca8fd38f3b93d8d640
Author: JH23X <165871467+JH23X@users.noreply.github.com>
Date:   Wed Apr 3 20:09:52 2024 +0200

    server : handle exception on wrong type in request (#6452)
    
    Co-authored-by: Jonas Holzner <jonas.holzner.external@hensoldt.net>

examples/server/utils.hpp

commit bb43cf7e9d86d69ffd9c7f008f75db890a35b45a
Author: bryanSwk <93190252+bryanSwk@users.noreply.github.com>
Date:   Thu Apr 4 02:05:10 2024 +0800

    llama : add SEA-LION support (#6448)
    
    * initial commit for sealion support
    
    * add sealion support
    
    * minor fix
    
    * q/k ln and pos_embd only if required
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * minor : clear whitespaces
    
    ---------
    
    Co-authored-by: bryan <bryansiow@aisingapore.org>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 9f62c0173d964972849251c8ad12fc356f5b7896
Author: Ewout ter Hoeven <E.M.terHoeven@student.tudelft.nl>
Date:   Wed Apr 3 20:01:13 2024 +0200

    ci : update checkout, setup-python and upload-artifact to latest (#6456)
    
    * CI: Update actions/checkout to v4
    
    * CI: Update actions/setup-python to v5
    
    * CI: Update actions/upload-artifact to v4

.github/workflows/bench.yml
.github/workflows/build.yml
.github/workflows/code-coverage.yml
.github/workflows/docker.yml
.github/workflows/editorconfig.yml
.github/workflows/gguf-publish.yml
.github/workflows/python-check-requirements.yml
.github/workflows/python-lint.yml
.github/workflows/server.yml
.github/workflows/zig-build.yml

commit 5d4f12e4624bf4435ba26753814bedd4e9b62803
Author: Ed Lepedus <ed.lepedus@googlemail.com>
Date:   Wed Apr 3 18:56:37 2024 +0100

    server: add cURL support to `server.Dockerfile` (#6461)

.devops/server.Dockerfile

commit 154d4ee39c38e231fb5c3910df14d2fc920fdf1b
Author: Francisco Melo <43780565+francis2tm@users.noreply.github.com>
Date:   Wed Apr 3 18:53:37 2024 +0100

    readme : add feature-rich rust bindings (#6465)

README.md

commit e69945d953b651674d6e6978022edf00c3a34d03
Author: Joyce <joycebrum@google.com>
Date:   Wed Apr 3 14:48:07 2024 -0300

    security : create policy (#6354)
    
    * Create SECURITY.md
    
    Signed-off-by: Joyce <joycebrum@google.com>
    
    * Fix: link on SECURITY.md
    
    Signed-off-by: Joyce <joycebrum@google.com>
    
    * Fix: link on SECURITY.md
    
    Signed-off-by: Joyce <joycebrum@google.com>
    
    * minor
    
    * fix
    
    * fix
    
    ---------
    
    Signed-off-by: Joyce <joycebrum@google.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

SECURITY.md

commit db214fa578e00b01e0884fc2725c9349608bdab5
Author: Abhishek Gopinath K <31348521+overtunned@users.noreply.github.com>
Date:   Wed Apr 3 21:12:52 2024 +0530

    Missing tokenizer.model error during gguf conversion (#6443)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

convert-hf-to-gguf.py

commit 1ff4d9f3d683f02ef8a12e04bfac84300c44bd3a
Author: kaizau <kaizau@users.noreply.github.com>
Date:   Wed Apr 3 23:24:31 2024 +0800

    Add OpenChat, Alpaca, Vicuna chat templates (#6397)
    
    * Add openchat chat template
    
    * Add chat template test for openchat
    
    * Add chat template for vicuna
    
    * Add chat template for orca-vicuna
    
    * Add EOS for vicuna templates
    
    * Combine vicuna chat templates
    
    * Add tests for openchat and vicuna chat templates
    
    * Add chat template for alpaca
    
    * Add separate template name for vicuna-orca
    
    * Remove alpaca, match deepseek with jinja output
    
    * Regenerate chat template test with add_generation_prompt
    
    * Separate deepseek bos from system message
    
    * Match openchat template with jinja output
    
    * Remove BOS token from templates, unprefix openchat

llama.cpp
tests/test-chat-template.cpp

commit 076b08649ecc3b0e1c0709c2a086a63eddd1bf32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 3 16:11:15 2024 +0300

    readme : update hot topics

README.md

commit 08a0c0206075556e82aca0feafad530dcc5f1426
Author: slaren <slarengh@gmail.com>
Date:   Wed Apr 3 15:07:05 2024 +0200

    ggml : mul_mat_id use the same tensor for all the experts (#6387)
    
    * ggml : update mul_mat_id to use the same tensor for all the experts
    
    * update cuda
    
    * minor
    
    * update metal
    
    * update test-backend-ops
    
    * fix cuda
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * update convert.py
    
    * update convert-hf-to-gguf.py
    
    * update convert.py for mixtral hf models
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * cuda : support non-pow-2 number of experts
    
    * allow quantize to work for split and merged experts models in the same way
    
    * cleanup + disable mmap automatically with split tensors models
    
    * update imatrix
    
    * test-backend-ops : test qwen argsort
    
    * update grok model loading
    
    * llama : add merged experts tensors to the grok tensor map
    
    * minor
    
    * gguf : bump version
    
    * fix quantizing of merged experts
    
    * convert-hf-to-gguf.py : update grok (untested)
    
    * make linter happy
    
    * cuda/argsort : use shared memory instead of pool memory
    
    * convert : fix grok tensor names
    
    * metal : add support for non-pow-2 argsort
    
    * llama : more loader cleanup, better error checking
    
    * cuda : fix warning
    
    * llama : still use mmap for loading old models, but copy the data to a host buffer
    
    * add review note
    
    * llama : remove ffn tensor counting + add sanity check
    
    ggml-ci
    
    * convert : fix handling of n_experts == None
    
    ggml-ci
    
    * imatrix : fix ncall counters
    
    * llama : produce error if imatrix size does not match
    
    * quantize : terminate on errors + trace logs
    
    ggml-ci
    
    * metal : pad shared memory to 16 bytes
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
convert.py
examples/imatrix/imatrix.cpp
examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-cuda/argsort.cu
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
gguf-py/pyproject.toml
llama.cpp
tests/test-backend-ops.cpp

commit 52604860f93063ef98863921da697576af1c7665
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed Apr 3 10:34:40 2024 +0800

    [SYCL] Disable iqx on windows as WA (#6435)
    
    * disable iqx on windows as WA
    
    * array instead of global_memory

ggml-common.h
ggml-sycl.cpp

commit f87f7b898651339fe173ddf016ca826163e899d8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 1 19:05:57 2024 +0300

    flake.lock: Update (#6402)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/44d0940ea560dee511026a53f0e2e2cde489b4d4' (2024-03-23)
      → 'github:NixOS/nixpkgs/d8fe5e6c92d0d190646fb9f1056741a229980089' (2024-03-29)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit 33a52448061cfd2ea44da9e6cb30b2ec22e2f6d0
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Apr 1 13:30:43 2024 +0200

    compare-llama-bench.py: fix long hexsha args (#6424)

scripts/compare-llama-bench.py

commit 226e819371eec0f298d9075198394a07b23ecfa9
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Apr 1 12:36:40 2024 +0200

    ci: server: verify deps are coherent with the commit (#6409)
    
    * ci: server: verify deps are coherent with the commit
    
    * ci: server: change the ref to build as now it's a pull event target

.github/workflows/server.yml

commit c50a82ce0f71558cbb8e555146ba124251504b38
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 31 11:56:30 2024 +0300

    readme : update hot topics

README.md

commit 37e7854c104301c5b5323ccc40e07699f3a62c3e
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 30 11:36:07 2024 +0100

    ci: bench: fix Resource not accessible by integration on PR event (#6393)

.github/workflows/bench.yml

commit c342d070c64a1ffe35d22c1b16b672e684a30297
Author: Mohammadreza Hendiani <hendiani.mohammadreza@gmail.com>
Date:   Sat Mar 30 01:29:56 2024 +0330

    Fedora build update (#6388)
    
    * fixed deprecated address
    
    * fixed deprecated address
    
    * fixed deprecated address
    
    * Added 'Apache-2.0' SPDX license identifier due to 'kompute.cc' submodule licensing. Explanation of licensing method: https://docs.fedoraproject.org/en-US/legal/spdx/#_and_expressions
    
    * Added 'Apache-2.0' SPDX license identifier due to 'kompute.cc' submodule licensing. Explanation of licensing method: https://docs.fedoraproject.org/en-US/legal/spdx/#_and_expressions
    
    * Added 'Apache-2.0' SPDX license identifier due to 'kompute.cc' submodule licensing. Explanation of licensing method: https://docs.fedoraproject.org/en-US/legal/spdx/#_and_expressions
    
    * reverted back to only the MIT license

.devops/llama-cpp-clblast.srpm.spec
.devops/llama-cpp-cuda.srpm.spec
.devops/llama-cpp.srpm.spec

commit f7fc5f6c6f3700da311de7fe93977c81798f02d3
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Mar 29 22:34:44 2024 +0100

    split: allow --split-max-size option (#6343)
    
    * split by max size
    
    * clean up arg parse
    
    * split: ok
    
    * add dry run option
    
    * error on 0 tensors
    
    * be positive
    
    * remove next_metadata_size

examples/gguf-split/gguf-split.cpp

commit ba0c7c70ab5b15f1f2be7fb0dfbe0366dda30d6c
Author: 0cc4m <picard12@live.de>
Date:   Fri Mar 29 17:29:21 2024 +0100

    Vulkan k-quant mmq and ggml-backend offload functionality (#6155)
    
    * Fix Vulkan no kv offload incoherence
    
    * Add k-quant mul mat mat shaders
    
    * Rework working buffer allocation, reduces vram use noticeably
    
    Clean up cpu assist code, replaced with ggml-backend offload function
    
    * Default to all dedicated GPUs
    
    * Add fallback for integrated GPUs if no dedicated GPUs are found
    
    * Add debug info which device is allocating memory
    
    * Fix Intel dequant issue
    
    Fix validation issue
    
    * Fix Vulkan GGML_OP_GET_ROWS implementation
    
    * Clean up merge artifacts
    
    * Remove Vulkan warning

README.md
ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml-vulkan.h
ggml.c
ggml_vk_generate_shaders.py
llama.cpp

commit d48ccf3ad4fea5b9ede209c7f40be65371987bfe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 29 17:45:46 2024 +0200

    sync : ggml (#6351)
    
    * sync : ggml
    
    ggml-ci
    
    * cuda : move GGML_CUDA_DMMV constants to dmmv.cuh
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-alloc.c
ggml-cuda/common.cuh
ggml-cuda/dmmv.cu
ggml-cuda/dmmv.cuh
scripts/sync-ggml-am.sh
scripts/sync-ggml.sh

commit 069574775cea67d8a977904e4d534aff47a671f4
Author: hxer7963 <hxer7963@gmail.com>
Date:   Fri Mar 29 21:37:03 2024 +0800

    [Model] Add support for xverse (#6301)
    
    * Support xverse model convert to gguf format.
    
    * 1. Convert xverse models to gguf;
    2. Add LLM_ARCH_XVERSE inference in llama.cpp;
    3. Add xverse item in Supported models in README.md;
    
    * * gguf-py: remove redundant logs
    * llama: remove the init_mapping_prefetch custom parameter
    
    * llama.cpp: Include the changes from #6122 to exclude the unused outputs of the last layers.
    
    * - Fix format issues
    - Remove duplicate set kqv_out to llm_build_kv
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: willhe <willhe@xverse.cn>
    Co-authored-by: willhe <hexin@xverse.cn>

README.md
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
llama.cpp

commit cfde806eb95de06d84162bdee593dad33a1d2693
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 29 14:34:28 2024 +0200

    ci : fix BGE wget (#6383)
    
    ggml-ci

ci/run.sh

commit b910287954d4462fd3d48938336770e258459677
Author: zhouwg <6889919+zhouwg@users.noreply.github.com>
Date:   Fri Mar 29 15:33:46 2024 +0800

    readme : add project (#6356)
    
    * readme: add Android UI binding
    
    * Update README.md

README.md

commit 809398709041ee854fbbad9b344bcfdcd3712d59
Author: Matt Clayton <156335168+mattjcly@users.noreply.github.com>
Date:   Fri Mar 29 03:27:42 2024 -0400

    cmake : add explicit metal version options (#6370)
    
    * cmake: add explicit metal version options
    
    * Update CMakeLists.txt
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt

commit 057400a3fd457f4f214684eeb171444663b47a23
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Mar 29 08:23:22 2024 +0100

    llama : remove redundant reshape in build_kv_store (#6369)
    
    * llama: remove redundant reshape in build_kv_store
    
    This commit removes the reshape of the V matrix in the build_kv_store.
    
    The motivation for this is that V matrix has the shape:
    ```console
    (gdb) p *v_cur
    $46 = {type = GGML_TYPE_F32, backend = GGML_BACKEND_TYPE_CPU,
           buffer = 0x0, ne = {4096, 512, 1, 1}, nb = {4, 16384, 8388608,
           8388608}, op = GGML_OP_MUL_MAT, op_params = {
           0 <repeats 16 times>}, flags = 0, grad = 0x0,
           src = {0xb496b0, 0x7ffef1c40950, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
           0x0, 0x0}, perf_runs = 0, perf_cycles = 0, perf_time_us = 0,
           view_src = 0x0, view_offs = 0, data = 0x0,
           name = "Vcur-0", '\000' <repeats 57 times>, extra = 0x0,
           padding = "\000\000\000\000\000\000\000"}
    ```
    And after reshaping this tensor we get:
    ```console
    gdb) p *ggml_reshape_2d(ctx, v_cur, n_embd_v_gqa, n_tokens)
    $44 = {type = GGML_TYPE_F32, backend = GGML_BACKEND_TYPE_CPU,
           buffer = 0x0, ne = {4096, 512, 1, 1}, nb = {4, 16384, 8388608,
           8388608}, op = GGML_OP_RESHAPE, op_params = {
           0 <repeats 16 times>}, flags = 0, grad = 0x0,
           src = {0x7ffef1c40e00, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
           0x0}, perf_runs = 0, perf_cycles = 0, perf_time_us = 0,
           view_src = 0x7ffef1c40e00, view_offs = 0, data = 0x0,
           name = "Vcur-0 (reshaped)", '\000' <repeats 46 times>, extra = 0x0,
           padding = "\000\000\000\000\000\000\000"}
    ```
    I noticed that the `src` and `view_src` fields are different but that the
    dimensions are the same. From the code comment it seems like the reshape
    call is not needed and perhaps the above can motivate the removal of the
    reshape call.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * llama : add assert
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp

commit b75c38166cf0c66793a32d5c3d6cb69929dd083d
Author: Pedro Cuenca <pedro@huggingface.co>
Date:   Fri Mar 29 08:15:00 2024 +0100

    convert : allow conversion of Mistral HF models (#6144)
    
    * Allow conversion of Mistral HF models
    
    * Homogenize Llama, Mistral, Mixtral under the same entry.
    
    * Fix tokenizer, permute tensors
    
    * Use sentencepiece tokenizer, or fall back to hfft.
    
    * convert-hf : small fix for mypy
    
    * convert-hf : fix duplicated block_count
    
    * convert-hf : add vocab size to metadata
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

convert-hf-to-gguf.py

commit bfe7dafc9cf96b9a09ead347fed9a547930fc631
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 28 22:56:03 2024 +0200

    readme : add notice for UI list

README.md

commit 5106ef482c65ac60ac14da9a68c7b37bca4c6993
Author: Ouadie EL FAROUKI <ouadie.elfarouki@codeplay.com>
Date:   Thu Mar 28 16:01:47 2024 +0000

    [SYCL] Revisited & updated SYCL build documentation (#6141)
    
    * Revisited & updated SYCL build documentation
    
    * removed outdated comment
    
    * Addressed PR comments
    
    * Trimed white spaces
    
    * added new end line

README-sycl.md

commit be55134a535f7218c53f39211755b1c7550851b2
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Mar 28 11:44:36 2024 -0400

    convert : refactor vocab selection logic (#6355)

convert-hf-to-gguf.py
convert-persimmon-to-gguf.py
convert.py
llama.h

commit 66ba56025602270152f5ba5234f3a80be3dee1c9
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Thu Mar 28 22:33:10 2024 +0800

    llava : fix MobileVLM (#6364)
    
    * fix empty bug
    
    * Update MobileVLM-README.md
    
    added more results on devices
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update MobileVLM-README.md
    
    * Update examples/llava/MobileVLM-README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update MobileVLM-README.md
    
    remove gguf links
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/llava/MobileVLM-README.md
examples/llava/clip.cpp

commit 0308f5e3d7bf9879f818b1a4ae589ff36b242af5
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Thu Mar 28 08:05:54 2024 -0400

    llama : fix command-r inference when omitting outputs (#6367)

llama.cpp

commit 28cb9a09c4d10a489be1238abe7a858dcd4d65f2
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Mar 28 11:27:56 2024 +0100

    ci: bench: fix master not schedule, fix commit status failed on external repo (#6365)

.github/workflows/bench.yml

commit cfc4d75df6399b36153ef739f2c1abee4c114bb8
Author: Ting Sun <suntcrick@gmail.com>
Date:   Thu Mar 28 16:51:06 2024 +0800

    doc: fix outdated default value of batch size (#6336)
    
    * doc: fix outdated default value of batch size
    
    * doc: add doc for ubatch-size

examples/main/README.md

commit 6902cb7f2e3479f364ee177118200fb7e4e9fc92
Author: Eric Zhang <34133756+EZForever@users.noreply.github.com>
Date:   Thu Mar 28 16:50:48 2024 +0800

    server : stop gracefully on SIGTERM (#6348)

examples/server/server.cpp

commit d2d8f389960a106b66313ff1621bdb1aaaaaa285
Author: hutli <hutli@hutli.hu>
Date:   Wed Mar 27 19:17:30 2024 +0100

    nix: removed unnessesary indentation

.devops/nix/package.nix

commit d39b308eaf0ac91c2e1f432bf66751193a470a56
Author: hutli <hutli@hutli.hu>
Date:   Wed Mar 27 19:14:28 2024 +0100

    nix: moved blas availability check to package inputs so it is still overridable

.devops/nix/package.nix

commit c87397664964e5a2a21de1877d504b23a2a35332
Author: hutli <hutli@hutli.hu>
Date:   Wed Mar 27 18:10:08 2024 +0100

    using blas.meta.available to check host platform

.devops/nix/package.nix

commit dbb03e2b9c4fead73062926b6a134f28d3a3b46d
Author: hutli <jensstaermose@hotmail.com>
Date:   Wed Mar 27 17:25:05 2024 +0100

    only using explicit blas if hostPlatform is allowed

.devops/nix/package.nix

commit e9f17dc3bf0da76c8b35130f9ca2fda5246c418e
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Tue Mar 26 16:22:42 2024 +0000

    nix: .#windows: proper cross-compilation set-up
    
    Take all dependencies from the cross stage, rather tha only stdenv

flake.nix

commit 22a462cc1f69873f7d4c6d0201bd93478afa2ecb
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Tue Mar 26 16:22:07 2024 +0000

    nix: package: don't introduce the dependency on python
    
    - The generic /usr/bin/env shebangs are good enough
    - Python deps are provisioned in the devShells
    - We need to be able to leave python out at least on windows (currently breaks eval)

.devops/nix/package.nix

commit f6a0f5c6422200764b7929064c39dcf4bb0e9cd6
Author: hutli <jensstaermose@hotmail.com>
Date:   Thu Feb 15 14:25:04 2024 +0100

    nix: .#widnows: init
    
    initial nix build for windows using zig
    
    mingwW64 build
    
    removes nix zig windows build
    
    removes nix zig windows build
    
    removed unnessesary glibc.static
    
    removed unnessesary import of pkgs in nix
    
    fixed missing trailing newline on non-windows nix builds
    
    overriding stdenv when building for crosscompiling to windows in nix
    
    better variables when crosscompiling windows in nix
    
    cross compile windows on macos
    
    removed trailing whitespace
    
    remove unnessesary overwrite of "CMAKE_SYSTEM_NAME" in nix windows build
    
    nix: keep file extension when copying result files during cross compile for windows
    
    nix: better checking for file extensions when using MinGW
    
    nix: using hostPlatform instead of targetPlatform when cross compiling for Windows
    
    using hostPlatform.extensions.executable to extract executable format

.devops/nix/package.nix
flake.nix

commit d0e2f6416bd43eddb70137f6b96c7bc3d0246102
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Thu Mar 28 12:03:30 2024 +0800

    doc: fix typo in MobileVLM-README.md (#6181)

examples/llava/MobileVLM-README.md

commit 25f4a613c4ed6451162a87cb90be10d610b49f0f
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Thu Mar 28 08:55:24 2024 +0800

    [SYCL] fix set main gpu crash (#6339)

ggml-sycl.cpp

commit a016026a3ac16d8c9b993a3573f19b9556d67de4
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Wed Mar 27 20:26:49 2024 +0100

    server: continuous performance monitoring and PR comment (#6283)
    
    * server: bench: init
    
    * server: bench: reduce list of GPU nodes
    
    * server: bench: fix graph, fix output artifact
    
    * ci: bench: add mermaid in case of image cannot be uploaded
    
    * ci: bench: more resilient, more metrics
    
    * ci: bench: trigger build
    
    * ci: bench: fix duration
    
    * ci: bench: fix typo
    
    * ci: bench: fix mermaid values, markdown generated
    
    * typo on the step name
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * ci: bench: trailing spaces
    
    * ci: bench: move images in a details section
    
    * ci: bench: reduce bullet point size
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

.github/workflows/bench.yml
examples/server/bench/bench.py
examples/server/bench/prometheus.yml
examples/server/bench/requirements.txt
examples/server/tests/features/steps/steps.py

commit 53c7ec53d5eca26b2c0c648605543a5fa6c12817
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Wed Mar 27 16:17:46 2024 +0000

    nix: ci: dont test cuda and rocm (for now)
    
    Until https://github.com/ggerganov/llama.cpp/issues/6346 is resolved

flake.nix

commit e5b89a441af23a74b861b0bf8db3239139041876
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 27 15:07:50 2024 +0100

    ggml : fix bounds checking of zero size views (#6347)

ggml.c

commit 3a0345970ed0353fa857df3c8a62a2b3318b1364
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 27 15:02:49 2024 +0200

    make : whitespace

Makefile

commit 1e13987fba5a536965ef942f2c86549d62cef50b
Author: howlger <eclipse@voormann.de>
Date:   Wed Mar 27 12:15:44 2024 +0100

    embedding : show full embedding for single prompt (#6342)
    
    * embedding : show full embedding for single prompt
    
    To support the use case of creating an embedding for a given prompt, the entire embedding and not just the first part needed to be printed.
    
    Also, show cosine similarity matrix only if there is more than one prompt, as the cosine similarity matrix for a single prompt is always `1.00`.
    
    * Update examples/embedding/embedding.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/embedding/embedding.cpp

commit e82f9e2b833d88cd2b30123ef57346c2cb8abd99
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Wed Mar 27 08:16:40 2024 +0000

    [SYCL] Fix batched impl for NVidia GPU (#6164)
    
    * Fix batched impl
    
    * Maintain previous behaviour for igpu
    
    * retrigger CI
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

ggml-sycl.cpp

commit cbc83436197cde617cad696e665879c20df77daa
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Mar 27 08:44:27 2024 +0100

    Make IQ1_M work for QK_K = 64 (#6327)
    
    * iq1_m: make it work for QK_K = 64 (WIP)
    
    * iq1_m: make it work for QK_K = 64 (scalar and AVX2)
    
    * iq1_m: QK_K = 64 seems to work on Metal and ARM_NEON
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-common.h
ggml-metal.metal
ggml-quants.c

commit e562b9714b9b3e242361a7f74bbbeb00f6bd99ac
Author: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>
Date:   Wed Mar 27 08:23:10 2024 +0100

    common : change --no-penalize-nl to --penalize-nl (#6334)
    
    * Change --no-penalize-nl to --penalize-nl
    
    * Update documentation too

common/common.cpp

commit 2ab4f00d25c0682a74472412d66454df211e4b0e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 27 09:16:02 2024 +0200

    llama2c : open file as binary (#6332)

examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp

commit 1740d6dd4e00dedda87d6820b0e0d8e70dade340
Author: Mateusz Charytoniuk <mateusz.charytoniuk@protonmail.com>
Date:   Wed Mar 27 08:08:59 2024 +0100

    readme : add php api bindings (#6326)
    
    * add php bindings to readme
    
    * readme : add link to PR
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md

commit 0642b22cd12af278d6e7e459b73411947c169381
Author: Eric Zhang <34133756+EZForever@users.noreply.github.com>
Date:   Wed Mar 27 13:55:29 2024 +0800

    server: public: use relative routes for static files (#6325)
    
    server: public: support custom `api_url`, default to relative base path

examples/server/completion.js.hpp
examples/server/index.html.hpp
examples/server/index.js.hpp
examples/server/public/completion.js
examples/server/public/index.html

commit a4f569e8a316cbd33d2b4de94b694d111507475a
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Mar 27 09:47:06 2024 +0800

    [SYCL] fix no file in win rel (#6314)

.github/workflows/build.yml

commit 32c8486e1f0297393cb22ac0a0d26a6b17ad4d54
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Mar 26 17:46:21 2024 -0400

    wpm : portable unicode tolower (#6305)
    
    Also use C locale for ispunct/isspace, and split unicode-data.cpp from unicode.cpp.

CMakeLists.txt
Makefile
Package.swift
build.zig
llama.cpp
unicode-data.cpp
unicode-data.h
unicode.cpp
unicode.h

commit 557410b8f06380560155ac7fcb8316d71ddc9837
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Tue Mar 26 10:46:41 2024 -0400

    llama : greatly reduce output buffer memory usage (#6122)
    
    * llama : greatly reduce logits memory usage
    
    * llama : more compact state saving and reloading
    
    * llama : fix lctx.n_outputs not being set before building graph
    
    * perplexity : adapt to the logits API changes
    
    * perplexity : fix Winogrande, use correct logits for second choice start
    
    The first logits used to evaluate the second choice were not from
    the end of the common prefix; instead, they were the logits from the end
    of the first choice. This has been corrected.
    
    The previous implementation sometimes had outliers in the scores of
    choices for some tasks, and the logic to skip choices words
    in the log-likelihood evaluation probably was an attempt to reduce those,
    but it was complex and didn't quite seem to be the right thing.
    
    This is simpler now, and the outlier scores aren't there anymore.
    
    * perplexity : normalize spaces and punctuation in Winogrande sentences
    
    * llama : fix embedding conditions
    
    * llama : fix llama_get_embeddings_ith when the resulting id is 0
    
    * llama : fix wrong n_outputs in llama_set_inputs
    
    A mismatch happened when using a smaller n_ubatch than n_batch and then using
    llama_batch_get_one(). The decision of what n_outputs should be now almost
    fully depends on how lctx.n_outputs is set in llama_decode_internal.
    The conditions are simpler this way.
    
    * llama : when saving the state, recalculate n_outputs
    
    This ensures the correct number of outputs for the entire previous batch
    is stored in the session file, even when n_ubatch is smaller than n_batch.
    
    * llama : fix not-skipping outputs of non-causal models
    
    * llama : fix running a batch with n_outputs == 0
    
    It previously worked because lctx.inp_out_ids was not initialized,
    so it pointed to some garbage address which was somehow still valid when I
    ran my tests.
    
    * llama : keep same graph topology even when n_outputs == 0
    
    * ggml : saner ggml_can_repeat with empty tensors
    
    *  ggml : future-proof ggml_is_empty by using GGML_MAX_DIMS - 1
    
    * ggml : do not multi-thread ops returning empty tensors
    
    * ggml : make ggml_is_empty public and work with views
    
    * llama : use a vector for ctx->output_ids
    
    * llama : rework reallocation logic for llama_output_reserve
    
    Now comparing the actual size with the new total size of the output buffer
    to allow more efficient enabling and disabling of the embeddings
    and/or logits output in the future.
    
    * ggml : skip empty tensors in all backends
    
    * llama : fix llama_output_reserve nullptr deref when new_size is 0
    
    * perplexity : make Winogrande work as it does on master
    
    The problems with the Winogrande implementation will
    need to be fixed in a separate PR to ease review.
    
    * llama : clearer error messages for invalid logits or embeddings ids
    
    * llama : assert all models that can have inp_out_ids
    
    Since the graph topology is now constant, this presence check
    can be done even when there are no outputs.
    
    * llama : assert logits and embd buffers exist before writing to them
    
    * llama : handle errors from llama_output_reserve at call sites
    
    * perplexity : make hellaswag and multiple-choice outputs identical to master
    
    Due to how the KV cache is updated, the logprobs for tokens in a batch
    are very slightly affected by the other tokens present in the batch,
    so to make hellaswag and multiple-choice return exactly the same results
    as on master, the last token of each sequence needs to be evaluated
    even though its output is not used at all.
    
    This will probably be changed back in the future to make these benchmarks
    a tiny bit faster.
    
    * perplexity : fix division by zero when using less than 100 multiple-choice tasks
    
    * llama : allow loading state saved with a different ctx size
    
    When loading a session file, the context size is now only required to be
    at least enough to load the KV cells contained in that session file,
    instead of requiring to use exactly the same context size as when saving.
    
    Doing this enables the use-case of extending or shrinking the context size
    of a saved session.
    
    This breaks existing session files because the meaning of kv_buf_size
    is slightly changed (previously it was the size of the whole KV cache,
    now it's only the size of the saved part of it). This allows for
    finer-grained sanity checks when loading in an effort to keep kv_buf_size
    useful even when the kv_size is changed.
    
    * llama : minor
    
    ggml-ci
    
    * readme : update recent API changes, and warn about Vulkan
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
examples/imatrix/imatrix.cpp
examples/parallel/parallel.cpp
examples/perplexity/perplexity.cpp
examples/server/server.cpp
examples/speculative/speculative.cpp
ggml-cuda.cu
ggml-kompute.cpp
ggml-metal.m
ggml-opencl.cpp
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
ggml.h
llama.cpp
llama.h

commit 55c1b2a3bbd470e9e2a3a0618b92cf64a885f806
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Mar 26 15:21:27 2024 +0100

    IQ1_M: 1.75 bpw quantization (#6302)
    
    * iq1_m: basics
    
    * iq1_m: basics-2
    
    * iq1_m: CUDA dequantize works
    
    Very 1st shot I get PPL = 9.76 for LLaMA-v2-7B.
    
    * iq1_m: separate shifts for each group of 8 in a block
    
    We get
    PPL(LLaMA-v2-7B ) = 9.2810
    PPL(LLaMA-v2-13B) = 6.8105
    
    Not bad, but slightly higher than
      sqrt(PPL(IQ1_S) * PPL(IQ2_XXS))
    which is the expected outcome given that IQ1_M is
    halfway between IQ1_S and IQ2_XXS in terms of bpw.
    From this, we would expect
     PPL = 9.14 for LLaMA-v2-7B
     PPL = 6.63 for LLaMA-v2-13B
    
    * iq1_m: go to 3-bit scales
    
    There is slight increase in PPL, but the 0.0625 bpw reduction
    in size is totally worth it.
    
    We now have
    PPL(LLaMA-v2-7B ) = 9.4469 at 1.96 bpw
    PPL(LLaMA-v2-13B) = 6.8717 at 1.93 bpw
    PPL(LLaMA-v2-70B) = 4.8568 at 1.85 bpw
    
    * iq1_m: scalar dot product
    
    * iq1_m: AVX2 dot product
    
    * iq1_m: very slightly faster AVX2 dot product
    
    * iq1_m: ARM_NEON dot product
    
    Works, but very slow (10.5 t/s)
    
    * iq1_m: Metal - dequantize works, dot product does not
    
    * iq1_m: Metal now works
    
    About the same performance as iq1_s.
    
    * iq1_m: minor
    
    * iq1_m: checking pure iq1_m quantization
    
    It is pretty bad: PPL(LLaMA-v2-7B) = 34 if we quantize output.weight
    with Q4_K.
    
    * iiq1_m: slightly faster ARM_NEON dot product
    
    10.5 t/s -> 11.65 t/s
    
    * iq1_m: faster ARM_NEON dot product
    
    11.65 t/s -> 14.9 t/s
    
    * iq1_m: another minor ARM_NEON dot product improvement
    
    14.9 -> 15.0 t/s
    
    * iq1_m: small PPL improvement via super-block scale adjustment
    
    After quantizing block scales redo the super-block scale fit.
    
    PPL(LLaMA-v2-7B ) = 9.3346
    PPL(LLaMA-v2-13B) = 6.8419
    PPL(LLaMA-v2-70B) = 4.8294
    PPL(Mistral-7B  ) = 8.1624
    
    * iq1_m: adapt to CUDA refactoring
    
    * iq1_m: remove unused variable
    
    We have progressed to warnings being errors.
    
    * iq1_m: add to backend-ops tests
    
    * iq1_m: fix Windows ARM
    
    * iq1_m: use common definition of iq1m_scale_t
    
    * cuda: assert -> NO_DEVICE_CODE
    
    * iq1_M: PR comments
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize/quantize.cpp
ggml-common.h
ggml-cuda.cu
ggml-cuda/convert.cu
ggml-cuda/mmvq.cu
ggml-cuda/vecdotq.cuh
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
gguf-py/gguf/constants.py
llama.cpp
llama.h
tests/test-backend-ops.cpp

commit e097633f63fdd26d492844f7eff056e4083fd9eb
Author: Pedro Cuenca <pedro@huggingface.co>
Date:   Tue Mar 26 13:32:19 2024 +0100

    convert-hf : fix exception in sentencepiece with added tokens (#6320)

convert-hf-to-gguf.py

commit d25b1c31b07c3675443a55a828dd58cfef5a241c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Mar 26 13:09:30 2024 +0100

    quantize : be able to override metadata by key (#6321)
    
    * quantize: be able to override metadata by key
    
    * minor : spacing
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/quantize/quantize.cpp
llama.cpp
llama.h

commit deb7240100da99555b9ab9dc635021e591fceaf5
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Tue Mar 26 18:11:46 2024 +0900

    embedding : adjust `n_ubatch` value (#6296)
    
    * embedding: assign `n_ubatch` value, print error on `n_batch` overflow
    
    * Update examples/embedding/embedding.cpp
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * use %ld instead of %lld
    
    * Revert "use %ld instead of %lld"
    
    This reverts commit ea753ede90a86a0699f65878cc8e2020ff5eabb8.
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

examples/embedding/embedding.cpp

commit 3d032ece8e6973441273601ca2981130608e287d
Author: Jan Boon <jan.boon@kaetemi.be>
Date:   Tue Mar 26 16:47:43 2024 +0800

    server : add `n_discard` parameter (#6300)

examples/server/server.cpp

commit e190f1fca6f60d80944f9e8709d343a025c4d245
Author: Joseph Stahl <1269177+josephst@users.noreply.github.com>
Date:   Mon Mar 25 20:51:46 2024 -0400

    nix: make `xcrun` visible in Nix sandbox for precompiling Metal shaders (#6118)
    
    * Symlink to /usr/bin/xcrun so that `xcrun` binary
    is usable during build (used for compiling Metal shaders)
    
    Fixes https://github.com/ggerganov/llama.cpp/issues/6117
    
    * cmake - copy default.metallib to install directory
    
    When metal files are compiled to default.metallib, Cmake needs to add this to the install directory so that it's visible to llama-cpp
    
    Also, update package.nix to use absolute path for default.metallib (it's not finding the bundle)
    
    * add `precompileMetalShaders` flag (defaults to false) to disable precompilation of metal shader
    
    Precompilation requires Xcode to be installed and requires disable sandbox on nix-darwin

.devops/nix/package.nix
CMakeLists.txt

commit 280345968dabc00d212d43e31145f5c9961a7604
Author: slaren <slarengh@gmail.com>
Date:   Tue Mar 26 01:16:01 2024 +0100

    cuda : rename build flag to LLAMA_CUDA (#6299)

.devops/full-cuda.Dockerfile
.devops/llama-cpp-cuda.srpm.spec
.devops/main-cuda.Dockerfile
.devops/nix/package.nix
.devops/server-cuda.Dockerfile
.github/workflows/build.yml
CMakeLists.txt
Makefile
README.md
ci/run.sh
common/common.cpp
docs/token_generation_performance_tips.md
examples/imatrix/README.md
examples/llama-bench/llama-bench.cpp
examples/llava/MobileVLM-README.md
examples/llava/clip.cpp
examples/main-cmake-pkg/README.md
examples/main/README.md
examples/server/README.md
examples/server/server.cpp
ggml-backend.c
ggml.c
ggml.h
llama.cpp
scripts/LlamaConfig.cmake.in
scripts/compare-commits.sh
scripts/pod-llama.sh
scripts/server-llm.sh

commit b06c16ef9f81d84da520232c125d4d8a1d273736
Author: Christian Kögler <ck3d@gmx.de>
Date:   Mon Mar 25 18:52:45 2024 +0100

    nix: fix blas support (#6281)
    
    Since no blas was provided to buildInputs, the executable is built without blas support.
    
    This is a backport of NixOS/nixpkgs#298567

.devops/nix/package.nix

commit 1f2fd4e727a707f85d58e0b56075c3e6334d18d8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Mar 25 18:33:15 2024 +0100

    tests : include IQ2_XXS and IQ2_XS in test-quantize-fns (#6303)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

tests/test-quantize-fns.cpp

commit 43139cc528909c3de8c144ed174e09a1f7912a80
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 25 17:22:27 2024 +0200

    flake.lock: Update (#6266)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/d691274a972b3165335d261cc4671335f5c67de9' (2024-03-14)
      → 'github:NixOS/nixpkgs/44d0940ea560dee511026a53f0e2e2cde489b4d4' (2024-03-23)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit 2f34b865b62b1d2b5eb8a27885e4de220deeacbd
Author: slaren <slarengh@gmail.com>
Date:   Mon Mar 25 15:43:22 2024 +0100

    cuda : fix LLAMA_CUDA_F16 build (#6298)

ggml-cuda/dmmv.cu

commit ae1f211ce2138448b47ebb148e25c58406845278
Author: slaren <slarengh@gmail.com>
Date:   Mon Mar 25 13:50:23 2024 +0100

    cuda : refactor into multiple files (#6269)

.clang-tidy
CMakeLists.txt
Makefile
ggml-cuda.cu
ggml-cuda/acc.cu
ggml-cuda/acc.cuh
ggml-cuda/alibi.cu
ggml-cuda/alibi.cuh
ggml-cuda/arange.cu
ggml-cuda/arange.cuh
ggml-cuda/argsort.cu
ggml-cuda/argsort.cuh
ggml-cuda/binbcast.cu
ggml-cuda/binbcast.cuh
ggml-cuda/clamp.cu
ggml-cuda/clamp.cuh
ggml-cuda/common.cuh
ggml-cuda/concat.cu
ggml-cuda/concat.cuh
ggml-cuda/convert.cu
ggml-cuda/convert.cuh
ggml-cuda/cpy.cu
ggml-cuda/cpy.cuh
ggml-cuda/dequantize.cuh
ggml-cuda/diagmask.cu
ggml-cuda/diagmask.cuh
ggml-cuda/dmmv.cu
ggml-cuda/dmmv.cuh
ggml-cuda/getrows.cu
ggml-cuda/getrows.cuh
ggml-cuda/im2col.cu
ggml-cuda/im2col.cuh
ggml-cuda/mmq.cu
ggml-cuda/mmq.cuh
ggml-cuda/mmvq.cu
ggml-cuda/mmvq.cuh
ggml-cuda/norm.cu
ggml-cuda/norm.cuh
ggml-cuda/pad.cu
ggml-cuda/pad.cuh
ggml-cuda/pool2d.cu
ggml-cuda/pool2d.cuh
ggml-cuda/quantize.cu
ggml-cuda/quantize.cuh
ggml-cuda/rope.cu
ggml-cuda/rope.cuh
ggml-cuda/scale.cu
ggml-cuda/scale.cuh
ggml-cuda/softmax.cu
ggml-cuda/softmax.cuh
ggml-cuda/sumrows.cu
ggml-cuda/sumrows.cuh
ggml-cuda/tsembd.cu
ggml-cuda/tsembd.cuh
ggml-cuda/unary.cu
ggml-cuda/unary.cuh
ggml-cuda/upscale.cu
ggml-cuda/upscale.cuh
ggml-cuda/vecdotq.cuh

commit ad3a0505e3b6cd777259ee35e61d428357ffc565
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Mar 25 09:42:17 2024 +0100

    Server: clean up OAI params parsing function (#6284)
    
    * server: clean up oai parsing function
    
    * fix response_format
    
    * fix empty response_format
    
    * minor fixes
    
    * add TODO for logprobs
    
    * update docs

examples/server/README.md
examples/server/server.cpp
examples/server/utils.hpp

commit 95ad616cddda50273e955bfe192328acd9aa4896
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Mon Mar 25 15:52:41 2024 +0800

    [SYCL] fix SYCL backend build on windows is break by LOG() error (#6290)
    
    * fix LOG() error for SYCL, enhance erro check by CI
    
    * rollback to bash
    
    * add newline at end of file

common/log.h
examples/sycl/win-build-sycl.bat

commit 64e7b47c6986221f2ff5c57c89dfc018bb0e9e6d
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Mon Mar 25 16:38:22 2024 +0900

    examples : add "retrieval" (#6193)
    
    * add `retrieval` example
    
    * add README
    
    * minor fixes
    
    * cast filepos on print
    
    * remove use of variable sized array
    
    * store similarities in separate vector
    
    * print error on insufficient batch size
    
    * fix error message printing
    
    * assign n_batch value to n_ubatch
    
    * fix param definitions
    
    * define retrieval-only parameters in retrieval.cpp
    
    * fix `--context-file` option to be provided multiple times for multiple files
    
    * use vector for `query_emb`
    
    * add usage description in README
    
    * fix merge conflict
    
    * fix usage printing
    
    * remove seed setting
    
    * fix lint
    
    * increase file read buffer size
    
    * retrieval : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile
common/common.cpp
common/common.h
common/log.h
examples/CMakeLists.txt
examples/retrieval/CMakeLists.txt
examples/retrieval/README.md
examples/retrieval/retrieval.cpp
retrieval

commit 7733f0c76081b2a69b5f8b192db2db7c43629d58
Author: Justine Tunney <jtunney@gmail.com>
Date:   Mon Mar 25 01:39:56 2024 -0400

    ggml : support AVX512VNNI (#6280)
    
    This change causes some quants (e.g. Q4_0, Q8_0) to go faster on some
    architectures (e.g. AMD Zen 4).

ggml-quants.c

commit a32b77c4b2c1808654d0b952f26c37d73d2e746b
Author: Rick G <26732651+TheFlipbook@users.noreply.github.com>
Date:   Sun Mar 24 14:45:56 2024 -0700

    Fix heap corruption from wmode out-of-bound writes on windows (#6272)
    
    * would throw error on VS2022 on GGML_FREE(wmode)
    * wchar_t is usually 2 bytes, but malloc wants bytes
      * therefore `*wmode_p++ = (wchar_t)*mode;` could write off the end of the allocation
    * Fixes error possibly introduced by https://github.com/ggerganov/llama.cpp/pull/6248

ggml.c

commit a0e584defd8c16e7a51ab895f595df0448d710d0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 24 16:18:45 2024 +0200

    imatrix : fix wname for mul_mat_id ops (#6271)
    
    * imatrix : fix wname for mul_mat_id ops
    
    * also filter tensor names in mul_mat_id ops
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

examples/imatrix/imatrix.cpp

commit 7aed0ffe6855e9cadcf413f288753af4566bfeb8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Mar 24 14:21:17 2024 +0100

    Fixed lookup compilation issues on Windows (#6273)

common/ngram-cache.cpp

commit ea279d56091b90fbbe063b598f5229d95f58ef68
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 24 09:57:06 2024 +0100

    ci : close inactive issue, increase operations per run (#6270)

.github/workflows/close-issue.yml

commit 586e7bc561be88e929a9afca7e67d8ead00c53bd
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Sun Mar 24 17:54:07 2024 +0900

    sampling : deduplicated code for probability distribution access (#6240)
    
    * sampling: remove duplicated code for probability distribution access
    
    * free original_logits
    
    * fix original_logits allocation
    
    * fixes based on review @cebtenzzre
    
    * change function name to `llama_sampling_prepare`

common/sampling.cpp
common/sampling.h
examples/speculative/speculative.cpp
retrieval

commit ddf65685105a39a57b1e7f80c3aa502a6313af24
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Sun Mar 24 12:04:25 2024 +0800

    [SYCL] offload op (#6217)
    
    
    * remove no USM methods
    
    * leave the schedule to ggml_backend_sched entirely

ggml-sycl.cpp
ggml-sycl.h
ggml.c
llama.cpp

commit d03224ac9840351023ff8abcf4aa0542258a53df
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Sun Mar 24 09:44:01 2024 +0800

    Support build win release for SYCL  (#6241)
    
    * support release win
    
    * fix value
    
    * fix value
    
    * fix value
    
    * fix error
    
    * fix error
    
    * fix format

.github/workflows/build.yml

commit 94d1b3b4119209efcdd08df0dceaecbd1fe7f85c
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Mar 23 18:48:02 2024 -0400

    use _wfopen instead of fopen on Windows (#6248)
    
    also fix missing #defines before windows.h, and BPE LF token on MSVC

ggml.c
ggml.h
llama.cpp

commit 95562175f83a49755ff6fd3bad09409417c8e6f9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 23 21:35:23 2024 +0200

    gitignore : gguf-split

.gitignore

commit f482bb2e4920e544651fb832f2e0bcb4d2ff69ab
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 23 18:07:00 2024 +0100

    common: llama_load_model_from_url split support  (#6192)
    
    * llama: llama_split_prefix fix strncpy does not include string termination
    common: llama_load_model_from_url:
     - fix header name case sensitive
     - support downloading additional split in parallel
     - hide password in url
    
    * common: EOL EOF
    
    * common: remove redundant LLAMA_CURL_MAX_PATH_LENGTH definition
    
    * common: change max url max length
    
    * common: minor comment
    
    * server: support HF URL options
    
    * llama: llama_model_loader fix log
    
    * common: use a constant for max url length
    
    * common: clean up curl if file cannot be loaded in gguf
    
    * server: tests: add split tests, and HF options params
    
    * common: move llama_download_hide_password_in_url inside llama_download_file as a lambda
    
    * server: tests: enable back Release test on PR
    
    * spacing
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * spacing
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * spacing
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/server.yml
common/common.cpp
common/common.h
examples/gguf-split/gguf-split.cpp
examples/server/README.md
examples/server/server.cpp
examples/server/tests/features/parallel.feature
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py
llama.cpp

commit 1997577d5e121568ae39f538021733ccd4278c23
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 23 18:00:38 2024 +0100

    server: docs: `--threads` and `--threads`, `--ubatch-size`, `--log-disable` (#6254)

examples/server/README.md

commit 476b0251b27fb64c575507024a671e639d675594
Author: Julius Arkenberg <arki05@users.noreply.github.com>
Date:   Sat Mar 23 17:41:53 2024 +0100

    llama : add grok-1 support (#6204)
    
    * Add support for Grok model architecture
    
    * Revert convert-hf-to-gguf to default options
    
    * Fixed f_norm_rms_eps bug
    
    * Fix whitespaces
    
    * llama : fix grok rope type
    
    * llama : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 21cad01b6e6e1a96f99391f95e8ea8ae25c8288e
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 23 17:18:13 2024 +0100

    split: add gguf-split in the make build target (#6262)

Makefile

commit 1b26aebe4de4f048ac99996efd8a2c9af150904d
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 23 13:18:45 2024 +0100

    server: flush stdout after logging in both text and json layout (#6253)

examples/server/utils.hpp

commit 50ccaf5eacb50a2ca378a4ef0dc7aeb45fead652
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Mar 23 01:24:36 2024 +0100

    lookup: complement data from context with general text statistics (#5479)
    
    * lookup: evaluation tools, use corpus/previous gens
    
    * fixup! lookup: evaluation tools, use corpus/previous gens
    
    * fixup! lookup: evaluation tools, use corpus/previous gens
    
    * fixup! lookup: evaluation tools, use corpus/previous gens
    
    * fixup! lookup: evaluation tools, use corpus/previous gens

.gitignore
Makefile
common/CMakeLists.txt
common/common.cpp
common/common.h
common/ngram-cache.cpp
common/ngram-cache.h
examples/lookup/CMakeLists.txt
examples/lookup/lookup-create.cpp
examples/lookup/lookup-merge.cpp
examples/lookup/lookup-stats.cpp
examples/lookup/lookup.cpp
scripts/get-wikitext-103.sh

commit 56a00f0a2f48a85376f48b5ce77699df781631ae
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 21:10:39 2024 +0200

    common : default --hf-file to --model (#6234)

common/common.cpp

commit 92397d87a45a09b5449d845a64856f177cd7a920
Author: fraxy-v <65565042+fraxy-v@users.noreply.github.com>
Date:   Fri Mar 22 20:49:06 2024 +0200

    convert-llama2c-to-ggml : enable conversion of GQA models (#6237)
    
    * convert-llama2c-to-ggml: enable conversion of multiqueries, #5608
    
    * add test in build action
    
    * Update build.yml
    
    * Update build.yml
    
    * Update build.yml
    
    * gg patch

.github/workflows/build.yml
examples/convert-llama2c-to-ggml/README.md
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp

commit 1d0331c12a2f2a6296b471232bd4e66fbf06e6a1
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Mar 22 19:47:14 2024 +0100

    quantize: options for output and token embedding tensors qtype (#6239)
    
    * quantize: be able to specify the output tensor type
    
    * quantize: be able to specify the token embedding tensor type
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize/quantize.cpp
llama.cpp
llama.h

commit dba1af612926cbd4ebe2d876277af1e3305177e0
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Mar 22 19:00:01 2024 +0100

    llama_model_loader: support multiple split/shard GGUFs (#6187)
    
    * split: support in llama_model_loader
    
    * avoid copying the entire vector
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * split: move llama_tensor_offset to llama_model_loader
    
    * llama_model_loader: PR feedbacks:
     - use only one gguf_context for metadata only
     - store all ggml_context in a vector as the files and mappings
     - store all weights in a vector along with the source tensor
     - rename ctx_gguf to meta
     - rename ctx_meta to contexts
    
    * avoid copying the entire vector
    
    * Simplify this by making these optional, switch some layer creation tensor optional
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Handle optional tensors
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * llama_model_loader: fail if backend cannot allocate buffer
    
    * fix mmap buffer management
    
    * llama_model_loader: map file to backend buffer if the allocation succeeds only
    
    * llama_model_loader: only map tensors included in the context
    
    * llama_model_loader: minor, use same variable name for consistency, fix spacing in types cast
    
    * llama_model_loader: fail if any of backend buffer cannot be allocated
    
    * spacing
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * fix loop over pointer
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * llama_model_loader: if n_tensors declared not equals to loaded tensors in split, throw an exception instead of asserting
    
    * llama_model_loader: ensure mappings vector has the expected size
    
    * llama_model_loader:  use at instead of operator[] if this should never add to the map.
    
    * llama_model_loader: immediately add the backend buffer to the model buffers in order to free them if an error occurs in the next allocation. Reserve the expected size.
    
    * llama_model_loader: be sure the model mappings has enough capacity before allocating backend buffer
    
    * llama_model_loader: fix map -> unordered map
    
    * llama_split_prefix: use a clearer version, not pass split path len but dest max len.
    
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    
    * llama : minor
    
    ggml-ci
    
    * llama : introduce some typedef helpers
    
    * docs: add model shard in hot topic
    
    * llama_model_loader: put mapping in a unique_ptr from the moment it is allocated
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * fix llama_split_prefix
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>

README.md
examples/gguf-split/gguf-split.cpp
llama.cpp
llama.h

commit ee804f6223777019cf921e0d99cc24669313ab98
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Sat Mar 23 02:15:06 2024 +0900

    ci: apply concurrency limit for github workflows (#6243)

.github/workflows/build.yml
.github/workflows/code-coverage.yml
.github/workflows/docker.yml
.github/workflows/editorconfig.yml
.github/workflows/nix-ci-aarch64.yml
.github/workflows/nix-ci.yml
.github/workflows/python-check-requirements.yml
.github/workflows/python-lint.yml
.github/workflows/server.yml
.github/workflows/zig-build.yml

commit 80bd33bc2c4be352697dc8473339f25e1085d117
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 15:33:38 2024 +0200

    common : add HF arg helpers (#6234)
    
    * common : add HF arg helpers
    
    * common : remove defaults

common/common.cpp
common/common.h

commit e80f06d2a194be62ab5b1cd7ef7c7a5b241dd4fb
Author: Nexesenex <124105151+Nexesenex@users.noreply.github.com>
Date:   Fri Mar 22 14:32:02 2024 +0100

    llama : correction of the attn.v.weight quantization for IQ3_XS (#6209)
    
    IQ3_XS was not mentioned, IQ3_S and IQ3_M were present twice.
    
    That PR corrects this in the manner which was probably intended initially.

llama.cpp

commit f77a8ffd3bbde77b7819823b0c006fd8c2d5cae4
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Mar 22 13:09:07 2024 +0000

    tests : conditional python & node json schema tests (#6207)
    
    * json: only attempt python & node schema conversion tests if their bins are present
    
    Tests introduced in https://github.com/ggerganov/llama.cpp/pull/5978
    disabled in https://github.com/ggerganov/llama.cpp/pull/6198
    
    * json: orange warnings when tests skipped
    
    * json: ensure py/js schema conv tested on ubuntu-focal-make
    
    * json: print env vars in test

.github/workflows/build.yml
tests/test-json-schema-to-grammar.cpp

commit 72114edf068a9b2f54d3b09e9a198f611be397e8
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Fri Mar 22 13:07:44 2024 +0000

    json-schema-to-grammar : fix order of props + non-str const/enum (#6232)
    
    * json: ordered json in server/schema converter to respect orig order
    
    * json: ws nits
    
    * json: support non-string const / enums

common/json-schema-to-grammar.cpp
common/json-schema-to-grammar.h
examples/json-schema-to-grammar.py
examples/server/json-schema-to-grammar.mjs.hpp
examples/server/public/json-schema-to-grammar.mjs
examples/server/server.cpp
examples/server/utils.hpp
tests/test-json-schema-to-grammar.cpp

commit 2f0e81e053b41ca28e73a841e7bdbf9820baaa57
Author: slaren <slarengh@gmail.com>
Date:   Fri Mar 22 14:05:31 2024 +0100

    cuda : add LLAMA_CUDA_NO_PEER_COPY to workaround broken ROCm p2p copy (#6208)
    
    * cuda : add LLAMA_CUDA_NO_PEER_COPY to workaround broken ROCm p2p copy
    
    * add LLAMA_CUDA_NO_PEER_COPY to HIP build

CMakeLists.txt
Makefile
ggml-cuda.cu

commit 29ab270e65975785cdca3243a3de71ccebc1252a
Author: Xiaoyi Chen <cxychina@gmail.com>
Date:   Fri Mar 22 04:29:49 2024 -0700

    readme : add RecurseChat to the list of UIs (#6219)

README.md

commit 6b8bb3a31d260a01020497c5f01025c34222fcb9
Author: Jan Boon <kaetemi@gmail.com>
Date:   Fri Mar 22 19:12:05 2024 +0800

    server : fix n_keep always showing as 0 in response (#6211)

examples/server/server.cpp

commit 68e210b3543e0cc71268bee0920441747679ee13
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 13:08:28 2024 +0200

    server : enable continuous batching by default (#6231)

common/common.h
examples/server/server.cpp

commit b3e94f26bab8e3b5338b5902e5af5bb01894cb4a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 11:35:53 2024 +0200

    metal : proper assert for mat-mat memory alignment (#6225)
    
    * metal : proper assert for mat-mat memory alignment
    
    ggml-ci
    
    * readme : add notice about the bug fix
    
    * metal : fix the fix
    
    ggml-ci

README.md
ggml-metal.m
ggml-metal.metal

commit b2075fd6a578f5685060df4baa90ae9e48e98c70
Author: Vaibhav Srivastav <vaibhavs10@gmail.com>
Date:   Fri Mar 22 08:53:43 2024 +0100

    ci : add CURL flag for the mac builds (#6214)

.github/workflows/build.yml

commit 95d576b48ebf582b112d1c9cf4eed7142fa4e464
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 22 09:36:03 2024 +0200

    metal : pad n_ctx by 32 (#6177)
    
    * metal : require ne00 >= 128 for mat-mat kernels
    
    ggml-ci
    
    * llama : pad n_ctx by 32
    
    ggml-ci

common/common.cpp
examples/batched/batched.cpp
llama.cpp
tests/test-backend-ops.cpp

commit 59c17f02de8fdf7b084d6100b875b7e2bc07a83b
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Fri Mar 22 15:19:37 2024 +0800

    add blog link (#6222)

README-sycl.md

commit fa046eafbc70bf97dcf39843af0323f19a8c9ac3
Author: DAN™ <dranger003@gmail.com>
Date:   Thu Mar 21 21:32:42 2024 -0400

    Fix params underscore convert to dash. (#6203)
    
    * Fix params underscore convert to dash.
    
    * Update common/common.cpp
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

common/common.cpp

commit be07a03217376c53b1d95e5f658adbbbb2831555
Author: Jan Boon <kaetemi@gmail.com>
Date:   Fri Mar 22 06:41:24 2024 +0800

    server : update readme doc from `slot_id` to `id_slot` (#6213)

examples/server/README.md

commit d0a71233fbf8ade8ef06ad8e6b81d1d7b254895f
Author: slaren <slarengh@gmail.com>
Date:   Thu Mar 21 19:54:28 2024 +0100

    cuda : disable host register by default (#6206)

ggml-cuda.cu

commit f372c49ccdc561ab96fb3c7d2b7cbc0f89a4b359
Author: semidark <me@semidark.net>
Date:   Thu Mar 21 11:52:35 2024 -0600

    Corrected typo to wrong file (#6199)
    
    The stated file `./devops/main-server.Dockerfile` does not exist. I figure that `.devops/server-intel.Dockerfile` was meant.

README-sycl.md

commit 924ce1dce7fdf54894b462d03e7b608a6e574c32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 21 16:20:05 2024 +0200

    tests : disable system() calls (#6198)
    
    ggml-ci

tests/test-json-schema-to-grammar.cpp

commit 03a8f8fafec2e21009be9fe7297d92e5d4538964
Author: slaren <slarengh@gmail.com>
Date:   Thu Mar 21 13:59:53 2024 +0100

    cuda : fix LLAMA_CUDA_F16 build (#6197)

ggml-cuda.cu

commit cfd3be76e37dab92c846d75a2421178f20db4a11
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Mar 21 13:59:38 2024 +0100

    ggml : same IQ4_NL quantization for CPU/CUDA/Metal (#6196)
    
    * Make quantize_row_iq4_nl do the same thing is quantization on CUDA
    
    * Make quantize_row_iq4_nl do the same thing is quantization on CUDA
    
    This time for real. backend-ops tests pass.
    
    * Now fix test-quantize-fns
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-quants.c

commit 5b7b0ac8dfdd800c0fd0dc69b69991e8cb19fb46
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Thu Mar 21 11:50:43 2024 +0000

    json-schema-to-grammar improvements (+ added to server) (#5978)
    
    * json: fix arrays (disallow `[,1]`)
    
    * json: support tuple types (`[number, string]`)
    
    * json: support additionalProperties (`{[k: string]: [string,number][]}`)
    
    * json: support required / optional properties
    
    * json: add support for pattern
    
    * json: resolve $ref (and support https schema urls)
    
    * json: fix $ref resolution
    
    * join: support union types (mostly for nullable types I think)
    
    * json: support allOf + nested anyOf
    
    * json: support any (`{}` or `{type: object}`)
    
    * json: fix merge
    
    * json: temp fix for escapes
    
    * json: spaces in output and unrestricted output spaces
    
    * json: add typings
    
    * json:fix typo
    
    * Create ts-type-to-grammar.sh
    
    * json: fix _format_literal (json.dumps already escapes quotes)
    
    * json: merge lit sequences and handle negatives
    
    {"type": "string", "pattern": "^({\"question\": \"[^\"]+\", \"response\": \"[^\"]+\"}\\n)+$"}
    
    * json: handle pattern repetitions
    
    * Update json-schema-to-grammar.mjs
    
    * Create regex-to-grammar.py
    
    * json: extract repeated regexp patterns to subrule
    
    * Update json-schema-to-grammar.py
    
    * Update json-schema-to-grammar.py
    
    * Update json-schema-to-grammar.py
    
    * json: handle schema from pydantic Optional fields
    
    * Update json-schema-to-grammar.py
    
    * Update json-schema-to-grammar.py
    
    * Update ts-type-to-grammar.sh
    
    * Update ts-type-to-grammar.sh
    
    * json: simplify nullable fields handling
    
    * json: accept duplicate identical rules
    
    * json: revert space to 1 at most
    
    * json: reuse regexp pattern subrules
    
    * json: handle uuid string format
    
    * json: fix literal escapes
    
    * json: add --allow-fetch
    
    * json: simplify range escapes
    
    * json: support negative ranges in patterns
    
    * Delete commit.txt
    
    * json: custom regex parser, adds dot support & JS-portable
    
    * json: rm trailing spaces
    
    * Update json-schema-to-grammar.mjs
    
    * json: updated server & chat `( cd examples/server && ./deps.sh )`
    
    * json: port fixes from mjs to python
    
    * Update ts-type-to-grammar.sh
    
    * json: support prefixItems alongside array items
    
    * json: add date format + fix uuid
    
    * json: add date, time, date-time formats
    
    * json: preserve order of props from TS defs
    
    * json: port schema converter to C++, wire in ./server
    
    * json: nits
    
    * Update json-schema-to-grammar.cpp
    
    * Update json-schema-to-grammar.cpp
    
    * Update json-schema-to-grammar.cpp
    
    * json: fix mjs implementation + align outputs
    
    * Update json-schema-to-grammar.mjs.hpp
    
    * json: test C++, JS & Python versions
    
    * json: nits + regen deps
    
    * json: cleanup test
    
    * json: revert from c++17 to 11
    
    * json: nit fixes
    
    * json: dirty include for test
    
    * json: fix zig build
    
    * json: pass static command to std::system in tests (fixed temp files)
    
    * json: fix top-level $refs
    
    * json: don't use c++20 designated initializers
    
    * nit
    
    * json: basic support for reserved names `{number:{number:{root:number}}}`
    
    * Revamp test cmake to allow args (WORKING_DIRECTORY needed for JSON test)
    
    * json: re-ran server deps.sh
    
    * json: simplify test
    
    * json: support mix of additional props & required/optional
    
    * json: add tests for some expected failures
    
    * json: fix type=const in c++, add failure expectations for non-str const&enum
    
    * json: test (& simplify output of) empty schema
    
    * json: check parsing in test + fix value & string refs
    
    * json: add server tests for OAI JSON response_format
    
    * json: test/fix top-level anyOf
    
    * json: improve grammar parsing failures
    
    * json: test/fix additional props corner cases
    
    * json: fix string patterns (was missing quotes)
    
    * json: ws nit
    
    * json: fix json handling in server when there's no response_format
    
    * json: catch schema conversion errors in server
    
    * json: don't complain about unknown format type in server if unset
    
    * json: cleaner build of test
    
    * json: create examples/json-schema-pydantic-example.py
    
    * json: fix date pattern
    
    * json: move json.hpp & json-schema-to-grammar.{cpp,h} to common
    
    * json: indent 4 spaces
    
    * json: fix naming of top-level c++ function (+ drop unused one)
    
    * json: avoid using namespace std
    
    * json: fix zig build
    
    * Update server.feature
    
    * json: iostream -> fprintf
    
    * json: space before & refs for consistency
    
    * json: nits

.gitignore
Makefile
build.zig
common/CMakeLists.txt
common/json-schema-to-grammar.cpp
common/json-schema-to-grammar.h
common/json.hpp
examples/json-schema-pydantic-example.py
examples/json-schema-to-grammar.py
examples/regex-to-grammar.py
examples/server/CMakeLists.txt
examples/server/chat.mjs
examples/server/completion.js.hpp
examples/server/index.html.hpp
examples/server/index.js.hpp
examples/server/json-schema-to-grammar.mjs.hpp
examples/server/public/index.html
examples/server/public/index.js
examples/server/public/json-schema-to-grammar.mjs
examples/server/server.cpp
examples/server/tests/features/security.feature
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py
examples/server/utils.hpp
examples/ts-type-to-grammar.sh
tests/CMakeLists.txt
tests/run-json-schema-to-grammar.mjs
tests/test-json-schema-to-grammar.cpp

commit 1943c0198125a0da1a200390e82cf461f9080d99
Author: Vaibhav Srivastav <vaibhavs10@gmail.com>
Date:   Thu Mar 21 10:30:40 2024 +0100

    ci : fix indentation error (#6195)

.github/workflows/build.yml

commit 5e43ba87429d85acbde97d16b2f48d9de992cc80
Author: Vaibhav Srivastav <vaibhavs10@gmail.com>
Date:   Thu Mar 21 10:13:12 2024 +0100

    build : add mac pre-build binaries (#6182)
    
    * Initial commit - add mac prebuilds.
    
    * forward contribution credits for building the workflow.
    
    * minor : remove trailing whitespaces
    
    ---------
    
    Co-authored-by: Nicolas Patry <Narsil@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml

commit 76aa30a26353f597e4fbe3cf776772ae812af89a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Mar 21 08:27:57 2024 +0100

    Add ability to use Q5_0, Q5_1, and IQ4_NL for quantized K cache (#6183)
    
    * k_cache: be able to use Q5_0
    
    * k_cache: be able to use Q5_1 on CODA
    
    * k_cache: be able to use Q5_0 on Metal
    
    * k_cache: be able to use Q5_1 on Metal
    
    * k_cache: be able to use IQ4_NL - just CUDA for now
    
    * k_cache: be able to use IQ4_NL on Metal
    
    * k_cache: add newly added supported types to llama-bench and CUDA supports_op
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

common/common.cpp
examples/llama-bench/llama-bench.cpp
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal

commit c5b8595e3f4f4ed319ef71c9c9d868d1b7a27626
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Thu Mar 21 06:10:52 2024 +0000

    Add nvidia and amd backends (#6157)

ggml-sycl.cpp

commit 42e21c68826f2e56b9592dccd9f3c43895b6890d
Author: slaren <slarengh@gmail.com>
Date:   Thu Mar 21 01:47:46 2024 +0100

    cuda : fix conflict with std::swap (#6186)

ggml-cuda.cu

commit 1c51f98adcbad40e3c41f0a6ffadeb723190b417
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 20 21:03:26 2024 +0100

    cuda : print the returned error when CUDA initialization fails (#6185)

ggml-cuda.cu

commit f9c7ba34476ffc4f13ae2cdb1aec493a16eb8d47
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Wed Mar 20 23:29:51 2024 +0800

    llava : update MobileVLM-README.md (#6180)

examples/llava/MobileVLM-README.md

commit 272935b281fee5c683e3d6d1eb580b84553cf503
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Wed Mar 20 23:02:32 2024 +0800

    llava : add MobileVLM_V2 backup (#6175)
    
    * Add MobileVLM_V2 backup
    
    * Update MobileVLM-README.md
    
    * Update examples/llava/MobileVLM-README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/llava/convert-image-encoder-to-gguf.py
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * clip :  fix whitespace
    
    * fix deifinition mistake in clip.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/llava/MobileVLM-README.md
examples/llava/clip.cpp
examples/llava/convert-image-encoder-to-gguf.py

commit ccf58aa3ec4d20b10162ba40898dc038ad4c3fad
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 20 14:42:59 2024 +0100

    cuda : refactor to remove global resources (#6170)
    
    * cuda : refactor to remove global resources

ggml-cuda.cu

commit 91f8ad167dcd24b54615b468d9dd764ebe1d37ad
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Mar 20 13:30:36 2024 +0100

    Server: version bump for httplib and json (#6169)
    
    * server: version bump for httplib and json
    
    * fix build
    
    * bring back content_length

examples/server/httplib.h
examples/server/json.hpp

commit 6b7e76d28cdf4361740054140708c71828453522
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 20 14:17:34 2024 +0200

    gitignore : ignore curl-related files

.gitignore

commit bc0baab2ea8f806961dd3fb9f534cfeb59a2e1fc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 20 14:14:32 2024 +0200

    server : allow to override -ngl in tests (#6170)

examples/server/tests/features/steps/steps.py

commit d795988d9e09f75412efe2134512914c42780ad5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 20 13:29:49 2024 +0200

    Revert "llava : add a MobileVLM_V2-1.7B backup (#6152)"
    
    This reverts commit f8c4e745e1e728204ab26dbadf52853545e6789c.

examples/llava/MobileVLM-README.md
examples/llava/clip.cpp
examples/llava/convert-image-encoder-to-gguf.py

commit f8c4e745e1e728204ab26dbadf52853545e6789c
Author: Ziang Wu <97337387+ZiangWu-77@users.noreply.github.com>
Date:   Wed Mar 20 19:20:37 2024 +0800

    llava : add a MobileVLM_V2-1.7B backup (#6152)
    
    * Add MobileVLM_V2 backup
    
    * Update MobileVLM-README.md
    
    * Update examples/llava/MobileVLM-README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/llava/convert-image-encoder-to-gguf.py
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * clip :  fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/llava/MobileVLM-README.md
examples/llava/clip.cpp
examples/llava/convert-image-encoder-to-gguf.py

commit 47cc7a7bf9793f81a6dfe7c2096c499403f64dd6
Author: Karthick <j.karthic2004@gmail.com>
Date:   Wed Mar 20 16:32:34 2024 +0530

    Server: Handle n_keep parameter in the request (#6174)

examples/server/utils.hpp

commit bd60d82d0cc8b6852ec535495a5042dbdf05de24
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Mar 20 01:33:49 2024 -0400

    server tests : more pythonic process management; fix bare `except:` (#6146)
    
    * server tests : remove seemingly redundant newlines in print()
    
    * server tests : use built-in subprocess features, not os.kill and psutil
    
    * server tests : do not catch e.g. SystemExit; use print_exc
    
    * server tests: handle TimeoutExpired exception
    
    * server tests: fix connect on dual-stack systems
    
    * server: tests: add new tokens regex on windows generated following new repeat penalties default changed in (#6127)
    
    * server: tests: remove the hack on windows since now we get the good socket family
    
    * server: tests: add new tokens regex following new repeat penalties default changed in (#6127)
    
    * server: tests: add new tokens regex following new repeat penalties default changed in (#6127)
    
    ---------
    
    Co-authored-by: Pierrick HYMBERT <pierrick.hymbert@gmail.com>

examples/server/tests/features/environment.py
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py
examples/server/tests/requirements.txt

commit 6c0b287748327741b113d7d6018b68c63039b1c5
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Mar 20 11:21:41 2024 +0800

    update readme sycl for new update (#6151)
    
    * update readme sycl for new update
    
    * Update README-sycl.md
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
    
    * Update README-sycl.md
    
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
    
    * update by review comments
    
    * update w64devkit link
    
    * update for verify device id part
    
    * Update README-sycl.md
    
    Co-authored-by: Meng, Hengyu <airdldl@163.com>
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    Co-authored-by: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
    Co-authored-by: Meng, Hengyu <airdldl@163.com>

README-sycl.md
examples/sycl/win-run-llama2.bat

commit d26e8b669dbf1f5f5a0afe4d2d885e86cf566302
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Wed Mar 20 08:28:49 2024 +0530

    increase igpu cluster limit (#6159)

ggml-sycl.h

commit d8b009a9456bf5284376149f3deb09300a37701a
Author: DAN™ <dranger003@gmail.com>
Date:   Tue Mar 19 12:16:09 2024 -0400

    Remove undeed header file. (#6158)

examples/gguf-split/gguf-split.cpp

commit d0d5de42e5a65865b5fddb6f5c785083539b74c3
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Tue Mar 19 12:05:44 2024 +0100

    gguf-split: split and merge gguf per batch of tensors (#6135)
    
    * gguf-split: split and merge gguf files per tensor
    
    * gguf-split: build with make toolchain
    
    * gguf-split: rename `--split-tensors-size` to `--split-max-tensors`. Set general.split_count KV to all split
    
    * split : minor style + fix compile warnings
    
    * gguf-split: remove --upload not implemented
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

Makefile
examples/CMakeLists.txt
examples/gguf-split/CMakeLists.txt
examples/gguf-split/README.md
examples/gguf-split/gguf-split.cpp

commit b80cf3b2d1dee0ad325f7a794fecc66befce7336
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 19 10:21:54 2024 +0200

    common : disable repeat penalties by default (#6127)

common/sampling.h

commit 970a48060ab9a6cc67aa063870323781c2a7bd7d
Author: slaren <slarengh@gmail.com>
Date:   Tue Mar 19 09:06:54 2024 +0100

    ci : exempt some labels from being tagged as stale (#6140)

.github/workflows/close-issue.yml

commit 4c28b8252907561165827125d2d1a4bad6926ac6
Author: DAN™ <dranger003@gmail.com>
Date:   Tue Mar 19 01:59:36 2024 -0400

    common : print usage on '-h' and '--help' (#6145)

common/common.cpp

commit 2d15886bb092c3b780c676b5cc57ff3337af9c83
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Mar 17 06:37:44 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/9df3e30ce24fd28c7b3e2de0d986769db5d6225d' (2024-03-06)
      → 'github:NixOS/nixpkgs/d691274a972b3165335d261cc4671335f5c67de9' (2024-03-14)

flake.lock

commit d199ca79f279e84ebe27caafe0aa59c461d88969
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Mar 18 12:49:02 2024 -0400

    mpt : implement backwards compatiblity with duped output tensor (#6139)

llama.cpp

commit 104f5e0fc156d48476258295457cafeec2a2af10
Author: Felix <stenbackfelix@gmail.com>
Date:   Mon Mar 18 16:40:22 2024 +0100

    clip : fix memory leak (#6138)

examples/llava/clip.cpp

commit 5e1b7f94a03e0b3b8e4578625bbdadc7bbd2b93c
Author: slaren <slarengh@gmail.com>
Date:   Mon Mar 18 16:33:44 2024 +0100

    backend : set max split inputs to GGML_MAX_SRC (#6137)

ggml-backend.c

commit ac9ee6a4ad740bc1ee484ede43e9f92b5af244c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 18 13:45:38 2024 +0200

    ci : disable stale issue messages (#6126)

.github/workflows/close-issue.yml

commit 4f6d1337ca5a409dc74aca8c479b7c34408a69c0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 18 13:45:27 2024 +0200

    ci : temporary disable sanitizer builds (#6128)

.github/workflows/build.yml
.github/workflows/server.yml

commit 2bf8d0f7c4cc1235755ad06961ca761e458c5e55
Author: slaren <slarengh@gmail.com>
Date:   Mon Mar 18 11:03:04 2024 +0100

    backend : offload large batches to GPU (#6083)
    
    * backend : offload large batches to GPU
    
    * fix hip
    
    * code cleanup
    
    * fix CUDA split buffers
    
    * Update ggml-backend-impl.h
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * cuda : fix memset without set_device
    
    * imatrix : remove sched affix from weight names
    
    * sched : add a new split if the current one has too many inputs
    reduce max inputs per split
    more cleanup
    
    * update backends
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

examples/imatrix/imatrix.cpp
examples/llama-bench/llama-bench.cpp
ggml-alloc.c
ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-cuda.h
ggml-kompute.cpp
ggml-metal.m
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
llama.cpp

commit 496bc79bc2b79bfd6124b8687a8dbd6a646e9b06
Author: DAN™ <dranger003@gmail.com>
Date:   Mon Mar 18 04:27:44 2024 -0400

    common : tidy-up argument parsing (#6105)
    
    * Tidy-up argument parsing.
    
    * Missing ref.
    
    * common : minor
    
    * common : add static classifier
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp

commit 9b03719ad712e2dc36c5c0c20f352bf3e4bda332
Author: Thérence <13496987+Royalphax@users.noreply.github.com>
Date:   Mon Mar 18 09:17:00 2024 +0100

    convert : add support for CamembertModel architecture (#6119)
    
    Adding support for CamembertModel architecture used by :
    https://huggingface.co/dangvantuan/sentence-camembert-large

convert-hf-to-gguf.py

commit 3a6efdd03c46c5ba08e43880d34260c02dd9999b
Author: Romain D <90720+Artefact2@users.noreply.github.com>
Date:   Mon Mar 18 09:04:41 2024 +0100

    convert : use f32 outtype for bf16 tensors (#6106)
    
    The old behaviour is to use f16, but bf16 to f16 is not a lossless conversion.
    Change the outtype to f32 to default to a lossless conversion.

convert.py

commit d01b3c4c32357567f3531d4e6ceffc5d23e87583
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 17 19:12:37 2024 +0100

    common: llama_load_model_from_url using --model-url (#6098)
    
    * common: llama_load_model_from_url with libcurl dependency
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
.github/workflows/server.yml
CMakeLists.txt
Makefile
common/CMakeLists.txt
common/common.cpp
common/common.h
examples/main/README.md
examples/server/README.md
examples/server/server.cpp
examples/server/tests/README.md
examples/server/tests/features/embeddings.feature
examples/server/tests/features/environment.py
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py
examples/server/tests/requirements.txt

commit cd776c37c945bf58efc8fe44b370456680cb1b59
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 17 19:51:57 2024 +0200

    ci : close all stale issues at once (#6115)

.github/workflows/close-issue.yml

commit dc0f6125487dcfbff913360f9d877bc0ccf6aa57
Author: GainLee <perfecter.gen@gmail.com>
Date:   Mon Mar 18 01:12:22 2024 +0800

    ggml:fix finding transfer queue family index error (#6094)
    
    Co-authored-by: GainLee <ligen@meizu.com>

ggml-vulkan.cpp

commit c47cf414efafb8f60596edc7edb5a2d68065e992
Author: AmirAli Mirian <37371367+amiralimi@users.noreply.github.com>
Date:   Sat Mar 16 11:52:02 2024 -0400

    ggml : add AVX512F SIMD (#6088)

ggml.c

commit b5f4ae09c3244ae1644b67c03ed9f4227ab25ad2
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sat Mar 16 16:46:29 2024 +0100

    gritlm : add initial README.md (#6086)
    
    * gritlm: add initial README.md to examples/gritlm
    
    This commit adds a suggestion for an initial README.md for the gritlm
    example.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! gritlm: add initial README.md to examples/gritlm
    
    Use the `scripts/hf.sh` script to download the model file.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * squash! gritlm: add initial README.md to examples/gritlm
    
    Fix editorconfig-checker error in examples/gritlm/README.md.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/gritlm/README.md

commit dfbfdd60f90207404039c6578d709231496831d9
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Mar 16 16:42:08 2024 +0100

    readme : add wllama as a wasm binding (#6100)

README.md

commit 15961ec04dbd59d21d8984d42e4c0f7e7e7d320a
Author: DAN™ <dranger003@gmail.com>
Date:   Sat Mar 16 11:39:15 2024 -0400

    common : refactor nested if causing error C1061 on MSVC (#6101)
    
    * Refactor nested if causing error C1061 on MSVC.
    
    * Revert back and remove else's.
    
    * Add flag to track found arguments.

common/common.cpp

commit a56d09a4407f29c21e149b44fd5308f83aa1cb09
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 16 13:20:53 2024 +0100

    ci : close inactive issue with workflow (#6053)
    
    * issues: ci - close inactive issue with workflow
    
    * ci: close issue, change workflow schedule time

.github/workflows/close-issue.yml

commit d84c48505f60bcd358b82a751d40418c4d235643
Author: slaren <slarengh@gmail.com>
Date:   Fri Mar 15 22:14:16 2024 +0100

    llama : fix Baichuan2 13B (#6092)

llama.cpp

commit 877b4d0c628cc70dddb5df72ed8fc14d126ca7e8
Author: Theia Vogel <theia@vgel.me>
Date:   Fri Mar 15 13:43:02 2024 -0700

    llama : add support for control vectors (#5970)
    
    * control vector api and implementation
    
    * control-vectors : minor code style updates
    
    * disable control vector when data == nullptr
    
    use -1 for disabled range (also on init) in case we ever support controlling layer 0 (embeddings)
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
llama.cpp
llama.h

commit 12247f4c69a173b9482f68aaa174ec37fc909ccf
Author: Andrew Canis <andrew.canis@gmail.com>
Date:   Fri Mar 15 16:41:22 2024 -0400

    llama : add Command-R support (#6033)
    
    Information about the Command-R 35B model (128k context) can be found at:
            https://huggingface.co/CohereForAI/c4ai-command-r-v01
    
    Based on the llama2 model with a few changes:
    
    1) New hyper parameter to scale output logits (logit_scale)
    2) Uses LayerNorm instead of RMSNorm
    3) Transfomer layers have a single shared LayerNorm that feeds into both the
       self-attention and FFN layers in parallel. There is no post-attention LayerNorm.
    4) No support for Rotary Position Embeddings (RoPE) scaling
    5) No biases used
    
    Find GGUF files here:
            https://huggingface.co/andrewcanis/c4ai-command-r-v01-GGUF
    
    To convert model to GGUF format yourself:
    
    1) Download Command-R Hugging Face safetensors:
            git lfs install
            git clone https://huggingface.co/CohereForAI/c4ai-command-r-v01
    
    2) Run:
            python3 convert-hf-to-gguf.py --outtype f16 ./c4ai-command-r-v01

README.md
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
llama.cpp

commit 4e9a7f7f7fb6acbddd1462909c8d696e38edbfcc
Author: Ting Lou <ting.lou@gmail.com>
Date:   Fri Mar 15 22:31:05 2024 +0800

    llava : change API to pure C style for Rust FFI bindgen (#6079)
    
    Co-authored-by: Lou Ting <louting.t@alibaba-inc.com>

examples/llava/clip.cpp
examples/llava/clip.h
examples/llava/llava.cpp
examples/llava/llava.h

commit 3020327f6cd6d2ce50528dd65f4b199d2ea8b1ae
Author: slaren <slarengh@gmail.com>
Date:   Fri Mar 15 13:24:03 2024 +0100

    cuda : disable unused cudaLaunchHostFunc code (#6078)

ggml-cuda.cu

commit 46acb3676718b983157058aecf729a2064fc7d34
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Fri Mar 15 18:53:53 2024 +0800

    fix set main gpu error (#6073)

examples/sycl/build.sh
examples/sycl/run-llama2.sh
ggml-sycl.cpp
ggml-sycl.h
llama.cpp

commit 131b0584096ee9df4d07cb28759dfea6efe6475f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 15 11:36:50 2024 +0200

    make : ggml-metal.o depends on ggml.h

Makefile

commit 753e36f650fa2a5869f89188d9ee745dc74cf14b
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Fri Mar 15 09:26:20 2024 +0000

    [SYCL] Fix non-intel device selection (#6042)
    
    * Fix non-intel device selection
    
    * Update ggml-sycl.cpp
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    * Update ggml-sycl.cpp
    
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    Co-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>

ggml-sycl.cpp

commit 7ce2c77f88e1ca66ec48417e56f91746bac018c2
Author: Ondřej Čertík <ondrej@certik.us>
Date:   Fri Mar 15 02:46:51 2024 -0600

    gguf : add support for I64 and F64 arrays (#6062)
    
    * gguf : add support for I64 and F64 arrays
    
    GGML currently does not support I64 or F64 arrays and they are not often
    used in machine learning, however if in the future the need arises, it
    would be nice to add them now, so that the types are next to the other
    types I8, I16, I32 in the enums, and it also reserves their type number.
    
    Furthermore, with this addition the GGUF format becomes very usable for
    most computational applications of NumPy (being compatible with the most
    common NumPy dtypes: i8, i16, i32, i64, f32, f64), providing a faster,
    and more versatile alternative to the `npz` format, and a simpler
    alternative to the `hdf5` format.
    
    The change in this PR seems small, not significantly increasing the
    maintenance burden. I tested this from Python using GGUFWriter/Reader
    and `gguf-dump`, as well as from C, everything seems to work.
    
    * Fix compiler warnings

ggml.c
ggml.h
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_reader.py
gguf-py/gguf/gguf_writer.py

commit aab606a11fc0a9740a7f297521c3eef851dfb351
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Mar 15 09:44:57 2024 +0100

    llama : add Orion chat template (#6066)

llama.cpp
tests/test-chat-template.cpp

commit b0bc9f4a9da7c19f4779106ea83b23feca747566
Author: slaren <slarengh@gmail.com>
Date:   Fri Mar 15 09:22:24 2024 +0100

    llama-bench : use random tokens to improve accuracy with mixtral (#6069)

examples/llama-bench/llama-bench.cpp

commit 4755afd1cbd40d93c017e5b98c39796f52345314
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 22:58:41 2024 +0200

    llama : fix integer overflow during quantization (#6063)

llama.cpp

commit 6e0438da3cc95b89cdbf55f45fa4e324d9076792
Author: Steve Grubb <ausearch.1@gmail.com>
Date:   Thu Mar 14 14:29:32 2024 -0400

    gguf : fix resource leaks (#6061)
    
    There several places where a gguf context is allocated. A call to gguf_free
    is missing in some error paths. Also on linux, llama-bench was missing a
    fclose.

examples/gguf/gguf.cpp
examples/llama-bench/llama-bench.cpp
examples/llava/clip.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp

commit 727107707a73b3dc8a497cf9fc9405722c16dd2b
Author: Ondřej Čertík <ondrej@certik.us>
Date:   Thu Mar 14 11:57:31 2024 -0600

    gguf-py : bump version to 0.8.0 (#6060)

gguf-py/pyproject.toml

commit 69ff61397d2b7b550dcdda4a35b35128892408b0
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Thu Mar 14 17:21:56 2024 +0100

    llama : support models without vocabulary (#5798)
    
    * additional methods to read model and ctx parameters
    
    * vocab size as a part of a model metadata
    
    * models without vocabulary, convert.py part
    
    * models without vocabulary, llama.cpp part
    
    * PR clean up
    
    * converter scrypt fixes
    
    * llama_vocab_type update (renamed the new key)
    
    * pr review fixes
    
    * revert function renaming
    
    * one more NoVocab assert

convert.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
llama.cpp
llama.h

commit 044ec4b2a567f649459ccd20af2f387c784faa51
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 15:14:14 2024 +0200

    embedding : add EOS token if not present (#899)

examples/embedding/embedding.cpp

commit 77178eedc83d49f31bf757d8e12315d76460be78
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 13:32:14 2024 +0200

    gguf-py : fix dtype check (#6045)

gguf-py/gguf/gguf_writer.py

commit 15a333260ab637a040ed0864c206a2ceaf806bb8
Author: Jian Liao <jianliao@users.noreply.github.com>
Date:   Thu Mar 14 04:18:23 2024 -0700

    readme : improve readme for Llava-1.6 example (#6044)
    
    Co-authored-by: Jian Liao <jianliao@adobe.com>

examples/llava/README.md

commit 43241adf22e8231ffaf3827d2c9310cc0ffd5ac5
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Thu Mar 14 12:15:39 2024 +0100

    server: disable debug release type sanitizer, simplify trigger (#6047)
    
    - increase time out for server
     - do not fail fast

.github/workflows/server.yml
examples/server/tests/features/steps/steps.py

commit a44bc969e4cd62ca9f4332e17fe3c51f2093e7c6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 13:13:06 2024 +0200

    llama : fix typo

llama.cpp

commit 2c4fb69246834503db7b78bcbedcef506bbc60c4
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Thu Mar 14 11:56:48 2024 +0100

    llama : optimize defrag moves + fix fragmentation calculation (#6037)
    
    * attempt to reduce the impact of a worst-case scenario
    
    * fragmentation calculation fix
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp

commit 3ca23481dd309bd51cc31c73a4cc34f922cc372f
Author: Ondřej Čertík <ondrej@certik.us>
Date:   Thu Mar 14 04:40:14 2024 -0600

    gguf-py : add support for I8, I16 and I32 (#6045)
    
    * Refactor dtype handling to be extensible
    
    This code is equivalent as before, but now it is prepared to easily add
    more NumPy dtypes.
    
    * Add support for I8, I16 and I32
    
    These types are allowed in the GGUF specification.
    
    * Add support for I8, I16 and I32 to gguf_writer
    
    * Add support for I8, I16, I32 to gguf_reader

gguf-py/gguf/constants.py
gguf-py/gguf/gguf_reader.py
gguf-py/gguf/gguf_writer.py

commit 3fe8d7a17f84bd721cd4d8db35365da44b69f68b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 12:38:37 2024 +0200

    ggml : designate enum vals for integer types (#6050)

ggml.h

commit 68265ebfc6a1bed022973ea0c3145be1450b7e70
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 12:37:20 2024 +0200

    embedding : print all resulting embeddings (#899)

examples/embedding/embedding.cpp

commit 381da2d9f0940d7009e3e918bed36338c8ff2fbb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 11:55:23 2024 +0200

    metal : build metallib + fix embed path (#6015)
    
    * metal : build metallib + fix embed path
    
    ggml-ci
    
    * metal : fix embed build + update library load logic
    
    ggml-ci
    
    * metal : fix embeded library build
    
    ggml-ci
    
    * ci : fix iOS builds to use embedded library

.github/workflows/build.yml
.gitignore
CMakeLists.txt
Makefile
ggml-metal.m
ggml-metal.metal

commit 0fd6c1f015f6cccf3b527f7dbd8386a434728126
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 14 10:12:29 2024 +0200

    embedding : print cosine similarity (#899)

common/common.cpp
common/common.h
examples/embedding/embedding.cpp
examples/gritlm/gritlm.cpp

commit 19885d205e768579ab090d1e99281cae58c21b54
Author: Linwei Wang <wanix1988@gmail.com>
Date:   Thu Mar 14 02:34:40 2024 +0800

    readme : update details about running llama in Termux on Android (#6039)

README.md

commit 76a936c8939c249a7c3e8e66dfefbab13eae194f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 13 20:33:56 2024 +0200

    readme : update API changes and hot topics

README.md

commit 463628372d5fe3a0c1e5864aa5fc57deb7387039
Author: Clint Herron <hanclinto@gmail.com>
Date:   Wed Mar 13 14:10:40 2024 -0400

    grammar : handle missing "root" node (#6004)

common/sampling.cpp

commit f30ea47a87ed4446ad55adb265755dc9102956a2
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 13 18:54:21 2024 +0100

    llama : add pipeline parallelism support (#6017)
    
    * llama : add pipeline parallelism support for batch processing with multiple CUDA GPUs
    
    ggml-ci
    
    * server : add -ub, --ubatch-size parameter
    
    * fix server embedding test
    
    * llama : fix Mamba inference for pipeline parallelism
    
    Tested to work correctly with both `main` and `parallel` examples.
    
    * llama : limit max batch size to n_batch
    
    * add LLAMA_SCHED_MAX_COPIES to configure the number of input copies for pipeline parallelism
    default increase to 4 (from 2)
    
    changing this value may improve performance for some systems, but increases memory usage
    
    * fix hip build
    
    * fix sycl build (disable cpy_tensor_async)
    
    * fix hip build
    
    * llama : limit n_batch and n_ubatch to n_ctx during context creation
    
    * llama : fix norm backend
    
    * batched-bench : sync after decode
    
    * swiftui : sync after decode
    
    * ggml : allow ggml_get_rows to use multiple threads if they are available
    
    * check n_ubatch >= n_tokens with non-casual attention
    
    * llama : do not limit n_batch to n_ctx with non-casual attn
    
    * server : construct batch with size of llama_n_batch
    
    * ggml_backend_cpu_graph_compute : fix return value when alloc fails
    
    * llama : better n_batch and n_ubatch comment
    
    * fix merge
    
    * small fix
    
    * reduce default n_batch to 2048
    
    ---------
    
    Co-authored-by: Francis Couture-Harpin <git@compilade.net>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
Makefile
common/common.cpp
common/common.h
examples/batched-bench/batched-bench.cpp
examples/embedding/embedding.cpp
examples/llama-bench/llama-bench.cpp
examples/llama.swiftui/llama.cpp.swift/LibLlama.swift
examples/perplexity/perplexity.cpp
examples/server/server.cpp
examples/server/tests/features/embeddings.feature
examples/server/tests/features/steps/steps.py
ggml-alloc.c
ggml-alloc.h
ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-kompute.cpp
ggml-metal.m
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
llama.cpp
llama.h

commit d8fd0ccf6ac8b07791ffd1575eed436930854ae3
Author: slaren <slarengh@gmail.com>
Date:   Wed Mar 13 14:58:30 2024 +0100

    test-backend-ops : skip CPU backend by default (#6028)

tests/test-backend-ops.cpp

commit b3d978600f07f22e94f2e797f18a8b5f6df23c89
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Wed Mar 13 13:17:54 2024 +0000

    Update get version (#6025)

ggml-sycl.cpp

commit 99b71c068f624521ad977e08e41589e2971fa1c7
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Mar 13 11:39:11 2024 +0100

    Server: Use multi-task for embeddings endpoint (#6001)
    
    * use multitask for embd endpoint
    
    * specify types
    
    * remove redundant {"n_predict", 0}

examples/server/server.cpp
examples/server/utils.hpp

commit 306d34be7ad19e768975409fc80791a274ea0230
Author: slaren <slarengh@gmail.com>
Date:   Tue Mar 12 16:55:19 2024 +0100

    ci : remove tidy-review (#6021)

.github/workflows/tidy-post.yml
.github/workflows/tidy-review.yml

commit 8030da7afea2d89f997aeadbd14183d399a017b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 12 14:27:20 2024 +0200

    ggml : reuse quantum structs across backends (#5943)
    
    * ggml : reuse quant blocks across backends
    
    ggml-ci
    
    * ggml : define helper constants only for CUDA and SYCL
    
    ggml-ci
    
    * ggml : define helper quantum constants for SYCL
    
    ggml-ci

ggml-common.h
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml-sycl.cpp

commit 184215e783dfad76aded2c68244c327a5c507df5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 12 13:49:55 2024 +0200

    ggml : fix UB in IQ2_S and IQ3_S (#6012)

ggml-quants.c

commit 48358b2e5b3983c41ba7e61a493e84d3901dc7b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 12 11:15:05 2024 +0200

    sycl : update IQ1_S kernels (WIP - not working!) (#5995)
    
    * sycl : try to fix after IQ1_S changes
    
    * sycl : iq1s_grid -> iq1s_grid_gpu
    
    * sycl : fix grid type

ggml-sycl.cpp

commit 5cdb371731caa2c41fcca42d4d2d43f94f6883b4
Author: gliptic <gliptic@users.noreply.github.com>
Date:   Mon Mar 11 20:59:03 2024 +0100

    grammar : fix unnecessarily retained pointer to rules (#6003)

llama.cpp

commit 44ca159faf4fbe1a7ace13a962845ba7cdfd95ec
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Mar 11 16:53:15 2024 +0100

    1.5 bit: we can do even better (#5999)
    
    * iq1_s: we can do even better
    
    Spent one of the 4 scale bits on a signs of a 0.125 shift.
    I.e., quants are now -1 + delta, delta, 1 + delta, where delta
    is +/- 0.125.
    
    CUDA works, same performance as before.
    PPL(LLaMA-v2-7B) is now 11.85!
    
    * iq1_s: make scalar and AVX2 work with the new version
    
    * iq1_s: make Neon work with new version.
    
    ~10% drop in performance, so will need some more work.
    
    * iq1_s: make Metal work with new version
    
    * iq1_s: very slightly faster dequantize on Metal
    
    * iq1_s: fix dequantize on the CPU
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-common.h
ggml-cuda.cu
ggml-metal.metal
ggml-quants.c

commit 05b06210c954491cf0f12034b0a62bd4d69ce78b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 11 17:49:47 2024 +0200

    llama : more consistent names of count variables (#5994)
    
    * llama : more consistent names of count variables
    
    ggml-ci
    
    * llama : n_parallel -> n_seq_max
    
    * common : fix param name
    
    * examples : fix param name

README.md
common/common.cpp
examples/batched-bench/batched-bench.cpp
examples/batched/batched.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
llama.cpp
llama.h

commit 83796e62bc9f6caae6228168e359890f51e60fee
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 11 17:47:47 2024 +0200

    llama : refactor unicode stuff (#5992)
    
    * llama : refactor unicode stuff
    
    ggml-ci
    
    * unicode : names
    
    * make : fix c++ compiler
    
    * unicode : names
    
    * unicode : straighten tables
    
    * zig : fix build
    
    * unicode : put nfd normalization behind API
    
    ggml-ci
    
    * swift : fix build
    
    * unicode : add BOM
    
    * unicode : add <cstdint>
    
    ggml-ci
    
    * unicode : pass as cpts as const ref

CMakeLists.txt
Makefile
Package.swift
build.zig
llama.cpp
tests/test-tokenizer-1-bpe.cpp
tests/test-tokenizer-1-llama.cpp
unicode.cpp
unicode.h

commit 828defefb66fc8a25404f5de845897145bf34061
Author: Jakub N <jakubniemczyk97@gmail.com>
Date:   Mon Mar 11 14:40:42 2024 +0100

    Update server docker image URLs (#5997)

examples/server/README.md

commit caa106d4e05a0ab94225c220b81f9e2cd522339b
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Mar 11 10:56:41 2024 +0100

    Server: format error to json (#5961)
    
    * server: format error to json
    
    * server: do not crash on grammar error
    
    * fix api key test case
    
    * revert limit max n_predict
    
    * small fix
    
    * correct coding style
    
    * update completion.js
    
    * launch_slot_with_task
    
    * update docs
    
    * update_slots
    
    * update webui
    
    * update readme

examples/server/README.md
examples/server/completion.js.hpp
examples/server/public/completion.js
examples/server/server.cpp
examples/server/tests/features/steps/steps.py
examples/server/utils.hpp

commit 3202361c5b1ba15e695b31209567ef42c22c5c32
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Mon Mar 11 10:28:51 2024 +0100

    ggml, ci : Windows ARM runner and build fixes (#5979)
    
    * windows arm ci
    
    * fix `error C2078: too many initializers` with ggml_vld1q_u32 macro for MSVC ARM64
    
    * fix `warning C4146: unary minus operator applied to unsigned type, result still unsigned`
    
    * fix `error C2065: '__fp16': undeclared identifier`

.github/workflows/build.yml
ggml-impl.h
ggml-quants.c
ggml.c
llama.cpp

commit 332bdfd7980718abf664bfa5460f2288a3314984
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Mon Mar 11 17:09:32 2024 +0900

    server : maintain chat completion id for streaming responses (#5988)
    
    * server: maintain chat completion id for streaming responses
    
    * Update examples/server/utils.hpp
    
    * Update examples/server/utils.hpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/server.cpp
examples/server/utils.hpp

commit ecab1c75de68de7c41c254e2ae170d3b07bee6d4
Author: Gilad S <giladgd@users.noreply.github.com>
Date:   Mon Mar 11 10:00:08 2024 +0200

    cmake : fix subdir for `LLAMA_METAL_EMBED_LIBRARY` (#5985)

CMakeLists.txt

commit ee35600b9061b1ea0c4ea87fce6844297632b2a8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 11 09:56:47 2024 +0200

    llama : fix F16/F32 downcast + improve names (#5980)

llama.cpp
llama.h

commit be858f620508385ad12d0e5e862010e666ca729c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Mar 11 07:51:49 2024 +0100

    Better 1.5 bit quantization  (#5971)
    
    * Trying blocvks of 16 for IQ1_S - seems slightly better
    
    * iq1s_blocks16: Adjust scale fudge factor to 1.125
    
    * iq1s_blocks16: going to blocks of 32
    
    with 2048 lattice points, so same bpw.
    This is even better than blocks of 16.
    Should I try blocks of 64? But to keep the same
    bpw, when I go to 4096 lattice points, I need to
    remove blocks alltogether and just have superblocks of
    256 weights.
    
    * iq1s_blocks16: Use 2*<x^2> as sigma2 in weight adjustment
    
    * iq1s_blocks16: scalar and AVX2 dot products
    
    * iq1s_blocks16: CUDA dot product
    
    * iq1s_blocks16: Metal works, Neon does not
    
    Metal works but TG is dog slow (35 t/s). PP is OKish (493 t/s).
    Not seeing the bug in the Neon implementation for now.
    
    * iq1s_blocks16: fixed Neon
    
    * iq1s_blocks16: very slightly faster TG on Metal
    
    Still pathetic at 37 t/s
    
    * iq1s_blocks16: speedup Metal by packing codebook into uint32_t's
    
    * Formatting
    
    * iq1s_blocks16: uint32_t codebook is also better in CUDA
    
    TG-128 is now 204 t/s up from 194 t/s.
    PP-512 is 5890 t/s, so significantly better than other quants
    
    * iq1s_blocks16: slightly faster Neon dot product
    
    * iq1s_blocks16: faster AVX2 dot product
    
    * iq1s_blocks16: adjust to ggml-common.h
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-common.h
ggml-cuda.cu
ggml-metal.metal
ggml-quants.c
ggml-quants.h

commit ef3ced26a3817d92890b97b83acaeb018ade02d0
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Mon Mar 11 10:27:56 2024 +0530

    [SYCL] Add q3_s and q1_s (#5886)
    
    * Add q3_s and q1_s
    
    * fix compilation
    
    * fix build
    
    * fix build
    
    * fix build
    
    * enable ops
    
    * rm macro
    
    * increase grid space

ggml-sycl.cpp

commit 3814a07392d2bdc22911652bc7c2f9bdb0ce042e
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon Mar 11 01:13:57 2024 +0000

    [SYCL] Add support for SYCL Nvidia target (#5738)
    
    * Add support for nvidia target in CMake
    
    * Update sycl read-me for Nvidia target
    
    * Fix errors

CMakeLists.txt
README-sycl.md

commit bb6d00bbf98476215596b9df3870b5504ef5a29a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 23:12:48 2024 +0200

    metal : move mm_id indices to shared mem (#5982)

ggml-metal.m
ggml-metal.metal

commit 7ab7b733bb48250b2df26c12b00256ef42c76932
Author: Dean <Dean.Sinaean@gmail.com>
Date:   Mon Mar 11 04:03:17 2024 +0800

    android : fix utf8 decoding error (#5935)
    
    * examples: fix utf8 decoding error
    
    some models have a tokenizer that decodes an id into an incomplete utf8 sequence, need to validate and wait for next token
    one example would be: https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q4_0.gguf and and an example of the token is 18137
    
    * android : minor
    
    ---------
    
    Co-authored-by: zhangfuwen <zhangfuwen@foxmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/llama.android/app/src/main/cpp/llama-android.cpp
examples/llama.android/app/src/main/java/com/example/llama/Llm.kt

commit d9f65c97c3dc3aa6fa27470b8c6e69b437ec1a27
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 20:58:26 2024 +0200

    readme : update hot topics

README.md

commit b838b53ad6de2e53f23ddf8f3ad5e6891cc3dd05
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 20:10:46 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit df4dc3e7cb43162862f339525638ba4febcb6158
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 8 23:45:07 2024 +0200

    ggml : try fix 32-bit arm compat (whisper/1938)
    
    * ggml : try fix 32-bit arm compat
    
    * ggml : fix cont

ggml-quants.c

commit bf47a5eefc669fdba71af096942af999bc1167d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 20:09:24 2024 +0200

    ggml : remove __constant__ specifier for CUDA tables (#5940)

ggml-common.h

commit fa8a809a9119411bc9c3f00026550d0343f4d9b7
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 10 18:17:47 2024 +0100

    server: ci: windows build and tests (#5968)
    
    * server: ci: windows build and tests
    
    * server: ci: remove tmp push branch
    
    * server: ci: EOF EOL
    
    * Use builti
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * server: tests: server graceful shutdown, then kill, then hard kill
    
    * server: tests: remove python2 unicode string
    
    * server: tests: remove wrong comment on server starting,  close_fds is always true
    
    * server: tests: server kill, if pid exists
    
    * server: tests: remove dependency to killall
    
    * server: tests: ci windows: pid exists better handling
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

.github/workflows/server.yml
examples/server/tests/features/environment.py
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py

commit bcebd7dbf62fd7b293d5ed089023e4e733269c71
Author: DAN™ <dranger003@gmail.com>
Date:   Sun Mar 10 11:56:30 2024 -0400

    llama : add support for GritLM (#5959)
    
    * add gritlm example
    
    * gritlm results match
    
    * tabs to spaces
    
    * comment out debug printing
    
    * rebase to new embed
    
    * gritlm embeddings are back babeee
    
    * add to gitignore
    
    * allow to toggle embedding mode
    
    * Clean-up GritLM sample code.
    
    * Fix types.
    
    * Flush stdout and output ending newline if streaming.
    
    * mostly style fixes; correct KQ_mask comment
    
    * add causal_attn flag to llama_cparams
    
    * gritml : minor
    
    * llama : minor
    
    ---------
    
    Co-authored-by: Douglas Hanley <thesecretaryofwar@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile
examples/CMakeLists.txt
examples/gritlm/CMakeLists.txt
examples/gritlm/gritlm.cpp
llama.cpp
llama.h

commit 2960eae847f8dbde23be6d170a61bcf44ebf32de
Author: Clint Herron <hanclinto@gmail.com>
Date:   Sun Mar 10 11:17:43 2024 -0400

    grammar : verify parsed state (#5950)

common/grammar-parser.cpp

commit c78541479cf835dd9eb568ccd9a2083198a7203d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 10 16:43:08 2024 +0200

    nix: update flake.lock (#5969)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8' (2024-02-29)
      → 'github:NixOS/nixpkgs/9df3e30ce24fd28c7b3e2de0d986769db5d6225d' (2024-03-06)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit 621e86b331f8b0e71f79fd82a4ae1cd54c3e4396
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 9 23:41:49 2024 +0100

    server: benchmark: chat/completions scenario and other llm servers comparison (#5941)
    
    * server: bench: Init a bench scenario with K6
    See #5827
    
    * server: bench: EOL EOF
    
    * server: bench: PR feedback and improved k6 script configuration
    
    * server: bench: remove llamacpp_completions_tokens_seconds as it include prompt processing time and it's misleading
    
    server: bench: add max_tokens from SERVER_BENCH_MAX_TOKENS
    
    server: bench: increase truncated rate to 80% before failing
    
    * server: bench: fix doc
    
    * server: bench: change gauge custom metrics to trend
    
    * server: bench: change gauge custom metrics to trend
    server: bench: add trend custom metrics for total tokens per second average
    
    * server: bench: doc add an option to debug http request
    
    * server: bench: filter dataset too short and too long sequences
    
    * server: bench: allow to filter out conversation in the dataset based on env variable
    
    * server: bench: fix assistant message sent instead of user message
    
    * server: bench: fix assistant message sent instead of user message
    
    * server : add defrag thold parameter
    
    * server: bench: select prompts based on the current iteration id not randomly to make the bench more reproducible
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/bench/README.md
examples/server/bench/script.js
examples/server/server.cpp

commit 77d1ac7e00bf049b9f2bba1b5a310a78318c49c4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 22:04:00 2024 +0200

    server : print chat template info

examples/server/server.cpp

commit d894f352bf433157232dc8dc54eacd50014e898e
Author: slaren <slarengh@gmail.com>
Date:   Sat Mar 9 19:55:54 2024 +0100

    perplexity : support using multiple sequences to allow larger batch sizes (#5946)
    
    * perplexity : support using multiple sequences to allow larger batch sizes
    
    ggml-ci
    
    * set cparams.n_parallel to the number of sequences
    
    * print tested n_ctx, add assert

examples/perplexity/perplexity.cpp
llama.cpp

commit 098dbaab449f5309a54871ba7e5acef72ae696de
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 18:14:13 2024 +0200

    readme : update hot topics

README.md

commit 8380ecfb219c2e73cc706fcc83935b7c806cc7c3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 17:36:20 2024 +0200

    ggml : fix unnecessary f32 -> f16 -> f32 casts (mmla) (#5951)

ggml-quants.c

commit 58308a0ecce7cc261b802f4803c38d420063db21
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 17:34:15 2024 +0200

    server : fix metrics init (#5964)

examples/server/server.cpp

commit 5b09797321430f08caf0473143a962916ab2ea89
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 15:53:59 2024 +0200

    ggml : remove old quantization functions (#5942)
    
    * ggml : remove old quantization functions
    
    ggml-ci
    
    * ggml : simplify ggml_quantize_chunk
    
    ggml-ci
    
    * ggml : restrict correctness
    
    ggml-ci
    
    * ggml : remove hist data from the quantization API
    
    ggml-ci
    
    * tests : remove hist usage in test-backend-ops
    
    ggml-ci
    
    * vulkan : remove hist and fix typo

examples/benchmark/benchmark-matmult.cpp
examples/llava/clip.cpp
ggml-quants.c
ggml-quants.h
ggml-vulkan.cpp
ggml.c
ggml.h
llama.cpp
tests/test-backend-ops.cpp

commit 97c09585d65a95864773b4d25d66d0f708baf38d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 15:47:47 2024 +0200

    server : clarify some items in the readme (#5957)
    
    * server : clarify some items in the readme
    
    * server : fix typo

examples/server/README.md

commit fb215c3832236fec7380c4fb618bd7154cb196ef
Author: SeungWon Jeong <65549245+redlion0929@users.noreply.github.com>
Date:   Sat Mar 9 21:27:58 2024 +0900

    server : normalize embeddings (#5956)
    
    * output normalize embedding in '/v1/embeddings'
    
    * common : reuse llama_embd_normalize
    
    * common : better normalize impl
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
examples/embedding/embedding.cpp
examples/server/server.cpp

commit 2c4f566c88322ebf2f9bd11b01b5ebdaa0130b89
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 14:17:11 2024 +0200

    tests : gitignore ggml-common.h

tests/.gitignore

commit 0db32beaf09d90b8959d3d0cc493ed1e45685353
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sat Mar 9 11:16:53 2024 +0000

    server : fix passing prompt as tokens (#5955)
    
    * server: fix passing prompt as tokens
    
    * Update examples/server/server.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/server.cpp

commit 8a3012a4ad08112bb3dc3f1399afec4e93780c44
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 12:47:57 2024 +0200

    ggml : add ggml-common.h to deduplicate shared code (#5940)
    
    * ggml : add ggml-common.h to shared code
    
    ggml-ci
    
    * scripts : update sync scripts
    
    * sycl : reuse quantum tables
    
    ggml-ci
    
    * ggml : minor
    
    * ggml : minor
    
    * sycl : try to fix build

CMakeLists.txt
Makefile
ggml-common.h
ggml-cuda.cu
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml-sycl.cpp
scripts/sync-ggml-am.sh
scripts/sync-ggml.sh

commit 9674aaf35cb81478eb38c3f3ebde713ec72fbb79
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 9 12:34:18 2024 +0200

    server : simplify logic for empty prompts (#5953)

examples/server/server.cpp

commit 950ba1ab84db199f0bbdecdb2bb911f35261b321
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Mar 9 11:27:53 2024 +0100

    Server: reorganize some http logic (#5939)
    
    * refactor static file handler
    
    * use set_pre_routing_handler for validate_api_key
    
    * merge embedding handlers
    
    * correct http verb for endpoints
    
    * fix embedding response
    
    * fix test case CORS Options
    
    * fix code style

examples/server/README.md
examples/server/server.cpp
examples/server/tests/features/security.feature
examples/server/tests/features/steps/steps.py

commit e1fa9569ba8ce276bc7801a3cebdcf8b1aa116ea
Author: Gabe Goodhart <gabe.l.hart@gmail.com>
Date:   Sat Mar 9 02:57:09 2024 -0700

    server : add SSL support (#5926)
    
    * add cmake build toggle to enable ssl support in server
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * add flags for ssl key/cert files and use SSLServer if set
    
    All SSL setup is hidden behind CPPHTTPLIB_OPENSSL_SUPPORT in the same
    way that the base httlib hides the SSL support
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * Update readme for SSL support in server
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    * Add LLAMA_SERVER_SSL variable setup to top-level Makefile
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>
    
    ---------
    
    Signed-off-by: Gabe Goodhart <ghart@us.ibm.com>

Makefile
examples/server/CMakeLists.txt
examples/server/README.md
examples/server/server.cpp

commit fd72d2d2a5e79d61ccef6af3d15f16e5e5cbc352
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 9 10:30:04 2024 +0100

    server: tests: add truncated prompt tests, better kv cache size (#5933)
    
    * server: tests: add truncated prompt tests, better size
    
    * server, tests : update regex
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/server.cpp
examples/server/tests/features/parallel.feature
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py

commit c2101a2e909ac7c08976d414e64e96c90ee5fa9e
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Fri Mar 8 17:31:00 2024 -0500

    llama : support Mamba Selective State Space Models (#5328)
    
    * mamba : begin working on support for Mamba SSM
    
    * mamba : begin figuring out how to (ab)use the kv cache for Mamba
    
    * mamba : recurrent inference almost works, but incoherent
    
    * mamba : recurrent inference WORKS!!!
    
    * convert : optionally use d_conv and d_state from config.json for Mamba
    
    * mamba : refactor recurrent conv, resulting in 20% perf increase
    
    It's still slower than I'd like, but I did not really optimize `ggml_exp` yet.
    
    I also refactored `ggml_exp` to work with tensors with more than 2 dimensions.
    
    * ggml : parallelize ggml_exp
    
    This results in 8% faster token generation for Mamba-130M.
    
    * mamba : simplify the conv step with a self-overlapping view
    
    Turns out the conv_state can be made smaller by one column.
    Note that this breaks existing GGUFs of Mamba,
    because the key_value_length field is tied to the conv_state size.
    
    Convolution with a self-overlapping view is cool!
    And it's much simpler than what I initially thought would be necessary
    to make the convolution step work with more than 1 token at a time.
    
    Next step is to make the SSM step work on batches of tokens too,
    and thus I need to figure out a way to make a parallel selective scan
    which will keep the ssm_state small and won't make it bigger
    by a factor of (n_layer * batch_size).
    
    * llama : fix Mamba KV self size wrongly displaying as f16 instead of f32
    
    Relatedly, I also tried to see if other types than f32 worked for the states,
    but they don't, because of the operators used.
    It's probably better anyway to keep lots of precision there,
    since the states are small anyway.
    
    * mamba : fix self-overlapping view depth stride
    
    * mamba : handle batches of more than 1 token
    
    This means running Mamba no longer crashes when using the default settings!
    And probably also slightly faster prompt processing.
    Both batched and non-batched processing yield the same output.
    
    Previously, the state was not cleared when starting a sequence.
    Next step is to make the KV cache API work as expected for Mamba models.
    
    * ggml: add ggml_ssm_scan to help with parallel selective scan
    
    If the selective scan was implemented without a custom operator,
    there would be waaay too many nodes in the graph. For example,
    for Mamba-130M, with a batch size of 512 (the default),
    a naive selective scan could add at least 24*512=12288 nodes,
    which is more than LLAMA_MAX_NODES (8192),
    and that's only for the smallest Mamba model.
    So it's much cleaner with a custom operator.
    Not sure about the name, though.
    
    * ggml : in ggml_ssm_scan, merge multiple rows in the same vec operation
    
    This will help with performance on CPU if ggml_vec_mul_f32
    and ggml_vec_add_f32 are ever optimized with SIMD.
    
    * mamba : very basic quantization support
    
    Mostly works, but there is currently no difference
    between the variants of a k-quant (e.g. Q4_K_S and Q4_K_M are the same).
    Most of the SSM-specific weights can be kept in f32 without affecting
    the size that much, since they are relatively small.
    (the linear projection weights are responsible for most of Mamba's size)
    
    Too much quantization seems to make the state degrade quite fast, and
    the model begins to output gibberish.
    It seems to affect bigger models to a lesser extent than small models,
    but I'm not sure by how much.
    
    Experimentation will be needed to figure out which weights are more important
    for the _M (and _L?) variants of k-quants for Mamba.
    
    * convert : fix wrong name for layer norm weight of offical Mamba models
    
    I was using Q-bert/Mamba-* models before, which have a slighlty different
    naming scheme for the weights.
    (they start with "model.layers" instead of "backbone.layers")
    
    * mamba : fuse more steps of the SSM scan in the ggml_ssm_scan operator
    
    This increases performance on CPU by around 30% for prompt processing,
    and by around 20% for text generation.
    
    However, it also makes the ggml_exp and ggml_soft_plus operators unused.
    Whether or not they should be kept will be decided later.
    
    * convert : for Mamba, also consider the "MambaLMHeadModel" arch name
    
    It's the name of the class of the official implementation,
    though they don't use it (yet) in the "architectures" field of config.json
    
    * mamba : fix vocab size problems with official models
    
    The perplexity was waaaay to high for models with a non-round vocab size.
    Not sure why, but it needed to be fixed in the metadata.
    
    Note that this breaks existing GGUF-converted Mamba models,
    but **only if** the vocab size was not already rounded.
    
    * ggml : remove ggml_exp and ggml_soft_plus
    
    They did not exist anyway outside of this branch,
    and since ggml_ssm_scan fused operations together, they are unused.
    It's always possible to bring them back if needed.
    
    * mamba : remove some useless comments
    
    No code change.
    
    * convert : fix flake8 linter errors
    
    * mamba : apply suggestions from code review
    
    * mamba : remove unecessary branch for row-wise ssm_state and C multiplication
    
    It was previously done to avoid permuting when only one token is processed
    at a time (like when generating text), but permuting is cheap,
    and dynamically changing the compute graph is not future-proof.
    
    * ggml : in ggml_ssm_scan, use more appropriate asserts
    
    * ggml : rename the destination pointer in ggml_compute_forward_ssm_scan_f32
    
    * mamba : multiple sequences, but one at a time
    
    This is a step towards making this Mamba implementation usable
    with the server example (the way the system prompt is kept when clearing
    the client slots will need to be changed before this can work, though).
    
    The KV cache size for this kind of model is tied to the maximum number
    of sequences kept at any single time.
    For now, this number is obtained from n_parallel (plus one,
    to have an extra sequence to dedicate to the system prompt),
    but there might be a better way to do this which won't also
    make the main example use 2 cells even if only 1 is really used.
    (for this specific case, --parallel 0 helps)
    
    Simultaneous sequence processing will probably require changes to
    ggml_ssm_scan, and possibly a new operator for the conv step.
    
    * mamba : support llama_kv_cache_seq_cp
    
    This (mis)uses the logic around K shifts, because tokens in a state
    can't be shifted anyway, and because inp_K_shift has the right shape and type.
    Using ggml_get_rows is a nice way to do copies, but copy chains can't work.
    Fortunately, copy chains don't really seem to be used in the examples.
    
    Each KV cell is dedicated to the sequence ID corresponding to its own index.
    
    * mamba : use a state mask
    
    It's cleaner than the previous heuristic of
    checking for the pos of the first token in the batch.
    
    inp_KQ_mask could not be re-used for this, because it has the wrong shape
    and because it seems more suited to the next step of
    simultaneous sequence processing (helping with the problem of
    remembering which token belongs to which sequence(s)/state(s)).
    
    * llama : replace the usage of n_ctx with kv_self.size in many places
    
    * mamba : use n_tokens directly instead of n_tok
    
    * mamba : in comments, properly refer to KV cells instead of slots
    
    * mamba : reduce memory usage of ggml_ssm_scan
    
    From 290.37 MiB to 140.68 MiB of CPU compute buffer size
    with Mamba 3B with a batch size of 512.
    
    The result tensor of ggml_ssm_scan was previously a big part
    of the CPU compute buffer size. To make it smaller,
    it does not contain the intermediate ssm states anymore.
    Both y and the last ssm state are combined in the result tensor,
    because it seems only a single tensor can be returned by an operator
    with the way the graph is built.
    
    * mamba : simultaneous sequence processing
    
    A batch can now contain tokens from multiple sequences.
    
    This is necessary for at least the parallel example, the server example,
    and the HellaSwag test in the perplexity example.
    
    However, for this to be useful, uses of llama_kv_cache_seq_rm/cp
    will need to be changed to work on whole sequences.
    
    * ggml : add ggml_ssm_conv as a new operator for the conv step of Mamba
    
    This operator makes it possible to use and update the correct states
    for each token of the batch in the same way as ggml_ssm_scan.
    Other solutions which use existing operators would need loops which would
    add too many nodes to the graph (at least the ones I thought of).
    
    Using this operator further reduces the size of the CPU compute buffer
    from 140.68 MiB to 103.20 MiB with Mamba 3B with a batch size of 512.
    And (at least on CPU), it's a bit faster than before.
    
    Note that "ggml_ssm_conv" is probably not the most appropriate name,
    and it could be changed if a better one is found.
    
    * llama : add inp_s_seq as a new input tensor
    
    The most convenient implementation to select the correct state (for Mamba)
    for each token is to directly get the correct index from a tensor.
    This is why inp_s_seq is storing int32_t and not floats.
    
    The other, less convenient way to select the correct state would be
    to have inp_KQ_mask contain 1.0f for each state used by a token
    and 0.0f otherwise. This complicates quickly fetching the first used
    state of a token, and is also less efficient because a whole row
    of the mask would always need to be read for each token.
    
    Using indexes makes it easy to stop searching when there are
    no more sequences for a token, and the first sequence assigned
    is always very quickly available (it's the first element of each row).
    
    * mamba : support llama_kv_cache_seq_cp copy chains
    
    * mamba : support shifting and dividing the kv cache pos
    
    * mamba : make the server and parallel examples work with whole sequences
    
    A seq_id is dedicated to the system prompt in both cases.
    
    * llama : make llama_kv_cache_seq_rm return whether it succeeded or not
    
    * mamba : dedicate an input tensor for state copy indices
    
    This is cleaner and makes it easier to adapt when/if token positions
    (and by extension, inp_K_shift) are no longer integers.
    
    * mamba : adapt perplexity, batched, and batched-bench examples
    
    * perplexity : limit the max number of sequences
    
    This adapts to what the loaded model can provide.
    
    * llama : add llama_n_max_seq to get the upper limit for seq_ids
    
    Used by the perplexity example.
    
    * batched : pass n_parallel to the model's context params
    
    This should have been there already, but it wasn't.
    
    * batched-bench : reserve sequences to support Mamba
    
    * batched-bench : fix tokens being put in wrong sequences
    
    Generation quality isn't what's measured in there anyway,
    but at least using the correct sequences avoids using non-consecutive
    token positions.
    
    * mamba : stop abusing attention metadata
    
    This breaks existing converted-to-GGUF Mamba models,
    but will allow supporting mixed architectures like MambaFormer
    without needing to break Mamba models.
    
    This will also allow changing the size of Mamba's states
    without having to reconvert models in the future.
    (e.g. using something else than d_conv - 1 columns for the conv_states
     will not require breaking existing converted Mamba models again)
    
    * gguf-py : add new KV metadata key-value pairs for Mamba
    
    * llama : add new metadata key-value pairs for Mamba
    
    * llama : guard against divisions by zero when n_head is 0
    
    * mamba : rename "unlimited" KV cache property to "recurrent"
    
    * mamba : more correctly update the "used" field of the KV cache
    
    * ggml : in ggml_ssm_scan, use a threshold for soft_plus
    
    This is how the official Mamba implementation does it,
    and it's also what torch.nn.Softplus does.
    
    * convert : for Mamba, fallback to internal NeoX tokenizer
    
    The resulting models are exactly the same
    as if the tokenizer.json and tokenizer_config.json of GPT-NeoX were there.
    
    * mamba : support state saving and restoring
    
    * ggml : implicitly pass src tensors through dst for Mamba-related ops
    
    * mamba : clarify some comments
    
    * server : fix cache_tokens not getting correctly resized
    
    Otherwise, when the "we have to evaluate at least 1 token" special case
    was triggered, an extra token was kept in cache_tokens even if it was
    removed from the KV cache.
    
    For Mamba, this caused useless prompt reprocessing when the previous
    request triggered the above case.
    
    * convert-hf : support new metadata keys for Mamba
    
    For the models available at
    https://huggingface.co/collections/state-spaces/transformers-compatible-mamba-65e7b40ab87e5297e45ae406
    
    * mamba : rename metadata to be more similar to transformers library
    
    This breaks existing converted-to-GGUF models,
    but the metadata names are more "standard".
    
    * mamba : support mamba-*-hf models
    
    These models share their token_embd.weight with their output.weight
    
    * mamba : add missing spaces
    
    This is purely a formatting change.
    
    * convert-hf : omit output.weight when identical with token_embd.weight
    
    Only for Mamba for now, but it might be relevant for other models eventually.
    Most Mamba models actually share these two tensors, albeit implicitly.
    
    * readme : add Mamba to supported models, and add recent API changes
    
    * mamba : move state_seq and state_mask views outside layer loop
    
    A few tensors were also missing `struct` in front of `ggml_tensor`.

README.md
common/common.cpp
convert-hf-to-gguf.py
examples/batched-bench/batched-bench.cpp
examples/batched/batched.cpp
examples/parallel/parallel.cpp
examples/perplexity/perplexity.cpp
examples/server/server.cpp
ggml.c
ggml.h
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/tensor_mapping.py
llama.cpp
llama.h

commit 515f7d0d4fce41c752fc253acf30707c3be2531e
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Fri Mar 8 10:53:37 2024 -0500

    llama : fix quantization of shared token_embd (#5944)

llama.cpp

commit 76e868821a94072fbc87cb1fcca291694319eae8
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Mar 8 12:25:04 2024 +0100

    server: metrics: add llamacpp:prompt_seconds_total and llamacpp:tokens_predicted_seconds_total, reset bucket only on /metrics. Fix values cast to int. Add Process-Start-Time-Unix header. (#5937)
    
    Closes #5850

examples/server/server.cpp
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py

commit e457fb3540e0aaec47cfde0abf784c213f9216ee
Author: Don Mahurin <dmahurin@users.noreply.github.com>
Date:   Fri Mar 8 02:41:50 2024 -0800

    llama : assume tied weights if lm_head/output weights is missing (#5824)
    
    This is to support model configurations with "tie_word_embeddings" set to true.
    
    Co-authored-by: Don Mahurin <2797413+dmahurin@users.noreply.github.com>

llama.cpp

commit af37fd8b30e37ccbffdd82e6f48559e2fb7ce7dd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 8 12:40:02 2024 +0200

    server : fix EOS token detection with disabled cache (#5938)

examples/server/server.cpp

commit 581ed5c4fe3a8909aaa8313633ac443f471ba755
Author: UEXTM.com <84163508+uextm@users.noreply.github.com>
Date:   Fri Mar 8 04:35:04 2024 -0500

    log : fix MSVC compile errors (#5643)
    
    MSVC gives the following error with the existing macros:
    `Error C2059 : syntax error: ','`
    
    This patch adds `##` as a prefix to `__VA_ARGS__` to address this error.

common/log.h

commit 6cdabe652695167263c8b447520987b11856f7ca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 7 16:32:38 2024 +0200

    llama-bench : add embeddings option (#5924)
    
    * llama-bench : add embeddings option
    
    * llama-bench : do not hard code embd default value
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

examples/llama-bench/llama-bench.cpp

commit 89fb735fcfd21781a8194b211cf32824beb3f71f
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Thu Mar 7 19:14:49 2024 +0800

    Revert "[SYCL] fix error when set main gpu to non-zero (#5901)" (#5918)
    
    This reverts commit ceca1aef0738b57951cd12c603c3477e75312dec.

ggml-sycl.cpp
ggml-sycl.h
llama.cpp

commit 55a2a900ff4a02fc33708ac7858d595d289a3f2a
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Thu Mar 7 19:42:39 2024 +0900

    server : add `/v1/completions` endpoint (#5914)
    
    * add-`/v1/completions`-endpoint
    
    * add legacy comment to `/completion` endpoint

examples/server/server.cpp

commit 2002bc96bf2cbf5ab981a17d7e994d817c9801f5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 7 11:41:53 2024 +0200

    server : refactor (#5882)
    
    * server : refactoring (wip)
    
    * server : remove llava/clip objects from build
    
    * server : fix empty prompt handling + all slots idle logic
    
    * server : normalize id vars
    
    * server : code style
    
    * server : simplify model chat template validation
    
    * server : code style
    
    * server : minor
    
    * llama : llama_chat_apply_template support null buf
    
    * server : do not process embedding requests when disabled
    
    * server : reorganize structs and enums + naming fixes
    
    * server : merge oai.hpp in utils.hpp
    
    * server : refactor system prompt update at start
    
    * server : disable cached prompts with self-extend
    
    * server : do not process more than n_batch tokens per iter
    
    * server: tests: embeddings use a real embeddings model (#5908)
    
    * server, tests : bump batch to fit 1 embedding prompt
    
    * server: tests: embeddings fix build type Debug is randomly failing (#5911)
    
    * server: tests: embeddings, use different KV Cache size
    
    * server: tests: embeddings, fixed prompt do not exceed n_batch, increase embedding timeout, reduce number of concurrent embeddings
    
    * server: tests: embeddings, no need to wait for server idle as it can timout
    
    * server: refactor: clean up http code (#5912)
    
    * server : avoid n_available var
    
    ggml-ci
    
    * server: refactor: better http codes
    
    * server : simplify json parsing + add comment about t_last
    
    * server : rename server structs
    
    * server : allow to override FQDN in tests
    
    ggml-ci
    
    * server : add comments
    
    ---------
    
    Co-authored-by: Pierrick Hymbert <pierrick.hymbert@gmail.com>

.github/workflows/server.yml
Makefile
examples/server-embd.py
examples/server/CMakeLists.txt
examples/server/README.md
examples/server/oai.hpp
examples/server/server.cpp
examples/server/tests/features/embeddings.feature
examples/server/tests/features/parallel.feature
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py
examples/server/tests/requirements.txt
examples/server/utils.hpp
llama.cpp

commit ceca1aef0738b57951cd12c603c3477e75312dec
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Thu Mar 7 16:34:31 2024 +0800

    [SYCL] fix error when set main gpu to non-zero (#5901)
    
    * fix error when set main gpu to non-zero
    
    * fix delete condition

ggml-sycl.cpp
ggml-sycl.h
llama.cpp

commit e04e04f8fad549bb0b3ec1c91f0413aeb08baf29
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Mar 6 15:42:23 2024 -0500

    ggml : use SYS_get_cpu if SYS_getcpu is not defined (#5906)
    
    Fixes #5694
    Fixes ggerganov/whisper.cpp#1894

ggml.c

commit e25fb4b18fcedb9bed6be4585cf842e9a669b28b
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Wed Mar 6 07:35:07 2024 +0000

    ggml : use `uint8x16_t` return type for `ggml_vqtbl1q_u8` (#5894)
    
    * use uint8x16_t
    
    * Update ggml-quants.c

ggml-quants.c

commit 1e35d619a6fb0b9c5e3dc955345980ff056ddbaf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 6 09:12:25 2024 +0200

    convert : remove AWQ remnants (#5768)

convert.py

commit 8ced9f7e3225adb8501e9821ed1bbd92e3a5c7ae
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Mar 6 12:08:32 2024 +0800

    add wait() to make code stable (#5895)

ci/run.sh
ggml-sycl.cpp

commit 652ca2bded3c818320d92c70d2b67f64bdbff5e5
Author: slaren <slarengh@gmail.com>
Date:   Tue Mar 5 22:27:29 2024 +0100

    compare-llama-bench.py : remove mul_mat_q (#5892)

scripts/compare-llama-bench.py

commit bd836944f826f07e19b7edcf994a78728da49c1c
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Mar 5 11:56:37 2024 -0500

    quants : use MM256_SET_M128I consistently to fix gcc 7 build (#5889)

ggml-quants.c

commit 3de31677d36aa4f82d4d99898902d7bcf398e666
Author: ExtReMLapin <3909752+ExtReMLapin@users.noreply.github.com>
Date:   Tue Mar 5 17:33:08 2024 +0100

    grammars : blacklists character control set (#5888)
    
    * Prevent control characters from being served in json string
    
    * Prevent control characters from being served in json string (array)

grammars/json.gbnf
grammars/json_arr.gbnf

commit 82cb31eb93fd19b74115e0f0133225d1dfdbfdbc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 5 15:56:24 2024 +0200

    Revert "grammars : don't allow to output unescaped new line in string (#5885)"
    
    This reverts commit b1a4e994fde929300d4aeb1deb8320c59cb6edec.

grammars/json.gbnf
grammars/json_arr.gbnf

commit b1a4e994fde929300d4aeb1deb8320c59cb6edec
Author: ExtReMLapin <3909752+ExtReMLapin@users.noreply.github.com>
Date:   Tue Mar 5 14:44:29 2024 +0100

    grammars : don't allow to output unescaped new line in string (#5885)
    
    * Don't allow grammar json array to output unescaped new line in string
    
    * Don't allow new line in json object string

grammars/json.gbnf
grammars/json_arr.gbnf

commit 61d1c88e155515dd03940913a5707ea84a8b119b
Author: 0cc4m <picard12@live.de>
Date:   Tue Mar 5 13:33:42 2024 +0100

    Vulkan Improvements (#5835)
    
    * Improve dequant shaders, add fast q4_0 dequant
    
    * Optimize dmmv non-kquants for GCN
    
    Remove unnecessary SPIR-V shader duplication
    
    * Fix q4_0 dequant dispatch sizes
    
    Fix backend free bug
    
    * Optimize dequant shaders for q4_1, q5_0, q5_1 and q8_0
    
    * Add unary and binary op shader templates
    
    * Fix Vulkan check results
    
    * Enable non-contiguous support for simple ops
    
    * Add argsort
    
    Basic q4_0 mmq shader and unit test
    
    * Speed up q4_0 dequant code, enable mmq for q4_0
    
    * Rework matmul pipeline selection
    
    * Add soft_max alibi support
    
    * Add q4_1, q5_0, q5_1 and q8_0 dequant mat mat mul shaders
    
    * Add environment variable GGML_VK_FORCE_MAX_ALLOCATION_SIZE to limit max buffer size
    
    Rename GGML_VULKAN_DISABLE_F16 to GGML_VK_DISABLE_F16 for consistency

ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml-vulkan.h
ggml_vk_generate_shaders.py
llama.cpp

commit 21b08674331e1ea1b599f17c5ca91f0ed173be31
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Tue Mar 5 16:08:35 2024 +0800

    [SYCL] fix mul_mat fault in CI/unit-test (#5862)
    
    * fix mul_mat fault in cpy_f32_f16
    
    * rm unused function
    
    * add wait() for memcpy
    
    * restore ci/run.sh, rename struct defination, fix bug in ggml_sycl_op_mul_mat_sycl
    
    * fix format issue
    
    * llama : fix segfault from unknown model arch name (#5820)
    
    * llama : fix segfault from unknown model arch name
    
    * llama : make all LLM maps const
    
    This also requires using `std::map::at` instead of its `operator[]`
    which does not exist for const maps.
    
    * llama : name LLM_ARCH_UNKNOWN to "(unknown)"
    
    This avoids errors from `std::map::at` when
    getting the general name of the model architecture.
    Using "(unknown)" instead of an empty string as per suggestion
    https://github.com/ggerganov/llama.cpp/pull/5820#issuecomment-1973735284
    
    * llama : remove redundant inner const for LLM_TENSOR_NAMES
    
    The extra const won't do anything here as const maps
    return const references to values.
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * llama : remove redundant nullptr check in llm_arch_from_string
    
    Since LLM_ARCH_NAMES is a const map, no spurious elements
    with a NULL name are inserted anymore, so this check is dead code.
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * llama : refactor internal quantization functions (#5830)
    
    * scripts : add pod-llama.sh
    
    * ggml : IQ3_S improvements (#5829)
    
    * iq3_s: somewhat faster AVX2 dot product
    
    On Ryzen a 7950X TG-128 increases to 16 t/s from 15.5 t/s using
    16 threads. For 8 threads it is 13.85 t/s vs 11.75 t/s.
    PP-512 increases to 28.5 t/s from 23.8 t/s.
    
    * iq3_s: somewhat faster ARM_NEON dot product
    
    Still dog slow - 10.7 t/s up from 9.9 t/s.
    
    * iq3_s: another small ARM_NEON improvement
    
    10.7 -> 11.0 t/s. Using vmulq_s8 is faster than the xor - sub trick
    that works best on AVX2.
    
    * iq3_s: minor improvement on Metal
    
    49.4 t/s -> 50.3 t/s
    
    * iq3_s: PPL improvement
    
    E.g., for a context of 4096 LLaMA-v2-7B goes to 5.1340 from 5.1653.
    
    * iq3_s: use new grid everywhere
    
    * Fix ARM_NEON
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    
    * convert-hf : make model class definitions self-contained (#5825)
    
    * convert : automatically fall back to HfVocab if tokenizer.model doesn't exist (#5821)
    
    * ggml : fix IQ3_S AVX implementation (#5834)
    
    ggml-ci
    
    * llama : add abort_callback to interrupt computation (#5409)
    
    * using abort_callback from ggml to stop llama computation
    
    * format fix
    
    * a brief explaining comment
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * server: tests: passkey challenge /  self-extend with context shift demo (#5832)
    
    * server: tests: add models endpoint scenario
    
    * server: /v1/models add some metadata
    
    * server: tests: add debug field in context before scenario
    
    * server: tests: download model from HF, add batch size
    
    * server: tests: add passkey test
    
    * server: tests: add group attention params
    
    * server: do not truncate prompt tokens if self-extend through group attention is enabled
    
    * server: logs: do not truncate log values
    
    * server: tests - passkey - first good working value of nga
    
    * server: tests: fix server timeout
    
    * server: tests: fix passkey, add doc, fix regex content matching, fix timeout
    
    * server: tests: fix regex content matching
    
    * server: tests: schedule slow tests on master
    
    * server: metrics: fix when no prompt processed
    
    * server: tests: self-extend add llama-2-7B and Mixtral-8x7B-v0.1
    
    * server: tests: increase timeout for completion
    
    * server: tests: keep only the PHI-2 test
    
    * server: tests: passkey add a negative test
    
    * flake.lock: Update (#5842)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/b253292d9c0a5ead9bc98c4e9a26c6312e27d69f' (2024-02-01)
      → 'github:hercules-ci/flake-parts/f7b3c975cf067e56e7cda6cb098ebe3fb4d74ca2' (2024-03-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/97b17f32362e475016f942bbdfda4a4a72a8a652?dir=lib' (2024-01-29)
      → 'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8?dir=lib' (2024-02-29)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/cbc4211f0afffe6dfd2478a62615dd5175a13f9a' (2024-02-23)
      → 'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8' (2024-02-29)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
    
    * server : init http requests thread pool with --parallel if set (#5836)
    
    * ci : schedule slow server tests only on Release or on demand (#5839)
    
    * llama : fix llama_copy_state_data with fragmented KV cache (#5840)
    
    The row size of the saved states was based on kv_self.head while
    it should be based on llama_kv_cache_cell_max.
    
    Existing session files should still work.
    
    * llama : fix llama_kv_cache_cell_max inability to return 1
    
    I've also changed its return type to uint32_t,
    because this function is always used to set the value of uint32_t variables,
    and because the index already has this type.
    
    * llama : fix state size calculation
    
    Some bytes in the state were unaccounted for in llama_get_state_size.
    Since the logits reserve so much space, it did not cause problems.
    
    * gguf-dump : support i-quants (#5841)
    
    Co-authored-by: Black_Fox <radekliska@gmail.com>
    
    * llama : allow for user specified embedding pooling type (#5849)
    
    * allow for user specified pooling type
    
    * llama : use enum types over int
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * readme : add API changes section
    
    * cuda : fix data race in soft max (#5853)
    
    * main : support special tokens as reverse/anti prompt (#5847)
    
    * Support special tokens as reverse/anti prompt.
    
    * Tokenize antiprompts only once.
    
    * main : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * common : use LLAMA_DEFAULT_SEED (#5855)
    
    * add some new ops, fix some operators and add batch operations to certain operators. (ggml/747)
    
    * cuda: fix group_norm
    
    * cuda: add batch inference support for ggml_pad/ggml_upscale
    
    * add ggml_arrange
    
    * add ggml_timestep_embedding
    
    * update ggml_arange/ggml_timestep_embedding tests
    
    * cuda: fix im2col
    
    * add ggml_arange/ggml_timestep_embbeding support for metal backend
    
    * fix some bugs
    
    * fix some bugs
    
    * Update ggml.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.metal
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * modify according to the review comments
    
    * ggml : fix compile warnings + code style
    
    * ggml : normalize compute_forward calls + fix seg fault in debug
    
    * minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * sync : ggml
    
    * add alias for chat template (#5858)
    
    * speculative : implement stochastic speculative sampling (#5625)
    
    * (WIP) Implement stochastic speculative decoding
    
    * sample from residual distribution on draft accept failure
    
    * fix #5657: force greedy sampling with probs when temp is 0
    
    * remove p_accept parameter
    
    * fix style
    
    * remove unused variables
    
    * add srand() in speculative.cpp
    
    * replace use of rand() with mt19937 sampling
    
    * fixes based on review (@JohannesGaessler)
    
    * fix r random generation
    
    * randomly select next sequence to verify + fix bug in memory freeing
    
    * fix bug in active_seqs sync
    
    * fix uniform int distribution initialization
    
    * remove warnings from comparison between int and size_t
    
    * check grammar in `llama_sample_probability_distribution_impl`
    
    * remove malloc code by utilizing vectors
    
    * add PR link to README
    
    * cmake : handle cases where git index is not found in .git (#5844)
    
    * Update CMakeLists.txt
    
    * Update CMakeLists.txt
    
    * ggml : introduce ggml_status (ggml/750)
    
    * using enum as an exit code instead of macros
    
    * update return type from enum to unsigned int
    
    * indentation fix
    
    * compound update
    ggml_compute_exit_code -> ggml_status
    changed ggml_status from a bit-field type to simple codes
    ggml_status to string cast
    
    * ggml_status to string cast
    
    * GGML_CALL was removed
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * sync : ggml
    
    ggml-ci
    
    * ggml : fix unknown status (#0)
    
    * flake : fix
    
    * llama : fix embeddings (#5796)
    
    * llama : fix embeddings
    
    ggml-ci
    
    * llama : do not use KV cache for non-causal models
    
    ggml-ci
    
    * embeddings : fix llama_batch_init arg
    
    * llama : add pooling switch
    
    * llama : distinguish token vs sequence embeddings
    
    ggml-ci
    
    * llama : assert pooling tensor
    
    * llama : simplify causal mask condition
    
    ggml-ci
    
    * llama : assert input batch with pooling enabled
    
    * readme : update API changes list
    
    * nix: static build (#5814)
    
    * fix speculative decoding build on windows (#5874)
    
    * rebase and rm tailing space
    
    ---------
    
    Co-authored-by: LiangtaoJin <liang-tao.jin@intel.com>
    Co-authored-by: compilade <113953597+compilade@users.noreply.github.com>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Xuan Son Nguyen <thichthat@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: Michael Podvitskiy <podvitskiymichael@gmail.com>
    Co-authored-by: Pierrick Hymbert <pierrick.hymbert@gmail.com>
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Nindaleth <Nindaleth@users.noreply.github.com>
    Co-authored-by: Black_Fox <radekliska@gmail.com>
    Co-authored-by: Douglas Hanley <thesecretaryofwar@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: DAN™ <dranger003@gmail.com>
    Co-authored-by: leejet <leejet714@gmail.com>
    Co-authored-by: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
    Co-authored-by: Dane Madsen <dane_madsen@hotmail.com>
    Co-authored-by: hutli <6594598+hutli@users.noreply.github.com>
    Co-authored-by: Jeffrey Quesnelle <emozilla@nousresearch.com>

ggml-sycl.cpp

commit 6a87ac3a52668e117d97bcea07b529c93188b303
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Tue Mar 5 15:12:23 2024 +0900

    fix editorconfig check break (#5879)

.devops/nix/package.nix

commit 29eee404746e4696143a4f3a642660a4793a15d8
Author: Jeffrey Quesnelle <emozilla@nousresearch.com>
Date:   Mon Mar 4 19:23:06 2024 -0800

    fix speculative decoding build on windows (#5874)

examples/speculative/speculative.cpp

commit 1d41d6f7c2a666eb9c18a686a4684c4b03289bf3
Author: hutli <6594598+hutli@users.noreply.github.com>
Date:   Tue Mar 5 02:33:08 2024 +0100

    nix: static build (#5814)

.devops/nix/package.nix

commit 29ae62d2ae163e2b68aa0ad3bf2ab4636de0c957
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 22:31:20 2024 +0200

    llama : fix embeddings (#5796)
    
    * llama : fix embeddings
    
    ggml-ci
    
    * llama : do not use KV cache for non-causal models
    
    ggml-ci
    
    * embeddings : fix llama_batch_init arg
    
    * llama : add pooling switch
    
    * llama : distinguish token vs sequence embeddings
    
    ggml-ci
    
    * llama : assert pooling tensor
    
    * llama : simplify causal mask condition
    
    ggml-ci
    
    * llama : assert input batch with pooling enabled
    
    * readme : update API changes list

README.md
common/common.cpp
examples/embedding/embedding.cpp
examples/server-embd.py
examples/server/server.cpp
llama.cpp
llama.h

commit e0843afe1b37890b631bc7d3d2da2ed36c862b91
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 21:50:50 2024 +0200

    flake : fix

convert-hf-to-gguf.py

commit a1c6d96ed8f906aa1cda439f7386b1171a22bf9f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 20:53:27 2024 +0200

    ggml : fix unknown status (#0)

ggml.c

commit efd8533ef8d0752cef7119eb5dbee412c4dba270
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 11:06:39 2024 +0200

    sync : ggml
    
    ggml-ci

scripts/sync-ggml.last

commit 9fa262734733573fa629ffc97dfcb971fe3f4832
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Mon Mar 4 10:05:42 2024 +0100

    ggml : introduce ggml_status (ggml/750)
    
    * using enum as an exit code instead of macros
    
    * update return type from enum to unsigned int
    
    * indentation fix
    
    * compound update
    ggml_compute_exit_code -> ggml_status
    changed ggml_status from a bit-field type to simple codes
    ggml_status to string cast
    
    * ggml_status to string cast
    
    * GGML_CALL was removed
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-kompute.cpp
ggml-metal.m
ggml-opencl.cpp
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
ggml.h

commit fe52be11e35358d2fd249f19d7ef5b6f9c08b16b
Author: Dane Madsen <dane_madsen@hotmail.com>
Date:   Tue Mar 5 05:26:55 2024 +1100

    cmake : handle cases where git index is not found in .git (#5844)
    
    * Update CMakeLists.txt
    
    * Update CMakeLists.txt

common/CMakeLists.txt

commit 6d341ab6c53cd51f2921d986d0090cc8b049b39a
Author: Minsoo Cheong <54794500+mscheong01@users.noreply.github.com>
Date:   Tue Mar 5 03:24:00 2024 +0900

    speculative : implement stochastic speculative sampling (#5625)
    
    * (WIP) Implement stochastic speculative decoding
    
    * sample from residual distribution on draft accept failure
    
    * fix #5657: force greedy sampling with probs when temp is 0
    
    * remove p_accept parameter
    
    * fix style
    
    * remove unused variables
    
    * add srand() in speculative.cpp
    
    * replace use of rand() with mt19937 sampling
    
    * fixes based on review (@JohannesGaessler)
    
    * fix r random generation
    
    * randomly select next sequence to verify + fix bug in memory freeing
    
    * fix bug in active_seqs sync
    
    * fix uniform int distribution initialization
    
    * remove warnings from comparison between int and size_t
    
    * check grammar in `llama_sample_probability_distribution_impl`
    
    * remove malloc code by utilizing vectors
    
    * add PR link to README

common/common.cpp
common/common.h
common/sampling.cpp
common/sampling.h
examples/speculative/README.md
examples/speculative/speculative.cpp

commit 4ffcdce2ff877ebb683cd217ea38faf20faa5ffe
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Mar 4 12:22:08 2024 +0100

    add alias for chat template (#5858)

examples/server/server.cpp
llama.cpp

commit a0fc62661f0fd2a9edd10ae5617345bbbf972f42
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 4 10:40:04 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit 7d43c585dc174bb586775c22c15e5db9242b5b4b
Author: leejet <leejet714@gmail.com>
Date:   Sun Mar 3 20:23:52 2024 +0800

    add some new ops, fix some operators and add batch operations to certain operators. (ggml/747)
    
    * cuda: fix group_norm
    
    * cuda: add batch inference support for ggml_pad/ggml_upscale
    
    * add ggml_arrange
    
    * add ggml_timestep_embedding
    
    * update ggml_arange/ggml_timestep_embedding tests
    
    * cuda: fix im2col
    
    * add ggml_arange/ggml_timestep_embbeding support for metal backend
    
    * fix some bugs
    
    * fix some bugs
    
    * Update ggml.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.m
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-metal.metal
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * modify according to the review comments
    
    * ggml : fix compile warnings + code style
    
    * ggml : normalize compute_forward calls + fix seg fault in debug
    
    * minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
tests/test-backend-ops.cpp

commit 82f3e668adafba647de703f835991e91a96b5ac4
Author: DAN™ <dranger003@gmail.com>
Date:   Mon Mar 4 03:08:19 2024 -0500

    common : use LLAMA_DEFAULT_SEED (#5855)

common/common.h

commit 5a51cc1bb4592f0d71f9af89cd08b11a066ba447
Author: DAN™ <dranger003@gmail.com>
Date:   Mon Mar 4 02:57:20 2024 -0500

    main : support special tokens as reverse/anti prompt (#5847)
    
    * Support special tokens as reverse/anti prompt.
    
    * Tokenize antiprompts only once.
    
    * main : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/main/main.cpp

commit 67be2ce1015d070b3b2cd488bcb041eefb61de72
Author: slaren <slarengh@gmail.com>
Date:   Sun Mar 3 14:26:18 2024 +0100

    cuda : fix data race in soft max (#5853)

ggml-cuda.cu

commit 231ae28f078c3148d097b301f2145f1e3e816cc1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 3 12:44:03 2024 +0200

    readme : add API changes section

README.md

commit 475df1d6cf817060028d3ff763cb8097d4ec40d6
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Sun Mar 3 04:40:27 2024 -0600

    llama : allow for user specified embedding pooling type (#5849)
    
    * allow for user specified pooling type
    
    * llama : use enum types over int
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
convert-hf-to-gguf.py
llama.cpp
llama.h

commit 87c2e8b2797860a06af3d6c06b8488a8ff1a09ab
Author: Nindaleth <Nindaleth@users.noreply.github.com>
Date:   Sun Mar 3 09:43:42 2024 +0100

    gguf-dump : support i-quants (#5841)
    
    Co-authored-by: Black_Fox <radekliska@gmail.com>

gguf-py/gguf/constants.py

commit de9692a7d2db66e29e5cb373c6551acc49145ccd
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Sun Mar 3 03:41:55 2024 -0500

    llama : fix llama_copy_state_data with fragmented KV cache (#5840)
    
    The row size of the saved states was based on kv_self.head while
    it should be based on llama_kv_cache_cell_max.
    
    Existing session files should still work.
    
    * llama : fix llama_kv_cache_cell_max inability to return 1
    
    I've also changed its return type to uint32_t,
    because this function is always used to set the value of uint32_t variables,
    and because the index already has this type.
    
    * llama : fix state size calculation
    
    Some bytes in the state were unaccounted for in llama_get_state_size.
    Since the logits reserve so much space, it did not cause problems.

llama.cpp

commit e6029348e86c3810d4435faee54ba822cb43e2ef
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 3 09:35:23 2024 +0100

    ci : schedule slow server tests only on Release or on demand (#5839)

.github/workflows/server.yml

commit 8ef969afcec1645d2d9c3ab1fc82263bba968989
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Mar 3 08:48:36 2024 +0100

    server : init http requests thread pool with --parallel if set (#5836)

examples/server/README.md
examples/server/server.cpp

commit fa974646e1a2024fc7dc9e6f27cf1f2f5d4a3763
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 3 06:11:31 2024 +0200

    flake.lock: Update (#5842)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/b253292d9c0a5ead9bc98c4e9a26c6312e27d69f' (2024-02-01)
      → 'github:hercules-ci/flake-parts/f7b3c975cf067e56e7cda6cb098ebe3fb4d74ca2' (2024-03-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/97b17f32362e475016f942bbdfda4a4a72a8a652?dir=lib' (2024-01-29)
      → 'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8?dir=lib' (2024-02-29)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/cbc4211f0afffe6dfd2478a62615dd5175a13f9a' (2024-02-23)
      → 'github:NixOS/nixpkgs/1536926ef5621b09bba54035ae2bb6d806d72ac8' (2024-02-29)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit 9731134296af3a6839cd682e51d9c2109a871de5
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Mar 2 22:00:14 2024 +0100

    server: tests: passkey challenge /  self-extend with context shift demo (#5832)
    
    * server: tests: add models endpoint scenario
    
    * server: /v1/models add some metadata
    
    * server: tests: add debug field in context before scenario
    
    * server: tests: download model from HF, add batch size
    
    * server: tests: add passkey test
    
    * server: tests: add group attention params
    
    * server: do not truncate prompt tokens if self-extend through group attention is enabled
    
    * server: logs: do not truncate log values
    
    * server: tests - passkey - first good working value of nga
    
    * server: tests: fix server timeout
    
    * server: tests: fix passkey, add doc, fix regex content matching, fix timeout
    
    * server: tests: fix regex content matching
    
    * server: tests: schedule slow tests on master
    
    * server: metrics: fix when no prompt processed
    
    * server: tests: self-extend add llama-2-7B and Mixtral-8x7B-v0.1
    
    * server: tests: increase timeout for completion
    
    * server: tests: keep only the PHI-2 test
    
    * server: tests: passkey add a negative test

.github/workflows/server.yml
examples/server/server.cpp
examples/server/tests/README.md
examples/server/tests/features/environment.py
examples/server/tests/features/issues.feature
examples/server/tests/features/parallel.feature
examples/server/tests/features/passkey.feature
examples/server/tests/features/security.feature
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py
examples/server/tests/features/wrong_usages.feature
examples/server/tests/requirements.txt
examples/server/tests/tests.sh
examples/server/utils.hpp

commit 4a6e2d6142ab815c964924896891e9ab3e050632
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Sat Mar 2 20:52:25 2024 +0100

    llama : add abort_callback to interrupt computation (#5409)
    
    * using abort_callback from ggml to stop llama computation
    
    * format fix
    
    * a brief explaining comment
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp
llama.h

commit 494c87032613e31c0be99b2735e732871f2c4e4d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 2 20:00:49 2024 +0200

    ggml : fix IQ3_S AVX implementation (#5834)
    
    ggml-ci

ggml-quants.c

commit 4d4d2366fc9c54d4a275065cfe9299c6cf7c5b78
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Mar 2 12:27:26 2024 -0500

    convert : automatically fall back to HfVocab if tokenizer.model doesn't exist (#5821)

README.md
convert-llama-ggml-to-gguf.py
convert.py
examples/infill/infill.cpp

commit c7a0ad8ec9ebb5ddb1c1c80c82f2ee041c525d47
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Mar 2 12:21:47 2024 -0500

    convert-hf : make model class definitions self-contained (#5825)

convert-hf-to-gguf.py
gguf-py/gguf/gguf_writer.py

commit bbde6eb2561153aabbdfac5001c690fe00cad639
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Mar 2 17:00:51 2024 +0200

    ggml : IQ3_S improvements (#5829)
    
    * iq3_s: somewhat faster AVX2 dot product
    
    On Ryzen a 7950X TG-128 increases to 16 t/s from 15.5 t/s using
    16 threads. For 8 threads it is 13.85 t/s vs 11.75 t/s.
    PP-512 increases to 28.5 t/s from 23.8 t/s.
    
    * iq3_s: somewhat faster ARM_NEON dot product
    
    Still dog slow - 10.7 t/s up from 9.9 t/s.
    
    * iq3_s: another small ARM_NEON improvement
    
    10.7 -> 11.0 t/s. Using vmulq_s8 is faster than the xor - sub trick
    that works best on AVX2.
    
    * iq3_s: minor improvement on Metal
    
    49.4 t/s -> 50.3 t/s
    
    * iq3_s: PPL improvement
    
    E.g., for a context of 4096 LLaMA-v2-7B goes to 5.1340 from 5.1653.
    
    * iq3_s: use new grid everywhere
    
    * Fix ARM_NEON
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu
ggml-metal.metal
ggml-quants.c

commit ef2cd694c4155fbf25bae61c5178c47eb3676dba
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 2 16:54:08 2024 +0200

    scripts : add pod-llama.sh

scripts/pod-llama.sh

commit 6c32d8c7ad8ba7b6ad2a162e929a21dd04fcdca0
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sat Mar 2 15:19:09 2024 +0100

    llama : refactor internal quantization functions (#5830)

llama.cpp

commit 802da0091ba646ecf02e1a8fae2da0b8e76409bd
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Sat Mar 2 08:42:56 2024 -0500

    llama : fix segfault from unknown model arch name (#5820)
    
    * llama : fix segfault from unknown model arch name
    
    * llama : make all LLM maps const
    
    This also requires using `std::map::at` instead of its `operator[]`
    which does not exist for const maps.
    
    * llama : name LLM_ARCH_UNKNOWN to "(unknown)"
    
    This avoids errors from `std::map::at` when
    getting the general name of the model architecture.
    Using "(unknown)" instead of an empty string as per suggestion
    https://github.com/ggerganov/llama.cpp/pull/5820#issuecomment-1973735284
    
    * llama : remove redundant inner const for LLM_TENSOR_NAMES
    
    The extra const won't do anything here as const maps
    return const references to values.
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * llama : remove redundant nullptr check in llm_arch_from_string
    
    Since LLM_ARCH_NAMES is a const map, no spurious elements
    with a NULL name are inserted anymore, so this check is dead code.
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

llama.cpp

commit 715641391dda1ff9762dc5d99d9a30acce99f2c6
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Sat Mar 2 19:49:30 2024 +0800

    Support multiple GPUs (split mode) on SYCL backend (#5806)
    
    * suport multiple cards: split-mode - layer|row
    
    * rm warning
    
    * rebase with master, support tow new OPs, close feature for -sm=row, fix for unit test
    
    * update news
    
    * fix merge error
    
    * update according to review comments

README-sycl.md
common/common.cpp
examples/llama-bench/llama-bench.cpp
examples/sycl/ls-sycl-device.cpp
examples/sycl/run-llama2.sh
ggml-sycl.cpp
ggml-sycl.h
llama.cpp

commit 9bf297a02bfbd474e51912409a470dd797e2fe13
Author: crasm <crasm@git.vczf.net>
Date:   Sat Mar 2 00:11:06 2024 -0500

    workflows : remove nocleanup arg for check-requirements.sh (#5826)
    
    Reduces peak tmpfs usage and should prevent the check from failing from
    running out of space.
    
    Fixes the 'No space left on device' issue mentioned in #5703.

.github/workflows/python-check-requirements.yml

commit cb5e8f7fc4ee57d4bcccafbe04a82cededd35486
Author: Tushar <ditsuke@protonmail.com>
Date:   Sat Mar 2 04:48:26 2024 +0530

    build(nix): Introduce flake.formatter for `nix fmt` (#5687)
    
    * build(nix): Introduce flake.formatter for `nix fmt`
    * chore: Switch to pkgs.nixfmt-rfc-style

.devops/nix/sif.nix
flake.nix

commit da3b9ba2b710c0f8b44398a0eb9e5a7ae2ad967a
Author: nold <Nold360@users.noreply.github.com>
Date:   Fri Mar 1 22:51:12 2024 +0100

    convert-hf-to-gguf : require einops for InternLM2ForCausalLM (#5792)

requirements/requirements-convert-hf-to-gguf.txt

commit c29af7e2252d288f2ea58a7d437c1cb7c0abf160
Author: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
Date:   Sat Mar 2 01:00:46 2024 +0530

    llama : add StarCoder2 support (#5795)
    
    * Add support for starcoder2
    
    * handle rope type
    
    * skip rope freq and rotary embeddings from being serialized
    
    * resolve comments
    
    * Update llama.cpp
    
    * remove redundant changes
    
    * handle `rope-theta`
    
    * llama : change starcoder2 rope type
    
    * address comment
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 38d16b142624bdd7c41d9955752b7f7b59c5e048
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 1 20:00:58 2024 +0200

    server : remove api_like_OAI.py proxy script (#5808)

README.md
examples/server/README.md
examples/server/api_like_OAI.py

commit c2224f003bf9cf558b1a3c57033563e11a4de9a5
Author: ddpasa <112642920+ddpasa@users.noreply.github.com>
Date:   Fri Mar 1 18:00:00 2024 +0100

    ggml-vulkan: fix VULKAN_CHECK_RESULTS flag, which was previously broken (#5813)

ggml-vulkan.cpp

commit e7433867288d2f142cffe596f3751bda5d7ee2c7
Author: kunal-vaishnavi <115581922+kunal-vaishnavi@users.noreply.github.com>
Date:   Fri Mar 1 06:08:08 2024 -0800

    gemma : fix bfloat16 -> float16 conversion issue (#5810)

convert-hf-to-gguf.py

commit f49a5356865ced0eca1df9f9d84631dfef71b9dc
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Fri Mar 1 22:48:56 2024 +0900

    common : fix flag `--logits-all` to `--all-logits` (#5805)

common/common.cpp

commit 3ab8b3a92ede46df88bc5a2dfca3777de4a2b2b6
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Mar 1 12:39:06 2024 +0100

    llama : cleanup unused mmq flags (#5772)
    
    * cleanup unused --no-mul-mat-q,-nommq, -mmq, --mul-mat-q, mul_mat_q
    
    * remove: mul_mat_q in compare llama bench and usage
    
    * update llama-bench
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

common/common.cpp
common/common.h
examples/batched-bench/batched-bench.cpp
examples/llama-bench/README.md
examples/llama-bench/llama-bench.cpp
examples/server/server.cpp
llama.cpp
llama.h
scripts/compare-llama-bench.py

commit 9600d59e010c18f5872580a21734ea1bf1968d04
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Fri Mar 1 03:15:36 2024 -0600

    unicode : switch to multimap based nfd_map (#5799)
    
    * switch to multimap based nfd_map due to compile time issues
    
    * simplify multimap keys
    
    * dont construct new locale every time

llama.cpp
unicode.h

commit 5cb02b4a012bb16c6c699c0c62c05ffa653eee0f
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Fri Mar 1 10:08:08 2024 +0100

    server: allow to override threads server pool with --threads-http (#5794)

examples/server/README.md
examples/server/server.cpp

commit 6ea0f010ff6967034528d9e0b8330b9b0f0b7c13
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Fri Mar 1 08:54:53 2024 +0000

    ci : add Ubuntu 22 Vulkan CI run (#5789)

.github/workflows/build.yml

commit f105471ef6aa4727afac8240da398590d7277f45
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 1 09:59:43 2024 +0200

    server : fix newlines in help (#5785)

examples/server/server.cpp

commit 38d152160898b0173ffe4dc7df5daadcbd2eceb0
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Fri Mar 1 07:36:47 2024 +0000

    [SYCL] Use batched mul_mat pathway (#5591)
    
    * Use batched mul_mat pathway
    
    * rm extra line
    
    * Explicitly state scaled data type
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

ggml-sycl.cpp

commit 052051d8ae4639a1c3c61e7da3237bcc572469d4
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Feb 29 21:42:11 2024 +0100

    Server: normalize naming (#5779)
    
    * server: normalize naming
    
    * fix spacing

examples/server/server.cpp
examples/server/utils.hpp

commit d5ab29757ebc59a30f03e408294ec20628a6374e
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Thu Feb 29 00:17:23 2024 -0800

    llama : constified `llama_set_state_data`'s `src` (#5774)

llama.cpp
llama.h

commit 87c91c07663b707e831c59ec373b5e665ff9d64a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 28 21:44:21 2024 +0200

    ci : reduce 3b ppl chunks to 1 to avoid timeout (#5771)
    
    ggml-ci

ci/run.sh

commit 317709b2a81dbaf87850202686ec5bb2602a504e
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed Feb 28 19:33:37 2024 +0000

    make portability_enumeration_ext apple only (#5757)

ggml-vulkan.cpp

commit 08c5ee87e4cceb603ecceac90734fcdade57311b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 28 18:43:38 2024 +0200

    llama : remove deprecated API (#5770)
    
    ggml-ci

llama.cpp
llama.h

commit 78aacf36344df724cdca9f1e1af849b2d2519cb8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 28 17:36:53 2024 +0200

    awq-py : remove (#5768)

awq-py/README.md
awq-py/awq/apply_awq.py
awq-py/requirements.txt

commit 8c0e8f4e73e275756ad69f9c99b26ead085ca9f0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 28 11:17:32 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit 2774b0c97427ee3ad3e2ee121354d078794e89d9
Author: slaren <slarengh@gmail.com>
Date:   Sun Feb 25 20:41:35 2024 +0100

    add google magika inference example (ggml/748)
    
    * add magika inference example
    
    * ggml : fix unaligned accesses in custom ops
    
    * ggml : fix FP32 GELU for values that exceed the FP16 range
    
    * use ggml_pool_1d
    
    * add README
    
    * Update README.md
    
    * pad inputs if the files are too small
    
    * cleanup
    
    ggml-ci

ggml.c

commit 5f706718566e3a5147916dc381f3b99de0ffad47
Author: UEXTM.com <84163508+uextm@users.noreply.github.com>
Date:   Sat Feb 24 11:27:36 2024 -0500

    Introduce backend GUIDs (ggml/743)
    
    * Introduce backend GUIDs
    
    Initial proposed implementation of backend GUIDs
    (Discussed in https://github.com/ggerganov/ggml/pull/741)
    
    Hardcoded CPU backend GUID (for now)
    Change ggml_backend_is_cpu logic to use GUID
    
    * Remove redundant functions
    
    Remove redundant functions `ggml_backend_i::get_name` and `ggml_backend_guid` which are not desired for future expansion
    
    * Add spaces to match style
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Fix brace style to match
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Add void to () in function signature
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Add back ggml_backend_guid and make CPU_GUID a local static in ggml_backend_cpu_guid
    
    * add guids to all backends
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-kompute.cpp
ggml-metal.m
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
ggml.h

commit a693bea1e6762a17b78b6ddf4611e54136941ea2
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Wed Feb 28 09:55:37 2024 +0100

    server : hit Ctrl+C twice to exit (#5734)
    
    * server: twice ctrl+C to exit
    
    * std::atomic_flag
    
    * sigint: message
    
    * sigint: stderr
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

examples/server/server.cpp

commit adcb12a9bad87bc96f2f158c95892b3d04aa7ffb
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Wed Feb 28 03:52:56 2024 -0500

    llama : fix non-quantization of expert gating tensors (#5754)
    
    This reverts a single line from #5475

llama.cpp

commit 177628bfd85565070916ad66a5ac4071ee0527d8
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Wed Feb 28 02:51:11 2024 -0600

    llama : improve BERT tokenization (#5740)
    
    * implement nfd for stripping accents in wpm tokenizer
    
    * sort nfd map; reuse iterator
    
    * use builtin tolower
    
    * add locale include
    
    * Simplify to_lower cases
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

llama.cpp
unicode.h

commit 6c4416868df2e5455da7d20547f62bcf9735ba8e
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Feb 28 09:39:39 2024 +0100

    readme : add link to LLaVA 1.6 models (#5758)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

README.md

commit efc72253f7987ed7bdc8bde9d9fa5c7cac2f6292
Author: Jorge A <161275481+jorgealias@users.noreply.github.com>
Date:   Wed Feb 28 01:39:15 2024 -0700

    server : add "/chat/completions" alias for "/v1/...` (#5722)
    
    * Add "/chat/completions" as alias for "/v1/chat/completions"
    
    * merge to upstream master
    
    * minor : fix trailing whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/server.cpp
examples/server/tests/features/parallel.feature
examples/server/tests/features/steps/steps.py

commit 7c4263d4261d6ee6f0539d53eb9e1b4d120ba8af
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Feb 28 10:37:02 2024 +0200

    ggml : make i-quants work with super-blocks of 64 (CPU,Metal) (#5760)
    
    * WIP: make i-quants work for QK_K = 64
    
    * iq2_xs: attempt to fix AVX dot product for QK_K = 64
    
    Tests pass, but I get gibberish.
    
    * QK_K = 64 tests pass on ARM_NEON and Metal
    
    Sadly, that does not mean it actually works.
    
    * Make CUDA compile with QK_K = 64
    
    Tests don't pass, plus we get misaligned access
    
    * Q2_K: fixed bug in imatrix quantization for QK_K = 64
    
    * iq1_s: turn off SIMD implementation for QK_K = 64 (it does not work)
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c

commit cb49e0f8c906e5da49e9f6d64a57742a9a241c6a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 27 19:16:49 2024 +0200

    Attempt to fix android build (#5752)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-quants.c

commit 0becb22ac05b6542bd9d5f2235691aa1d3d4d307
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 27 16:34:24 2024 +0200

    IQ4_XS: a 4.25 bpw quantization (#5747)
    
    * Try IQ4_NL with blocks of 64 - does not look good
    
    * iq4_xs: go to super-blocks of 256 and 6-bit scales for blocks of 32
    
    * iq4_xs: CUDA works - 133.2 t/s
    
    * iq4_xs: AVX2 dot product
    
    * iq4_xs: ARM_NEON dot product
    
    * iq4_nl: Metal implementation
    
    As usual, Metal / Apple Silicon don't like my quants.
    
    * iq3_xs: minor fix
    
    * iq4_xs: shrink by using IQ3_S for attn_k and attn_q
    
    * iq4_xs: revert using IQ3_S for attn_k and attn_v
    
    PPL vs size is good, but CPU performance suffers: on M2 Max
    TG-128 drops to 21.7 t/s from 28.8, and on a Ryzen-7950X
    to 14.5 t/s from 15.8 t/s. On CUDA we have 135 t/s when
    using IQ3_S vs 133 t/s with pure IQ4_XS.
    
    * Fix CI
    
    * iq4_xs: Added forgotten check for 256 divisibility
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp

commit c24a2a6e6005e5d424301525a42ba45a4a362d30
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Tue Feb 27 07:22:45 2024 -0600

    cuda : replace remaining shfl_xor with calls to warp_reduce functions (#5744)

ggml-cuda.cu

commit 1f30b7a9f1b86baa455072d3182b9ebeee0cd845
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Tue Feb 27 06:50:18 2024 -0600

    ggml-quants : fix avx2 iq1_s vec_dot when compiled with gcc (#5742)

ggml-quants.c

commit 9d533a77d0c3850ce09d736bc1baa67fd6ad27b3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 27 14:35:51 2024 +0200

    llama : fix defrag bugs + add parameter (#5735)
    
    * llama : fix defrag bugs + enable by default
    
    ggml-ci
    
    * llama : add defrag_thold parameter
    
    ggml-ci
    
    * llama : cont
    
    * llama : disable log message
    
    ggml-ci
    
    * llama : fix graph size check during defrag

common/common.cpp
common/common.h
examples/passkey/passkey.cpp
llama.cpp
llama.h

commit cbbd1efa06f8c09f9dff58ff9d9af509cc4c152b
Author: le.chang <cljs118@126.com>
Date:   Tue Feb 27 10:03:06 2024 +0800

    Makefile: use variables for cublas (#5689)
    
    * make: use arch variable for cublas
    
    * fix UNAME_M
    
    * check opt first
    
    ---------
    
    Co-authored-by: lindeer <le.chang118@gmail.com>

Makefile

commit b11a93df41921846a10628a7c306d5c82a549939
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Feb 26 23:15:48 2024 +0100

    fix server hangs on empty prompt (#5733)

examples/server/server.cpp

commit a33e6a0d2a66104ea9a906bdbf8a94d050189d91
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Feb 26 18:28:38 2024 +0200

    Adding IQ2_S and IQ2_M to complete coverage of the 2-3 bit quantization range (#5721)
    
    * Adding IQ2_S and IQ2_M as a single cumulative commit
    
    * Update examples/quantize/quantize.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp
tests/test-quantize-fns.cpp

commit 47bb7b48c7cec9d8f57d56812ce811ec130b89a3
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Feb 26 15:36:38 2024 +0100

    CUDA: fix DEBUG_CUDA_MALLOC (#5729)

ggml-cuda.cu

commit c4d7f8178608440506e5489bae0109e4ca12e44a
Author: Artem <guinmoon@gmail.com>
Date:   Mon Feb 26 17:15:28 2024 +0300

    readme : update ui list (#5731)
    
    * Add LLMFarm (ui for iOS) to list

README.md

commit e849078c6e09e72fdd2c95ba61f5fba9a7b2d9ef
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon Feb 26 14:02:11 2024 +0000

    [SYCL] Add support for soft_max ALiBi (#5639)
    
    * Add support for bias
    
    * Update pre-processor
    
    * rm commented code
    
    * fix format
    
    * fix CI
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

ggml-sycl.cpp
llama.cpp

commit 67fd33132fab93e6c2087bd6fa656a8a57419efa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 26 14:02:12 2024 +0200

    unicode : reuse iterator (#5726)

unicode.h

commit 4804215cb833841ffb15a710a16b77ca0a29eb4b
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Feb 26 11:41:34 2024 +0100

    server: CI fix trailing space (#5728)

.github/workflows/server.yml

commit 8a533f0d9078396ebaee9ba213038a1322976dee
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Mon Feb 26 09:56:10 2024 +0100

    server: CI tests reduce build matrix (#5725)

.github/workflows/server.yml

commit 269de86ba073b5dc9ce687c11a3bc4d7d873b962
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 26 08:30:17 2024 +0200

    llama : fix Gemma rope type (#5691)

llama.cpp

commit c39373398803c669056304090050fe3f44b41bf9
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Feb 25 00:17:11 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/5863c27340ba4de8f83e7e3c023b9599c3cb3c80' (2024-02-16)
      → 'github:NixOS/nixpkgs/cbc4211f0afffe6dfd2478a62615dd5175a13f9a' (2024-02-23)

flake.lock

commit e3965cf35aac00d4e24998c8a3d0093ae1d98bd3
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 25 22:48:33 2024 +0100

    server: tests - slow inference causes timeout on the CI (#5715)
    
    * server: tests - longer inference timeout for CI

common/sampling.cpp
examples/server/tests/features/steps/steps.py

commit 8b350356b28f782deab63d8b0e9ae103ceb25fcd
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 25 21:46:29 2024 +0100

    server: docs - refresh and tease a little bit more the http server (#5718)
    
    * server: docs - refresh and tease a little bit more the http server
    
    * Rephrase README.md server doc
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/server/README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/server/README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
examples/server/README.md

commit bf08e00643fd529f748f0a858fd79f3061e3fa18
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 25 22:12:24 2024 +0200

    llama : refactor k-shift implementation + KV defragmentation (#5691)
    
    * llama : refactor k-shift implementation
    
    ggml-ci
    
    * llama : rename llama_kv_cache_seq_shift to llama_kv_cache_seq_add
    
    * llama : cont k-shift refactoring + normalize type names
    
    ggml-ci
    
    * minor : fix MPI builds
    
    * llama : reuse n_rot from the build context
    
    ggml-ci
    
    * llama : revert enum name changes from this PR
    
    ggml-ci
    
    * llama : update llama_rope_type
    
    * llama : add comment about rope values
    
    * llama : fix build
    
    * passkey : apply kv cache updates explicitly
    
    ggml-ci
    
    * llama : change name to llama_kv_cache_update()
    
    * llama : add llama_kv_cache_seq_pos_max()
    
    * passkey : fix llama_kv_cache_seq_pos_max() usage
    
    * llama : some llama_kv_cell simplifications
    
    * llama : add llama_kv_cache_compress (EXPERIMENTAL)
    
    * llama : add alternative KV cache merging (EXPERIMENTAL)
    
    * llama : add llama_kv_cache_defrag
    
    * llama : comments
    
    * llama : remove llama_kv_cache_compress
    
    will add in a separate PR
    
    ggml-ci
    
    * llama : defragment via non-overlapping moves
    
    * llama : ggml_graph based defrag implementation
    
    ggml-ci
    
    * llama : switch the loop order in build_defrag
    
    * llama : add comments

examples/infill/infill.cpp
examples/main/main.cpp
examples/passkey/passkey.cpp
examples/server/server.cpp
llama.cpp
llama.h

commit f7625019c51ca437a5840576d92362cfa710e4a2
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Sun Feb 25 13:43:50 2024 -0500

    server : fix crash when system prompt is bigger than batch size (#5714)
    
    The system prompt is now decoded in batches.
    
    * server : fix off-by-one n_past when start of prompt matches whole cache
    
    The tokens right after the matching part would otherwise skip a pos value.

examples/server/server.cpp

commit abbabc5e51d0d4656b438aec10b7fae9479ef37d
Author: Radosław Gryta <radek.gryta@gmail.com>
Date:   Sun Feb 25 19:43:00 2024 +0100

    ggml-quants : provide ggml_vqtbl1q_u8 for 64bit compatibility (#5711)
    
    * [ggml-quants] Provide ggml_vqtbl1q_u8 for 64bit compatibility
    
    vqtbl1q_u8 is not part of arm v7 neon library
    
    * [android-example] Remove abi filter after arm v7a fix
    
    * [github-workflows] Do not skip Android armeabi-v7a build

.github/workflows/build.yml
examples/llama.android/app/build.gradle.kts
ggml-quants.c

commit f1a98c52546d009f742bdec2154c2a314ea950a6
Author: kwin1412 <42286931+kwin1412@users.noreply.github.com>
Date:   Mon Feb 26 00:46:49 2024 +0800

    make : fix nvcc version is empty (#5713)
    
    fix nvcc version is empty

Makefile

commit 7d548a1827f6fc6aece6db74c9d112da42c40d68
Author: Ashok Gelal <401055+ashokgelal@users.noreply.github.com>
Date:   Sun Feb 25 10:57:34 2024 -0500

    readme : add Msty to UI list (#5618)

README.md

commit 930b1780269a69948d106e2d1b838ab7661f679a
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 25 13:50:32 2024 +0100

    server: logs - unified format and --log-format option (#5700)
    
    * server: logs - always use JSON logger, add add thread_id in message, log task_id and slot_id
    
    * server : skip GH copilot requests from logging
    
    * server : change message format of server_log()
    
    * server : no need to repeat log in comment
    
    * server : log style consistency
    
    * server : fix compile warning
    
    * server : fix tests regex patterns on M2 Ultra
    
    * server: logs: PR feedback on log level
    
    * server: logs: allow to choose log format in json or plain text
    
    * server: tests: output server logs in text
    
    * server: logs switch init logs to server logs macro
    
    * server: logs ensure value json value does not raised error
    
    * server: logs reduce level VERBOSE to VERB to max 4 chars
    
    * server: logs lower case as other log messages
    
    * server: logs avoid static in general
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * server: logs PR feedback: change text log format to: LEVEL [function_name] message | additional=data
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/README.md
examples/server/server.cpp
examples/server/tests/README.md
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py
examples/server/utils.hpp

commit d52d7819b8ced70c642a88a59da8c78208dc58ec
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 25 13:49:43 2024 +0100

    server: concurrency fix + monitoring - add /metrics prometheus compatible endpoint (#5708)
    
    * server: monitoring - add /metrics prometheus compatible endpoint
    
    * server: concurrency issue, when 2 task are waiting for results, only one call thread is notified
    
    * server: metrics - move to a dedicated struct

examples/server/README.md
examples/server/server.cpp
examples/server/tests/features/environment.py
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py
examples/server/tests/requirements.txt
examples/server/utils.hpp

commit 12894088170f62e4cad4f8d6a3043c185b414bab
Author: Radosław Gryta <radek.gryta@gmail.com>
Date:   Sun Feb 25 11:53:11 2024 +0100

    cmake : fix compilation for Android armeabi-v7a (#5702)

CMakeLists.txt

commit ab336a9d5e5352ecdcdf4c12d2d54cf4ef82ce31
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 25 12:09:09 2024 +0200

    code : normalize enum names (#5697)
    
    * coda : normalize enum names
    
    ggml-ci
    
    * code : cont
    
    * code : cont

common/common.cpp
common/common.h
common/train.cpp
examples/baby-llama/baby-llama.cpp
examples/finetune/finetune.cpp
examples/llama-bench/llama-bench.cpp
examples/llava/llava.cpp
examples/server/server.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-cuda.cu
ggml-metal.m
ggml-opencl.cpp
ggml-sycl.cpp
ggml-vulkan.cpp
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp
tests/test-opt.cpp

commit 69917dfa55674c608360638bb4d6a12a315e2810
Author: Anas Ahouzi <112881240+aahouzi@users.noreply.github.com>
Date:   Sun Feb 25 10:54:04 2024 +0100

    py : fix StableLM conversion after config.json changes (#5703)
    
    * Fix issues during StableLM models conversion
    
    * Fix hard coded layer_norm_eps
    
    * Support layer_norm_eps for LlavaStableLM
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Add missing parenthesis
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Support rotary_factor for LlavaStableLM
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * fix typo
    
    * Add StableLMEpochForCausalLM for safety
    
    Co-authored-by: compilade <113953597+compilade@users.noreply.github.com>
    
    * Add StableLMEpochForCausalLM for safety 2
    
    Co-authored-by: compilade <113953597+compilade@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: compilade <113953597+compilade@users.noreply.github.com>

convert-hf-to-gguf.py

commit 9e359a4f47c1b2dceb99e29706c9f7403d32ab5e
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Feb 24 19:16:04 2024 +0100

    server: continue to update other slots on embedding concurrent request (#5699)
    
    * server: #5655 - continue to update other slots on embedding concurrent request.
    
    * server: tests: add multi users embeddings as fixed
    
    * server: tests: adding OAI compatible embedding concurrent endpoint
    
    * server: tests: adding OAI compatible embedding with multiple inputs

examples/server/server.cpp
examples/server/tests/features/issues.feature
examples/server/tests/features/parallel.feature
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py

commit 4c4cb30736582cacb1a164a9d4bc8e17b1014be7
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Feb 24 16:23:52 2024 +0200

    IQ3_S: a much better alternative to Q3_K (#5676)
    
    * iq4_nl: squash commits for easier rebase
    
    * Basics (quantize, dequantize)
    * CUDA dequantize and dot product
    * Slightly faster CUDA dot product (120 t/s)
    * Switch to 6-bit scales
    * Scalar dot product
    * AVX2 dot product
    * ARM_NEON dot product
    * Works on metal, but still slow
    * Slightly better Metal dot product
    * Another small Metal improvement
    * Metal dot product is getting there
    * Faster CUDA dot product
    * Add 1/8 ffn_down layers as Q5_K when no imatrix has been provided
    * Report the actual bpw
    * Add _xs mix that is 4.05 bpw for non-MoE models
    * Remove IQ4_XS for now, slightly adjust kvalues_iq4nl
    * AVX2 dot product uses Q8_0 instead of Q8_K
    * Add to test-backend-ops
    * Minor fix
    * Also use use Q5_K for attn_output in MoE models
    * Fixes after merging latest master
    * Switching to blocks of 32
    * AVX2 for blocks of 32
    * Scaler dot product for blocks of 32
    * ARM_NEON dot product for blocks of 32
    * Metal kernels for blocks of 32
    * Slightly faster Metal kernels
    
    * Resurrecting iq3_xs
    
    After all the experimentation, nothing was better than this.
    
    * Minor PPL improvement via a block scale fudge factor
    
    * Minor improvement via 3 neighbours
    
    * iq3_xs: working scalar and AVX2 dot products
    
    * iq3_xs: ARM_NEON dot product - works but extremely slow (10 t/s)
    
    * iq3_xs: working Metal implementation
    
    * Adding IQ3_M - IQ3_XS mix with mostly Q4_K
    
    * iiq3_xs: a 3.4375 bpw variant
    
    * iq3_xs: make CUDA work for new version
    
    * iq3_xs: make scalar and AVX2 work for new version
    
    * iq3_s: make ARM_NEON work with new version
    
    * iq3_xs: make new version work on metal
    
    Performance is very similar to Q3_K_S
    
    * iq3_xs: tiny Metal speed improvement
    
    * iq3_xs: tiny Metal speed improvement
    
    * Fix stupid warning
    
    * Q3_K_XS now uses a mix of IQ3_XS and IQ3_XXS
    
    * iq3_xs: rename to iq3_s
    
    * iq3_s: make tests pass
    
    * Move Q3_K_XS mix to 3.25 bpw
    
    * Attempt to fix failing tests
    
    * Another attempt to fix the Windows builds
    
    * Attempt to fix ROCm
    
    * ROCm again
    
    * iq3_s: partial fix for QK_K = 64
    
    * iq3_s: make it work on metal for QK_K = 64
    
    Pleasent surprise: the coding was super-block size independent,
    so all it took was to delete some QK_K == 256 guards.
    
    * Will this fix ROCm?
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp
tests/test-quantize-fns.cpp

commit 525213d2f5da1eaf4b922b6b792cb52b2c613368
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sat Feb 24 12:28:55 2024 +0100

    server: init functional tests (#5566)
    
    * server: tests: init scenarios
     - health and slots endpoints
     - completion endpoint
     - OAI compatible chat completion requests w/ and without streaming
     - completion multi users scenario
     - multi users scenario on OAI compatible endpoint with streaming
     - multi users with total number of tokens to predict exceeds the KV Cache size
     - server wrong usage scenario, like in Infinite loop of "context shift" #3969
     - slots shifting
     - continuous batching
     - embeddings endpoint
     - multi users embedding endpoint: Segmentation fault #5655
     - OpenAI-compatible embeddings API
     - tokenize endpoint
     - CORS and api key scenario
    
    * server: CI GitHub workflow
    
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/ISSUE_TEMPLATE/bug.md
.github/workflows/server.yml
examples/server/README.md
examples/server/server.cpp
examples/server/tests/README.md
examples/server/tests/features/environment.py
examples/server/tests/features/issues.feature
examples/server/tests/features/parallel.feature
examples/server/tests/features/security.feature
examples/server/tests/features/server.feature
examples/server/tests/features/steps/steps.py
examples/server/tests/features/wrong_usages.feature
examples/server/tests/requirements.txt
examples/server/tests/tests.sh

commit fd43d66f46ee3b5345fb8a74a252d86ccd34a409
Author: AlpinDale <52078762+AlpinDale@users.noreply.github.com>
Date:   Fri Feb 23 19:31:54 2024 +0000

    server : add KV cache quantization options (#5684)

examples/server/server.cpp

commit 54fbcd2ce6c48c9e22eca6fbf9e53fb68c3e72ea
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Fri Feb 23 13:39:14 2024 -0500

    convert : fix missing ftype for gemma (#5690)

convert-hf-to-gguf.py

commit 15499eb94227401bdc8875da6eb85c15d37068f7
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Feb 22 17:05:23 2024 -0500

    mpt : do not duplicate token_embd.weight on disk (#5670)

convert-hf-to-gguf.py
llama.cpp

commit 96633eeca1265ed03e57230de54032041c58f9cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 23:23:46 2024 +0200

    gemma : use more bits for the token_embd.weight tensor (#5650)
    
    * gemma : use Q8_0 for the token_embd.weight tensor
    
    * llama : quantize token_embd.weight using output type

llama.cpp

commit 847eedbdb2d1ebf14ef56eb507d4b4b975510908
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 23:22:48 2024 +0200

    py : add Gemma conversion from HF models (#5647)
    
    * py : add gemma conversion from HF models
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: Aarni Koskela <akx@iki.fi>
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: Aarni Koskela <akx@iki.fi>
    
    * Update convert-hf-to-gguf.py
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    
    ---------
    
    Co-authored-by: Aarni Koskela <akx@iki.fi>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

convert-hf-to-gguf.py
llama.cpp

commit 7e4f339c404dbe029d4a117c03b37a9bf646cf0e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 23:21:39 2024 +0200

    ggml : always define ggml_fp16_t as uint16_t (#5666)
    
    * ggml : always define ggml_fp16_t as uint16_t
    
    ggml-ci
    
    * ggml : cont
    
    ggml-ci
    
    * ggml : cont
    
    * ggml : cont
    
    ggml-ci
    
    * ggml : cont
    
    ggml-ci
    
    * cuda : no longer ggml headers last
    
    ggml-ci
    
    * ggml : fix q6_K FP16 -> FP32 conversion
    
    ggml-ci
    
    * ggml : more FP16 -> FP32 conversion fixes
    
    ggml-ci

ggml-cuda.cu
ggml-impl.h
ggml-quants.c
ggml.c
ggml.h

commit 334f76fa385ed81095165e5ae068756214893901
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 23:21:05 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit efd56b1c2139d50b9b4381a212feb75d69598fda
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 18:31:40 2024 +0200

    ggml : 32-bit arm compat (whisper/1891)
    
    * ggml : 32-bit arm compat
    
    * ggml : add ggml_vqtbl1q_s8 impl
    
    * ggml : cont

ggml-quants.c

commit 201294ae177b308fb3a99dc504dd6d27e8afa907
Author: Someone <sergei.kozlukov@aalto.fi>
Date:   Thu Feb 22 19:44:10 2024 +0000

    nix: init singularity and docker images (#5056)
    
    Exposes a few attributes demonstrating how to build [singularity](https://docs.sylabs.io/guides/latest/user-guide/)/[apptainer](https://apptainer.org/) and Docker images re-using llama.cpp's Nix expression.
    
    Built locally on `x86_64-linux` with `nix build github:someoneserge/llama.cpp/feat/nix/images#llamaPackages.{docker,docker-min,sif,llama-cpp}` and it's fast and effective.

.devops/nix/docker.nix
.devops/nix/scope.nix
.devops/nix/sif.nix

commit 5a9e2f60ba3d8362ba17c77ac3092906d49b813f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 20:13:25 2024 +0200

    py : minor fixes (#5668)

convert-hf-to-gguf.py

commit 373ee3fbbabc4c1508eed4f5c3795b23a20939a3
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Feb 22 19:10:21 2024 +0100

    Add Gemma chat template (#5665)
    
    * add gemma chat template
    
    * gemma: only apply system_prompt on non-model message

llama.cpp
tests/test-chat-template.cpp

commit 4cb4d8b22d4fda971621a68c570ce84d66897c37
Author: Someone <sergei.kozlukov@aalto.fi>
Date:   Thu Feb 22 16:32:09 2024 +0000

    workflows: nix: hardcode cachix ids, build unconditionally (#5663)
    
    GitHub does not expose environment and repository variables to PRs coming from forks implies that we've been disabling the Nix CI actions for most PRs.
    
    The `if:` also didn't make much sense, because we can always pull from cachix, and there's no point (albeit no risk either) in pushing cache for the untrusted code.

.github/workflows/nix-ci-aarch64.yml
.github/workflows/nix-ci.yml

commit 3a03541cedea474fa9d41214484cc3fbcf468a9e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 13:54:03 2024 +0200

    minor : fix trailing whitespace (#5638)

llama.cpp

commit 56d03d92be57f5880b9ed94542d87bb6effae31f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 22 10:35:54 2024 +0200

    readme : update hot topics

README.md

commit a46f50747b2028f7f9c9883b26bfba12bf92556e
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Feb 22 09:33:24 2024 +0100

    server : fallback to chatml, add AlphaMonarch chat template (#5628)
    
    * server: fallback to chatml
    
    * add new chat template
    
    * server: add AlphaMonarch to test chat template
    
    * server: only check model template if there is no custom tmpl
    
    * remove TODO

examples/server/server.cpp
llama.cpp
tests/test-chat-template.cpp

commit c5688c6250430d2b8e0259efcf26c16dfa4c1f46
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Thu Feb 22 08:27:32 2024 +0000

    server : clarify some params in the docs (#5640)

examples/server/README.md

commit 4ef245a92a968ba0f18a5adfd41e51980ce4fdf5
Author: Dat Quoc Nguyen <2412555+datquocnguyen@users.noreply.github.com>
Date:   Thu Feb 22 18:15:13 2024 +1000

    mpt : add optional bias tensors (#5638)
    
    Update for MPT with optional bias parameters: to work with PhoGPT and SEA-LION models that were pre-trained with 'bias'.

llama.cpp

commit 973053d8b0d04809836b3339a50f68d9c842de90
Author: slaren <slarengh@gmail.com>
Date:   Thu Feb 22 00:42:09 2024 +0100

    llama : fix loading models with shared tok_embd and output (#5651)
    
    ggml-ci

llama.cpp

commit 7c8bcc11dc61cf5930b70cd0168b84afcebe12a9
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Feb 22 00:31:00 2024 +0100

    Add docs for llama_chat_apply_template (#5645)
    
    * add docs for llama_chat_apply_template
    
    * fix typo

examples/server/README.md
llama.h

commit 7fe4678b0244ba7b03eae66ebeaa947e2770bb1a
Author: slaren <slarengh@gmail.com>
Date:   Wed Feb 21 22:52:39 2024 +0100

    llama : fix session save/load with quantized KV (#5649)

llama.cpp

commit ba2135ccae7462470b3865c6e41d2e1d734eac05
Author: slaren <slarengh@gmail.com>
Date:   Wed Feb 21 22:18:23 2024 +0100

    gemma : allow offloading the output tensor (#5646)

llama.cpp

commit 89febfed9322c8849520dc63c93ee4f5fd72556e
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Feb 21 10:33:54 2024 -0500

    examples : do not assume BOS when shifting context (#5622)

examples/main/main.cpp
examples/server/server.cpp

commit 5022cf242d689e15defd133f96c4345ad30c5d19
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 21 16:52:39 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit 1ecea255ebb70750b52688393f37a63606b90e3f
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Wed Feb 21 15:47:48 2024 +0100

    server: health: fix race condition on slots data using tasks queue (#5634)
    
    * server: health: fix race condition on slots data using tasks queue
    
    * server: health:
        * include_slots only if slots_endpoint
        * fix compile warning task.target_id not initialized.

examples/server/README.md
examples/server/server.cpp
examples/server/utils.hpp

commit a00a35cef93e057eace8351a667d14d152a91ebc
Author: Ettore Di Giacinto <mudler@users.noreply.github.com>
Date:   Wed Feb 21 15:39:10 2024 +0100

    readme : add LocalAI to the availables UI (#5629)

README.md

commit eccd7a26ddbff19e4b8805648f5f14c501957859
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 21 16:17:10 2024 +0200

    sync : ggml (#5633)
    
    * ggml : fix conv_2d batch mode (ggml/737)
    
    Co-authored-by: bssrdf <bssrdf@gmail.com>
    
    * ggml : compute forward no longer pass src tensors (ggml/729)
    
    * sync : ggml
    
    ggml-ci
    
    ---------
    
    Co-authored-by: bssrdf <merlintiger@hotmail.com>
    Co-authored-by: bssrdf <bssrdf@gmail.com>

ggml.c
scripts/sync-ggml.last

commit c14f72db9c62d71d35eb1c141745c0bd0cb27b49
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Feb 21 15:39:54 2024 +0200

    readme : update hot topics

README.md

commit cc6cac08e38e32bf40bbe07e9e8f8f0130b5fd94
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Feb 21 14:36:57 2024 +0100

    llava : add --skip-unknown to 1.6 convert.py (#5632)
    
    This commit adds the `--skip-unknown` option to the convert.py script
    and removes the saving of the updated checkpoints to avoid updating
    possibly checked out files.
    
    The motivation for this change is that this was done for 1.5
    in Commit fc0c8d286a533363a9a663510b62af85ffad58b3 ("llava :
    update surgery script to not remove tensors") and makes the examples
    more consistent.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/llava/README.md
examples/llava/llava-surgery-v2.py

commit 580111d42b3b6ad0a390bfb267d6e3077506eb31
Author: postmasters <namnguyen@google.com>
Date:   Wed Feb 21 05:08:22 2024 -0800

    llama : add `gemma` model (#5631)
    
    There are couple things in this architecture:
    
    1. Shared input and output embedding parameters.
    2. Key length and value length are not derived from `n_embd`.
    
    More information about the models can be found at
    https://ai.google.dev/gemma. GGUFs can be downloaded from
    https://huggingface.co/google.

README.md
gguf-py/gguf/constants.py
llama.cpp

commit 88c46cbdac05cebd936511b1d3c74112e721615f
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Wed Feb 21 17:52:06 2024 +0800

    [SYCL] conext add name (#5624)
    
    * [SYCL] conext add name
    
    * name should start with SYCL*

ggml-sycl.cpp

commit a14679cc30c785e75d38028bae6ec39c6209ddef
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Feb 21 11:39:52 2024 +0200

    IQ4_NL: 4-bit non-linear quants with blocks of 32 (#5590)
    
    * iq4_nl: squash commits for easier rebase
    
    * Basics (quantize, dequantize)
    * CUDA dequantize and dot product
    * Slightly faster CUDA dot product (120 t/s)
    * Switch to 6-bit scales
    * Scalar dot product
    * AVX2 dot product
    * ARM_NEON dot product
    * Works on metal, but still slow
    * Slightly better Metal dot product
    * Another small Metal improvement
    * Metal dot product is getting there
    * Faster CUDA dot product
    * Add 1/8 ffn_down layers as Q5_K when no imatrix has been provided
    * Report the actual bpw
    * Add _xs mix that is 4.05 bpw for non-MoE models
    * Remove IQ4_XS for now, slightly adjust kvalues_iq4nl
    * AVX2 dot product uses Q8_0 instead of Q8_K
    * Add to test-backend-ops
    * Minor fix
    * Also use use Q5_K for attn_output in MoE models
    * Fixes after merging latest master
    * Switching to blocks of 32
    * AVX2 for blocks of 32
    * Scaler dot product for blocks of 32
    * ARM_NEON dot product for blocks of 32
    * Metal kernels for blocks of 32
    * Slightly faster Metal kernels
    
    * iq4_nl: Fix after merging with master
    
    * iq4_nl: another fix after merging with master
    
    * Use IQ4_NL instead of Q4_K when using k-quants is not possible
    
    * Fix typo that makes several tests fail
    
    * It was the ggml_vdotq thing missed inside the brackets
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp

commit 6560bed3f066c876682464762cad90f1e28e3f1b
Author: CJ Pais <cj@cjpais.com>
Date:   Tue Feb 20 11:07:22 2024 -0800

    server : support llava 1.6 (#5553)
    
    * server: init working 1.6
    
    * move clip_image to header
    
    * remove commented code
    
    * remove c++ style from header
    
    * remove todo
    
    * expose llava_image_embed_make_with_clip_img
    
    * fix zig build

Makefile
build.zig
examples/llava/llava.cpp
examples/llava/llava.h
examples/server/server.cpp

commit 06bf2cf8c406e6b70dbf9b431a02fa0ad845b9df
Author: slaren <slarengh@gmail.com>
Date:   Tue Feb 20 20:06:17 2024 +0100

    make : fix debug build with CUDA (#5616)

Makefile

commit 4ed8e4fbef6a15afd993bfcd9ffa279841e18ef1
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Feb 20 18:30:27 2024 +0100

    llava : add explicit instructions for llava-1.6 (#5611)
    
    This commit contains a suggestion for the README.md in the llava
    example. The suggestion adds explicit instructions for how to convert
    a llava-1.6 model and run it using llava-cli.
    
    The motivation for this is that having explicit instructions similar to
    the 1.5 instructions will make it easier for users to try this out.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/llava/README.md

commit 9c405c9f9a7cfd23511fd6b2de05dc72481119b4
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Feb 20 15:58:27 2024 +0100

    Server: use llama_chat_apply_template (#5593)
    
    * server: use llama_chat_apply_template
    
    * server: remove trailing space
    
    * server: fix format_chat
    
    * server: fix help message
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * server: fix formatted_chat
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/oai.hpp
examples/server/server.cpp
examples/server/utils.hpp
llama.cpp

commit 5207b3fbc500f89dfe528693e96540956dbaed96
Author: Dane Madsen <dane_madsen@hotmail.com>
Date:   Tue Feb 20 21:00:23 2024 +1100

    readme : update UI list (#5605)
    
    * Add maid to ui list
    
    * Specify licence

README.md

commit 8dbbd75754d43ec7b4bbe42fb287cc2553fdf0e9
Author: Haoxiang Fei <tonyfettes@tonyfettes.com>
Date:   Mon Feb 19 22:58:36 2024 -1100

    metal : add build system support for embedded metal library (#5604)
    
    * add build support for embedded metal library
    
    * Update Makefile
    
    ---------
    
    Co-authored-by: Haoxiang Fei <feihaoxiang@idea.edu.cn>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
Makefile

commit c0a8c6db371cb3e4379900867b948879f5842201
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Tue Feb 20 08:48:19 2024 +0100

    server : health endpoint configurable failure on no slot (#5594)

examples/server/README.md
examples/server/server.cpp

commit b9111bd209c7b11b0592450a6ed2e0ca545b2c84
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Tue Feb 20 07:01:25 2024 +0000

    Update ggml_sycl_op_mul_mat_vec_q (#5502)
    
    * Update ggml_sycl_op_mul_mat_vec_q
    
    * Apply suggestions from code review
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
    
    * revert suggestion on macro
    
    * fix bug
    
    * Add quant type GGML_TYPE_IQ1_S to unsupported
    
    * fix format
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

ggml-sycl.cpp

commit 633782b8d949f24b619e6c68ee37b5cc79167173
Author: Mathijs de Bruin <mathijs@mathijsfietst.nl>
Date:   Tue Feb 13 20:28:02 2024 +0000

    nix: now that we can do so, allow MacOS to build Vulkan binaries
    
    Author:    Philip Taron <philip.taron@gmail.com>
    Date:      Tue Feb 13 20:28:02 2024 +0000

flake.nix

commit 22f83f0c383e12106692b8afc224d61b8993a52c
Author: 0cc4m <picard12@live.de>
Date:   Sat Feb 10 22:18:33 2024 +0100

    Enable Vulkan MacOS CI

.devops/nix/package.nix

commit bb9dcd560a7e81265398b0d463c40f3e467daf19
Author: 0cc4m <picard12@live.de>
Date:   Wed Feb 14 20:57:17 2024 +0100

    Refactor validation and enumeration platform checks into functions to clean up ggml_vk_instance_init()

ggml-vulkan.cpp

commit f50db6ae0bdcb5f8593ca6ca46dfa03b177faa2f
Author: 0cc4m <picard12@live.de>
Date:   Sat Feb 10 22:14:52 2024 +0100

    Add check for VK_KHR_portability_enumeration for MoltenVK support

ggml-vulkan.cpp

commit d8c054517dc24f1316f3be12a98fff383e1e93e3
Author: Mathijs de Bruin <mathijs@mathijsfietst.nl>
Date:   Tue Feb 6 14:39:22 2024 +0000

    Add preprocessor checks for Apple devices.
    
    Based on work by @rbourgeat in https://github.com/ggerganov/llama.cpp/pull/5322/files

ggml-vulkan.cpp

commit 42f664a3825dfde13a32c3577ab66d10c56f3aa6
Author: Mathijs de Bruin <mathijs@mathijsfietst.nl>
Date:   Sat Feb 3 18:00:11 2024 +0000

    Resolve ErrorIncompatibleDriver with Vulkan on MacOS.
    
    Refs:
    - https://chat.openai.com/share/7020ce72-65fc-45ec-b7be-9d9d798a5f3f
    - https://github.com/SaschaWillems/Vulkan/issues/954
    - https://github.com/haasn/libplacebo/issues/128
    - https://github.com/KhronosGroup/Vulkan-Samples/issues/476

ggml-vulkan.cpp

commit 5dde5408978eda22242b87e22e306d1c2d1a5834
Author: Mathijs de Bruin <mathijs@mathijsfietst.nl>
Date:   Sat Feb 3 17:56:46 2024 +0000

    Allow for Vulkan build with Accelerate.
    
    Closes #5304

ggml.c

commit 40c3a6c1e11040088b4a1ce0abc4651cb3011dd4
Author: slaren <slarengh@gmail.com>
Date:   Mon Feb 19 23:40:26 2024 +0100

    cuda : ignore peer access already enabled errors (#5597)
    
    * cuda : ignore peer access already enabled errors
    
    * fix hip

ggml-cuda.cu

commit f24ed14ee0ce28dfe98115c378b37da144912016
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Feb 19 15:54:12 2024 -0500

    make : pass CPPFLAGS directly to nvcc, not via -Xcompiler (#5598)

Makefile

commit 9d679f0fccd4030779ed3c7684a40122fe41806c
Author: nopperl <54780682+nopperl@users.noreply.github.com>
Date:   Mon Feb 19 14:14:07 2024 +0000

    examples : support minItems/maxItems in JSON grammar converter (#5039)
    
    * support minLength and maxLength in JSON schema grammar converter
    
    * Update examples/json-schema-to-grammar.py
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/json-schema-to-grammar.py

commit 1387cf60f758efb218fa06b670182c38ff149b7b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 15:23:17 2024 +0200

    llava : remove extra cont (#5587)

examples/llava/clip.cpp

commit 6fd413791a754598a54a366145960f2e27eec015
Author: slaren <slarengh@gmail.com>
Date:   Mon Feb 19 14:02:36 2024 +0100

    llava : replace ggml_cpy with ggml_cont

examples/llava/clip.cpp

commit 337c9cbd52918ae5fb9a9d9e25d7fae4e238c9f1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 14:54:21 2024 +0200

    sync : ggml
    
    ggml-ci

scripts/sync-ggml.last

commit a3145bdc305422973e25f0b066da6f469ed5dc45
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 14:53:48 2024 +0200

    ggml-alloc : apply ggml/731

ggml-alloc.c

commit 890559ab28e354052e16e770155ad007fd0856e8
Author: Didzis Gosko <didzis@users.noreply.github.com>
Date:   Sun Feb 11 16:41:41 2024 +0200

    metal : option to embed MSL source into compiled binary (whisper/1842)
    
    * ggml : embed Metal library source (ggml-metal.metal) into binary
    
    enable by setting WHISPER_EMBED_METAL_LIBRARY
    
    * rename the build option
    
    * rename the preprocessor directive
    
    * generate Metal library embedding assembly on-fly during build process

ggml-metal.m

commit d0e3ce51f45bd6a646da1952d7e5d143a087db3e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 14:45:41 2024 +0200

    ci : enable -Werror for CUDA builds (#5579)
    
    * cmake : pass -Werror through -Xcompiler
    
    ggml-ci
    
    * make, cmake : enable CUDA errors on warnings
    
    ggml-ci

CMakeLists.txt
Makefile
ggml-cuda.cu

commit 68a6b98b3c8af7e5baade3ee45fe1d2c7b9323a9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 13:41:51 2024 +0200

    make : fix CUDA build (#5580)

Makefile

commit 70d45af0efce9ed360e1858b827989d971dd9caf
Author: valiray <133289098+valiray@users.noreply.github.com>
Date:   Mon Feb 19 02:37:10 2024 -0800

    readme : fix typo in README-sycl.md (#5353)

README-sycl.md

commit 13e2c771aa4212cd5405cf310203848d50f7f859
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Mon Feb 19 14:45:18 2024 +0530

    cmake : remove obsolete sycl compile flags (#5581)
    
    * rm unwanted sycl compile options
    
    * fix bug
    
    * fix bug
    
    * format fix

CMakeLists.txt

commit f53119cec4f073b6d214195ecbe1fad3abdf2b34
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 19 10:34:10 2024 +0200

    minor : fix trailing whitespace (#5538)

llama.cpp

commit 70847553963c85e86051d06df848236829f5f951
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Feb 19 09:31:59 2024 +0100

    llava : avoid changing the original BakLLaVA model (#5577)
    
    This is a follup of Commit fc0c8d286a533363a9a663510b62af85ffad58b3
    ("llava : update surgery script to not remove tensors") but this time
    the change is to the BakLLaVA specific part of the surgery script.
    
    I've been able to test this using SkunkworksAI/BakLLaVA-1 and it works
    as expected using the instructions in README.md.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/llava/llava-surgery.py

commit 4480542b2271ba1438f0daff8e5f3a74b1dc8609
Author: NawafAlansari <72708095+NawafAlansari@users.noreply.github.com>
Date:   Mon Feb 19 03:25:38 2024 -0500

    baby-llama : allocate graphs in ggml_context (#5573)
    
    * Fixed the baby-llama issue (see issue #4830)
    
    * minor : fix whitespaces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/baby-llama/baby-llama.cpp

commit 11b12de39bd787c0494da0cd405958fdfedc29c4
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Mon Feb 19 09:23:37 2024 +0100

    llama : add llama_chat_apply_template() (#5538)
    
    * llama: add llama_chat_apply_template
    
    * test-chat-template: remove dedundant vector
    
    * chat_template: do not use std::string for buffer
    
    * add clarification for llama_chat_apply_template
    
    * llama_chat_apply_template: add zephyr template
    
    * llama_chat_apply_template: correct docs
    
    * llama_chat_apply_template: use term "chat" everywhere
    
    * llama_chat_apply_template: change variable name to "tmpl"

Makefile
llama.cpp
llama.h
tests/CMakeLists.txt
tests/test-chat-template.cpp

commit 3a9cb4ca6408c29423373dd6cd7aa78a58286c00
Author: slaren <slarengh@gmail.com>
Date:   Mon Feb 19 09:04:45 2024 +0100

    cuda, metal : fix nans in soft_max (#5574)
    
    * cuda : fix nans in soft_max
    
    * metal : fix nans in soft_max
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-cuda.cu
ggml-metal.metal

commit 769a716e30ba1da46f709df1c00727d6869d30e7
Author: Mirko185 <mirkosig@gmail.com>
Date:   Mon Feb 19 08:39:31 2024 +0100

    readme : update (#5572)
    
    Added 1.5-bit on README.md

README.md

commit f0d1fafc029a056cd765bdae58dcaa12312e9879
Author: bmwl <brian.marshall@tolko.com>
Date:   Sun Feb 18 23:38:32 2024 -0800

    ggml : android and old glibc NUMA incompatibility bugfixes (#5557)
    
    * #ifdef out some code NUMA blocks for Android due to lack of support
    
    * added in some __ANDROID__ if def gates around numa code and forced GLIBC prior to 2.29 to use a syscall for getcpu instead of the wrapper
    
    * Changed gates on numa platform specific stuff to __gnu_linux__ to skip any platforms without glibc
    
    * harmonizing #if defined blocks for numa code to __gnu_linux__ since that's the only model that's being followed anyways
    
    ---------
    
    Co-authored-by: root <root@nenya.lothlorien.ca>

ggml.c

commit a0c2dad9d43456c677e205c6240a5f8afb0121ac
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sun Feb 18 16:21:52 2024 -0500

    build : pass all warning flags to nvcc via -Xcompiler (#5570)
    
    * build : pass all warning flags to nvcc via -Xcompiler
    * make : fix apparent mis-merge from #3952
    * make : fix incorrect GF_CC_VER for CUDA host compiler

CMakeLists.txt
Makefile
scripts/get-flags.mk

commit 14278f55d2e2c6a53022075c7f2719b71e1cd61d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 22:58:57 2024 +0200

    ggml : restore vec dot stride arg names (#5453)

ggml-quants.c

commit b1de96824bdbeb91ea458abcb3e5478690ad0727
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 22:39:30 2024 +0200

    ci : fix wikitext url + compile warnings (#5569)
    
    ggml-ci

README.md
ci/run.sh
examples/perplexity/perplexity.cpp
ggml-quants.c
scripts/get-wikitext-2.sh

commit 7ad554f90e735cf2a0f612ce44f9aa4fad6ae46a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 21:39:58 2024 +0200

    metal : fix unused warnings (#0)

ggml-metal.metal

commit 5ee99c32f5e47c8d32634eff9a47fb32a24c276b
Author: Robey Holderith <robey@flaminglunchbox.net>
Date:   Sun Feb 18 11:11:16 2024 -0800

    common, server : surface min_keep as its own parameter (#5567)
    
    * Feature - surface min_keep as its own parameter
    
    * Updated README with min_keep param

common/common.cpp
common/sampling.cpp
common/sampling.h
examples/server/README.md
examples/server/public/index.html
examples/server/server.cpp

commit c145f8a132b2fe1d1e65987faddbd9a40bef7a12
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 18 18:39:57 2024 +0100

    server : slots monitoring endpoint (#5550)

examples/server/README.md
examples/server/server.cpp

commit 689a091bbe0537ee9abff3e15a1d74f5f3561165
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 19:38:06 2024 +0200

    sampling : do not set min_keep to n_probs (#5564)

common/sampling.cpp

commit f3f28c5395cd25b371617981b341616dbdd31e85
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 19:17:00 2024 +0200

    cmake : fix GGML_USE_SYCL typo (#5555)

CMakeLists.txt

commit e75c6279d1c8e7abb82a331f5de7124eed402de2
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 18 17:31:28 2024 +0100

    server : enhanced health endpoint (#5548)
    
    * server: enrich health endpoint with available slots, return 503 if not slots are available
    
    * server: document new status no slot available in the README.md

examples/server/README.md
examples/server/server.cpp

commit 36376abe05a12a8cb3af548a4af9b8d0e2e69597
Author: Pierrick Hymbert <pierrick.hymbert@gmail.com>
Date:   Sun Feb 18 17:30:09 2024 +0100

    server : --n-predict option document and cap to max value (#5549)
    
    * server: document --n-predict
    
    * server: ensure client request cannot override n_predict if set
    
    * server: fix print usage LF in new --n-predict option

examples/server/README.md
examples/server/server.cpp

commit 66c1968f7a2e895675425e875b6589f1233a1b52
Author: Daniel Hiltgen <dhiltgen@users.noreply.github.com>
Date:   Sun Feb 18 08:23:16 2024 -0800

    server : graceful server shutdown (#5244)
    
    This updates the server queue to support graceful shutdown of the server on signals.

examples/server/server.cpp
examples/server/utils.hpp

commit 1dcc3fde004787e6fc4d84c9de0bb34cd2901a3e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 18 18:21:52 2024 +0200

    common : fix ub (#5530)

common/common.cpp

commit 5d3de51f972055702a1859186fe7acb8f0b43dc4
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Sun Feb 18 16:20:12 2024 +0000

    ggml, common, examples, tests : fixed type arguments in printf (#5528)

common/common.cpp
examples/batched-bench/batched-bench.cpp
examples/batched/batched.cpp
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
examples/perplexity/perplexity.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml.c
tests/test-grammar-parser.cpp
tests/test-llama-grammar.cpp

commit fc0c8d286a533363a9a663510b62af85ffad58b3
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Sun Feb 18 17:19:23 2024 +0100

    llava : update surgery script to not remove tensors (#5536)
    
    This commit updates the surgery script to not remove the tensors from the
    model file. For this to work the `--skip-unknown` flag is added as an
    argument to the convert.py script in README.md.
    
    The motivation for this change is that the surgery script currently
    removes the projector tensors from the model file. If the model was
    checked out from a repository, the model file will have been updated
    and have to be checked out again to reset this effect. If this can be
    avoided I think it would be preferable.
    
    I did not perform this change for BakLLaVA models as I am not sure
    how that part works.

examples/llava/README.md
examples/llava/llava-surgery.py

commit bd2d4e393b2b7d2a1b2e201058e26017c9728ead
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Feb 18 18:16:55 2024 +0200

    1.5 bit quantization (#5453)
    
    * iq1_s: WIP basics
    
    * iq1_s: CUDA is working
    
    * iq1_s: scalar CPU dot product
    
    * iq1_s: WIP AVX2 dot product - something is not right
    
    * Fix tests
    
    * Fix shadow warnings
    
    * Fix after merge with latest master
    
    * iq1_s: AVX2 finally works
    
    * iq1_s: ARM_NEON dot product. Works, but not very fast
    
    * iq1_s: better grid
    
    * iq1_s: use IQ2_XXS for attn_output
    
    At a cost of 0.04 extra bpw this gives a big improvement in PPL.
    
    * iq1_s: Metal basics
    
    Dequantize works, but not dot product
    
    * iq1_s: Metal works, but quite slow
    
    As usual, Apple Silicon does not like the code I write.
    
    * iq1_s: Tests
    
    * iq1_s: slightly faster dot product
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize/quantize.cpp
ggml-backend.c
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp

commit c8e0d7efeb7634ecc2e9832e879ab9fca4510e71
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Feb 18 00:17:07 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/f8e2ebd66d097614d51a56a755450d4ae1632df1' (2024-02-07)
      → 'github:NixOS/nixpkgs/5863c27340ba4de8f83e7e3c023b9599c3cb3c80' (2024-02-16)

flake.lock

commit 8f1be0d42f23016cb6819dbae01126699c4bd9bc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Feb 17 23:04:16 2024 +0200

    ggml : add ALiBi support for ggml_soft_max_ext (#5488)
    
    * ggml : avoid recomputing alibi slopes (CPU)
    
    * llama : reuse hparams.f_max_alibi_bias in all cases
    
    ggml-ci
    
    * ggml : support alibi bias in ggml_soft_max_ext (CPU + Metal)
    
    ggml-ci
    
    * ggml : handle all SRCs (do not break on first null)
    
    ggml-ci
    
    * tests : do not use slope for large soft_max
    
    accumulates too much error
    
    ggml-ci
    
    * ggml : alternative ALiBi without extra tensor
    
    We compute the slopes in the kernel
    
    ggml-ci
    
    * cuda : add ALiBi support in ggml_soft_max_ext
    
    ggml-ci
    
    * ggml : deprecate ggml_alibi
    
    * ggml : support multi-sequence ALiBi (Metal)
    
    ggml-ci
    
    * cuda : add multi-seq ALiBi + remote F16 soft_max
    
    ggml-ci
    
    * ggml : update deprecation message
    
    * ggml : fix pos ptr when no ALiBi
    
    ggml-ci
    
    * cuda : fix performance (pow -> powf)
    
    * cuda : precompute ALiBi constants
    
    * metal : pre-compute ALiBi slopes
    
    ggml-ci
    
    * llama : init kq_pos only if needed
    
    ggml-ci
    
    * test-backend-ops : add null pos test to soft_max
    
    test-backend-ops : replace soft_max tests
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-alloc.c
ggml-backend.c
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
llama.cpp
tests/test-backend-ops.cpp

commit 6e4e973b2615f8d390b1c4f4a7e05a119078bb0f
Author: Ananta Bastola <anantarajbastola@gmail.com>
Date:   Sat Feb 17 16:03:14 2024 -0500

    ci : add an option to fail on compile warning (#3952)
    
    * feat(ci): add an option to fail on compile warning
    
    * Update CMakeLists.txt
    
    * minor : fix compile warnings
    
    ggml-ci
    
    * ggml : fix unreachable code warnings
    
    ggml-ci
    
    * ci : disable fatal warnings for windows, ios and tvos
    
    * ggml : fix strncpy warning
    
    * ci : disable fatal warnings for MPI build
    
    * ci : add fatal warnings to ggml-ci
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
CMakeLists.txt
Makefile
ci/run.sh
examples/export-lora/export-lora.cpp
ggml-backend.c
ggml-metal.m
ggml.c

commit d250c9d61d4d9f7346930814cc4aef3f3673dc3e
Author: clibdev <52199778+clibdev@users.noreply.github.com>
Date:   Sat Feb 17 18:28:37 2024 +0200

    gitignore : update for CLion IDE (#5544)

.gitignore

commit 5bf2b94dd4fb74378b78604023b31512fec55f8f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Feb 16 19:05:56 2024 +0200

    cmake : fix VULKAN and ROCm builds (#5525)
    
    * cmake : fix VULKAN and ROCm builds
    
    * cmake : fix (cont)
    
    * vulkan : fix compile warnings
    
    ggml-ci
    
    * cmake : fix
    
    ggml-ci
    
    * cmake : minor
    
    ggml-ci

CMakeLists.txt
ggml-vulkan.cpp

commit d2819d5577b35507be83d0c3f4d2d3c0ab1488ca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Feb 16 15:14:40 2024 +0200

    scripts : add helpers script for bench comparing commits (#5521)
    
    * scripts : add helpers script for bench comparing commits
    
    * scripts : detect CUDA
    
    * set flags after checking the command line
    
    * fix make flags
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

scripts/compare-commits.sh

commit 4cb072769804c77ab466bc8351c76ede9d5ba49d
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri Feb 16 12:43:23 2024 +0000

    llava : removed excess free(NULL) operation (#5531)

examples/llava/llava.cpp

commit 65085c713e14f78cdda6abc275b1a5d8c2b8ca15
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri Feb 16 11:45:48 2024 +0000

    llama : minor fixed return int value (#5529)

llama.cpp

commit 6dcc02d2444c779c18d49c364c5d5c5728b6b484
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Fri Feb 16 11:33:25 2024 +0000

    server : add "samplers" param to control the samplers order (#5494)

common/common.cpp
common/common.h
common/sampling.cpp
common/sampling.h
examples/server/README.md
examples/server/server.cpp

commit 5f5808ca7b7f23a1fa7a77241842bb84a0e55108
Author: Rőczey Barnabás <31726601+An0nie@users.noreply.github.com>
Date:   Fri Feb 16 11:00:56 2024 +0100

    server : fix system prompt cli (#5516)

examples/server/server.cpp

commit f486f6e1e5e9d01603d9325ab3e05f1edb362a95
Author: bmwl <brian.marshall@tolko.com>
Date:   Fri Feb 16 01:31:07 2024 -0800

    ggml : add numa options (#5377)
    
    * Added numa options to allow finer grained control as well as plumbing for a new mirror mode that will require numa.h
    
    * Reverted Makefile
    
    * Fixed include
    
    * Removed sched.h from ggml.h, moved ggml_get_numa_affinity into ggml.c, removed trailing whitespace and fixed up a few inconsistent variables
    
    * removed trailing whitespace
    
    * Added numa options to allow finer grained control as well as plumbing for a new mirror mode that will require numa.h
    
    * Reverting Makefile
    
    * Fixed a number of issues with the move from BOOL to ggml_numa_strategies. Added a note about mirror mode note being implemented yet
    
    * Removing MIRROR_MODE code for this PR
    
    * Removing last bit of MIRROR_MODE code for this PR
    
    * Removing unneeded branch in server.cpp example and moving get_numa_affinity and making it static
    
    * Fixed lingering init_llama_backend() bool calls in tests and examples
    
    * Remote enum llama_numa_strategies
    
    * Revert bad merge with dynatemp flags
    
    * add missing enum ggml_numa_strategies declaration and revert sync problem with master
    
    * add missing enum ggml_numa_strategies declaration
    
    * fixed ggml_init_numa variable
    
    * Update ggml.h
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update READMEs with info about numa flags, change INTERLEAVE strategy name to DISTRIBUTE everywhere, implement the improved distribution strategy from @rankaiyx, fix a spelling mistake and un-merge some bad merges
    
    * split numa init out from llama_backend_init and created llama_numa_init. Updated all code paths and samples
    
    * Fix up some boolean vs enum comparisons
    
    * Added #ifdefs for non-Linux OS that don't have cpu_set_t datatype
    
    * Update ggml.h
    
    Align enum values
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml.c
    
    Remove whitespace
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml.c
    
    align paremeters
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/server/server.cpp
    
    remove whitespace and align brace
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update common/common.cpp
    
    Remove whitespace and align brace
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * unified ggml_numa_strategy enum and fixed text alignment in server.cpp example
    
    * Update ggml.c
    
    simplified return for platforms without NUMA support
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * removed redundant else from cli argument processing of --numa
    
    * whitespace
    
    ---------
    
    Co-authored-by: root <root@nenya.lothlorien.ca>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

common/common.cpp
common/common.h
examples/batched-bench/batched-bench.cpp
examples/batched.swift/Sources/main.swift
examples/batched/batched.cpp
examples/beam-search/beam-search.cpp
examples/embedding/embedding.cpp
examples/imatrix/imatrix.cpp
examples/infill/infill.cpp
examples/llama-bench/llama-bench.cpp
examples/llama.android/app/src/main/cpp/llama-android.cpp
examples/llama.swiftui/llama.cpp.swift/LibLlama.swift
examples/llava/llava-cli.cpp
examples/lookahead/lookahead.cpp
examples/lookup/lookup.cpp
examples/main/README.md
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/passkey/passkey.cpp
examples/perplexity/perplexity.cpp
examples/quantize/quantize.cpp
examples/server/README.md
examples/server/server.cpp
examples/simple/simple.cpp
examples/speculative/speculative.cpp
examples/tokenize/tokenize.cpp
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-autorelease.cpp
tests/test-model-load-cancel.cpp
tests/test-tokenizer-0-falcon.cpp
tests/test-tokenizer-0-llama.cpp
tests/test-tokenizer-1-bpe.cpp
tests/test-tokenizer-1-llama.cpp

commit 60ed04cf82dc91ade725dd7ad53f0ee81f76eccf
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Feb 16 10:24:39 2024 +0100

    llava : fix clip-model-is-vision flag in README.md (#5509)
    
    * llava: fix clip-model-is-vision flag in README.md
    
    This commit fixes the flag `--clip_model_is_vision` in README.md which
    is does not match the actual flag:
    ```console
    $ python convert-image-encoder-to-gguf.py --help
    ...
      --clip-model-is-vision
                            The clip model is a pure vision model
                            (ShareGPT4V vision extract for example)
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * llava: update link to vit config in README.md
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/llava/README.md

commit 594845aab1c6775877f6d9545a51dc0f8d0b3d77
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Feb 16 09:57:55 2024 +0200

    ci : fix BERT model download and convert

ci/run.sh

commit 4524290e87b8e107cc2b56e1251751546f4b9051
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Thu Feb 15 11:21:49 2024 -0600

    Use correct type of pooling for embedding models (#5500)
    
    Use correct type of pooling for embedding models

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
llama.cpp
llama.h

commit c06e45d72983d9ace7b1535f7e7ea258d212169e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 15 18:49:08 2024 +0200

    clip : fix wrong loop condition

examples/llava/clip.cpp

commit 9060a1e9dfca6038906e819be5fa42217f49028c
Author: slaren <slarengh@gmail.com>
Date:   Thu Feb 15 16:49:01 2024 +0100

    cuda : print message when initialization fails (#5512)
    
    * cuda : print message when initialization fails
    
    * use CUDA_NAME both times

ggml-cuda.cu

commit 9350a1cf21b1492c69b20175b73a419b897d6a3a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 15 15:41:15 2024 +0200

    scripts : add hf.sh helper script (#5501)
    
    * scripts : add hf.sh helper scripts
    
    * hf : add error logs
    
    * hf : add support for --repo and --file

scripts/hf.sh

commit 73122473ffd73030146276dbb85da7c8021a3ee4
Author: Michaël de Vries <vriesdemichael@gmail.com>
Date:   Thu Feb 15 14:14:37 2024 +0100

    fix(gguf-py): special tokens are no longer skipped when add_<token>_token is set to false (#5487)
    
    * fix(gguf-py): special tokens are no longer skipped when add_<token>_token is set to false
    
    * fix(gguf-py): added missing cls and mask token ids to the gguf metadata

gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/vocab.py

commit 0d4177126b0556e202efb85bf3f768be81076400
Author: Elbios <141279586+Elbios@users.noreply.github.com>
Date:   Thu Feb 15 09:01:57 2024 +0100

    llava : fix memory management bug (#5491)
    
    * Fix memory management in llava and server code
    
    Fixes this error:
    
    llama_new_context_with_model: graph splits (measure): 3
    Available slots:
     -> Slot 0 - max context: 6000
    {"timestamp":1707926446,"level":"INFO","function":"main","line":2623,"message":"model loaded"}
    all slots are idle and system prompt is empty, clear the KV cache
    slot 0 - loaded image
    slot 0 is processing [task id: 0]
    slot 0 : kv cache rm - [0, end)
    slot 0 - encoding image [id: 1]
    munmap_chunk(): invalid pointer
    Aborted
    
    * Make it cleaner by checking size in batch free wrapper

examples/llava/clip.cpp
examples/llava/clip.h
examples/server/server.cpp

commit 7930a8a6e89a04c77c51e3ae5dc1cd8e845b6b8f
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Thu Feb 15 08:59:18 2024 +0100

    llaba : hotfix for llava-1.6 image number (#5495)
    
    Co-authored-by: John <cmt-nct@users.noreply.github.com>

examples/llava/llava.cpp

commit 704359e29985a06a389337a2617b7f3fa8eff908
Author: Neuman Vong <neuman.vong@gmail.com>
Date:   Thu Feb 15 17:11:15 2024 +1100

    vulkan: Find optimal memory type but with fallback (#5381)
    
    * @0cc4m feedback
    
    * More feedback @0cc4m

ggml-vulkan.cpp

commit 594fca3fefe27b8e95cfb1656eb0e160ad15a793
Author: Rune <43761327+Rune-AI@users.noreply.github.com>
Date:   Wed Feb 14 16:15:49 2024 +0100

    readme : fix typo (#5490)
    
    executabhle -> executable

README.md

commit ccbb277f4642fc0d84c72dbc0d51ed2df418d6ce
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Wed Feb 14 15:49:42 2024 +0100

    llava : update README.md (#5489)
    
    * Update README.md
    
    * Update README.md
    
    * Update examples/llava/README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/llava/README.md

commit 8084d554406b767d36b3250b3b787462d5dd626f
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Wed Feb 14 11:49:01 2024 +0300

    cmake : ARM intrinsics detection for MSVC (#5401)

CMakeLists.txt

commit aa2341298924ac89778252015efcb792f2df1e20
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Wed Feb 14 08:38:35 2024 +0100

    llava : support v1.6 (#5267)
    
    * Create llava-survery-v2.py
    
    * Update convert-image-encoder-to-gguf.py
    
    * Update convert-image-encoder-to-gguf.py
    
    * Rename llava-survery-v2.py to llava-surgery-v2.py
    
    * Update convert-image-encoder-to-gguf.py
    
    will now search for projector
    
    * Update convert-image-encoder-to-gguf.py
    
    whoops
    
    * Update llava-surgery-v2.py
    
    * Clip: Bugfix for normalization (it did not loat the 3 std and mean values)
    Clip: bicubic resize function
    Clip: added save-to-bmp/pil for debugging and conversion from/to 32/8 images
    Clip: added normalization with FP16 precision simulation (image tensors match HF implementation, can be switched off, only used for llava-1.6)
    Clip: added newline tensor, mergetype kv, image-grid kv, new resize-pad function with resolution from gridpoints
    Clip: clip_image_preprocess now returns a float * vector instead of float, this way llava 1.5 and 1.6 is supported
    llava: added ggml cpu graph for embedding patching, added spatial_unpad preliminary support, added a lot of comments that need to be cleaned when all is final
    convert-image-encoder: fixed image-grid flattening
    
    * whitespace corrections
    
    * ws
    
    * Tensors are now properly permuted.
    Before the embeddings were inserted 1:1, now they are split into the 24x24 patches as in reference.
    
    * ws
    
    * added verbose_prompt support into cli
    added stopwords for llava-1.6 into cli
    
    * moved llava functions to llava.cpp, made clip.h C compatible API, replaced vector style functions with pointers, added a debug define to remove functions from compilation while not needed
    
    * ws
    
    * convert : skip unknown tensors (need for LLaVA)
    
    * llava : update readme
    
    * llava : fix compile warnings
    
    * llava : style
    
    * convert : add --skip-unknown CLI arg
    
    * server : remove clip structs
    
    * bugfix for non llava-1.6
    
    It should now work with llava-1.5 as well
    
    * clip : minor code rearrange
    
    * llava : update readme a bit
    
    ---------
    
    Co-authored-by: John <cmt-nct@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert.py
examples/llava/README.md
examples/llava/clip.cpp
examples/llava/clip.h
examples/llava/convert-image-encoder-to-gguf.py
examples/llava/llava-cli.cpp
examples/llava/llava-surgery-v2.py
examples/llava/llava.cpp
examples/llava/llava.h
examples/server/server.cpp

commit f5ca054855dea83f424003162f26de376e5643f6
Author: AT <manyoso@users.noreply.github.com>
Date:   Tue Feb 13 15:44:25 2024 -0600

    Early return for zero size calls to get_tensor. (#5482)
    
    * Early return for zero size calls to get_tensor.
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    
    * Update ggml-kompute.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update ggml-kompute.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Add an early return to the get/set tensor when the size is null.
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    
    * Early return after the assertions.
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    
    * Since we do the early return in the generic backend now no reason to do so here as well.
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    
    ---------
    
    Signed-off-by: Adam Treat <treat.adam@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-backend.c

commit 6c00a066928b0475b865a2e3e709e2166e02d548
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Tue Feb 13 18:56:38 2024 +0100

    gguf : add python reader example (#5216)
    
    * Update CMakeLists.txt
    
    * Create reader.py
    
    * Update reader.py
    
    * Update reader.py
    
    another whitespace :|
    
    * Update reader.py
    
    * lintlintlint

examples/CMakeLists.txt
gguf-py/examples/reader.py

commit ea9c8e11436ad50719987fa23a289c74b7b40d40
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Feb 13 12:03:53 2024 -0500

    llama : add support for Nomic Embed (#5468)

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit c4e6dd59e45ef7b14f7763fb073b517395dc176c
Author: Aarni Koskela <akx@iki.fi>
Date:   Tue Feb 13 18:18:16 2024 +0200

    llama : allow raw byte in SPM vocabs; don't crash on nl 404 (#5478)
    
    * common : don't crash if newline token is not found
    
    * common : llama_byte_to_token: allow falling back to finding just the token byte in SPM vocabs

llama.cpp

commit 037259be689353081e7bae3c1ab4ab18e7fbe8c9
Author: Aarni Koskela <akx@iki.fi>
Date:   Tue Feb 13 15:24:50 2024 +0200

    llama : make load error reporting more granular (#5477)
    
    Makes it easier to pinpoint where e.g. `unordered_map::at: key not found` comes from.

llama.cpp

commit 263978904c7472db1865409a7ff1129599f6a40b
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Feb 13 14:15:42 2024 +0100

    finetune : rename feed-forward tensors (w1/w2/w3) (#4839)
    
    * finetune: rename feed-forward tensors (w1/w2/w3)
    
    This commit renames the feed-forward tensors w1, w2 and w3 to ffn_gate,
    ffn_down and ffn_up respectively.
    
    The motivation for this change is to make it easier to understand the
    purpose of the tensors. This also seems to be inline with the names
    used in the llama_layer struct in llama.cpp.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * train-text-from-scratch: rename ff tensors
    
    This commit renames the feed-forward tensors w1, w2 and w3 to ffn_gate,
    ffn_down and ffn_up respectively.
    
    The motivation for this change is to make it easier to understand the
    purpose of the tensors. This also seems to be inline with the names
    used in the llama_layer struct in llama.cpp
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/finetune/README.md
examples/finetune/finetune.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp

commit cf45252a7cfcb998bade46a886e20477cecc538a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 13 15:14:22 2024 +0200

    tests : multi-thread the tokenizer tests (#5474)
    
    * tests : multi-thread the tokenizer tests
    
    ggml-ci
    
    * unicode : fix data race for unidentified codepoints
    
    ggml-ci
    
    * unicode : minor style fixes
    
    ggml-ci

llama.cpp
tests/test-tokenizer-1-bpe.cpp
tests/test-tokenizer-1-llama.cpp
unicode.h

commit 03bf161eb6dea6400ee49c6dc6b69bdcfa9fd3fc
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Tue Feb 13 06:06:58 2024 -0600

    llama : support batched embeddings (#5466)
    
    * batched embedding: pool outputs by sequence id. updated embedding example
    
    * bring back non-causal attention
    
    * embd : minor improvements
    
    * llama : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
examples/embedding/embedding.cpp
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
llama.cpp
llama.h

commit ad014bba97ef6ef6c3e2f78b2fc463e91ae94579
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Feb 13 12:38:37 2024 +0100

    make: add error message for bad CUDA version (#5444)
    
    * make: add error message for bad CUDA version
    
    * Update Makefile
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

Makefile

commit 49cc1f7d67de2da99f3ac185f9ff1319b7bf35f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 13 13:01:29 2024 +0200

    bert : add tests + fix quantization (#5475)
    
    * llama : do not quantize pos embd and token type tensors
    
    * ci : add BERT tests
    
    ggml-ci
    
    * ci : do not do BERT tests on low-perf nodes
    
    ggml-ci

ci/run.sh
llama.cpp

commit 99b8b43d7b185a6483f28cf798a2d968b2e16ca7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 13 11:20:24 2024 +0200

    tests : disable moe test (#5473)

tests/test-backend-ops.cpp

commit 895407f31b358e3d9335e847d13f033491ec8a5b
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 13 09:07:57 2024 +0200

    ggml-quants : fix compiler warnings (shadow variable) (#5472)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-quants.c

commit 099afc6274c859ca67146e725839f2d97a5ef313
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 12 20:14:39 2024 +0200

    llama : fix quantization when tensors are missing (#5423)

llama.cpp

commit df334a11251b81fd0b6a0e51e7146e0ba9e973f2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 12 19:54:29 2024 +0200

    swift : package no longer use ggml dependency (#5465)
    
    * Revert "swift : update Package.swift to use ggml as dependency (#4691)"
    
    This reverts commit ece9a45e8ffb73ad461c792720c2fec28b0137bc.
    
    * spm : add ggml headers

Package.swift
spm-headers/ggml-alloc.h
spm-headers/ggml-backend.h
spm-headers/ggml.h

commit dbd8828eb03b9aa8d0af7e4c533d3c2f5b38aba6
Author: Lee <44310445+lx200916@users.noreply.github.com>
Date:   Tue Feb 13 01:29:57 2024 +0800

    py : fix persimmon `n_rot` conversion (#5460)
    
    * convert : fix persimmon offical weight conversion to write correct n_rot.
    
    * Update convert-persimmon-to-gguf.py
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-persimmon-to-gguf.py

commit 43fe07c1a4f3a58612e1d9543f7c6b556710f5d0
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Mon Feb 12 20:22:05 2024 +0530

    ggml-sycl: Replace 3d ops with macro  (#5458)
    
    * use macro
    
    * use macro
    
    * fix format

ggml-sycl.cpp

commit 4a46d2b7923be83d6019251671ee63aa1fa0d6bc
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Feb 12 09:38:44 2024 +0100

    llava : remove prog parameter from ArgumentParser (#5457)
    
    * llava: remove prog parameter from ArgumentParser
    
    This commit removes the `prog` parameter from `ArgumentParser`
    so that it uses the default value which is the name of the script.
    
    The motivation for this change is that currently the usage output looks
    like this:
    ```console
    $ python examples/llava/convert-image-encoder-to-gguf.py --help
    usage: convert_hf_to_gguf.py [-h] ...
    ```
    And with this change it will look like this:
    ```console
    $ python examples/llava/convert-image-encoder-to-gguf.py --help
    usage: convert-image-encoder-to-gguf.py [-h] ...
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * ci: add W503 to flake8 ignore list
    
    This commit adds W503 to the ignore list for flake8. This is done to
    avoid the following error:
    W503 line break before binary operator
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

.github/workflows/python-lint.yml
examples/llava/convert-image-encoder-to-gguf.py

commit 3b169441dfe8e420f88d1592708cc2a871daadb9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 12 09:16:06 2024 +0200

    sync : ggml (#5452)
    
    * ggml-alloc : v3 (ggml/727)
    
    * ggml-alloc v3
    
    ggml-ci
    
    * fix ci
    
    ggml-ci
    
    * whisper : check for backend buffer allocation failures
    
    * whisper : avoid leaks when initialization fails
    
    * cleanup
    
    ggml-ci
    
    * style fixes
    
    ggml-ci
    
    * sync : ggml
    
    * update llama.cpp, clip.cpp, export-lora.cpp
    
    * update finetune.cpp, train-text-from-scratch.cpp
    
    ggml-ci
    
    * ggml-backend : reduce alignment to 32 to match gguf and fix mmap
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

examples/export-lora/export-lora.cpp
examples/finetune/finetune.cpp
examples/llava/clip.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-alloc.c
ggml-alloc.h
ggml-backend.c
ggml-backend.h
ggml.c
ggml.h
llama.cpp
scripts/sync-ggml.last

commit 3bdc4cd0f595a6096cca4a64aa75ffa8a3503465
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Feb 11 19:08:39 2024 +0100

    CUDA: mul_mat_vec_q tiling, refactor mul mat logic (#5434)
    
    * CUDA: mul_mat_vec_q tiling, refactor mul mat logic
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu

commit 2891c8aa9af17f4ff636ff3868bc34ff72b56e25
Author: Douglas Hanley <thesecretaryofwar@gmail.com>
Date:   Sun Feb 11 10:21:38 2024 -0600

    Add support for BERT embedding models (#5423)
    
    * BERT model graph construction (build_bert)
    * WordPiece tokenizer (llm_tokenize_wpm)
    * Add flag for non-causal attention models
    * Allow for models that only output embeddings
    * Support conversion of BERT models to GGUF
    * Based on prior work by @xyzhang626 and @skeskinen
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.flake8
convert-hf-to-gguf.py
examples/embedding/embedding.cpp
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/tensor_mapping.py
llama.cpp
llama.h

commit 97a336507ed9b971d72262bec7e2b8b7016a054a
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Feb 11 00:17:31 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/b8b232ae7b8b144397fdb12d20f592e5e7c1a64d' (2024-01-31)
      → 'github:NixOS/nixpkgs/f8e2ebd66d097614d51a56a755450d4ae1632df1' (2024-02-07)

flake.lock

commit c88c74f967028ae3d5ebade40ae586d20a961abc
Author: Sergio López <slp@sinrega.org>
Date:   Sun Feb 11 15:12:00 2024 +0100

    vulkan: only use M-sized matmul on Apple GPUs (#5412)
    
    * vulkan: refactor guess_matmul_pipeline for vendor
    
    Refactor ggml_vk_guess_matmul_pipeline to simplify adding per-vendor
    conditionals.
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    * vulkan: only use M-sized matmul on Apple GPUs
    
    L-sized and S-sized matmuls are broken on Apple GPUs, force using
    M-size with this vendor.
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>
    
    ---------
    
    Signed-off-by: Sergio Lopez <slp@redhat.com>

ggml-vulkan.cpp

commit a803333a4e6fc534c93afe90d741bc2388bdec87
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sun Feb 11 13:43:31 2024 +0000

    common : use enums for sampler types (#5418)
    
    * common: use enums for sampler types
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * minor : spaces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
common/sampling.cpp
common/sampling.h

commit 684780141a08200ec98eba3e982dbafd1d0b5000
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sun Feb 11 13:38:14 2024 +0000

    server : allow to specify tokens as strings in logit_bias (#5003)
    
    * server: allow to specify tokens as strings in logit_bias
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/README.md
examples/server/server.cpp

commit 85910c5b30f6e268321be8df044f5528a6efac52
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 11 15:35:50 2024 +0200

    main : ctrl+C print timing in non-interactive mode (#3873)

examples/main/main.cpp

commit 139b62a839825ef20084ed75ed624db7a5ad554a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 11 15:33:43 2024 +0200

    common : fix compile warning

common/sampling.cpp

commit 0f2411f154db46780d3aaa3a0664691b2170c83f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Feb 11 15:33:01 2024 +0200

    ggml : fix compile warnings (unused vars) (#4966)

ggml-quants.c

commit a07d0fee1f05c5c1dc49948ae1a3293db017275f
Author: snadampal <87143774+snadampal@users.noreply.github.com>
Date:   Sun Feb 11 07:22:33 2024 -0600

    ggml : add mmla kernels for quantized GEMM (#4966)
    
    * ggml: aarch64: implement smmla kernel for q8_0_q8_0 quantized gemm
    
    armv8.2-a and above supports MMLA instructions that have higher
    throughput than DOT. this commit adds mmla kernel for
    q8_0_q8_0 gemm. The feature is enabled if the platform supports
    "__ARM_FEATURE_MATMUL_INT8"
    
    On AWS Graviton3 processors this kernel resulted up to 1.5x
    improvement for prompt evaluation throughput compared to the
    default sdot kernel.
    
    * ggml: aarch64: implement smmla kernel for q4_0_q8_0 quantized gemm
    
    armv8.2-a and above supports MMLA instructions that have higher
    throughput than DOT. this commit adds mmla kernel for
    q4_0_q8_0 gemm. The feature is enabled if the platform supports
    "__ARM_FEATURE_MATMUL_INT8"
    
    On AWS Graviton3 processors this kernel resulted up to 1.5x
    improvement for prompt evaluation throughput compared to the
    default sdot kernel.
    
    * ggml: aarch64: implement smmla kernel for q4_1_q8_1 quantized gemm
    
    armv8.2-a and above supports MMLA instructions that have higher
    throughput than DOT. this commit adds mmla kernel for
    q4_1_q8_1 gemm. The feature is enabled if the platform supports
    "__ARM_FEATURE_MATMUL_INT8"
    
    On AWS Graviton3 processors this kernel resulted up to 1.5x
    improvement for prompt evaluation throughput compared to the
    default sdot kernel.
    
    * ggml: update unit tests for the new vec_dot interface
    
    * llama.cpp: add MATMUL_INT8 capability to system_info

common/common.cpp
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
pocs/vdot/q8dot.cpp
pocs/vdot/vdot.cpp
tests/test-quantize-fns.cpp
tests/test-quantize-perf.cpp

commit e4640d8fdf56f14a6db3d092bcd3d2d315cb5d04
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Feb 11 12:44:51 2024 +0100

    lookup: add print for drafting performance (#5450)

examples/lookup/lookup.cpp

commit 907e08c1109f498b01036367804cff3082c44524
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Sun Feb 11 11:16:22 2024 +0100

    server : add llama2 chat template (#5425)
    
    * server: add mistral chat template
    
    * server: fix typo
    
    * server: rename template mistral to llama2
    
    * server: format_llama2: remove BOS
    
    * server: validate "--chat-template" argument
    
    * server: clean up using_chatml variable
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

examples/server/oai.hpp
examples/server/server.cpp
examples/server/utils.hpp

commit f026f8120f97090d34a52b3dc023c82e0ede3f7d
Author: Ian Bull <irbull@eclipsesource.com>
Date:   Sat Feb 10 02:53:28 2024 -0800

    metal : use autoreleasepool to avoid memory leaks (#5437)
    
    There appears to be a known memory leak when using the
    `MLTCommandBuffer`. It is suggested to use `@autoreleasepool` in
    [1,2]
    
    [1] https://developer.apple.com/forums/thread/662721
    [2] https://forums.developer.apple.com/forums/thread/120931
    
    This change-set wraps the `ggml_metal_graph_compute` in a
    `@autoreleasepool`.
    
    This commit addresses https://github.com/ggerganov/llama.cpp/issues/5436

ggml-metal.m

commit cd9aea63b577a83def84dbd6dcd90a6fa02af745
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Feb 10 09:53:05 2024 +0200

    scripts : update sync scripts with new backends

scripts/sync-ggml-am.sh
scripts/sync-ggml.sh

commit 43b65f5eb85e8741aba573a8f65bb8efad245d31
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Feb 10 09:30:36 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit 4633d93af08d890ecd00fa6e4f61d76f21cded4c
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Fri Feb 9 10:42:27 2024 +0100

    ggml : add abort_callback for cpu backend (ggml/725)
    
    * a way to use abort_callback with the cpu backend
    
    * whisper update

ggml-backend.c
ggml-backend.h
ggml.c
ggml.h

commit 4b7b38bef5addbd31f453871d79647fbae6bec8a
Author: Neuman Vong <neuman.vong@gmail.com>
Date:   Sat Feb 10 05:30:19 2024 +1100

    vulkan: Set limit for task concurrency (#5427)
    
    A common default for the maximum number of open files is 256, which can
    lead to `asyncio.gather(*tasks)` failing with Too many open files.
    
        $ python ggml_vk_generate_shaders.py --glslc=$ANDROID_NDK_PATH/shader-tools/darwin-x86_64/glslc
        ggml_vulkan: Generating and compiling shaders to SPIR-V
        Traceback (most recent call last):
          File "/Users/neuman/Code.noindex/github/llama.cpp/ggml_vk_generate_shaders.py", line 2326, in <module>
            asyncio.run(main())
          File "/Users/neuman/Code.noindex/miniforge3/lib/python3.10/asyncio/runners.py", line 44, in run
            return loop.run_until_complete(main)
          File "/Users/neuman/Code.noindex/miniforge3/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
            return future.result()
          File "/Users/neuman/Code.noindex/github/llama.cpp/ggml_vk_generate_shaders.py", line 2294, in main
            await asyncio.gather(*tasks)
        [...snip...]
        OSError: [Errno 24] Too many open files
    
    This change sets a reasonable concurrency limit for tasks (and therefore
    open files), without significant impact on run time.

ggml_vk_generate_shaders.py

commit e00d2a62dd1441e3b089570ec06d05c18800d368
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Feb 9 14:00:59 2024 +0100

    llava : add requirements.txt and update README.md (#5428)
    
    * llava: add requirements.txt and update README.md
    
    This commit adds a `requirements.txt` file to the `examples/llava`
    directory. This file contains the required Python packages to run the
    scripts in the `examples/llava` directory.
    
    The motivation of this to make it easier for users to run the scripts in
    `examples/llava`. This will avoid users from having to possibly run into
    missing package issues if the packages are not installed on their system.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * llava: fix typo in llava-surgery.py output
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/llava/README.md
examples/llava/llava-surgery.py
examples/llava/requirements.txt

commit 7c777fcd5dd4af7079e33390cf6a19c328a2666f
Author: Riley Stewart <ristew@users.noreply.github.com>
Date:   Fri Feb 9 02:49:49 2024 -0800

    server : fix prompt caching for repeated prompts (#5420)

examples/server/server.cpp

commit e5ca3937c685d6e012ac4db40555d6ec100ff03c
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Fri Feb 9 10:48:06 2024 +0000

    llama : do not cap thread count when MoE on CPU (#5419)
    
    * Not capping thread count when MoE inference is running on CPU
    
    * Whitespace

llama.cpp

commit e4124c24775f2cb5b3d7acc93bf9dc5471c172ef
Author: Marko Tasic <mtasic85@gmail.com>
Date:   Fri Feb 9 11:17:00 2024 +0100

    readme : add JavaScript/Wasm repo (#5415)

README.md

commit b2f87cb64db47d799b6f3656855c9caf9792ab2a
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Fri Feb 9 10:56:43 2024 +0100

    ggml : fix `error C2078: too many initializers` for MSVC ARM64 (#5404)

ggml-quants.c

commit 44fbe34360dd760f9e68b4271f21533436397f84
Author: 0cc4m <picard12@live.de>
Date:   Fri Feb 9 06:52:33 2024 +0100

    Fix Vulkan crash on APUs with very little device memory (#5424)
    
    * Fix Vulkan crash on APUs with very little device memory
    
    * Fix debug output function names

ggml-vulkan.cpp

commit 8e6a9d2de0096af7120606c74ee2f26684e87b41
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Feb 8 21:56:40 2024 +0100

    CUDA: more warps for mmvq on NVIDIA (#5394)

ggml-cuda.cu

commit 41f308f58edc2a04bcf9e245100b0a9b10e9a0fb
Author: slaren <slarengh@gmail.com>
Date:   Thu Feb 8 21:33:03 2024 +0100

    llama : do not print "offloading layers" message in CPU-only builds (#5416)

llama.cpp

commit 6e99f2a04f1871d637dd77eb4d81de31a5510253
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Thu Feb 8 22:39:10 2024 +0530

    Fix f16_sycl cpy call from Arc (#5411)
    
    * fix f16_sycl cpy call
    
    * rm old logic
    
    * add fp16 build CI
    
    * use macro
    
    * format fix

.github/workflows/build.yml
ggml-sycl.cpp

commit ff4ff05c5ff4311c05a8ce1f984c7d8def4f07a5
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Feb 8 15:20:03 2024 +0100

    llava : add missing .py, and fix paths in README.md (#5414)
    
    This commit adds the missing .py extension to the convert-image-encoder-to-gguf
    script. It also fixes the paths for the `model` and `mmproj` options in the
    example llava-cli command.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/llava/README.md

commit b7b74cef36a93ae01e0b9af8986d131761742d0e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Feb 8 11:36:54 2024 +0100

    fix trailing whitespace (#5407)

llama.cpp

commit 4aa43fab569215a13495a7f1a0f8afc541b16d03
Author: runfuture <runfuture@users.noreply.github.com>
Date:   Thu Feb 8 18:36:19 2024 +0800

    llama : fix MiniCPM (#5392)
    
    * fix bug for norm_rms_eps missing
    
    * to align with the same order as convert.py for model write
    
    * fix: undo HF models permute tensor
    
    * update for flake8 lint

convert-hf-to-gguf.py
llama.cpp

commit a6e514a85f0fda38ff78ec91782877ea3d19ed98
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Feb 8 09:58:19 2024 +0100

    llava: fix typo/formatting in README.md (#5405)
    
    This commit fixes a typo in the README.md file for the llava example
    which is causing the formatting to look a little off:
    
    Clone llava-v15-7b`` and clip-vit-large-patch14-336`` locally
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/llava/README.md

commit 26d4efd11e48908e14e2ee9471a7fc4c57079a1d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Feb 8 09:46:30 2024 +0100

    sampling: fix top_k <= 0 (#5388)
    
    * sampling: fix top_k <= 0
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/sampling.cpp
llama.cpp
tests/test-sampling.cpp

commit 8504d2d0da8cc7a1f2eee0e9e56949f960510b75
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Feb 8 09:46:47 2024 +0200

    tests : .gitignore obj files

tests/.gitignore

commit c4fbb6717c684196bd13b72d21747557130914e8
Author: Michael Podvitskiy <podvitskiymichael@gmail.com>
Date:   Wed Feb 7 22:39:23 2024 +0100

    CMAKE_OSX_ARCHITECTURES for MacOS cross compilation (#5393)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

CMakeLists.txt

commit 8c933b70c21e05b685d476d0a1f36b34cbda7365
Author: Ebey Abraham <ebey97@gmail.com>
Date:   Wed Feb 7 21:11:30 2024 +0000

    fix typo in readme (#5399)
    
    Co-authored-by: Ebey Abraham <ebeyabraham@microsoft.com>

README.md

commit b906596bb775b17656c2e51d5ab1b347faab6860
Author: Kamil Tomšík <info@tomsik.cz>
Date:   Wed Feb 7 19:44:52 2024 +0100

    Add Ava in the list of llama.cpp UIs (#4362)

README.md

commit aa7ab99be29b633263803f2e185265734c2d9427
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Feb 7 12:40:26 2024 +0100

    CUDA: fixed mmvq kernel for bs 2,3,4 and -sm row (#5386)

ggml-cuda.cu

commit 10afa6f1d11ebc9fcc1085f468170002cbf6e2b5
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Feb 7 18:16:55 2024 +0800

    [SYCL] update install make by w64devkit (#5297)

README-sycl.md

commit 0ef46da632c32faa1a538e5dc180994e8bbb46e1
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Wed Feb 7 02:17:25 2024 -0600

    llava-cli : always tokenize special tokens (#5382)
    
    * llava-cli: tokenize special tokens in prompt
    
    * llava-cli: use the escape CLI argument, remove incomplete separate escaping process

examples/llava/llava-cli.cpp

commit ee1628bdfea8b0079fed0140ac2f00ef1b465b57
Author: 0cc4m <picard12@live.de>
Date:   Wed Feb 7 07:54:50 2024 +0100

    Basic Vulkan Multi-GPU implementation (#5321)
    
    * Initial Vulkan multi-gpu implementation
    
    Move most global variables into backend context
    
    * Add names to backend device functions
    
    * Add further missing cleanup code
    
    * Reduce code duplication in tensor split layer assignment
    
    * generalize LLAMA_SPLIT_LAYER for all backends, do not expose device count and memory in llama.h
    
    * Only do device info print in the beginning and initialize one backend for cpu assist
    
    Add missing cleanup code
    
    * Rework backend memory management to make sure devices and buffers get properly allocated and freed
    
    * Rename cpu assist free function
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

common/common.cpp
ggml-vulkan.cpp
ggml-vulkan.h
ggml.c
llama.cpp

commit ed0bf32290ee5b30ffad5becd99cbecef74aedd7
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed Feb 7 06:21:30 2024 +0000

    readme : modernize (#5379)
    
    * first cleanup, update everything to Llama 2 and remove outdated content
    
    * Delete SHA256SUMS
    
    * make build instructions generic
    
    * recommend Q4_K_M quantization method
    
    * Update README.md

README.md
SHA256SUMS

commit 9a697d842bc0cfce8268ebd2ba703ffc1c904f98
Author: Ben Williams <ben@719ben.com>
Date:   Tue Feb 6 22:16:48 2024 -0800

    readme : update ui list (#5354)

README.md

commit 316c7faf7740fa98ea68f1445f4505810f706b9e
Author: runfuture <runfuture@users.noreply.github.com>
Date:   Wed Feb 7 14:15:56 2024 +0800

    llama : add MiniCPM support (#5346)
    
    * support minicpm arch.
    
    * fix tab/space typo.
    
    * convert minicpm model via convert-hf-gguf.py
    
    * try to make tokenizer work
    
    * fix bug for quantize minicpm
    
    * fix for flake8 lint
    
    * remove convert-minicpm.py
    
    * fix for editorconfig
    
    * correct minicpm model type (size)
    
    * constants expanded for minicpm
    
    * Minor change of the constant names for minicpm

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
llama.cpp

commit f3e2b4fa3f81a410ecb7dec929c259ef8d8dbb7d
Author: Justin Parker <jparkerweb@gmail.com>
Date:   Wed Feb 7 01:15:19 2024 -0500

    server : update `/props` with "total_slots" value (#5373)
    
    * include total "num_slots" in default_generation_settings_for_props
    
    * cleanup total_slots return value in /props endpoint
    
    * update /props endpoint docs with total_slots
    
    * remove num_slots from default_generation_settings_for_props
    
    * update /props endpoint section

examples/server/README.md
examples/server/server.cpp

commit f68664ac241a6b5c233d8f1051eef20929b06008
Author: Sang-Kil Park <sang.park@42dot.ai>
Date:   Wed Feb 7 13:28:00 2024 +0900

    convert : fix TypeError on GPT-2 vocab.json (#5288)

convert.py

commit 213d1439fadefe182f69c5f7e8dd3b4b6572ebcb
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Tue Feb 6 18:08:38 2024 +0000

    server : remove model.json endpoint (#5371)

examples/server/completion.js.hpp
examples/server/public/completion.js
examples/server/server.cpp

commit 17c97fb0620448b37516a3f53fea6c482b0a30a4
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Feb 6 18:43:06 2024 +0100

    CUDA: mul_mat_vec_q max. batch size 8 -> 4 (#5370)

ggml-cuda.cu

commit b08f22c882a1443e6b97081f3ce718a4d1a741f8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 6 19:00:16 2024 +0200

    Update README.md (#5366)
    
    Add some links to quantization related PRs

README.md

commit f57fadc009cbff741a1961cb7896c47d73978d2c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Feb 6 17:28:02 2024 +0200

    Slight quantization improvement for Q4_K and Q5_K (#5361)
    
    * Q4_K: slightly better quantization
    
    * Q5_K: slightly better quantization
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-quants.c

commit 2e9c0bd6b301155ce749e162527fc55e9fb5b832
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Tue Feb 6 09:06:48 2024 -0500

    readme : add phi, orion 14b, internlm2, and yi-VL to readme (#5362)

README.md

commit 2c516611f1d0f1e5e9754f8ea1cf97cb1b17bf2c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Feb 6 14:44:06 2024 +0100

    CUDA: mul_mat_vec_q for batch sizes > 1 (#5351)

ggml-cuda.cu

commit 8a79c591de9b7ff3242a94f68b7fb5a17ed8c2be
Author: Justin Parker <jparkerweb@gmail.com>
Date:   Tue Feb 6 04:20:59 2024 -0500

    server : include total "num_slots" in props endpoint (#5349)

examples/server/server.cpp

commit 31e790322133a4b1d0684527ea446e765e8a96cf
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Tue Feb 6 04:20:00 2024 -0500

    server : add `dynatemp_range` and `dynatemp_exponent` (#5352)
    
    * server: added `dynatemp_range` and `dynatemp_exponent`
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

examples/server/README.md
examples/server/server.cpp

commit 4ffc7a17d4e80c5f3f905139cb570ed9b6934fcb
Author: Niall Coates <1349685+Niall-@users.noreply.github.com>
Date:   Tue Feb 6 08:16:23 2024 +0000

    server : various fixes for the prompt field in /completion (#5300)
    
    server : fix deadlock when prompt array contains strings and numbers
    
    server : removed an unnecessary generation when generating multi-prompts
    
    server : removed an unnecessary assert

examples/server/server.cpp

commit 906cff55c2848fda091d888a1585915ec0c9ea9e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Feb 6 07:47:22 2024 +0200

    py : handle byte tokens in `get_token_type` (#5341)
    
    * py : handle byte tokens in `get_token_type`
    
    * py : fix empty bytes arg

convert.py

commit 098f6d737b65134cf220d12b9b706e8cfc5e4610
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Feb 5 19:33:00 2024 +0100

    make: Use ccache for faster compilation (#5318)
    
    * make: Use ccache for faster compilation

CMakeLists.txt
Makefile

commit 78b00dda6c0d62c34f5371d47718defff6ed2b22
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Feb 5 15:55:10 2024 +0100

    README: updated introduction (#5343)
    
    * README: updated introduction
    
    * readme : update
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md

commit c6b395535a6874d749ef47c33eacd466cb252cd5
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Feb 5 14:09:47 2024 +0200

    ggml : make use of ggml-quants.h possible in C++ code (#5338)
    
    * Make use of ggml-quants.h possible in C++ code
    
    * One cannot possibly be defining static_assert in a C++ compilation
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-impl.h
ggml-quants.h

commit abb61944a5f64dec62c893ed0db10790169b672a
Author: Dr. Tom Murphy VII Ph.D <499244+tom7@users.noreply.github.com>
Date:   Mon Feb 5 06:13:57 2024 -0500

    ggml : avoid duplicating function calls using MIN/MAX macros (#5325)
    
    * Avoid duplicating function calls when using MIN/MAX macros.
    
    Since these copy "a" and "b" they ask the compiler to evaluate one of them twice. The compiler doesn't have a problem with removing the duplication in something like MAX(0, x + 2), but in some cases we're calling functions, and those calls just happen twice.
    By explicitly evaluating at the expression we get smaller and faster code without duplicate calls. See ggml_rope_yarn_corr_dims in Compiler Explorer:
    
    https://godbolt.org/z/Ee4KMrvKh
    
    Code behaves exactly the same.
    
    * Update ggml.c
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c

commit 89503dcb5f764a5cc7093db1f395f5121876a2cc
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Feb 5 12:32:27 2024 +0200

    iq3_xxs: quards for the no-imatrix situation (#5334)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit 7e1ae372f36d98fa66b1d778c5862904b4d80c88
Author: Guoteng <32697156+SolenoidWGT@users.noreply.github.com>
Date:   Mon Feb 5 17:04:06 2024 +0800

    py : fix internlm2-hf convert to gguf (#5305)
    
    * py : fix internlm2-hf convert to gguf
    
    * ggml-ci

convert-hf-to-gguf.py

commit 6fdfa2ecc684000a25a4ad91823bc82a6652b645
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Feb 5 10:46:06 2024 +0200

    iq2_xxs: tune quantization (#5320)
    
    We get slightly better PPL, and we cut quantization time in
    nearly half.
    
    The trick is to 1st quantize without forcing points onto the E8-lattice.
    We can then use a narrower search range around the block scale that we
    got that way.
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-quants.c

commit a2d60c9158435ae9a6f14632f07f1acf7a3becef
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Mon Feb 5 08:10:22 2024 +0000

    server : allow to get default generation settings for completion (#5307)

examples/server/README.md
examples/server/server.cpp

commit e6f81775323f6f4e4a30abf022a6028fa86b79ac
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Mon Feb 5 17:00:47 2024 +0900

    common : add dynamic temperature parameters to main example cli (#5295)
    
    * added dynamic temp params in main
    
    * added help text

common/common.cpp

commit 30679d438d5225b3aecf5cec6482cbc9f8f87ba5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Feb 5 09:48:03 2024 +0200

    scripts : fix typos, cleanup (#5303)

scripts/server-llm.sh

commit 4be04c8965578edc09194fab769b4b922b8444f5
Author: Нияз Гарифзянов <112617865+garrnizon@users.noreply.github.com>
Date:   Mon Feb 5 10:43:57 2024 +0300

    scripts : add non-interactive server-llm.sh (#5303)
    
    * Update server-llm.sh
    
    Add flag --non-interactive that allows run script without asking a permission
    
    * Update scripts/server-llm.sh
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

scripts/server-llm.sh

commit 5d55b0cd827bb0fcfedfa329a82bd5d6ef2c93ca
Author: chiranko <96988916+chiranko@users.noreply.github.com>
Date:   Mon Feb 5 15:41:38 2024 +0800

    readme : add CodeShell models to the supported models list (#5330)

README.md

commit 4833ac209da6a427de64f97e8f403dcdc5de6bc3
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Mon Feb 5 07:08:24 2024 +0000

    [SYCL] Fix cpy with dims of 3 (#5289)
    
    * Fix cpy with dims of 3
    
    * rm asserts
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

ggml-sycl.cpp

commit 9392ebd49ea5ae236a55b47cbf6a13247e8a3b8c
Author: github-actions[bot] <github-actions[bot]@users.noreply.github.com>
Date:   Sun Feb 4 00:17:24 2024 +0000

    flake.lock: Update
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/07f6395285469419cf9d078f59b5b49993198c00' (2024-01-11)
      → 'github:hercules-ci/flake-parts/b253292d9c0a5ead9bc98c4e9a26c6312e27d69f' (2024-02-01)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/b0d36bd0a420ecee3bc916c91886caca87c894e9?dir=lib' (2023-12-30)
      → 'github:NixOS/nixpkgs/97b17f32362e475016f942bbdfda4a4a72a8a652?dir=lib' (2024-01-29)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/ae5c332cbb5827f6b1f02572496b141021de335f' (2024-01-25)
      → 'github:NixOS/nixpkgs/b8b232ae7b8b144397fdb12d20f592e5e7c1a64d' (2024-01-31)

flake.lock

commit 5ed26e1fc9fab4ce96ecf2d84183fe45bdcab0d4
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Feb 4 10:39:58 2024 +0200

    Adding some imatrix tools (#5302)
    
    * imatrix: adding --combine and --continue-from
    
    * imatrix: be able to start from a specific chunk
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/imatrix/imatrix.cpp

commit 277fad30c60ef3559dc2d01b19d05e659d40a824
Author: Welby Seely <welbyseely@gmail.com>
Date:   Sat Feb 3 23:18:51 2024 -0500

    cmake : use set() for LLAMA_WIN_VER (#5298)
    
    option() is specifically for booleans.
    
    Fixes #5158

CMakeLists.txt

commit 3c0d25c4756742ebf15ad44700fabc0700c638bd
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Feb 3 20:15:13 2024 +0100

    make: add nvcc info print (#5310)

Makefile

commit 3cc5ed353c07201d8d5b98b0a4713ab633da6d04
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Feb 3 20:14:59 2024 +0100

    make: fix nvcc optimization flags for host code (#5309)

Makefile

commit 60ecf099eddfe70fec797ef6790572e452054add
Author: Martin Schwaighofer <mschwaig@users.noreply.github.com>
Date:   Sun Jan 28 12:59:43 2024 +0100

    add Vulkan support to Nix flake

.devops/nix/package.nix
flake.nix

commit e920ed393d989ed35625ddaf182ebb52cda07fcd
Author: 0cc4m <picard12@live.de>
Date:   Sat Feb 3 18:15:00 2024 +0100

    Vulkan Intel Fixes, Optimizations and Debugging Flags (#5301)
    
    * Fix Vulkan on Intel ARC
    
    Optimize matmul for Intel ARC
    
    Add Vulkan dequant test
    
    * Add Vulkan debug and validate flags to Make and CMakeLists.txt
    
    * Enable asynchronous transfers in Vulkan backend
    
    * Fix flake8
    
    * Disable Vulkan async backend functions for now
    
    * Also add Vulkan run tests command to Makefile and CMakeLists.txt

CMakeLists.txt
Makefile
ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py

commit 52bb63c7082c859c3f1dfc527227e6a95b299c7c
Author: Michael Klimenko <mklimenko29@gmail.com>
Date:   Sat Feb 3 12:23:37 2024 +0100

    refactor : switch to emplace_back to avoid extra object (#5291)

common/common.cpp
examples/llama-bench/llama-bench.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
examples/quantize/quantize.cpp
examples/server/server.cpp
tests/test-llama-grammar.cpp

commit 1ec3332ade60aeb1494ace2211cf1a966db6d770
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Feb 3 06:22:06 2024 -0500

    YaRN : store rope scaling type as int32_t in memory (#5285)
    
    * YaRN : store rope scaling type as int32_t in memory
    
    * llama : store mapped names as const char *

common/common.h
llama.cpp
llama.h

commit 6a66c5071a74a96c4f52cf1015a092acd18c3714
Author: BADR <contact@pythops.com>
Date:   Sat Feb 3 12:20:26 2024 +0100

    readme : add tenere in the ui tools list (#5284)

README.md

commit a305dba8ff642e57f538f42010868fe0bc5262a1
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Sat Feb 3 08:11:37 2024 +0000

    Fix im2col with 32fp (#5286)

ggml-sycl.cpp

commit 191221178f51b6e81122c5bda0fd79620e547d07
Author: kalomaze <66376113+kalomaze@users.noreply.github.com>
Date:   Fri Feb 2 08:15:30 2024 -0600

    perplexity : fix KL divergence calculations on Windows (#5273)

examples/perplexity/perplexity.cpp

commit e437b37fd0b2b97e6c6ff1045ec7f901faa6498a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Feb 2 14:23:40 2024 +0200

    scripts : parse wtype in server-llm.sh (#5167)
    
    * scripts : parse wtype in server-llm.sh
    
    * scripts : fix check for wfile

scripts/server-llm.sh

commit 2d40085c26794e29c434480b9e06738e89e5686f
Author: Mirror Azure <54669636+MirrorAzure@users.noreply.github.com>
Date:   Fri Feb 2 14:39:09 2024 +0300

    py : add check for '.attn.masked_bias' layers to GPT2model (#5281)

convert-hf-to-gguf.py

commit b05102fe8cfa9893851c6bf6efd15cdc20b6afa2
Author: AidanBeltonS <87009434+AidanBeltonS@users.noreply.github.com>
Date:   Fri Feb 2 08:39:48 2024 +0000

    Tidy ggml-sycl (#5261)
    
    * Tidy some code in ggml-sycl
    
    * Remove blank space
    
    * Remove std::printf comments
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

ggml-sycl.cpp

commit 6b91b1e0a92ac2e4e269eec6361ca53a61ced6c6
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Feb 2 08:56:31 2024 +0100

    docker : add build for SYCL, Vulkan + update readme (#5228)
    
    * add vulkan dockerfile
    
    * intel dockerfile: compile sycl by default
    
    * fix vulkan dockerfile
    
    * add docs for vulkan
    
    * docs: sycl build in docker
    
    * docs: remove trailing spaces
    
    * docs: sycl: add docker section
    
    * docs: clarify install vulkan SDK outside docker
    
    * sycl: use intel/oneapi-basekit docker image
    
    * docs: correct TOC
    
    * docs: correct docker image for Intel oneMKL

.devops/main-intel.Dockerfile
.devops/main-vulkan.Dockerfile
.devops/server-intel.Dockerfile
.devops/server-vulkan.Dockerfile
README-sycl.md
README.md

commit e805f0fa9951081ce0a86378a7aa52b6f636b82d
Author: Meng, Hengyu <hengyu.meng@intel.com>
Date:   Fri Feb 2 15:54:14 2024 +0800

    [SYCL] get MAX_MEM_ALLOC from device property (#5270)
    
    * get max alloc size from device prop
    
    * fix macro typo

ggml-sycl.cpp

commit af3ba5d94627d337e32a95129e31a3064c459f6b
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Fri Feb 2 15:53:27 2024 +0800

    [SYCL] update guide of SYCL backend (#5254)
    
    * update guide for make installation, memory, gguf model link,  rm todo for windows build
    
    * add vs install requirement
    
    * update for gpu device check
    
    * update help of llama-bench
    
    * fix grammer issues

README-sycl.md
examples/llama-bench/README.md
examples/sycl/win-run-llama2.bat

commit e1e721094d8169636d55f68efe37f222cd3f0677
Author: Ian Bull <irbull@gmail.com>
Date:   Thu Feb 1 23:20:13 2024 -0800

    llama : fix memory leak in llama_batch_free (#5252)
    
    The llama_batch_init allocates memory for a fixed number of tokens.
    However, the llama_batch_free only frees memory for the number of
    tokens that were added to the batch.
    
    This change-set uses a null terminated array for the batch seq_id, and
    frees all the elements until the nullptr is reached. This change-set
    also changes the name of the first parameter from `n_tokens` to
    `n_tokens_alloc` to more clearly indicate that this value is the number
    of tokens allocated to the batch, not the number of tokens in the batch.

llama.cpp

commit 128dcbd3c9c4b12f42b560a4430427d7b2828628
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Fri Feb 2 03:48:53 2024 +0800

    add --no-mmap in llama-bench (#5257)
    
    * add --no-mmap, show sycl backend
    
    * fix conflict
    
    * fix code format, change print for --no-mmap
    
    * ren no_mmap to mmap, show mmap when not default value in printer
    
    * update guide for mmap
    
    * mv position to reduce model reload

README-sycl.md
examples/llama-bench/llama-bench.cpp
ggml-sycl.cpp
ggml-sycl.h

commit 4d0924a8902010d31bd737b6f1f594943d120d0f
Author: 0cc4m <picard12@live.de>
Date:   Thu Feb 1 19:25:24 2024 +0100

    Vulkan Phi Fix for AMD Proprietary Drivers (#5260)
    
    * Replace tanh to avoid NaN in gelu shader on AMD proprietary driver
    
    * Fix another Vulkan CPY buffer size bug

ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py

commit 8ca511cadee2c67f0bd8c7034a2513778ee9a1b7
Author: slaren <slarengh@gmail.com>
Date:   Thu Feb 1 18:30:17 2024 +0100

    cuda : fix LLAMA_CUDA_F16 (#5262)

ggml-cuda.cu

commit d71ac90985854b0905e1abba778e407e17f9f887
Author: Ali Nehzat <ali.nehzat@thanks.dev>
Date:   Fri Feb 2 02:18:53 2024 +1100

    make : generate .a library for static linking (#5205)

Makefile

commit ce32060198b7e2d6a13a9b8e1e1369e3c295ae2a
Author: Guoteng <32697156+SolenoidWGT@users.noreply.github.com>
Date:   Thu Feb 1 17:19:51 2024 +0800

    llama : support InternLM2 (#5184)
    
    * support InternLM2 inference
      * add add_space_prefix KV pair

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 1cfb5372cf5707c8ec6dde7c874f4a44a6c4c915
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed Jan 31 19:21:55 2024 +0000

    Fix broken Vulkan Cmake (properly) (#5230)
    
    * build vulkan as object
    
    * vulkan ci

.github/workflows/build.yml
CMakeLists.txt

commit d3bac7d58408c602ec1f1e423695f1df8410bb03
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 31 18:47:10 2024 +0200

    llama : reorder build_orion() at correct place (#5118)

llama.cpp

commit 5cb04dbc16d1da38c8fdcc0111b40e67d00dd1c3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 31 17:30:17 2024 +0200

    llama : remove LLAMA_MAX_DEVICES and LLAMA_SUPPORTS_GPU_OFFLOAD (#5240)
    
    * llama : remove LLAMA_MAX_DEVICES from llama.h
    
    ggml-ci
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * server : remove LLAMA_MAX_DEVICES
    
    ggml-ci
    
    * llama : remove LLAMA_SUPPORTS_GPU_OFFLOAD
    
    ggml-ci
    
    * train : remove LLAMA_SUPPORTS_GPU_OFFLOAD
    
    * readme : add deprecation notice
    
    * readme : change deprecation notice to "remove" and fix url
    
    * llama : remove gpu includes from llama.h
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

README.md
common/common.cpp
common/common.h
common/train.cpp
examples/batched-bench/batched-bench.cpp
examples/llama-bench/llama-bench.cpp
examples/server/server.cpp
llama.cpp
llama.h

commit efb7bdbbd061d087c788598b97992c653f992ddd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 31 15:35:41 2024 +0200

    metal : add im2col F32 dst support (#5132)

ggml-metal.m
ggml-metal.metal

commit 15606309a05ccf7fadbaad5538cb7c32acb1e06b
Author: JidongZhang-THU <1119708529@qq.com>
Date:   Wed Jan 31 21:10:15 2024 +0800

    llava : add MobileVLM support (#5132)
    
    * New Feature:
        1. Sum_Rows:
            fix cuda kernel overflow
            fix block shape error when nrows too big
        2. Im2Col:
            Support Batch in cuda
            Support f32 to f32 both in cpu && cuda
        3. DepthWiseConv:
            Support by Im2Col && MulMat
        4. Pool_2d:
            Supoort avg pooling in cuda
        5. HardSigmoid:
            Imp in cuda
        6. HardSwish:
            Imp in cuda
    
    * fix tabs instead of spaces
    
    * code clean
    
    * CUDA POOL2D
    
    * ADD POOL2D test case in test-backend-ops.cpp
    
    * code clean
    
    * fix pool2d_kernel
    
    nits
    
    * fix bug in pool2d kernel
    
    * fix avg pooling, count_include_pad
    
    nits
    
    * test-backend-ops : add more pool_2d tests
    
    * cuda : fix warnings and formatting
    
    * ggml : check types in release builds too in pool_2d
    
    * test-backend-ops : remove f16 pool_2d tests
    
    * cuda : more style fixes
    
    * Add assert in ggml_cuda_op_pool2d
    
    * pool2d float padding fallback
    
    * test-backend-ops : add dst_type to im2col
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

examples/llava/MobileVLM-README.md
ggml-cuda.cu
ggml.c
ggml.h
tests/test-backend-ops.cpp

commit b2b9f025e7821e78bd501d75d01838c26de07a57
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Jan 31 21:04:46 2024 +0800

    format license text, restore apache license by legal suggestion (#5233)

examples/sycl/ls-sycl-device.cpp
ggml-sycl.cpp
ggml-sycl.h

commit dabcc5b471348e4ae03ddacc41e19ad75fb2f041
Author: slaren <slarengh@gmail.com>
Date:   Wed Jan 31 13:43:03 2024 +0100

    ggml : limit n_threads to the max n_tasks (#5238)

ggml.c

commit f8e9140cb46eebaa867e1184a9946e4840eec772
Author: 0cc4m <picard12@live.de>
Date:   Wed Jan 31 11:44:19 2024 +0100

    Vulkan Fixes (#5223)
    
    * Fix Vulkan F16 models
    
    * Fix Vulkan context shift crash
    
    * Add Vulkan to common.cpp dump_non_result_info_yaml function
    
    * Fix bug in Vulkan CPY op
    
    * Fix small matrix multiplication errors in AMD GPUs on Windows or with amdvlk
    
    Co-authored-by: Engininja2 <139037756+Engininja2@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: Engininja2 <139037756+Engininja2@users.noreply.github.com>

common/common.cpp
ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml_vk_generate_shaders.py

commit d62520eb2cc1d7168a30edec6110e1daefbd959f
Author: Yiming Cui <conandiy@vip.qq.com>
Date:   Wed Jan 31 11:04:21 2024 +0800

    Fix typos of IQ2_XXS and IQ3_XXS in llama.cpp (#5231)

llama.cpp

commit 01684139c352561840ae55ec627ab58abc3e06ab
Author: Neo Zhang Jianyu <jianyu.zhang@intel.com>
Date:   Wed Jan 31 10:38:07 2024 +0800

    support SYCL backend windows build (#5208)
    
    * support SYCL backend windows build
    
    * add windows build in CI
    
    * add for win build CI
    
    * correct install oneMKL
    
    * fix install issue
    
    * fix ci
    
    * fix install cmd
    
    * fix install cmd
    
    * fix install cmd
    
    * fix install cmd
    
    * fix install cmd
    
    * fix win build
    
    * fix win build
    
    * fix win build
    
    * restore other CI part
    
    * restore as base
    
    * rm no new line
    
    * fix no new line issue, add -j
    
    * fix grammer issue
    
    * allow to trigger manually, fix format issue
    
    * fix format
    
    * add newline
    
    * fix format
    
    * fix format
    
    * fix format issuse
    
    ---------
    
    Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

.github/workflows/build.yml
.github/workflows/editorconfig.yml
.gitignore
CMakeLists.txt
README-sycl.md
README.md
examples/sycl/win-build-sycl.bat
examples/sycl/win-run-llama2.bat
scripts/install-oneapi.bat

commit e8dc55d0065d076d4c20f3c4bfca562701b4edfe
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Jan 30 19:04:37 2024 -0500

    kompute : llama-bench support and ggml_cpu_has_kompute() (#5226)

common/common.cpp
examples/llama-bench/llama-bench.cpp
ggml.c
ggml.h
llama.cpp

commit e0085fdf7c758f0bc2746fc106fb29dd9df959de
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 30 21:19:26 2024 +0200

    Revert "server : change deps.sh xxd files to string literals (#5221)"
    
    This reverts commit 4003be0e5feef320f3707786f22722b73cff9356.

examples/server/completion.js.hpp
examples/server/deps.sh
examples/server/index.html.hpp
examples/server/index.js.hpp
examples/server/json-schema-to-grammar.mjs.hpp

commit e6f291d15844398f8326940fe5ad7f2e02b5aa56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 30 20:17:30 2024 +0200

    server : fix context shift (#5195)
    
    * server : fix context shift + simplify self-extend
    
    * server : take system_tokens into account
    
    * server : more n_past fixes
    
    * server : rever n_past_se changes

examples/server/chat.sh
examples/server/server.cpp

commit 4003be0e5feef320f3707786f22722b73cff9356
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Tue Jan 30 12:15:05 2024 -0600

    server : change deps.sh xxd files to string literals (#5221)
    
    * Changed ugly xxd to literals.
    
    HPP files are much more readable as multiline literals rather than hex arrays.
    
    * Dashes in literal variable names.
    
    Replace . and - with _ in file names -> variable names.
    
    * Comment on removing xxd.
    
    XXD-> string literals
    
    * XXD to string literals.
    
    Replaced these unreadable headers with string literal versions using new deps.sh.

examples/server/completion.js.hpp
examples/server/deps.sh
examples/server/index.html.hpp
examples/server/index.js.hpp
examples/server/json-schema-to-grammar.mjs.hpp

commit fea4fd4ba7f6b754ac795387b275e1a014a77bde
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 30 19:15:28 2024 +0200

    ggml : fix IQ3_XXS on Metal (#5219)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.metal

commit 8f8ddfcfadc830b936318c3ea9fe2e8e3365aa85
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 30 16:21:57 2024 +0200

    sync : ggml (#0)

ggml-cuda.cu
scripts/sync-ggml.last

commit 6fb50ebbf036ac57a20fe8d8da31731a543582d5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 29 21:08:18 2024 +0200

    gguf : fix comparison (ggml/715)
    
    ggml-ci

ggml.c

commit 625a699b5456994bc32a8093d53818f60ceda6d1
Author: John Balis <phobossystems@gmail.com>
Date:   Mon Jan 29 06:37:33 2024 -0600

    `ggml_cuda_cpy` support for 4d tensors and float16->float32 upcasting (ggml/686)
    
    * added cuda float16->float32 upcasting to ggml_cuda_cpy
    
    * added ability to copy 4d tensors with the cuda backend
    
    * added tests for float16_>float32 upcast and 4d tensor cuda copys
    
    * added 4d copy test for float32->float16 copy
    
    * applied patch suggested by @iamlemec
    
    * simplify cpy tests
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu
tests/test-backend-ops.cpp

commit a4b07c057a553b1ac253051efc3f040351e2eae1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 29 14:00:10 2024 +0200

    gguf : add input validation, prevent integer overflows (ggml/709)
    
    * gguf : add input validation, prevent integer overflows
    
    ggml-ci
    
    * gguf : fix switch default case
    
    * gguf : sanitize info->n_dims and info->type
    
    ggml-ci
    
    * gguf : assert GGUF_TYPE_SIZE access
    
    ggml-ci
    
    * ggml : assert mallocs are successful
    
    ggml-ci
    
    * gguf : prevent integer overflow
    
    * gguf : sanitize tensor info
    
    ggml-ci
    
    * gguf : stricter limit on the number of items
    
    ggml-ci

ggml.c

commit 549a1e6cd5b39fe0dc3d4ea5515c65f17797a31e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 29 13:29:46 2024 +0200

    ci : fix yolo URLs + fix metal capture (ggml/712)

ggml-metal.m

commit 5f14ee0b0cd06f1c4790e6123df4b38ace637e88
Author: Jack Mousseau <jmousseau@users.noreply.github.com>
Date:   Mon Jan 29 01:22:23 2024 -0800

    metal : add debug capture backend function (ggml/694)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.h
ggml-metal.m

commit 8e14e3ddb3744566aef7bc0fa734180e47ae6bdf
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 30 15:15:07 2024 +0200

    Faster AVX2 dot product for IQ2_XS (#5187)
    
    * iq2xs: faster AVX2 dot product
    
    * iq2xs: small AVX2 imrovement
    
    * Speed up computing sign bits in AVX2 iq2_xs dot product
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Peter Reid <peter@peterreid.net>

ggml-quants.c

commit f4d7e5497485ce6ce0e322533930b7da4657dd2d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 30 15:14:12 2024 +0200

    SOTA 3-bit quants  (#5196)
    
    * iq3_xxs: quantize/dequantize
    
    RMSE seems a bit high-ish at about half-way between q2_K and
    q3_K, so need to check more.
    
    * iq3_xxs: CUDA dequantize works
    
    * iq2_xxs: tuning quantization
    
    * iq3_xxs: starting to look better
    
    PPL on wiki.test.raw
    LLaMA-v1-7B: 6.4218
    LLaMA-v2-7B: 6.3560
    Mistral-7B : 6.0717
    
    This is better than Q3_K_XS, with a 5% reduction in quantized model
    size.
    
    * iq3_xxs: CUDA dot product
    
    We have
    PP-512: 5891 t/s
    TG-128: 143.9 t/s
    
    * iq3_xxs: scalar and AVX2 dot products
    
    * iq3_xxs: ARM_NEON and Metal
    
    Metal performance is decent, ARM_NEON is pathetic
    
    * iq3_xxs: slightly better grid points
    
    * Faster iq3_xxs and iq2_xs dot products on CUDA
    
    * iq3_xxs: add some quant mix
    
    * iq3_xxs: fix failing quantization test
    
    Dot product still fails. Is this real?
    
    * iq3_xxs: hopefully fix ROCm
    
    * iq3_xxs: failing tests
    
    This time the dot product accuracy did find an actual bug
    in the AVX2 implementation.
    
    * Add IQ3_XXS to test-backend-ops
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize-stats/quantize-stats.cpp
examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp
tests/test-quantize-fns.cpp
tests/test-quantize-perf.cpp

commit 2256f36b79a932a478d4dcdf02c1e5a60056e5f3
Author: 0cc4m <picard12@live.de>
Date:   Tue Jan 30 13:59:30 2024 +0100

    Vulkan Windows APU Memory Handling (#5199)
    
    * Add basic UMA memory handling
    
    Improve memory OOM behavior
    
    Fix tests
    
    * Fix UMA handling
    
    * Also fix UMA handling for prealloc buffers
    
    * Remove unnecessary warning message
    
    * Remove outdated comment

ggml-vulkan.cpp

commit 7359016c7c0f65dc30cf79791212b06f15866450
Author: Vladimir Malyutin <first-leon@yandex.ru>
Date:   Tue Jan 30 17:57:07 2024 +0700

    quantize : fix typo (#5211)
    
    Fix misprint in quantize help

examples/quantize/quantize.cpp

commit 813416991ab0d1caa0d12f93ac4e8a24a2add0a3
Author: divinity76 <divinity76@gmail.com>
Date:   Tue Jan 30 10:18:02 2024 +0100

    main : allow empty --prompt-cache file (#5176)
    
    * allow empty --prompt-cache file
    
    This allows the use of std::tmpnam(), std::tmpfile(), Python's tempfile.NamedTemporaryFile(), and similar create-empty-file API's for the user.
    
    I switched from the C fopen API to the C++ filesystem api to get around the fact that, to the best of my knowledge, C has no portable way to get the file size above LONG_MAX, with std::ftell() returning long? fallback to std::ifstream for c++  < 17
    (the project is currently targeting C++11 it seems - file_exists() and file_size() can be removed when we upgrade to c++17)
    
    * formatting
    
    (requested in codereview)
    
    * remove c++17, file_is_empty

examples/main/main.cpp

commit 5589921ef84a4fb1c6d1c9c34d626a5a83033db6
Author: Romain Neutron <romain@neutron.io>
Date:   Tue Jan 30 10:16:38 2024 +0100

    readme : minor (#5204)
    
    This is about tuning the code formatting of the README file

README.md

commit 49f44b5c55d801e3d51ddcf409d866047d718905
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 30 11:14:44 2024 +0200

    readme : update hot topics

README.md

commit 6685cc41c237544623b4b5651eceb9c4280728cc
Author: Wu Jian Ping <wujp@greatld.com>
Date:   Tue Jan 30 17:11:46 2024 +0800

    server : improve README (#5209)

examples/server/README.md

commit ceebbb5b21b971941b2533210b74bf359981006c
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Mon Jan 29 22:19:29 2024 +0000

    ggml alloc: Fix for null dereference on alloc failure (#5200)
    
    * Fix for a null pointer dereference if a metal GGML buffer fails to be allocated
    
    * Freeing the allocated buffers rather than the pointer in ggml-alloc.c
    
    * Fixed the fix of the fix

ggml-alloc.c

commit 6daa69ee81851ab26ca8aefca1a4202941fc0262
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Jan 29 17:11:27 2024 -0500

    kompute : fix fallback to CPU (#5201)

llama.cpp

commit fbf1ddec69f7001cc707de17fa74d7200813bbac
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Mon Jan 29 15:50:50 2024 -0500

    Nomic Vulkan backend (#4456)
    
    Signed-off-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: niansa <anton-sa@web.de>
    Co-authored-by: Adam Treat <treat.adam@gmail.com>
    Co-authored-by: Aaron Miller <apage43@ninjawhale.com>
    Co-authored-by: ToKiNoBug <tokinobug@163.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

.ecrc
.github/workflows/build.yml
.gitmodules
CMakeLists.txt
ggml-backend.c
ggml-kompute.cpp
ggml-kompute.h
kompute
kompute-shaders/common.comp
kompute-shaders/op_add.comp
kompute-shaders/op_addrow.comp
kompute-shaders/op_cpy_f16_f16.comp
kompute-shaders/op_cpy_f16_f32.comp
kompute-shaders/op_cpy_f32_f16.comp
kompute-shaders/op_cpy_f32_f32.comp
kompute-shaders/op_diagmask.comp
kompute-shaders/op_gelu.comp
kompute-shaders/op_getrows.comp
kompute-shaders/op_getrows_f16.comp
kompute-shaders/op_getrows_q4_0.comp
kompute-shaders/op_getrows_q4_1.comp
kompute-shaders/op_getrows_q6_k.comp
kompute-shaders/op_mul.comp
kompute-shaders/op_mul_mat_f16.comp
kompute-shaders/op_mul_mat_mat_f32.comp
kompute-shaders/op_mul_mat_q4_0.comp
kompute-shaders/op_mul_mat_q4_1.comp
kompute-shaders/op_mul_mat_q6_k.comp
kompute-shaders/op_mul_mat_q8_0.comp
kompute-shaders/op_mul_mv_q_n.comp
kompute-shaders/op_mul_mv_q_n_pre.comp
kompute-shaders/op_norm.comp
kompute-shaders/op_relu.comp
kompute-shaders/op_rmsnorm.comp
kompute-shaders/op_rope_f16.comp
kompute-shaders/op_rope_f32.comp
kompute-shaders/op_scale.comp
kompute-shaders/op_scale_8.comp
kompute-shaders/op_silu.comp
kompute-shaders/op_softmax.comp
kompute-shaders/rope_common.comp
llama.cpp
llama.h
tests/test-backend-ops.cpp
tests/test-c.c

commit 2aed77eb06a329f0d82bb1c467f4244904d4073f
Author: divinity76 <divinity76@gmail.com>
Date:   Mon Jan 29 15:45:41 2024 +0100

    fix typo "RLIMIT_MLOCK" (#5175)

llama.cpp

commit c82d18e863fcde91b4b1109b1d0c73ea4470c405
Author: Wu Jian Ping <wujjpp@hotmail.com>
Date:   Mon Jan 29 21:48:10 2024 +0800

    server : embeddings compatibility for OpenAI (#5190)

examples/server/oai.hpp
examples/server/server.cpp

commit 14fef85e2d5361e6b4dd7a9ecf22bb817362997d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 29 15:35:54 2024 +0200

    py : fix except (#5194)
    
    ggml-ci

convert.py

commit e76627bcce9f77adb6034cb127b7ec93d4287b69
Author: Sang-Kil Park <sang.park@42dot.ai>
Date:   Mon Jan 29 18:24:19 2024 +0900

    py : improve BPE tokenizer support (#5189)

convert.py

commit fbe7dfa53caff0a7e830b676e6e949917a5c71b4
Author: slaren <slarengh@gmail.com>
Date:   Mon Jan 29 09:05:13 2024 +0100

    ggml : add max buffer sizes to opencl and metal backends (#5181)

ggml-metal.m
ggml-opencl.cpp

commit 172ac82629815d038702b049070f4c8c02662da5
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Mon Jan 29 08:04:47 2024 +0000

    cmake : fix Vulkan build (#5182)

CMakeLists.txt

commit d2f650cb5b04ee2726663e79b47da5efe196ce00
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Sun Jan 28 19:50:16 2024 +0000

    metal : free metal objects (#5161)
    
    * Releasing MTLFunction references after Metal pipeline construction
    
    * Keeping the `ggml_metal_kernel` structure
    
    * Spacing fix
    
    * Whitespace fix

ggml-metal.m

commit 35dec26cc25a9ff7d8c3ed52326b94f772b911ce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 28 19:48:05 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit d460510c7222d43a458a17e01d4bbe72437cdd3c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 28 18:44:58 2024 +0200

    ggml : minor type fix (int64_t -> size_t)

ggml.c

commit 2307523d322af762ae06648b29ec5a9eb1c73032
Author: 0cc4m <picard12@live.de>
Date:   Sun Jan 28 18:03:59 2024 +0100

    ggml : add Vulkan backend (#2059)
    
    * Vulkan loader code
    
    * Fix matmul kernel, continue implementation
    
    * Continue implementation
    
    * Vulkan memory management
    
    * Vulkan development
    
    * Matmul call
    
    * Add aligned malloc and free for VMA
    
    * Continue implementation
    
    * First matmul success
    
    * GEMM Kernel optimization
    
    * 1D Blocktiling
    
    * 2D Blocktiling
    
    * Write coalescing
    
    * Continue vulkan implementation and optimization
    
    * First FP16 attempt, disabled for now
    
    * Code abstraction, FP16 implementation, fix kernel, add FP16 to FP32 kernel
    
    * Enable device extensions properly, restore fp16 matmul op
    
    * Fix mulmat_f16
    
    * Output FP32 in fp16 matmul shader
    
    * Fix f16_to_f32 kernel
    
    * dequant_q4_0 kernel
    
    * Add VMA library
    
    * Avoid requesting dedicated memory, VMA can decide that by itself
    
    * Add bounds checking to matmul kernels, improve implementation, fix command buffers not freed properly
    
    * add cmake commands
    
    * Add 2d write operation, profiling code
    
    * Fix 2d write
    
    * Fix queue selection for AMD RADV
    
    * Fix trailing whitespace in vk_mem_alloc.h
    
    * Add WIP warp tile mat mul shaders
    
    * Disable glslc optimization
    
    * Disable glslc optimization for CMake
    
    * Optimize warptile matmul shader, replace blocktile with it
    
    * Add split-k optimization for small matrix multiplication
    
    Use semaphores for synchronization instead of fences or waitidle
    
    Rework async write/read for synchronization
    
    * Fix validation errors, improve compatibility with AMD GPUs
    
    * Rework command buffer handling
    
    * Variable matmul kernel using specialization constants
    
    * Fix synchronization on AMD, add barriers for buffer ownership transfer, add debug flag and prints
    
    * Reuse semaphores
    
    * Handle stage flags during command buffer submission properly
    
    * Increase matmul test runs for consistent results
    
    * Fix F32 matmul
    
    * Add vectorized loading and zeropadding for matrix multiplication
    
    * Use pinned memory for f16 preprocessing
    
    * Don't force aligned matmul
    
    * Don't free before queue done
    
    * Replace VMA library with native Vulkan buffer management
    
    * Basic offloading support with mul_f32 and dmmv for q4_0
    
    * Run glslc commands in parallel
    
    * Unroll loops in dmmv shader
    
    * Reduce usage of waitIdle
    
    * Reuse pinned allocation for f16 conversion
    
    * Handle devices with only a single queue
    
    * Fix trailing whitespace in CMakeLists.txt
    
    * Allow parallel execution of kernels, parallelize third and fourth dimension calls
    
    * Add fallback for devices only supporting one DescriptorSet per DescriptorPool
    
    * Move to graph function similar to CUDA implementation
    
    * Use F16 kernel for most things, replace q_f32 with mul_mat_q_f16 function
    
    * Add F32 dmmv shaders
    
    * Batch submissions
    
    * Add .spv to gitignore
    
    * Split off matrix vector multiplication for separate optimization
    
    * Use single command buffer for matrix vector multiplication ops
    
    * Reduce overhead of mul_f32 calls by using a single command buffer
    
    * Add submission batching to mul_f32
    
    * Fix tests
    
    * Add missing barrier
    
    * Add further missing barrier
    
    * Add further ops
    
    * Replace vk::QueueFamilyIgnored with VK_QUEUE_FAMILY_IGNORED to support more Vulkan header versions
    
    * Remove unnecessary cblas link
    
    * Fix descriptor set pre-allocation assert
    
    * Add runtime shader compilation, start transferring shaders to this approach
    
    * Transfer remaining shaders to header and compile on runtime
    
    * Fix fp32 fallback if device doesn't support fp16, add force disable env var GGML_VULKAN_DISABLE_F16
    
    * Add support for q4_1, q5_0, q5_1 and q8_0
    
    * Remove unnecessary scalar layout extension
    
    * Parse graph early to pre-record command buffers
    
    * Add q6_k support
    
    * Add multi-submit for command buffers
    
    * Fix q6_k dequant shader for AMD
    
    * Fix q6_k for GPUs without fp16 support
    
    * Simplify q6_k fp16 fix
    
    * Minor fixes
    
    * Fix wg_denom of m-mulmat shaders
    
    * Add Python-based Vulkan shader generator
    
    * Replace shaderc dependency with precompiled shaders
    
    Fix python script to generate shaders
    
    * Clean up code
    
    * Fix shader generator script Windows compatibility
    
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>
    
    * Close file before deletion
    
    * Fix vulkan shader fp32 name
    
    * Add q2_k and q3_k support
    
    Add validation check to compare shader results to cpu results
    
    * Add q4_k support
    
    * Add q5_k support
    
    * Bake SPIR-V bytecode into the library instead of loading shaders from file
    
    * Switch to signal semaphores for flexibility
    
    Prepare broadcasting support for mul mat
    
    * Finish broadcasting mul mat support for GQA
    
    * Clean up unused functions
    
    Add repeat op
    
    * Add further ops, not yet enabled. Improve semaphore code
    
    * Reduce number of used semaphores by utilizing timelines more properly
    
    * Remove queue information
    
    * Reuse timeline semaphores, allow parallel operation with binary semaphores to work around nvidia driver limitations
    
    * Add Vulkan to llama-bench
    
    * Remove cblas dependency
    
    * Fix matmul k-split bug
    
    * Fix q4_k dmmv K_QUANTS_PER_ITERATION 1 shader
    
    * Add RMS Norm shader, rework op_f32 shader setup, fix matmul bug
    
    * Fix issues with float16 overflows in shaders
    
    * Fix issues with older Vulkan headers on Ubuntu 22.04
    
    * Allow multi-op partial offloading by parsing the graph to preallocate enough between-op buffers
    
    * Implement further ops, rework op_f32 calls, fix bugs
    
    * Finish full offloading support, add last remaining ops, fix bugs, remove redundant code
    
    * Upload generated file ggml-vulkan-shaders.hpp, remove redundant shaders
    
    * Merge upstream changes, fix conflicts, adapt soft_max op
    
    * Fix Python and shader header format
    
    * Free model gpu buffers on exit
    
    * Use single queue per device to simplify code
    
    * Add matmul shader support for running multiple calculations in parallel
    
    * Switch from semaphore-synchronized multiple command buffers per op to single command buffer for multiple ops, whole graph if possible
    
    * Fix missing event cast
    
    * Replace uint64_t(-1) with UINT64_MAX, rename function for clarity
    
    * Fix warning about empty C function parameters
    
    * Fix compiler warnings
    
    * Properly implement Vulkan backend buffer handling
    
    * Fix oversized host staging buffers
    
    * Simplify barrier synchronization calls
    
    * Fix gcc warnings
    
    * Implement max_size for backend buffer types to limit the size of a single allocation
    
    * Use min of maxMemoryAllocationSize and maxBufferSize for device max allocation size
    
    * refactor multi buf
    
    * Disable unsupported ops to fix tests
    
    * Check for maintenance4 support before using it
    
    * Handle devices with only a single queue
    
    * Fix single queue logic
    
    * propagate buffer usage in multi buffers
    
    * Implement rope_neox op
    
    * Cleanup header and other files
    
    * Simplify gpu_extras by removing events and putting staging memcpys into contexts
    
    * Move queue into context
    
    Add not-yet-enabled async backend ops
    
    * Simplify context use, optimize matmul shader for warp size 64 (AMD GCN), fix split_k matmul shader optimization
    
    * Add get_max_size to SYCL backend.
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * llama : fix trailing whitespace
    
    ---------
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
Makefile
examples/llama-bench/llama-bench.cpp
ggml-alloc.c
ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-metal.m
ggml-opencl.cpp
ggml-sycl.cpp
ggml-vulkan-shaders.hpp
ggml-vulkan.cpp
ggml-vulkan.h
ggml.c
ggml.h
ggml_vk_generate_shaders.py
llama.cpp
llama.h

commit 0f648573dde61c510560f68244f70ece7e60d8c1
Author: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Date:   Sun Jan 28 21:26:23 2024 +0530

    ggml : add unified SYCL backend for Intel GPUs (#2690)
    
    * first update for migration
    
    * update init_cublas
    
    * add debug functio, commit all help code
    
    * step 1
    
    * step 2
    
    * step3 add fp16, slower 31->28
    
    * add GGML_LIST_DEVICE function
    
    * step 5 format device and print
    
    * step6, enhance error check, remove CUDA macro, enhance device id to fix none-zero id issue
    
    * support main device is non-zero
    
    * step7 add debug for code path, rm log
    
    * step 8, rename all macro & func from cuda by sycl
    
    * fix error of select non-zero device, format device list
    
    * ren ggml-sycl.hpp -> ggml-sycl.h
    
    * clear CMAKE to rm unused lib and options
    
    * correct queue: rm dtct:get_queue
    
    * add print tensor function to debug
    
    * fix error: wrong result in 658746bb26702e50f2c59c0e4ada8e9da6010481
    
    * summary dpct definition in one header file to replace folder:dpct
    
    * refactor device log
    
    * mv dpct definition from folder dpct to ggml-sycl.h
    
    * update readme, refactor build script
    
    * fix build with sycl
    
    * set nthread=1 when sycl, increase performance
    
    * add run script, comment debug code
    
    * add ls-sycl-device tool
    
    * add ls-sycl-device, rm unused files
    
    * rm rear space
    
    * dos2unix
    
    * Update README_sycl.md
    
    * fix return type
    
    * remove sycl version from include path
    
    * restore rm code to fix hang issue
    
    * add syc and link for sycl readme
    
    * rm original sycl code before refactor
    
    * fix code err
    
    * add know issue for pvc hang issue
    
    * enable SYCL_F16 support
    
    * align pr4766
    
    * check for sycl blas, better performance
    
    * cleanup 1
    
    * remove extra endif
    
    * add build&run script, clean CMakefile, update guide by review comments
    
    * rename macro to intel hardware
    
    * editor config format
    
    * format fixes
    
    * format fixes
    
    * editor format fix
    
    * Remove unused headers
    
    * skip build sycl tool for other code path
    
    * replace tab by space
    
    * fix blas matmul function
    
    * fix mac build
    
    * restore hip dependency
    
    * fix conflict
    
    * ren as review comments
    
    * mv internal function to .cpp file
    
    * export funciton print_sycl_devices(), mv class dpct definition to source file
    
    * update CI/action for sycl code, fix CI error of repeat/dup
    
    * fix action ID format issue
    
    * rm unused strategy
    
    * enable llama_f16 in ci
    
    * fix conflict
    
    * fix build break on MacOS, due to CI of MacOS depend on external ggml, instead of internal ggml
    
    * fix ci cases for unsupported data type
    
    * revert unrelated changed in cuda cmake
    remove useless nommq
    fix typo of GGML_USE_CLBLAS_SYCL
    
    * revert hip cmake changes
    
    * fix indent
    
    * add prefix in func name
    
    * revert no mmq
    
    * rm cpu blas duplicate
    
    * fix no_new_line
    
    * fix src1->type==F16 bug.
    
    * pass batch offset for F16 src1
    
    * fix batch error
    
    * fix wrong code
    
    * revert sycl checking in test-sampling
    
    * pass void as arguments of ggml_backend_sycl_print_sycl_devices
    
    * remove extra blank line in test-sampling
    
    * revert setting n_threads in sycl
    
    * implement std::isinf for icpx with fast math.
    
    * Update ci/run.sh
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/sycl/run-llama2.sh
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/sycl/run-llama2.sh
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update CMakeLists.txt
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update CMakeLists.txt
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update CMakeLists.txt
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update CMakeLists.txt
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * add copyright and MIT license declare
    
    * update the cmd example
    
    ---------
    
    Co-authored-by: jianyuzh <jianyu.zhang@intel.com>
    Co-authored-by: luoyu-intel <yu.luo@intel.com>
    Co-authored-by: Meng, Hengyu <hengyu.meng@intel.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
CMakeLists.txt
README.md
README_sycl.md
ci/README.md
ci/run.sh
common/common.cpp
examples/CMakeLists.txt
examples/server/server.cpp
examples/sycl/CMakeLists.txt
examples/sycl/README.md
examples/sycl/build.sh
examples/sycl/ls-sycl-device.cpp
examples/sycl/run-llama2.sh
ggml-backend.c
ggml-sycl.cpp
ggml-sycl.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp

commit b764b8f1d079ba44d912801ce6d29bd0d94d51cf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 28 16:54:54 2024 +0200

    flake.lock: Update (#5162)

flake.lock

commit 9241c3a2ace544aef708334e54bbdddb90208ee8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jan 28 09:59:49 2024 +0100

    Apply min_p to unsorted tokens (#5115)

llama.cpp

commit b2b2bf988c098851b4f3831f0cf38394bff75121
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jan 28 09:35:14 2024 +0100

    Tests for min_p, sampling queue (#5147)

llama.cpp
tests/test-sampling.cpp

commit af4980bfedfd8df43b9e4cd1442895e85fee37bc
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Sun Jan 28 00:30:44 2024 -0800

    readme : add link to rust bindings (#5148)
    
    * added link to another set of rust bindings with brief note on differences.
    
    * fixed link name

README.md

commit f2e69d28c01303ca9dc79907f89ef120a6ac4a92
Author: sharpHL <132747147+sharpHL@users.noreply.github.com>
Date:   Sun Jan 28 16:00:30 2024 +0800

    llama : add support for Orion-14B (#5118)
    
    * add support for Orion-14B(https://huggingface.co/OrionStarAI/Orion-14B-Chat)
    
    * flake8 support
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update llama.cpp
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: lixiaopu <lixiaopu@cmcm.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
llama.cpp

commit 39baaf55a160909bb9428bd981014218761a20cb
Author: Kyle Mistele <kyle@mistele.com>
Date:   Sun Jan 28 01:55:31 2024 -0600

    docker : add server-first container images (#5157)
    
    * feat: add Dockerfiles for each platform that user ./server instead of ./main
    
    * feat: update .github/workflows/docker.yml to build server-first docker containers
    
    * doc: add information about running the server with Docker to README.md
    
    * doc: add information about running with docker to the server README
    
    * doc: update n-gpu-layers to show correct GPU usage
    
    * fix(doc): update container tag from `server` to `server-cuda` for README example on running server container with CUDA

.devops/server-cuda.Dockerfile
.devops/server-intel.Dockerfile
.devops/server-rocm.Dockerfile
.devops/server.Dockerfile
.github/workflows/docker.yml
README.md
examples/server/README.md

commit 6db2b41a76ee78d5efdd5c3cddd5d7ad3f646855
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Sat Jan 27 16:09:18 2024 +0100

    llava : support for Yi-VL and fix for mobileVLM (#5093)
    
    * Support for Yi-VL, templating fix for mobileVLM
    
    * ws
    
    * Update examples/llava/clip.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llava-cli.cpp
    
    * Update clip.cpp
    
    bugfix for new conversions
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/llava/clip.cpp
examples/llava/llava-cli.cpp

commit 753eafed0ebd07af6903771327a1786a7c02cf98
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 27 16:59:20 2024 +0200

    sync : ggml

examples/server/utils.hpp
scripts/sync-ggml.last

commit e9764230054e01553bdead6f2bfd8e001869599d
Author: Judd <foldl@users.noreply.github.com>
Date:   Fri Jan 26 21:04:01 2024 +0800

    ggml : check ggml_add src1 type (ggml/708)
    
    Co-authored-by: Judd <foldl@boxvest.com>

ggml.c

commit 35a2ee914308c85ab5cb576467381443ad23f0ac
Author: Michael Klimenko <mklimenko29@gmail.com>
Date:   Sat Jan 27 15:25:55 2024 +0100

    Remove unused data and add fixes (#5154)
    
    * Remove unused data and add fixes
    
    * Add missing file
    
    * Address review comments
    
    * Replace the scope of vq allocation

common/sampling.cpp
examples/infill/infill.cpp
examples/llava/clip.cpp
examples/server/server.cpp
pocs/vdot/vdot.cpp
tests/test-backend-ops.cpp
tests/test-llama-grammar.cpp

commit ec903c034131848da9222536ff18da07ec0882a0
Author: Maximilian Winter <maximilian.winter.91@gmail.com>
Date:   Sat Jan 27 14:38:05 2024 +0100

    server : add self-extend support (#5104)
    
    * Ported self extension to server example
    
    * Update server.cpp
    
    * Fixed prompt caching without self extend
    
    * Update server.cpp
    
    * Added description to server readme.
    
    * Update server.cpp
    
    * Update server.cpp
    
    * Update server.cpp
    
    * Update server.cpp
    
    * Update README.md
    
    * Changed descriptions
    
    * server : formatting
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update server.cpp
    
    * Update server.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/README.md
examples/server/server.cpp

commit a1d6df129bcd3d42cda38c09217d8d4ec4ea3bdd
Author: 0cc4m <picard12@live.de>
Date:   Fri Jan 26 23:07:32 2024 +0100

    Add OpenCL add kernel (#5151)
    
    * Add OpenCL add kernel
    
    * Put add kernel into different string to stay within MSVC string length limit, disable float16 support due to bad results

ggml-opencl.cpp
ggml-opencl.h
ggml.c

commit bbe7c56c9993af86aa2d84cbe1fd69e1b4300cea
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Fri Jan 26 15:34:06 2024 -0500

    cmake : pass CPU architecture flags to nvcc (#5146)

CMakeLists.txt

commit 62fead3ea0a30c8d424f4a8373fa14165c7c707f
Author: slaren <slarengh@gmail.com>
Date:   Fri Jan 26 18:59:43 2024 +0100

    cuda : fix tensor size calculation for non-split buffer (#5145)

ggml-backend.c
ggml-cuda.cu

commit 15b4538ff29b280a395a1406d711497d8eaa2564
Author: slaren <slarengh@gmail.com>
Date:   Fri Jan 26 18:18:26 2024 +0100

    ggml-alloc : add 10% margin to the buffer sizes (#5149)

ggml-alloc.c

commit 7032f4f6349c17a8352f9f93f7d2122f45469e59
Author: snadampal <87143774+snadampal@users.noreply.github.com>
Date:   Fri Jan 26 11:17:59 2024 -0600

    ggml : update softmax n_task calculation (#5126)
    
    updated the n_task calculation to use max number of
    threads possible. This has improved the prompt eval
    performance by around 5% for DOT kernels and by
    around 10% for MMLA kernels on AWS Graviton3.

ggml.c

commit 5f1925a8cef81eb9b372faaae34b0dd76d5361d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 26 17:09:44 2024 +0200

    scripts : move run-with-preset.py from root to scripts folder

scripts/run-with-preset.py

commit 3b7c914de25c6851396d7f9178249f1ed278120e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 26 14:48:15 2024 +0200

    tests : gitignore test-c.o

tests/.gitignore

commit 48c857aa10aea73210a4a72da3f1a6f99269e75d
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Fri Jan 26 13:42:20 2024 +0100

    server : refactored the task processing logic (#5065)
    
    * server: add llama_server_queue struct
    
    * server: add llama_server_response_event
    
    * server: add comments
    
    * server: move all mutexes away from server.cpp
    
    * server: correct multitask response
    
    * server: only add back deferred tasks when one slot is available
    
    * server: fix a race condition cause by "request_completion"

Makefile
examples/server/CMakeLists.txt
examples/server/oai.hpp
examples/server/server.cpp
examples/server/utils.hpp

commit 413e7b0559f922bd4de5e9eec548829d111651b1
Author: crasm <crasm@git.vczf.net>
Date:   Fri Jan 26 07:18:00 2024 -0500

    ci : add model tests + script wrapper (#4586)
    
    * scripts : add lib.sh and lib_test.sh
    
    * scripts : stub out new ci-run.sh script
    
    * scripts : switch to PascalCase for functions
    
    This looks a little odd at first, but I find it very useful as a
    convention to know if a command is part of our code vs a builtin.
    
    * scripts : add some fancy conversion from snake_case to PascalCase
    
    * Add venv to ci/run.sh
    
    * Revert scripts work
    
    * scripts : add wrapper script for local use of ci/run.sh
    
    * Simplify .gitignore for tests, clang-tidy fixes
    
    * Label all ctest tests
    
    * ci : ctest uses -L main
    
    * Attempt at writing ctest_with_model
    
    * Update test-model-load-cancel
    
    * ci : add ctest_with_model for debug and release
    
    ggml-ci
    
    * Fix gg_get_model function
    
    ggml-ci
    
    * got stuck on CMake
    
    * Add get_model.cpp to tests/CMakeLists.txt
    
    ggml-ci
    
    * Fix README.md output for ctest_with_model
    
    ggml-ci
    
    * workflows : use `-L main` for all ctest
    
    ggml-ci
    
    * Fixes
    
    * GG_RUN_CTEST_MODELFILE => LLAMACPP_TESTMODELFILE
    * Always show warning rather than failing if model file variable is not
      set
    
    * scripts : update usage text for ci-run.sh

.github/workflows/build.yml
.gitignore
Makefile
ci/run.sh
scripts/ci-run.sh
tests/.gitignore
tests/CMakeLists.txt
tests/get-model.cpp
tests/get-model.h
tests/test-autorelease.cpp
tests/test-model-load-cancel.cpp

commit 6dd3c28c9cd1ef74b49d79f47d668759346a3c6c
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Fri Jan 26 12:16:07 2024 +0000

    metal : remove unused `n_buffers` and `buffers` (#5129)

ggml-metal.m

commit 38b431de232d1b736b5af19b8c7d72f7075a70bc
Author: Riceball LEE <snowyu.lee@gmail.com>
Date:   Fri Jan 26 17:10:28 2024 +0800

    gguf : fix "general.alignment" type in gguf_reader.py (#5136)

gguf-py/gguf/gguf_reader.py

commit aad0b01d7380a7cdfe0dd42307b18c7b6bac9575
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 26 10:52:33 2024 +0200

    readme : update hot topics

README.md

commit 1182cf4d4f6ee383b92695c2e3fe438086dcdba7
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jan 26 09:14:39 2024 +0200

    Another bucket sort (#5109)
    
    * Initial bucket sort
    
    * Bucket sort: slightly better version
    
    * Bucket sort: another minor improvement
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit fe54033b69b83164cabb5f3ed92dc0ff7ea47605
Author: XiaotaoChen <chenxiaotao1234@gmail.com>
Date:   Fri Jan 26 04:14:32 2024 +0800

    readme : add MobileVLM 1.7B/3B to the supported models list (#5107)
    
    Co-authored-by: Chenxiaotao03 <chenxiaotao03@meituan.com>

README.md

commit 5eaf9964fc797d4585c214db32a463d557f3ed33
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Fri Jan 26 05:06:22 2024 +0900

    llama : dynamic temperature sampling (#4972)
    
    * implemented dynamic temperature sampling from koboldcpp
    
    * removed trailing whitespace
    
    * removed unused temp parameter in llama_sample_entropy
    
    * exposed exponent_val in dynamic temp sampler
    
    * added debug check for printf statements
    
    * use nullptr in llama_sample_softmax call during llama_sample_entropy
    
    this avoids counting the time taken stats twice
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * return earlier if there is only 1 candiate (i.e. max_entropy == 0)
    
    * reformat 't' case in llama_sample_queue
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * check for one or zero candidates case in llama_sample_entropy
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

common/sampling.cpp
common/sampling.h
llama.cpp
llama.h

commit d292f4f2047963f558dd516f1baaa71793e9acf2
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Jan 25 14:51:24 2024 -0500

    examples : make pydantic scripts pass mypy and support py3.8 (#5099)

examples/pydantic-models-to-grammar-examples.py
examples/pydantic_models_to_grammar.py

commit 256d1bb0ddce6a0a21f5a7503019bdd5c1933cba
Author: Valentin Konovalov <valle.ketsujin@gmail.com>
Date:   Thu Jan 25 12:05:51 2024 -0500

    android : use release cmake build type by default (#5123)

examples/llama.android/app/build.gradle.kts

commit faa3526a1eba458120987ed8269e5616385a76f4
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 25 17:58:53 2024 +0200

    Fix Q3_K_XS for MoE models (#5113)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit ddc5a5033f948dc7ab0a3a6ec2d914d13c274077
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 25 11:26:17 2024 +0200

    metal : show compile log messages

ggml-metal.m

commit cd4fddb29f81d6a1f6d51a0c016bc6b486d68def
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Wed Jan 24 16:18:15 2024 -0600

    cuda : fix 2-bit quants on amd hip (#5105)
    
    * cuda : fix 2-bit quants on amd hip
    
    * use __low2float intrinsic function for new quants

ggml-cuda.cu

commit c9b316c78fba31e65879a2ec91cbafd341b88cce
Author: Michael Hueschen <m@mhueschen.dev>
Date:   Mon Jan 22 16:44:10 2024 -0700

    nix-shell: use addToSearchPath
    
    thx to @SomeoneSerge for the suggestion!

.devops/nix/package.nix

commit bf63d695b804b1c995c7ae4427a8a86936ea6d25
Author: Michael Hueschen <m@mhueschen.dev>
Date:   Mon Jan 22 03:17:05 2024 -0700

    nix: add cc to devShell LD_LIBRARY_PATH
    
    this fixes the error I encountered when trying to run the convert.py
    script in a venv:
    
    ```
    $ nix develop
    
    [...]$ source .venv/bin/activate
    (.venv)
    [...]$ pip3 install -r requirements.txt
    <... clipped ...>
    [...]$ python3 ./convert.py
    Traceback (most recent call last):
      File "/home/mhueschen/projects-reference/llama.cpp/./convert.py", line 40, in <module>
        from sentencepiece import SentencePieceProcessor
      File "/home/mhueschen/projects-reference/llama.cpp/.venv/lib/python3.11/site-packages/sentencepiece/__init__.py", line 13, in <module>
        from . import _sentencepiece
    ImportError: libstdc++.so.6: cannot open shared object file: No such file or directory
    ```
    
    however, I am not sure this is the cleanest way to address this linker
    issue...

.devops/nix/package.nix

commit 1387ea21178f9f154944013d4dd9764b54c69deb
Author: slaren <slarengh@gmail.com>
Date:   Wed Jan 24 12:48:14 2024 +0100

    llama : pre-allocate input tensors in a separate buffer (#5100)

ggml-alloc.c
llama.cpp

commit 26d607608d794efa56df3bdb6043a2f94c1d632c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 23 15:50:56 2024 +0200

    metal : disable support for MUL_MAT F32 x F16

ggml-metal.m

commit 44879ee885f48ecf4675dd216b373dce0a6f3690
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 23 15:17:20 2024 +0200

    Additional KL-divergence statistics (#5081)
    
    * perplexity: add top-token probability
    
    * perplexity: add additional KL-divergence statistics
    
    * perplexity: a better organized KL-divergence statistics output
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp

commit 9ecdd12e95aee20d6dfaf5f5a0f0ce5ac1fb2747
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jan 23 13:31:56 2024 +0100

    CUDA: more info when no device code (#5088)

ggml-cuda.cu

commit 89758723c75ba594e401f6513751beeba7ca1d28
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 23 14:12:57 2024 +0200

    minor : clean-up some warnings and style (#5094)
    
    * minor : clean-up some warnings and style
    
    ggml-ci
    
    * ggml : add comment

common/common.cpp
examples/llava/clip.cpp
examples/perplexity/perplexity.cpp
ggml.c
ggml.h
llama.cpp

commit 2bed4aa3f37cb4e39e16e9ec7b595a7738fd5faf
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Tue Jan 23 08:11:39 2024 +0100

    devops : add intel oneapi dockerfile (#5068)
    
    Co-authored-by: Xuan Son Nguyen <xuanson.nguyen@snowpack.eu>

.devops/main-intel.Dockerfile
.github/workflows/docker.yml
CMakeLists.txt

commit 125d03a5036a02a983c8e98c2cdc126e061afb8e
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Tue Jan 23 01:51:27 2024 -0500

    llama.vim : added api key support (#5090)
    
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

examples/llama.vim

commit 011e8ec577fd135cbc02993d3ea9840c516d6a1c
Author: slaren <slarengh@gmail.com>
Date:   Mon Jan 22 23:42:41 2024 +0100

    llama : fix not enough space in buffer with Qwen (#5086)

llama.cpp

commit 6f9939d119b2d004c264952eb510bd106455531e
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 22 16:10:14 2024 +0200

    KL-divergence (#5076)
    
    * kl-divergence: be able to save all logits to a file
    
    * Add ability to compute KL-divergence
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

common/common.cpp
common/common.h
examples/perplexity/perplexity.cpp

commit 780e24a22eb595b705cbe8284771e9ceff1c4dd2
Author: Reinforce-II <fate@eastal.com>
Date:   Mon Jan 22 21:15:08 2024 +0800

    ggml : parallelize FP32 conversion when using BLAS (#5045)
    
    * make GGML_TASK_INIT phase can be run in multithread
    
    * multithreaded dequantize in mul_mat when using blas library
    
    * minor fixes
    
    * update outdated comment
    * fix coding style
    
    * simplify code
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c

commit 3ce7e8f8e7ccfce07e5947ac5f1f3f4628cf68ea
Author: XiaotaoChen <chenxiaotao1234@gmail.com>
Date:   Mon Jan 22 21:09:35 2024 +0800

    llava : MobileVLM support (#4954)
    
    * MobileVLM native implementation
    
    * delete depthwise_conv_2d and permute_cpy relative code, replace the two by the existed functions, and opt ldp definition, support LLAMA_PERF option for CMake
    
    * move android script to example/llava directory
    
    * Fix the editor config checks
    
    ---------
    
    Co-authored-by: Chenxiaotao03 <chenxiaotao03@meituan.com>

CMakeLists.txt
examples/llava/MobileVLM-README.md
examples/llava/android/adb_run.sh
examples/llava/android/build_64.sh
examples/llava/clip.cpp
examples/llava/convert-image-encoder-to-gguf.py
ggml.c
ggml.h

commit b2d80e105a59b54822edf7ce7f3ed5f317e96e21
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sun Jan 21 03:41:37 2024 +0000

    flake.nix: add a comment about flakes vs nix

flake.nix

commit 28603cd2833cedd4434f398d847f87fc83546dbb
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sun Jan 21 03:29:38 2024 +0000

    nix: add a comment on the many nixpkgs-with-cuda instances

.devops/nix/nixpkgs-instances.nix

commit 5e97ec91ae3038720a5b15cde4c52d2a53ec2137
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sun Jan 21 03:15:13 2024 +0000

    nix: add a comment about makeScope

.devops/nix/scope.nix

commit 7251870780e2d572dd6f239d7a0bfe438c82fa74
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 17:45:01 2024 +0000

    nix: refactor the cleanSource rules

.devops/nix/package.nix

commit fe8b3c0d4b0d806e8b46660e24eaf4b90b8b385f
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 17:38:32 2024 +0000

    workflows: nix-ci: drop the redundant "paths" filter

.github/workflows/nix-ci.yml

commit f4dd059259d0234913b9e9780e1662811744c09d
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 17:16:54 2024 +0000

    workflows: nix-build-aarch64: rate limit

.github/workflows/nix-ci-aarch64.yml

commit f7276f7500f7ea588836dd1fc6f126334c517878
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 17:10:19 2024 +0000

    workflows: nix-ci: rebuild on flake.lock updates

.github/workflows/nix-ci.yml

commit 15bceec2d73d4166340b46b27677c45ac1b4cdad
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 22 14:18:43 2024 +0200

    imatrix : keep intermediate imatrix results (#5077)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/imatrix/imatrix.cpp

commit d6bd4d46ddb6926087c11e0f6633ab1c81da58c3
Author: compilade <113953597+compilade@users.noreply.github.com>
Date:   Mon Jan 22 06:21:52 2024 -0500

    llama : support StableLM 2 1.6B (#5052)
    
    * llama : support StableLM 2 1.6B
    
    * convert : fix Qwen's set_vocab wrongly naming all special tokens [PAD{id}]
    
    * convert : refactor Qwen's set_vocab to use it for StableLM 2 too
    
    * nix : add tiktoken to llama-python-extra
    
    * convert : use presence of tokenizer.json to determine StableLM tokenizer loader
    
    It's a less arbitrary heuristic than the vocab size.

.devops/nix/package.nix
convert-hf-to-gguf.py
llama.cpp

commit 152d9d05e097e35f1cac21262946d57faec7542a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Jan 22 12:11:01 2024 +0100

    finetune : print sample-start/include-sample-start (#5072)
    
    This commit adds `--sample-start` and `--include-sample-start` to the
    output from the main function in finetune.cpp.
    
    The motivation for this is that even though these are set explicitly by
    the user via the command line, if one forgets to set them then it is
    useful to have their values printed out. Otherwise it is possible to go
    through the whole training process before realizing that the values are
    not what one expected.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/finetune/finetune.cpp

commit 66d575c45c5a370d668f9c3283cdf348e2329fa2
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 22 12:43:33 2024 +0200

    llama : add Q3_K_XS (#5060)
    
    * Add Q3_K_XS - intermediate size between Q2_K and Q3_K_S
    
    * Q3_K_XS: quanize first 1/8 of ffn_down layers with Q4_K
    
    Together with an importance matrix, this brings perplexity
    for LLaMA-v2-70B below the perplexity of the former Q2_K
    with a 800 MB smaller quantized model size.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize/quantize.cpp
llama.cpp
llama.h

commit 57744932c64266359ee905518de7e096c0295d8c
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Mon Jan 22 08:55:05 2024 +0000

    ci : fix Windows CI by updating Intel SDE version (#5053)

.github/workflows/build.yml

commit 3466c6ebcf668cac453f891b493ead19283347a8
Author: Shijie <821898965@qq.com>
Date:   Mon Jan 22 15:33:19 2024 +0800

    llama : add more qwen2 models (#5071)

llama.cpp

commit 504dc37be8446fb09b1ede70300250ad41be32a2
Author: iSma <ismail.senhaji@gmail.com>
Date:   Sun Jan 21 22:37:13 2024 +0100

    Revert LLAMA_NATIVE to OFF in flake.nix (#5066)

.devops/nix/package.nix

commit 05490fad7f7f60ff2bed9ad05cd81b44e82ccde3
Author: kuronekosaiko <EvanChanJ@163.com>
Date:   Mon Jan 22 00:28:14 2024 +0800

    add safetensors support to convert-lora-to-ggml.py (#5062)
    
    * add safetensors support to convert-lora-to-ggml.py
    
    * Update convert-lora-to-ggml.py
    
    Remove white space in line 69.

convert-lora-to-ggml.py

commit 6c5629d4d2d15557c38a0e609b30c1a42abad80d
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Sun Jan 21 15:17:35 2024 +0000

    add `#include <string>` to unicode.h (#5051)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

unicode.h

commit 7dcbe39d36b76389f6c5cd3b151928472b7e22ff
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 21 14:42:44 2024 +0200

    Add ability to evauate multiple choice tasks  (#5047)
    
    * TruthfulQA: 1st attempt, does not look like it is working
    
    The same implementation can be used for HellaSwag as well,
    so I converted a HellaSwag validation dataset to the binary
    format used here and tested with that. The score is only
    around 50, so something is not quite right.
    
    * TruthfulQA: works but the result is bad
    
    I know it works because if I convert the HellaSwag validation
    data to the binary format used in the truthful_qa_score() function
    I get the exact same result as from the hellaswag_score() function.
    But I guess, the questions are tricky and the way I have done
    the combination of question + answer is very likely not the best.
    The TruthfulQA validation dataset contains 817 questions, with
    random chance result around 19%. With this version I get
    29.1% for Mistral-7B and 55.2% for Mistral-7B-Instruct-v0.2.
    The HF leader board results for these two models are
    42.2% and 68.3%, respectively.
    
    * TruthfulQA: fix random sample
    
    * TruthfulQA: prepare tasks in parallel for large test datasets
    
    * Rename truthful_qa to multiple_choice
    
    * Make MSVC happy
    
    I had forgotten that MSVC does not make constexpr's available
    inside a lambda.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

common/common.cpp
common/common.h
examples/perplexity/perplexity.cpp

commit 726c0fa9a2da976e9c5d5c51e185d9dd453fc9e5
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 21 08:01:20 2024 +0200

    Slightly faster imatrix (#5050)
    
    * imatrix: speedup by avoiding unnecessary allocations and copies
    
    * imatrix: add --no-ppl option to skip PPL calculations altogether
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/imatrix/imatrix.cpp

commit 942c0107a7301434c0a5e7da46bc4cf2393aa556
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 21 05:17:27 2024 +0200

    flake.lock: Update (#5054)
    
    Flake lock file updates:
    
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/9b19f5e77dd906cb52dade0b7bd280339d2a1f3d' (2024-01-13)
      → 'github:NixOS/nixpkgs/bbe7d8f876fbbe7c959c90ba2ae2852220573261' (2024-01-19)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit b43ebde3b0ccbc42d9dd782b32e2fd8eb35b43b5
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Jan 20 18:14:18 2024 -0500

    convert : partially revert PR #4818 (#5041)

convert-hf-to-gguf.py
convert-llama-ggml-to-gguf.py
convert-lora-to-ggml.py
convert-persimmon-to-gguf.py
convert.py
mypy.ini

commit 97c1549808d2742d37584a3c9df28154bdf34417
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sat Jan 20 10:08:08 2024 -0500

    perplexity : fix MSVC build after #5020 (#5043)
    
    * perplexity : fix MSVC build after #5020
    
    * try a differerent fix

examples/perplexity/perplexity.cpp

commit 6df465a91d402370bcba6676b19fad85b06ce7e0
Author: slaren <slarengh@gmail.com>
Date:   Sat Jan 20 16:05:49 2024 +0100

    llama : run all KQV ops on the CPU with no KV offload (#5049)
    
    ggml-ci

ggml-backend.c
llama.cpp

commit 77bc1bbd05f0c31cb45773eb5eb59b9ff2b07e1b
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Sat Jan 20 08:11:31 2024 +0000

    cmake : add support for ccache (#5002)
    
    * Added support ccache for speedup recompilation
    
    * cmake : option to disable ccache
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt

commit 48e2b1337257bb77573bf76cfffcff3a5efa8704
Author: adel boussaken <netdur@gmail.com>
Date:   Sat Jan 20 09:05:43 2024 +0100

    Add a dart/flutter binding to README.md (#4882)

README.md

commit cca894f16a5eade15afd07b015e4cb3d8658943f
Author: Kylin <56434533+KyL0N@users.noreply.github.com>
Date:   Sat Jan 20 15:01:46 2024 +0800

    cuda : fix compile error in jetson platform (#4975)
    
    * cuda: fix compile error in jetson platform
    
    * cuda: update comment in ggml-cuda.cu
    
    * cuda: update ggml-cuda.cu comment

ggml-cuda.cu

commit 381ee195721d8e747ee31a60c0751822b3072f02
Author: Uzo Nweke <uzoechi@gmail.com>
Date:   Fri Jan 19 13:20:50 2024 -0500

    finetune : fix ggml_allocr lifetimes (tmp workaround) (#5033)
    
    * Fix issue with alloc causing max_compute_size to be calculated
    
    * remove ggml_allocr_free as suggested in issue #4791

examples/train-text-from-scratch/train-text-from-scratch.cpp

commit a5cacb22b2114fd9adf61c00cbb237384d86bced
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 19 15:24:47 2024 +0200

    imatrix : add README.md

examples/imatrix/README.md

commit 9b75cb2b3ccbed3df2e14c1202168db3e5145095
Author: Shijie <821898965@qq.com>
Date:   Fri Jan 19 19:53:13 2024 +0800

    llama : support upcoming Qwen2 (#5037)

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
llama.cpp

commit de9a147df14e62f54f879d2d15e6c4793107f4fc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 19 13:52:22 2024 +0200

    py : fix flake8 lint

convert-hf-to-gguf.py

commit 7051aacfac0057fa5fac9ea46c55bffc3892d810
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jan 19 11:39:11 2024 +0200

    winogrande: evaluate log-probs in parallel (#5036)
    
    This is a relatively minor performance tweak resulting in
    ~10% speedup on my system.
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp

commit 2b3b999cacc7ad1207c32fbdf3479a19c06e1a34
Author: chiranko <96988916+chiranko@users.noreply.github.com>
Date:   Fri Jan 19 17:07:27 2024 +0800

    llama : add CodeShell support (#5016)
    
    * llama: add codeshell support
    
    * llama.cpp: fix codeshell with NeoX rope
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 993fba81807e55d27b570945af8e416d535eced1
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jan 19 11:02:39 2024 +0200

    perplexity: avoid unnecessary alloocations and logit copies (#5035)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp

commit 8b20858e5e9c44b99b4b31ae9c40b8f20d01d94f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 19 10:45:06 2024 +0200

    perplexity : faster Winogrande via batching (#5024)
    
    * perplexity : faster Winogrande via batching
    
    ggml-ci
    
    * perplexity : remove unused function
    
    * perplexity : only tokenize selected tasks for Winogrande

examples/perplexity/perplexity.cpp

commit 57e2a7a52a819883f40dada8a2edc24ecf48186b
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Thu Jan 18 23:12:15 2024 +0100

    llama : fix falcon arch for tied output embeddings (#4978)
    
    * falcon arch fix for tied output embeddings
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp

commit 9b6ea4263ab45e02ff905bf7a29dc143ca1facc3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 23:36:07 2024 +0200

    cmake : add ggml public headers (#5011)

CMakeLists.txt

commit 821f0a271e7c9ee737945245dd7abfa22cc9b5b0
Author: Xuan Son Nguyen <thichthat@gmail.com>
Date:   Thu Jan 18 21:33:05 2024 +0100

    server : defer tasks when "slot unavailable" (#5018)
    
    * server: defer task when no slot is available
    
    * remove unnecessary log
    
    ---------
    
    Co-authored-by: Xuan Son Nguyen <xuanson.nguyen@snowpack.eu>

examples/server/server.cpp

commit 96d7f56d2918ffde1995dbb32392571deb76d7fc
Author: slaren <slarengh@gmail.com>
Date:   Thu Jan 18 21:12:15 2024 +0100

    llama : fix mlock with no-mmap with Metal (#5025)

llama.cpp

commit 2d5419d08ab1131623e6a1d554607b7663435e87
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 21:45:51 2024 +0200

    imatrix : fix assert for src0 non-cont check

examples/imatrix/imatrix.cpp

commit d391ae9b4919e24624cc963d82162450848beaf4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 20:49:00 2024 +0200

    perplexity : fix winogrande N tasks option

examples/perplexity/perplexity.cpp

commit e9240cdfa06a50c1b5dbafa367cb8cd698e65103
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 20:45:39 2024 +0200

    scripts : add get-winogrande.sh

scripts/get-hellaswag.sh
scripts/get-wikitext-2.sh
scripts/get-winogrande.sh

commit b46757735d30f5c6ed4f20ebeccc684e02d4f3bf
Author: David Sommers <12738+databyte@users.noreply.github.com>
Date:   Thu Jan 18 12:20:59 2024 -0500

    convert.py : fix llama/llama2 conversion due to vocab_size=-1 (#5019)
    
    PR #4818 (merged last week) reintroduced a config check for vocab_size that was addressed in PR #4258 (merged 2023-11-30).
    
    Without the fix, llama2 models can't be converted. The error is:
    
    `ValueError: The model's vocab size is set to -1 in params.json. Please update it manually. Maybe 32000?`

convert.py

commit 3e945cc1e9c06d2001031360e4e303e9548fb02c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 18 19:18:21 2024 +0200

    HellaSwag: speed up by parallelizing log-prob evaluation (#5020)
    
    For Mistral-7B and fp16, time on my system goes down from 536 seconds
    to 423 seconds for the full evaluation dataset (10042 tasks).
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp

commit ad19812cda4062c9f154ef16315df41fbe6a770a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 15:33:01 2024 +0200

    perplexity : faster HellaSwag via batching (#5017)
    
    * perplexity : faster HellaSwag
    
    ggml-ci
    
    * perplexity : clean-up
    
    ggml-ci
    
    * perplexity : no need for decode_helper
    
    ggml-ci
    
    * perplexity : add comments
    
    * perplexity : option to specify max batched tasks via `n_parallel`
    
    * perplexity : remove HellaSwag restruction for n_batch

examples/perplexity/perplexity.cpp

commit 682986a08eb5cb04865d2e713449f17304d266d8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 18 13:46:27 2024 +0200

    Add Winogrande evaluation (#5015)
    
    * winogrande: simple implementation
    
    It doesn't look like it is working - why?
    For Mistral-7B it is barely better than
    random chance (score ~60% for 1267 tasks), while I see
    Mistral-7B scoring 78.4% on the HF leader board.
    1-sigma statistical uncertainty for 1267 tasks is ~1.4,
    so no way the difference is due to statistics.
    
    * winogrande: somewhat better
    
    Score for Mistrali7-B is now 68.9 on the validation set of
    winogrande_debiased. Still far from the reported 78.4, but
    better than what I had before.
    
    * winogrande: improving
    
    Mistral-7B score is now 73.56.
    Still not quite 78.4 but getting there.
    We are also getting a lower score on HellaSwag
    compared to HF leader board, so I'm not expecting
    we will get up to 78.4 anyway.
    
    It looks like it is better to skip the choice word(s)
    when evaluating the average log-likelihood. This kind of
    makes sense because a more common word (in Winogrande this is
    often a name) will have a higher probability without knowing
    about the follow up context, and this will skew the log-likelihood
    towards the more common word. We can only do this if the
    choice words are not last in the sentence.
    
    It also looks like it is better to skip the punctuation at the
    end of the sentence, provided the choice words are not last.
    
    * winogrande: add dataset instructions
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

common/common.cpp
common/common.h
examples/perplexity/perplexity.cpp

commit dcad445d0c83ad49bca1b58cf9c139cfcebee5d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 18 11:44:49 2024 +0200

    scritps : add helper script to get hellaswag data in txt format

scripts/get-hellaswag.sh

commit 1e605f4102c7ea8dc0dff82f5eaa6a71973d549f
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Thu Jan 18 08:47:24 2024 +0000

    metal : fix memory leak, dangling pointer and unused autorel (#5007)
    
    * Metal memory: Small memory leak on init, dangling pointer, and unused autorelease pool in graph compute
    
    * SPM header potential fix
    
    * Reverting symlinks

ggml-metal.m

commit 6b6916b215251e09bd57cdbf870dc8a73345edc2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 20:54:50 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit 38566680cdfe982a495562332c25b9227de9cf8d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:54:56 2024 +0200

    ggml : add IQ2 to test-backend-ops + refactoring (#4990)
    
    * ggml : add IQ2 to test-backend-ops + refactoring
    
    ggml-ci
    
    * cuda : update supports_op for IQ2
    
    ggml-ci
    
    * ci : enable LLAMA_CUBLAS=1 for CUDA nodes
    
    ggml-ci
    
    * cuda : fix out-of-bounds-access in `mul_mat_vec_q`
    
    ggml-ci
    
    * tests : avoid creating RNGs for each Q tensor
    
    ggml-ci
    
    * tests : avoid creating RNGs for each tensor
    
    ggml-ci

ci/run.sh
ggml-backend.c
ggml-cuda.cu
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
tests/test-backend-ops.cpp

commit ba69bbc84ced580fe4fdb0713ca2d95634325b7a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:46:30 2024 +0200

    imatrix : offload to GPU support (#4957)
    
    * backend : add eval callback
    
    ggml-ci
    
    * backend : group nodes in a single compute when user don't need them
    
    * backend : clean-up the implementation
    
    ggml-ci
    
    * simple : do not perform tensor data copy if not needed
    
    * simple : fix
    
    * imatrix : offload to GPU support
    
    * imatrix : fix ggml_mul_mat_id hanlding
    
    ggml-ci
    
    * ci : add imatrix test
    
    ggml-ci
    
    * ci : rearrange output
    
    ggml-ci

ci/run.sh
examples/imatrix/imatrix.cpp
ggml.c
ggml.h

commit 44a1a4a41a4c0b03afaa7d9e06bcbc7cf95aa1e6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:39:41 2024 +0200

    backend : add eval callback (#4935)
    
    * backend : add eval callback
    
    ggml-ci
    
    * backend : group nodes in a single compute when user don't need them
    
    * backend : clean-up the implementation
    
    ggml-ci
    
    * simple : do not perform tensor data copy if not needed
    
    * simple : fix
    
    * simple : no need for ggml_is_contiguous + fix bool parse
    
    * llama : fix callback placement in llama_context_params
    
    * backend : avoid double-ask callback calls
    
    * simple : restore examples, imatrix will serve as a demo

ggml-backend.c
ggml-backend.h
llama.cpp
llama.h

commit c918fe8dca8fa1c4602427e0a4b88e20046f6c34
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:38:39 2024 +0200

    metal : create autorelease pool during library build (#4970)
    
    * metal : create autorelease pool during library build
    
    ggml-ci
    
    * test : simplify
    
    ggml-ci

.gitignore
Makefile
ci/run.sh
ggml-metal.m
tests/CMakeLists.txt
tests/test-autorelease.cpp

commit 0f83e727af0a7cadf90b7ecc1f8e35de1d0880bc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 18:37:36 2024 +0200

    py : fix whitespace

convert.py

commit 4f4bf35f46600441dec2f941e667291eeb9a18d8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 17 15:45:03 2024 +0200

    py : fix missing added_tokens_dict for SPM and BPE vocabs (#4971)
    
    * py : fix missing added_tokens_dict for SPM vocab
    
    * py : pad with unknown tokens when data is missing
    
    ggml-ci
    
    * py : fix BPE vocab conversion
    
    ggml-ci
    
    * py : fix padded dummy tokens (I hope)

convert.py

commit 2b3a665d3917edf393761a24c4835447894df74a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Jan 17 12:36:37 2024 +0200

    llama : use Q4_K for attn_v for Q2_K_S when n_gqa >= 4 (#4996)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit 75632936659772d5b2ce54b0b65319fecbaac2e6
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Wed Jan 17 08:07:24 2024 +0000

    metal : remove unnecessary nil check (#4986)

ggml-metal.m

commit f46c0c1b0ea0bc67e24e4bf026a7e898c1af22a9
Author: David Renshaw <dwrenshaw@gmail.com>
Date:   Wed Jan 17 02:17:50 2024 -0500

    llama : fix copy/paste error in llama_sampling_params comment (#4994)

common/sampling.h

commit 5c999609013a30c06e6fd28be8db5c2074bcc196
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 16 20:59:31 2024 +0200

    py : remove unnecessary hasattr (#4903)

convert-hf-to-gguf.py

commit bee938da74c33f42242c3a1058ac0a0a6eeef531
Author: Philip Taron <philip.taron@gmail.com>
Date:   Tue Jan 16 09:56:21 2024 -0800

    nix: remove nixConfig from flake.nix (#4984)

flake.nix

commit cec8a4847062fbd76253e3b085683f39d91e80d3
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jan 16 18:54:24 2024 +0100

    finetune : add training data file to log message (#4979)
    
    This commit adds the name of the training data file to the log message
    printed when the training data is tokenized.
    
    The motivation for this change is that it can be useful to show which
    file is being tokenized when running the finetune example.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/finetune/finetune.cpp

commit 334a835a1ccc8106a5fa355683a965efb1bfa24b
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jan 16 19:51:26 2024 +0200

    ggml : importance matrix support for legacy quants (#4969)
    
    * imatrix: adding support for legacy quants
    
    * imatrix: guard Q4_0/Q5_0 against ffn_down craziness
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-quants.c
ggml-quants.h
ggml.c
llama.cpp

commit 4feb4b33eeb1756e46084a4db9230b279af1a480
Author: Maximilian Winter <maximilian.winter.91@gmail.com>
Date:   Tue Jan 16 18:41:42 2024 +0100

    examples : add complete parallel function calling example (#4974)

examples/pydantic-models-to-grammar-examples.py

commit 959ef0c0df725c013c7f712eaa7790b8e38a8e20
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 16 19:34:54 2024 +0200

    perplexity : fix kv cache handling for hellaswag (#4981)
    
    ggml-ci

examples/perplexity/perplexity.cpp

commit c37b3474e61d609d43cccc3bde5d559e80e4f5d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 16 19:13:54 2024 +0200

    flake.lock: update flake-parts, flake-parts/nixpkgs-lib, and nixpkgs (#4920)
    
    Flake lock file updates:
    
    • Updated input 'flake-parts':
        'github:hercules-ci/flake-parts/34fed993f1674c8d06d58b37ce1e0fe5eebcb9f5' (2023-12-01)
      → 'github:hercules-ci/flake-parts/07f6395285469419cf9d078f59b5b49993198c00' (2024-01-11)
    • Updated input 'flake-parts/nixpkgs-lib':
        'github:NixOS/nixpkgs/e92039b55bcd58469325ded85d4f58dd5a4eaf58?dir=lib' (2023-11-29)
      → 'github:NixOS/nixpkgs/b0d36bd0a420ecee3bc916c91886caca87c894e9?dir=lib' (2023-12-30)
    • Updated input 'nixpkgs':
        'github:NixOS/nixpkgs/cfc3698c31b1fb9cdcf10f36c9643460264d0ca8' (2023-12-27)
      → 'github:NixOS/nixpkgs/317484b1ead87b9c1b8ac5261a8d2dd748a0492d' (2024-01-08)
    
    Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>

flake.lock

commit 158f8c9e21302114bac3c646f80ea85b52ffa0bd
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Tue Jan 16 17:05:19 2024 +0000

    metal : localized logic in `ggml_metal_graph_compute` (#4924)
    
    * Metal: Localized logic in `ggml_metal_graph_compute`, minor performance improvement
    
    * Whitespace
    
    * Collecting command buffer completions on single thread
    
    * Whitespace
    
    * Reduce diff noise

ggml-metal.h
ggml-metal.m

commit 862f5e41ab1fdf12d6f59455aad3f5dd8258f805
Author: Neuman Vong <neuman.vong@gmail.com>
Date:   Wed Jan 17 00:47:34 2024 +1100

    android : introduce starter project example (#4926)
    
    * Introduce starter project for Android
    
    Based on examples/llama.swiftui.
    
    * Add github workflow
    
    * Set NDK version
    
    * Only build arm64-v8a in CI
    
    * Sync bench code
    
    * Rename CI prop to skip-armeabi-v7a
    
    * Remove unused tests

.github/workflows/build.yml
examples/llama.android/.gitignore
examples/llama.android/README.md
examples/llama.android/app/.gitignore
examples/llama.android/app/build.gradle.kts
examples/llama.android/app/proguard-rules.pro
examples/llama.android/app/src/main/AndroidManifest.xml
examples/llama.android/app/src/main/cpp/CMakeLists.txt
examples/llama.android/app/src/main/cpp/llama-android.cpp
examples/llama.android/app/src/main/java/com/example/llama/Downloadable.kt
examples/llama.android/app/src/main/java/com/example/llama/Llm.kt
examples/llama.android/app/src/main/java/com/example/llama/MainActivity.kt
examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt
examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Color.kt
examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Theme.kt
examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Type.kt
examples/llama.android/app/src/main/res/drawable/ic_launcher_background.xml
examples/llama.android/app/src/main/res/drawable/ic_launcher_foreground.xml
examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher.xml
examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher_round.xml
examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher.webp
examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher_round.webp
examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher.webp
examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher_round.webp
examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher.webp
examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher_round.webp
examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher.webp
examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher_round.webp
examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher.webp
examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher_round.webp
examples/llama.android/app/src/main/res/values/colors.xml
examples/llama.android/app/src/main/res/values/strings.xml
examples/llama.android/app/src/main/res/values/themes.xml
examples/llama.android/app/src/main/res/xml/backup_rules.xml
examples/llama.android/app/src/main/res/xml/data_extraction_rules.xml
examples/llama.android/build.gradle.kts
examples/llama.android/gradle.properties
examples/llama.android/gradle/wrapper/gradle-wrapper.jar
examples/llama.android/gradle/wrapper/gradle-wrapper.properties
examples/llama.android/gradlew
examples/llama.android/settings.gradle.kts

commit 3a48d558a69c88ac17efcaa5900cd9eb19596ac4
Author: Alex Azarov <alex@azarov.by>
Date:   Tue Jan 16 14:41:27 2024 +0100

    metal : replace loop of dispatch_async with dispatch_apply (#4934)
    
    * Replace loop of dispatch_async with dispatch_apply
    
    * Update ggml-metal.m
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.m

commit 7c8d3abd1a17c28fc56b1a4814bc4b29f91d7454
Author: Alex Azarov <alex@azarov.by>
Date:   Tue Jan 16 14:33:02 2024 +0100

    metal : log `recommendedMaxWorkingSetSize` on iOS 16+ (#4936)
    
    * metal: Log `recommendedMaxWorkingSetSize` on iOS 16+
    
    * Only log on iOS and macOS, ignoring tvOS and other platforms
    
    * Check for Xcode version before using recommendedMaxWorkingSetSize
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.m

commit 122ed4840cc6d209df6043e027f9f8a03aee01da
Author: Maximilian Winter <maximilian.winter.91@gmail.com>
Date:   Tue Jan 16 13:10:48 2024 +0100

    examples : fix and improv docs for the grammar generator (#4909)
    
    * Create pydantic-models-to-grammar.py
    
    * Added some comments for usage
    
    * Refactored Grammar Generator
    
    Added example and usage instruction.
    
    * Update pydantic_models_to_grammar.py
    
    * Update pydantic-models-to-grammar-examples.py
    
    * Renamed module and imported it.
    
    * Update pydantic-models-to-grammar.py
    
    * Renamed file and fixed grammar generator issue.
    
    * Fixed some issues and bugs of the grammar generator. Imporved Documentation
    
    * Update pydantic_models_to_grammar.py

examples/pydantic_models_to_grammar.py

commit a0b3ac8c48b66206b9c5921ce57bd5c0ea6557c3
Author: Justine Tunney <jtunney@gmail.com>
Date:   Tue Jan 16 03:16:33 2024 -0800

    ggml : introduce GGML_CALL function annotation (#4850)
    
    This change makes it possible to build ggml-cuda.cu and ggml-metal.m as
    independent dynamic shared objects, that may be conditionally linked at
    runtime in a multiplatform binary. It introduces a GGML_CALL annotation
    that documents which functions have a cyclic call relationship, between
    the application code and GPU modules.
    
    This change does nothing, unless the build defines -DGGML_MULTIPLATFORM
    which causes back-references and function pointers to conform to MS ABI
    which is supported by NVCC, ROCm, XCode, GCC and Clang across platforms

ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-cuda.h
ggml-metal.h
ggml-metal.m
ggml.c
ggml.h

commit d75c232e1da56f19ac4d2530dadbe0ab3a11fde5
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jan 16 12:14:19 2024 +0100

    finetune : use LLAMA_FILE_MAGIC_GGLA (#4961)
    
    This commit replaces the magic number LLAMA_FILE_MAGIC_LORA used in
    finetune.cpp with LLAMA_FILE_MAGIC_GGLA defined in llama.h.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/finetune/finetune.cpp

commit e0324285a569d0583cf2f4a07a2402221ee25f58
Author: stduhpf <stephduh@live.fr>
Date:   Tue Jan 16 12:04:32 2024 +0100

    speculative : threading options (#4959)
    
    * speculative: expose draft threading
    
    * fix usage format
    
    * accept -td and -tbd args
    
    * speculative: revert default behavior when -td is unspecified
    
    * fix trailing whitespace

common/common.cpp
common/common.h
examples/speculative/speculative.cpp

commit 3e5ca7931c68152e4ec18d126e9c832dd84914c8
Author: ngc92 <7938269+ngc92@users.noreply.github.com>
Date:   Mon Jan 15 20:40:48 2024 +0200

    pass cpu-architecture arguments only to host code (C;C++) (#4943)

CMakeLists.txt

commit 4483396751c79dea540808b9cb9238245d06da2b
Author: David Friehs <david@friehs.info>
Date:   Mon Jan 15 14:06:52 2024 +0100

    llama : apply classifier-free guidance to logits directly (#4951)

common/sampling.cpp
llama.cpp
llama.h

commit d9aa4ffa6e0296d42f1f676dd85de97c8491eb73
Author: Victor Z. Peng <ziliangdotme@gmail.com>
Date:   Mon Jan 15 04:41:46 2024 -0800

    awq-py : fix typo in awq-py/README.md (#4947)

awq-py/README.md

commit ddb008d845cd50bb090bf051f570130524042936
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 15 13:27:00 2024 +0200

    cuda : fix dequantize kernel names (#4938)

ggml-cuda.cu

commit 2faaef39799c97a53bec3898141478700da25757
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 15 10:09:38 2024 +0200

    llama : check for 256 divisibility for IQ2_XS, IQ2_XXS (#4950)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit 4a3156de2fac9a8ee4279de7804d4e352dcfe121
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 15 07:48:06 2024 +0200

    CUDA: faster dequantize kernels for Q4_0 and Q4_1 (#4938)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu

commit a836c8f534ab789b02da149fbdaf7735500bff74
Author: David Pflug <david@pflug.email>
Date:   Sun Jan 14 10:46:00 2024 -0500

    llama : fix missing quotes (#4937)

llama.cpp

commit 467a882fd2e5b6172897b49aa45aa29bd3f27685
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 14 16:21:12 2024 +0200

    Add ability to use importance matrix for all k-quants (#4930)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize/quantize.cpp
ggml-quants.c
ggml-quants.h
ggml.c

commit bb0c1392479398f9aba86d9ec98db0b95ede6e6d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 14 13:26:53 2024 +0200

    llama : check LLAMA_TRACE env for extra logging (#4929)
    
    * llama : minor fix indent
    
    * llama : check LLAMA_TRACE env for extra logging
    
    ggml-ci

llama.cpp

commit 9408cfdad6b1c090a7e1419d4434edc260b7e47e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 14 11:08:09 2024 +0200

    scripts : sync-ggml-am.sh option to skip commits

scripts/sync-ggml-am.sh
scripts/sync-ggml.last

commit 03c526749041c863b0cd842b26b8907e1ea0e0b1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 14 11:03:19 2024 +0200

    llama : use LLAMA_LOG_ macros for logging

llama.cpp

commit a128c38de862431f1aae9ccc40b792fbc1b8b682
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 14 10:53:39 2024 +0200

    Fix ffn_down quantization mix for MoE models (#4927)
    
    * Fix ffn_down quantization mix for MoE models
    
    In #4872 I did not consider the part where every third
    tensor is quantized with more bits. Fir MoE this leads to tensors
    of the same layer being quantized with different number of bits,
    which is not considered as a possibility in the inference implementation
    (it is assumed all experts use the same quantization).
    
    * Fix the fix
    
    * Review suggestion
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit 5f5fe1bd608fa2ed42af97b5f2ea31be6625fc48
Author: Alex Azarov <alex@azarov.by>
Date:   Sun Jan 14 09:44:39 2024 +0100

    metal : correctly set SIMD support flags on iOS (#4923)
    
    * Correctly set support_simdgroup_reduction and support_simdgroup_mm on iPhone/iPad
    
    * log a little bit more info on iOS

ggml-metal.m

commit ac32902a87147f78d63c931aa8a23dee762660e7
Author: Karthik Kumar Viswanathan <195178+guilt@users.noreply.github.com>
Date:   Sun Jan 14 00:41:44 2024 -0800

    llama : support WinXP build with MinGW 8.1.0 (#3419)

CMakeLists.txt
llama.cpp

commit 147b17ac94a24d524e367cda26a9ff6245689f34
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 14 09:45:56 2024 +0200

    2-bit quantizations (#4897)
    
    * imatrix: load
    
    * imatrix: WIP
    
    * imatrix: Add Q2_K quantization
    
    * imatrix: also guard against Q2_K_S quantization without importance matrix
    
    * imatrix: guard even more against low-bit quantization misuse
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/benchmark/benchmark-matmult.cpp
examples/quantize/quantize.cpp
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp

commit 807179ec583dcb882f97d9704577c06beb2c5ec9
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jan 14 09:44:30 2024 +0200

    Make Q3_K_S be the same as olf Q3_K_L for Mixtral-8x7B (#4906)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit 76484fbfd355df388f71d6edaa98e1692a74de7e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 14 00:14:46 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit c71d608ce7a1584bf5072f197919dd24f3a6163f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jan 13 21:41:37 2024 +0100

    ggml: cache sin/cos for RoPE (#4908)

ggml.c

commit 4be5ef556de830c5c4f6e45c05ef4427823fe607
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 20:45:45 2024 +0200

    metal : remove old API (#4919)
    
    ggml-ci

Makefile
examples/CMakeLists.txt
examples/metal/CMakeLists.txt
examples/metal/metal.cpp
ggml-metal.h
ggml-metal.m
llama.cpp

commit 0ea069b87bd296c556824e57455433b6c0357340
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 19:31:26 2024 +0200

    server : fix prompt caching with system prompt (#4914)

examples/server/server.cpp

commit f172de03f11465dc6c5a0fc3a22f8ec254c6832c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 18:47:38 2024 +0200

    llama : fix detokenization of non-special added-tokens (#4916)
    
    Co-authored-by: goerch <jhr.walter@t-online.de>

llama.cpp

commit 2d57de525541247132e354f561ff48775fba5d85
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 18:46:37 2024 +0200

    metal : disable log for loaded kernels (#4794)

ggml-metal.m

commit df845cc982e7e2ea7b9900e29d55b15338faa78d
Author: David Friehs <david@friehs.info>
Date:   Sat Jan 13 17:29:43 2024 +0100

    llama : minimize size used for state save/load (#4820)
    
    * examples : save-load-state: save only required state
    
    * llama : only reserve n_vocab * n_batch at most for logits
    
    llama_decode asserts that only n_batch tokens are passed each call, and
    n_ctx is expected to be bigger than n_batch.
    
    * llama : always reserve n_vocab * n_batch for logits
    
    llama_context de-serialization breaks if the contexts have differing
    capacity for logits and llama_decode will at maximum resize to
    n_vocab * n_batch.
    
    * llama : only save and restore used logits
    
    for batch sizes of 512 this reduces save state in the best case by
    around 62 MB, which can be a lot if planning to save on each message
    to allow regenerating messages.
    
    * llama : use ostringstream and istringstream for save and load
    
    * llama : serialize rng into minimum amount of space required
    
    * llama : break session version due to serialization changes

examples/save-load-state/save-load-state.cpp
llama.cpp
llama.h

commit 6b48ed089377330cdb362970a51c1c89b6d857a8
Author: Someone <sergei.kozlukov@aalto.fi>
Date:   Sat Jan 13 16:29:16 2024 +0000

    workflows: unbreak nix-build-aarch64, and split it out (#4915)
    
    The fix should be just the `sudo apt-get update`

.github/workflows/nix-ci-aarch64.yml
.github/workflows/nix-ci.yml

commit 722d33f34ec74c6f7046109f936d0928ffe171bc
Author: Yann Follet <131855179+YannFollet@users.noreply.github.com>
Date:   Sun Jan 14 00:09:08 2024 +0800

    main : add parameter --no-display-prompt (#4541)
    
    * add the parameter : --no-display-prompt , combine with --log-disable it will display only the generated tokens
    
    * remove empty line
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
examples/main/main.cpp

commit c30b1ef39aeba497a943416d2897d69fee055b96
Author: texmex76 <40733439+texmex76@users.noreply.github.com>
Date:   Sat Jan 13 17:06:20 2024 +0100

    gguf : fix potential infinite for-loop (#4600)
    
    Co-authored-by: Bernhard Gstrein <gstrein@informatik.uni-freiburg.de>

ggml.c

commit b38b5e93ae31019e87f692b69d27124eae6aac02
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 18:03:45 2024 +0200

    metal : refactor kernel loading code (#4794)
    
    * metal : detect more GPU families
    
    * metal : refactor kernel loading
    
    * metal : set kernel family requirements
    
    * metal : fix kernel init + fix compile options
    
    * metal : take into account simdgroup reduction support
    
    * metal : print only skipped kernels
    
    * metal : fix check for simdgroup reduction support
    
    * metal : check for Metal 3
    
    * metal : free allocations
    
    * metal : normalize encoder:setComputePipelineStatus calls
    
    ggml-ci
    
    * metal : fix Metal3 family check
    
    ggml-ci
    
    * metal : check for simdgroup matrix mul. feature
    
    ggml-ci

ggml-metal.m

commit 7dc78764e2ff86512e6e31cb0fcb8087df4b4708
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jan 13 15:52:53 2024 +0100

    compare-llama-bench: tweak output format (#4910)

scripts/compare-llama-bench.py

commit 356327feb3f66980ab687040495d722696d98970
Author: Ziad Ben Hadj-Alouane <zied.benhadjalouane@gmail.com>
Date:   Sat Jan 13 09:20:46 2024 -0500

    server : fix deadlock that occurs in multi-prompt scenarios (#4905)
    
    * * fix deadlock
    
    * * dont ruint all whitespace

examples/server/server.cpp

commit ee8243adaa9a9f51ff449213383874e49efe368f
Author: makomk <makosoft@googlemail.com>
Date:   Sat Jan 13 14:16:11 2024 +0000

    server : fix crash with multimodal models without BOS token (#4904)

examples/server/server.cpp

commit 15ebe59210e7fd9817ff67f51fa1a5ee2d004294
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 13 13:44:37 2024 +0200

    convert : update phi-2 to latest HF repo (#4903)
    
    * convert : update phi-2 to latest HF repo
    
    ggml-ci
    
    * py : try to fix flake stuff

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit de473f5f8e19ba5e659cdf5af65fb9251dce16c5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 22:02:43 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit f238461236f4e0e18cac1a554af23c7deadc9b01
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 14:02:30 2024 +0200

    ggml : fix 32-bit ARM compat for IQ2_XS (whisper/1758)
    
    * ggml : fix 32-bit ARM compat
    
    * ggml : fix fix
    
    * ggml : fix fix fix

ggml-quants.c

commit fa5c1fb44a2724292da545d6b7cf2a1ac0e0b989
Author: slaren <slarengh@gmail.com>
Date:   Fri Jan 12 20:38:34 2024 +0100

    backend_sched : fix assignments
    
    ggml-ci

ggml-backend.c

commit 52ee4540c0f2e11d52c839db6eb51d014ce060e1
Author: Maximilian Winter <maximilian.winter.91@gmail.com>
Date:   Fri Jan 12 20:46:45 2024 +0100

    examples : add pydantic models to GBNF grammar generator (#4883)
    
    * Create pydantic-models-to-grammar.py
    
    * Added some comments for usage
    
    * Refactored Grammar Generator
    
    Added example and usage instruction.
    
    * Update pydantic_models_to_grammar.py
    
    * Update pydantic-models-to-grammar-examples.py
    
    * Renamed module and imported it.
    
    * Update pydantic-models-to-grammar.py
    
    * Renamed file and fixed grammar generator issue.

examples/pydantic-models-to-grammar-examples.py
examples/pydantic_models_to_grammar.py

commit 3fe81781e3bf98b8e44946240a19f3a6ad08a11a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jan 12 20:38:54 2024 +0100

    CUDA: faster q8_0 -> f16 dequantization (#4895)

ggml-cuda.cu

commit e7e4df031b9e29d4b55a4e0b0295187f6b213db1
Author: slaren <slarengh@gmail.com>
Date:   Fri Jan 12 20:07:38 2024 +0100

    llama : ggml-backend integration (#4766)
    
    * llama : ggml-backend integration
    
    * ggml-backend : add names to buffers
    
    * fix unmap after loading
    
    * batched-bench : add tensor_split param
    
    * llama : check for null tensor_split
    
    * ggml-backend : increase GGML_MAX_BACKENDS
    
    * improve graph splitting, partial fix for --no-kv-offload
    
    * cuda : add ggml-backend split buffer support
    
    * cuda : do not create buffer types for devices that don't exist (fixes usage without CUDA devices available)
    
    * ggml : fix null backend dereference (#4807)
    
    * ggml : fix null backend dereference
    
    * ggml : also check ggml_backend_is_cpu
    
    * test-backend-ops : check buffer allocation failures
    
    * llama : add cparam (split_mode) and command line argument (--split-mode, -sm) to configure the split mode (none, layer or row)
    
    * ggml : fix mul_mat_id work size
    
    * llama : rewrite session kv load/set without graphs
    
    * minor
    
    * llama : only initialize used backends, free backends on context free
    
    * llama : abort ctx if cuda backend init fails
    
    * llama : rewrite lora with ggml-backend and compute on CPU
    
    ggml-ci
    
    * llama : only map to a backend buffer the region of the file mapping containing the tensors used in the buffer
    
    * opencl : add ggml-backend buffer type
    
    * cuda : only use batched_cublas with batched mat muls (fixes fp16 tg perf)
    
    * llama : on Metal, by default offload the full model
    
    ggml-ci
    
    * metal : page align the data ptr (#4854)
    
    * Apply suggestions from code review
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * cuda : fix split buffer free
    
    * address review comments
    
    * llama-bench : add split-mode parameter
    
    * fix whitespace
    
    * opencl : fix double initialization
    
    * server : add --split-mode parameter
    
    * use async copy and compute to improve multi-gpu performance
    
    ggml-ci
    
    * use async memcpys to copy the graph outputs to the CPU
    
    * fix opencl
    
    * use a host buffer for the cpu compute buffer for faster copies to the gpu
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

common/common.cpp
common/common.h
examples/batched-bench/batched-bench.cpp
examples/llama-bench/llama-bench.cpp
examples/server/server.cpp
ggml-alloc.c
ggml-alloc.h
ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-cuda.h
ggml-impl.h
ggml-metal.m
ggml-opencl.cpp
ggml-opencl.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-backend-ops.cpp

commit 584d674be622fbf1578694ada6e62eebedbfd377
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 20:54:12 2024 +0200

    llama : remove redundant assert for StableLM (#4901)

llama.cpp

commit 930f907d3ece1eb5b0a1ec5e209983a66dcbfa68
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Jan 12 18:54:53 2024 +0100

    export-lora : use LLAMA_FILE_MAGIC_GGLA (#4894)
    
    This commit replaces the magic number used in export-lora.cpp with
    the one defined in llama.h, which is indirectly included via common.h.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/export-lora/export-lora.cpp

commit e790eef21ce659f5c16d59f8a5c8dcf6cde0692a
Author: Zay <95888118+isaiahbjork@users.noreply.github.com>
Date:   Fri Jan 12 05:48:00 2024 -0700

    llama.swiftui : update models layout (#4826)
    
    * Updated Models Layout
    
    - Added a models drawer
    - Added downloading directly from Hugging Face
    - Load custom models from local folder
    - Delete models by swiping left
    
    * trimmed trailing white space
    
    * Updated Models Layout

examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj
examples/llama.swiftui/llama.swiftui/Models/LlamaState.swift
examples/llama.swiftui/llama.swiftui/UI/ContentView.swift
examples/llama.swiftui/llama.swiftui/UI/DownloadButton.swift
examples/llama.swiftui/llama.swiftui/UI/InputButton.swift

commit 5537d9d36bfdb4379555431f574d3d78ce6e7955
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 14:33:21 2024 +0200

    gitignore : imatrix

.gitignore

commit 1b280c9fffd682b6924010a4437f0275f2921fa9
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jan 12 12:30:41 2024 +0100

    CUDA: fix softmax compile for old CUDA versions (#4862)

ggml-cuda.cu

commit 3cabe80630c7eeb57713cd02249053a8cf6894fa
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 13:10:19 2024 +0200

    llama : fix typo "imp_embd" -> "inp_embd"

llama.cpp

commit 4315a94366708828f949f9db89d2a8d99b634459
Author: howlger <eclipse@voormann.de>
Date:   Fri Jan 12 12:05:32 2024 +0100

    common : streamline the formatting of help (#4890)
    
    * common : streamline the formatting of help
    
    - Separate alternative parameters by a comma
    
    - Do not indent `--version` differently
    
    * Update common/common.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp

commit 2d00741e12c5db4a33dfccd1125f5de4adec9a5b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 13:03:38 2024 +0200

    py : fix lint (#4889)

convert-hf-to-gguf.py

commit f445c0e68cf8e1faca0b2aa8dfb9d48231cec301
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 12 13:01:56 2024 +0200

    llama : fix llm_build_k_shift to use correct n_rot (#4889)
    
    * llama : fix llm_build_k_shift to use correct n_rot
    
    ggml-ci
    
    * llama : always use hparams.n_rot for ggml_rope_custom
    
    ggml-ci
    
    * convert : fix persimmon conversion to write correct n_rot

common/common.cpp
convert-hf-to-gguf.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 326b418b59b6d48d854c4461a2303e8ac0a311e6
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jan 12 06:59:57 2024 +0100

    Importance Matrix calculation (#4861)
    
    * imatrix: 1st version
    
    * imatrix: WIP
    
    * Cleanup
    
    * Update examples/imatrix/imatrix.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

Makefile
examples/CMakeLists.txt
examples/imatrix/CMakeLists.txt
examples/imatrix/imatrix.cpp
ggml.c
ggml.h

commit 1d118386fea031f01550f8cd47a5c86296e5333f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 23:23:49 2024 +0200

    server : fix infill when prompt is empty (#4833)

examples/server/server.cpp

commit 7edefbd79cc6dea96640edc54c6b94b2b2496d8b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 22:46:26 2024 +0200

    main : better name for variable n_print (#4874)

common/common.cpp
common/common.h
examples/main/main.cpp

commit 3ca63b4538dfc78aaec88cd2c3e3f8417c1924e3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 22:43:05 2024 +0200

    main : disable token count by default (#4874)

common/common.cpp
common/common.h
examples/main/main.cpp

commit b0377875488b33f7114138687d828da1de61775d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 21:58:28 2024 +0200

    swift : track ggml release branch (#4867)

Package.swift

commit 469e75d0a35b08de549a4fd87f082ca7a8a539ba
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 11 20:43:15 2024 +0100

    llama : restore intended k-quants mixes for MoE models (#4872)
    
    * Restore intended k-quants quantization mixes for MoE models
    
    * Update Q2_K_S values in the quantize tool
    
    Still using LLaMA-v1 PPL values in the quant description
    today does not make much sense. But let's leave this update
    for another PR.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/quantize/quantize.cpp
llama.cpp
llama.h

commit 49662cbed3e95f5976c070b85b9fd53fd577038d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jan 11 20:39:39 2024 +0100

    ggml : SOTA 2-bit quants (add IQ2_XS) (#4856)
    
    * iq2_xs: basics
    
    * iq2_xs: this should have been in the basics
    
    * iq2_xs: CUDA and scalar CPU works
    
    * iq2_xs: WIP Metal
    
    * iq2_xs: Metal now works
    
    * iq2_xs: working, but dog slow, ARM_NEON dot product
    
    * iq2_xs: better ARM_NEON dot product
    
    We are now at 19.5 t/s for TG-128 and 61 t/s for PP-512 when
    running on the CPU.
    
    * iq2_xs: AVX2 dot product - 19.5 t/s
    
    * iq2_xs: faster AVX2 dit product
    
    21.4 t/s for TG-128, 59.2 t/s for PP-512.
    The latter is 2x compared to the previous version.
    
    * iq2_xs: had forgotten to delete iq2-data.h
    
    * Add llama enum for IQ2_XS
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-quantize-fns.cpp

commit 3ba5b8ca8e6181a5c712c5b77595a29f1d3e2b97
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 21:31:31 2024 +0200

    swift : pin ggml commit + remove ggml.h from spm-headers (#4878)
    
    ggml-ci

Package.swift
spm-headers/ggml.h

commit 4330bd83feb39683de4bd7a34cfcf672ff8ac3e4
Author: Laura <Tijntje_7@msn.com>
Date:   Thu Jan 11 19:02:48 2024 +0100

    server : implement credentialed CORS (#4514)
    
    * Implement credentialed CORS according to MDN
    
    * Fix syntax error
    
    * Move validate_api_key up so it is defined before its first usage

examples/server/server.cpp

commit 27379455c38cb13f24de92dbd6fcdd04eeb1b9d9
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Thu Jan 11 12:51:17 2024 -0500

    server : support for multiple api keys (#4864)
    
    * server: added support for multiple api keys, added loading api keys from file
    
    * minor: fix whitespace
    
    * added file error handling to --api-key-file, changed code to better
    reflect current style
    
    * server: update README.md for --api-key-file
    
    ---------
    
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

examples/server/README.md
examples/server/server.cpp

commit eab67950068e4b125007d027232c47d2a5831cd0
Author: Behnam M <58621210+ibehnam@users.noreply.github.com>
Date:   Thu Jan 11 12:41:39 2024 -0500

    server : add `LOG_INFO` when model is successfully loaded (#4881)
    
    * added /health endpoint to the server
    
    * added comments on the additional /health endpoint
    
    * Better handling of server state
    
    When the model is being loaded, the server state is `LOADING_MODEL`. If model-loading fails, the server state becomes `ERROR`, otherwise it becomes `READY`. The `/health` endpoint provides more granular messages now according to the server_state value.
    
    * initialized server_state
    
    * fixed a typo
    
    * starting http server before initializing the model
    
    * Update server.cpp
    
    * Update server.cpp
    
    * fixes
    
    * fixes
    
    * fixes
    
    * made ServerState atomic and turned two-line spaces into one-line
    
    * updated `server` readme to document the `/health` endpoint too
    
    * used LOG_INFO after successful model loading

examples/server/server.cpp

commit d8d90aa343c22fe01429d3540e47ded87e9dcb9d
Author: Someone <sergei.kozlukov@aalto.fi>
Date:   Thu Jan 11 17:22:34 2024 +0000

    ci: nix-flake-update: new token with pr permissions (#4879)
    
    * ci: nix-flake-update: new token with pr permissions
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/nix-flake-update.yml

commit 43f76bf1c362c067fce46bb8dcda0b64af8a9533
Author: pudepiedj <pudepiedj@gmail.com>
Date:   Thu Jan 11 16:14:52 2024 +0000

    main : print total token count and tokens consumed so far (#4874)
    
    * Token count changes
    
    * Add show token count
    
    * Updating before PR
    
    * Two requested changes
    
    * Move param def posn

common/common.cpp
common/common.h
examples/main/main.cpp
llama.cpp

commit 2f043328e3116724d15b915b5c6078e2df860a69
Author: Isaac McFadyen <isaac@imcf.me>
Date:   Thu Jan 11 09:33:26 2024 -0500

    server : fix typo in model name (#4876)

examples/server/server.cpp

commit 2a7c94db5fb67b2f8882d2d16a11bf5d8d12d397
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Thu Jan 11 14:31:52 2024 +0000

    metal : put encoder debug group behind a define (#4873)

ggml-metal.m

commit 64802ec00d6383784a9dacf616095eaced16c3c3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 09:39:08 2024 +0200

    sync : ggml

scripts/sync-ggml.last

commit 3267c2abc72e34608224408ace3c048831050f97
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 09:34:59 2024 +0200

    metal : fix deprecation warning (ggml/690)

ggml-metal.m

commit f85a973aa139ae6f37e8b8e1966f1d278b5e0372
Author: Timothy Cronin <40186632+4imothy@users.noreply.github.com>
Date:   Thu Jan 11 02:27:48 2024 -0500

    ggml : remove ggml_cpy_inplace and ggml_cont_inplace (ggml/693)

ggml.c
ggml.h

commit 5362e43962e84d61e20b91f34991d7ccaef4a7d5
Author: Jack Mousseau <jmousseau@users.noreply.github.com>
Date:   Wed Jan 10 06:19:19 2024 -0800

    metal : wrap each operation in debug group (ggml/690)

ggml-metal.m

commit e739de790921e6abbc8c70398303cacd74913f61
Author: leejet <leejet714@gmail.com>
Date:   Wed Jan 10 21:13:42 2024 +0800

    ggml : change GGML_MAX_NAME at compile time (ggml/682)
    
    * change GGML_MAX_NAME to 128
    
    * allow controlling the value of GGML_MAX_NAME through external macro definitions

ggml.h

commit c910e3c28a1caee8cb1398143d582dd9ab697e68
Author: Halalaluyafail3 <55773281+Halalaluyafail3@users.noreply.github.com>
Date:   Tue Jan 9 11:16:37 2024 -0500

    Fix execlp call (ggml/689)
    
    NULL can be an integer constant expression with the value zero, in this case the behavior would be undefined because of an incorrect type being passed to the variable arguments.

ggml.c

commit f34432ca1e0b288129390c1db8296a82aaf1e632
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Fri Jan 5 16:00:00 2024 +0100

    fix : cuda order of synchronization when setting a buffer (ggml/679)
    
    * fix : cuda order of synchronization when setting a buffer
    
    * also sync before memcpy
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu

commit 7a9f75c38b5e62fe27b8a5a3ed823b4a3714024b
Author: Behnam M <58621210+ibehnam@users.noreply.github.com>
Date:   Thu Jan 11 02:12:05 2024 -0500

    server : update readme to document the new `/health` endpoint (#4866)
    
    * added /health endpoint to the server
    
    * added comments on the additional /health endpoint
    
    * Better handling of server state
    
    When the model is being loaded, the server state is `LOADING_MODEL`. If model-loading fails, the server state becomes `ERROR`, otherwise it becomes `READY`. The `/health` endpoint provides more granular messages now according to the server_state value.
    
    * initialized server_state
    
    * fixed a typo
    
    * starting http server before initializing the model
    
    * Update server.cpp
    
    * Update server.cpp
    
    * fixes
    
    * fixes
    
    * fixes
    
    * made ServerState atomic and turned two-line spaces into one-line
    
    * updated `server` readme to document the `/health` endpoint too

examples/server/README.md

commit 5c1980d8d4c4e0c0af77359f81cc44d90b3f250b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 11 09:10:34 2024 +0200

    server : fix build + rename enums (#4870)

examples/server/server.cpp

commit cd108e641dbdedd8c5641c4cec1762f751f38136
Author: Behnam M <58621210+ibehnam@users.noreply.github.com>
Date:   Wed Jan 10 14:56:05 2024 -0500

    server : add a `/health` endpoint (#4860)
    
    * added /health endpoint to the server
    
    * added comments on the additional /health endpoint
    
    * Better handling of server state
    
    When the model is being loaded, the server state is `LOADING_MODEL`. If model-loading fails, the server state becomes `ERROR`, otherwise it becomes `READY`. The `/health` endpoint provides more granular messages now according to the server_state value.
    
    * initialized server_state
    
    * fixed a typo
    
    * starting http server before initializing the model
    
    * Update server.cpp
    
    * Update server.cpp
    
    * fixes
    
    * fixes
    
    * fixes
    
    * made ServerState atomic and turned two-line spaces into one-line

examples/server/server.cpp

commit 57d016ba2d46a6e22517a31a75cebb48f9e234b6
Author: Brian <mofosyne@gmail.com>
Date:   Thu Jan 11 01:09:53 2024 +1100

    llama : add additional suffixes for model params (#4834)
    
    * llm_load_print_meta: Add additional suffixs for model params
    
    * Update llama.cpp model param log
    
    remove unneeded comments and convert from > to >=

llama.cpp

commit 329ff615699d32f596d4ebf8baba654c30064e0d
Author: Austin <77757836+teleprint-me@users.noreply.github.com>
Date:   Wed Jan 10 08:39:09 2024 -0500

    llama : recognize 1B phi models (#4847)
    
    This update categorizes models with 24 layers as MODEL_1B, ensuring compatibility with different Phi model variants without impacting existing Phi-2 model functionality.

llama.cpp

commit d34633d8db6c2e400355de4862cd699154ecc73f
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Wed Jan 10 14:37:09 2024 +0100

    clip : support more quantization types (#4846)
    
    Uses ggml functions instead of hardcoded names and adds support to quantize into the modern Q-K variants.
    This is just the bare minimum to get k-types working - a more refined choice of types would be needed to get best quality on low quantizations.
    
    I ran a few tests, it doesn't break anything I could notice and a Q6_K ViT works almost as well as Q8_0 but 3 times the inference speed.

examples/llava/clip.cpp

commit 4f56458d34cb13dcbf69aca650e9bf77d5497e6f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jan 10 01:04:33 2024 +0100

    Python script to compare commits with llama-bench (#4844)

scripts/compare-llama-bench.py

commit 6efb8eb30e7025b168f3fda3ff83b9b386428ad6
Author: Austin <77757836+teleprint-me@users.noreply.github.com>
Date:   Tue Jan 9 13:46:46 2024 -0500

    convert.py : fix vanilla LLaMA model conversion (#4818)
    
    * Update Imports and Add Notes for Future Reference
    
    - Updated import statements in `convert.py`.
    - Added import for `AutoTokenizer` from `transformers` module.
    - Added conditional import for `gguf` from the local directory.
    - Added comments and notes for future reference.
    
    Additional Notes:
    
    - Noted removal of a redundant `TypeAlias` import.
    - Noted the removal of a `gguf` debug statement.
    - Commented on the presence of `ARCH` and `NDArray` definitions.
    - Commented on cleaning up and refactoring data type definitions.
    
    * Refine Model Hyperparameters and Params Class
    
    - Updated type annotations to use `Optional` for clarity.
    - Improved method names and attribute consistency.
    - Removed unnecessary variables for better code readability.
    
    Additional Notes:
    
    - Highlighted the use of `Optional` for clearer intent.
    - Ensured backward and forward compatibility.
    
    * Restore BpeVocab and SentencePieceVocab classes
    
    - Restored the BpeVocab class for handling BPE tokenization.
    - Restored the SentencePieceVocab class for SentencePiece tokenization.
    
    These classes are essential for maintaining the original behavior of the codebase.
    
    * refactor: Standardize vocabulary handling with HfVocab
    
    - Replaced VocabLoader with HfVocab, aligning vocabulary handling across classes.
    - Updated initialization of HfVocab with local_files_only=True for AutoTokenizer.
    - Introduced optional parameter fname_added_tokens for flexible added token management.
    - Streamlined added token handling for clarity and conciseness.
    - Maintained special tokens and IDs, enhancing token management.
    - Simplified token processing methods for improved readability.
    - Added a placeholder for score computation with a default value of -1000.0.
    - Optimized newline token check for efficiency.
    - Updated __repr__ function for clarity in representation.
    - Adjusted type alias Vocab to include BpeVocab, SentencePieceVocab, and HfVocab.
    - Removed redundant code related to special token handling, reverse vocabulary mapping, and vocabulary file detection.
    
    This refactoring promotes a standardized and modular approach to vocabulary management, facilitating future integration with a VocabFactory and improving code maintainability and scalability.
    
    * refactor: Enhance readability, functionality, and code quality
    
    - Improved code formatting and readability for better maintainability.
    - Refactored LazyUnpickler's CLASSES dictionary for clarity.
    - Added print statements and warnings in check_vocab_size for user feedback.
    - Removed find_vocab_file_path, as it's superseded by VocabFactory.
    - Preparatory changes for upcoming classes: OutputFile and VocabFactory.
    - Overall focus on code quality, error handling, and consistency.
    
    These changes reflect a continuous effort to refine the codebase, ensuring it meets best practices and prepares for future enhancements, such as the VocabFactory.
    
    * refactor: Update OutputFile class for enhanced model vocabulary management
    
    - Restructured the constructor for improved readability.
    - Updated `add_meta_arch` method for flexible model name determination.
    - Introduced `handle_tokenizer_model` for mapping vocab types to supported tokenizer models.
    - Streamlined vocabulary extraction with `extract_vocabulary_from_model`.
    - Simplified vocabulary metadata addition using `add_meta_vocab`.
    - Refactored `add_tensor_info` for clarity and consistency.
    - Improved error handling for better user feedback.
    
    These changes signify the development of a versatile and comprehensive `OutputFile` class, enabling efficient management of model conversion output, metadata, vocabulary, and tensor information.
    
    * feat: Introduce VocabFactory for flexible vocabulary management in model conversion
    
    - The VocabFactory class is added to facilitate modular vocabulary handling.
    - The constructor initializes a directory path and detects vocabulary-related files.
    - The _select_file method provides file paths based on vocabulary type (e.g., BPE, SentencePiece).
    - _create_special_vocab generates special vocabularies, accommodating different types.
    - The load_vocab method loads vocabularies, handling BPE, SentencePiece, and Hugging Face Fast Tokenizer.
    - Error handling and logging enhance debugging and user feedback.
    - The modular and flexible design simplifies vocabulary management and supports future extensions.
    
    The VocabFactory class enhances code modularity and maintainability, allowing versatile vocabulary handling in the model conversion process.
    
    * refactor: Improve code organization, argument parsing, and user interface
    
    - Renamed 'default_outfile' to 'default_output_file' for clarity.
    - Refactored argument parser setup into 'get_argument_parser' function.
    - Introduced descriptive comments for each argument in the parser.
    - Added '--vocab-type' argument with choices ["spm", "bpe", "hfft"] for vocabulary processing.
    - Improved flag naming consistency: '--outfile' to '--out-file' and '--bigendian' to '--big-endian'.
    - Enhanced error handling to prevent overwriting input data in 'default_output_file'.
    - Made 'argv' in 'main' an optional parameter for flexibility.
    - Introduced dynamic import for 'awq.apply_awq' based on 'args.awq_path' for conditional dependency.
    
    These changes enhance code clarity, organization, and the user interface of the script, aligning it with Python best practices and improving maintainability.
    
    * refactor: Further refine functionality, improve user interaction, and streamline vocabulary handling
    
    - Renamed command-line arguments for clarity and consistency.
    - Improved path resolution and import adjustments for robustness.
    - Thoughtfully handled 'awq-path' and conditional logic for the weighted model.
    - Enhanced model and vocabulary loading with the 'VocabFactory' class for structured and adaptable loading.
    - Strengthened error handling and user feedback for a more user-friendly experience.
    - Structured output file handling with clear conditions and defaults.
    - Streamlined and organized the 'main' function for better logic flow.
    - Passed 'sys.argv[1:]' to 'main' for adaptability and testability.
    
    These changes solidify the script's functionality, making it more robust, user-friendly, and adaptable. The use of the 'VocabFactory' class is a notable enhancement in efficient vocabulary handling, reflecting a thoughtful and iterative approach to script development.
    
    * chore: Apply ruff formatting to convert.py
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    
    * Revert to commit 0614c33
    
    * chore: Apply flake8 formatting rules
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    
    * refactor: Revise `check_vocab_size` for Enhanced Clarity and Correctness
    
    - Resolved an unreachable branch issue by reorganizing the conditional structure.
    - Moved the special case check for `params.n_vocab == -1` to the top for immediate assertion.
    - Flattened the conditional logic for improved clarity and predictability of the function's behavior.
    
    These changes enhance the readability and functional correctness of the `check_vocab_size` function without altering its intended functionality.
    
    * py : fix outfile and outtype
    
    * py : suggest hint for missing vocab size
    
    ---------
    
    Signed-off-by: teleprint-me <77757836+teleprint-me@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert.py

commit 36e5a08b203542dca53cca4eaf172c5dc4bbc991
Author: Justine Tunney <jtunney@gmail.com>
Date:   Tue Jan 9 09:59:14 2024 -0800

    llava-cli : don't crash if --image flag is invalid (#4835)
    
    This change fixes an issue where supplying `--image missing-file` would
    result in a segfault due to a null pointer being dereferenced. This can
    result in distracting info being printed if robust crash analysis tools
    are being used.

examples/llava/llava-cli.cpp

commit 4dccb38d9abab7f9f2d1f9a6977df4185d490132
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 9 19:37:08 2024 +0200

    metal : improve dequantize precision to match CPU (#4836)
    
    ggml-ci

ggml-metal.metal

commit 9a818f7c42761984ac99e08e613cc20634f8410e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 9 19:20:45 2024 +0200

    scripts : improve get-pg.sh (#4838)

scripts/get-pg.sh

commit 18adb4e9bb340b7b4565d8b6715b4449283e7641
Author: iohub <rickyang.pro@gmail.com>
Date:   Wed Jan 10 00:45:54 2024 +0800

    readme : add 3rd party collama reference to UI list (#4840)
    
    Add a VSCode extension for llama.cpp reference to UI list

README.md

commit d9653894dffbfd3a58616f31b0967b34faf6f611
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 9 16:23:05 2024 +0200

    scripts : script to get Paul Graham essays in txt format (#4838)

scripts/get-pg.sh

commit 128de3585b0f58b1e562733448fc00109f23a95d
Author: Behnam M <58621210+ibehnam@users.noreply.github.com>
Date:   Tue Jan 9 05:02:05 2024 -0500

    server : update readme about token probs (#4777)
    
    * updated server readme to reflect the gg/server-token-probs-4088 commit
    
    added explanation for the API's completion result which now includes `completion_probabilities`. Also added a JSON schema that shows the type/structure of `completion_probabilities`.
    
    * simplified the `completion_probabilities` JSON schema
    
    It's now easier to understand what the structure of `completion_probabilities` looks like.
    
    * minor : fix trailing whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/README.md

commit 8c5833031857c9e9ada61948bae894ab9c785f86
Author: Zsapi <martin1.zsapka@gmail.com>
Date:   Tue Jan 9 10:12:43 2024 +0100

    server : add api-key flag to documentation (#4832)
    
    Document the api-key flag added to server in https://github.com/ggerganov/llama.cpp/pull/4441

examples/server/README.md

commit 18c2e1752c3b387689e9e73d7d8a1a3b1511ce23
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 9 10:42:06 2024 +0200

    ggml : fix vld1q_s8_x4 32-bit compat (#4828)
    
    * ggml : fix vld1q_s8_x4 32-bit compat
    
    ggml-ci
    
    * ggml : fix 32-bit ARM compat (cont)
    
    ggml-ci

ggml-quants.c

commit 8f900abfc09851e281bc9027e0ab2f16bf079b29
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jan 9 08:58:55 2024 +0100

    CUDA: faster softmax via shared memory + fp16 math (#4742)

ggml-cuda.cu
tests/test-backend-ops.cpp

commit 1fc2f265ff9377a37fd2c61eae9cd813a3491bea
Author: howlger <eclipse@voormann.de>
Date:   Mon Jan 8 20:05:53 2024 +0100

    common : fix the short form of `--grp-attn-w`, not `-gat` (#4825)
    
    See https://github.com/ggerganov/llama.cpp/blob/master/common/common.cpp#L230C53-L230C57

common/common.cpp

commit a9a8c5de3d2028701c239d821b220214fcaefbf1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 20:25:17 2024 +0200

    readme : add link to SOTA models

README.md

commit dd5ae06405c5565b99889bdb3f168f4351252cfb
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jan 8 16:02:32 2024 +0100

    SOTA 2-bit quants (#4773)
    
    * iq2_xxs: basics
    
    * iq2_xxs: scalar and AVX2 dot products
    
    Needed to change Q8_K to have quants in the -127...127 range,
    else the IQ2_XXS AVX implementation becomes very awkward.
    The alternative would have been to use Q8_0 instead. Perhaps
    I'll change later, for now this is what we have.
    
    * iq2_xxs: ARM_NEON dot product
    
    Somehow strangely slow (112 ms/token).
    
    * iq2_xxs: WIP Metal
    
    Dequantize works, something is still wrong with the
    dot product.
    
    * iq2_xxs: Metal dot product now works
    
    We have
    PP-512 = 475 t/s
    TG-128 = 47.3 t/s
    
    Not the greatest performance, but not complete garbage either.
    
    * iq2_xxs: slighty faster dot product
    
    TG-128 is now 48.4 t/s
    
    * iq2_xxs: slighty faster dot product
    
    TG-128 is now 50.9 t/s
    
    * iq2_xxs: even faster Metal dot product
    
    TG-128 is now 54.1 t/s.
    
    Strangely enough, putting the signs lookup table
    into shared memory has a bigger impact than the
    grid values being in shared memory.
    
    * iq2_xxs: dequantize CUDA kernel - fix conflict with master
    
    * iq2_xxs: quantized CUDA dot product (MMVQ)
    
    We get TG-128 = 153.1 t/s
    
    * iq2_xxs: slightly faster CUDA dot product
    
    TG-128 is now at 155.1 t/s.
    
    * iq2_xxs: add to llama ftype enum
    
    * iq2_xxs: fix MoE on Metal
    
    * Fix missing MMQ ops when on hipBLAS
    
    I had put the ggml_supports_mmq call at the wrong place.
    
    * Fix bug in qequantize_row_iq2_xxs
    
    The 0.25f factor was missing.
    Great detective work by @ggerganov!
    
    * Fixing tests
    
    * PR suggestion
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-quantize-fns.cpp

commit 668b31fc7d86245435ad6574e0e1126e734049e2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 16:40:51 2024 +0200

    swift : exclude ggml-metal.metal from the package (#4822)

Package.swift

commit 42ea63c5a3da01d4a94e906d8565868012c79f4f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 15:57:36 2024 +0200

    llama.swiftui : update readme

examples/llama.swiftui/README.md

commit 52531fdff88764282c1b233174721aab8347252d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 11:18:32 2024 +0200

    main : add self-extend support (#4815)
    
    * examples : add passkey test
    
    * passkey : better prints
    
    * passkey : select pass key pos from CLI
    
    * passkey : simplify n_past logic
    
    * llama : "self-extend"-like context extension
    
    * passkey : add comment
    
    * main : add Self-Extend support
    
    * llama : add comment about llama_kv_cache_seq_div

common/common.cpp
common/common.h
examples/main/main.cpp
llama.h

commit b0034d93ce2949ce7d9c098ca02e56f66cd484e2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jan 8 11:14:04 2024 +0200

    examples : add passkey test (#3856)
    
    * examples : add passkey test
    
    * passkey : better prints
    
    * passkey : select pass key pos from CLI
    
    * passkey : simplify n_past logic
    
    * make : add passkey target
    
    * passkey : add "self-extend"-like context extension (#4810)
    
    * llama : "self-extend"-like context extension
    
    * passkey : add comment
    
    * passkey : add readme

.gitignore
Makefile
examples/CMakeLists.txt
examples/batched/batched.cpp
examples/passkey/CMakeLists.txt
examples/passkey/README.md
examples/passkey/passkey.cpp
llama.cpp
llama.h

commit b7e7982953f80a656e03feb5cfb17a17a173eb26
Author: Lars Grammel <lars.grammel@gmail.com>
Date:   Sun Jan 7 21:24:11 2024 +0100

    readme : add lgrammel/modelfusion JS/TS client for llama.cpp (#4814)

README.md

commit 226460cc0d5b185bc6685fb76f418fd9418d7add
Author: slaren <slarengh@gmail.com>
Date:   Sun Jan 7 17:59:01 2024 +0100

    llama-bench : add no-kv-offload parameter (#4812)

examples/llama-bench/llama-bench.cpp

commit d5a410e8556191672465f7ff58682ea2474038b0
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jan 7 17:24:08 2024 +0100

    CUDA: fixed redundant value dequantization (#4809)

ggml-cuda.cu

commit 9dede37d812604897496dd9d276ae9fbe13d1042
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 7 14:29:36 2024 +0200

    llama : remove unused vars (#4796)

llama.cpp

commit 3c36213df850a2353e95572b3636797c79b7c815
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 7 11:21:53 2024 +0200

    llama : remove redundant GQA check (#4796)

llama.cpp

commit 72d8407b3696dd1293bd233b6db392be108bc377
Author: Alex Azarov <alexander.azarov@mapbox.com>
Date:   Sun Jan 7 09:20:50 2024 +0100

    llama.swiftui : use llama.cpp as SPM package (#4804)

examples/llama.swiftui/llama.cpp.swift/LibLlama.swift
examples/llama.swiftui/llama.cpp.swift/bridging-header.h
examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj
examples/llama.swiftui/llama.swiftui/Assets.xcassets/AccentColor.colorset/Contents.json
examples/llama.swiftui/llama.swiftui/Preview Content/Preview Assets.xcassets/Contents.json

commit d117d4dc5dadb46831036bfa4d6e5e8c86babaf1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 7 09:50:31 2024 +0200

    llama : print tensor meta for debugging

llama.cpp

commit 3418c03ecc149fd657527ebb06776239b60a3f3b
Author: Alex Azarov <alexander.azarov@mapbox.com>
Date:   Sun Jan 7 08:46:55 2024 +0100

    llama.swiftui : add visionOS target (#4805)

examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj

commit 63ee677efd92060b14894b984597c62e3742b8da
Author: Konstantin Zhuravlyov <konstantin.zhuravlyov@amd.com>
Date:   Sun Jan 7 01:52:42 2024 -0500

    ggml : use __builtin_amdgcn_sudot4 in __dp4a for gfx11 (#4787)

ggml-cuda.cu

commit 67984921a70a7e680a24494aeb7575a66e90685d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jan 7 08:45:26 2024 +0200

    server : fix n_predict check (#4798)

examples/server/server.cpp

commit c75ca5d96f902564cbbbdd7f5cade80d53c288bb
Author: Daniel Illescas Romero <illescas.daniel@protonmail.com>
Date:   Sat Jan 6 16:12:59 2024 +0100

    llama.swiftui : use correct pointer for llama_token_eos (#4797)

examples/llama.swiftui/llama.cpp.swift/LibLlama.swift

commit 96e80dabc6e73ff68b09b68947b1fc25883c5094
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jan 6 11:40:24 2024 +0200

    examples : improve base-translate.sh script (#4783)

examples/base-translate.sh

commit eec22a1c6378d9a013943cbddb4330c0da621442
Author: a-n-n-a-l-e-e <150648636+a-n-n-a-l-e-e@users.noreply.github.com>
Date:   Fri Jan 5 08:04:40 2024 -0800

    cmake : check for openblas64 (#4134)
    
    openblas v0.3.22 64-bit pkg-config file is named openblas64.pc
    https://github.com/OpenMathLib/OpenBLAS/issues/3790

CMakeLists.txt

commit be36bb946a6336238e92706464de6a30495fe825
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Sat Jan 6 01:02:44 2024 +0900

    flake.nix : fix typo (#4700)
    
    betwen -> between

.devops/nix/package.nix

commit 91d38876dfa10332359ac671b62353aeceb448d3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 5 16:30:52 2024 +0200

    metal : switch back to default.metallib (ggml/681)
    
    ggml-ci

CMakeLists.txt
examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj
ggml-metal.m
scripts/sync-ggml.last

commit d061bf9405cc5fd50792fb2dbdff9c9ea53d6bf9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 5 15:36:04 2024 +0200

    ggml : fix q2_k bpw in comments (ggml/680)

ggml-quants.h

commit 1bf681f90ef4cf37b36e6d604d3e30fc57eda650
Author: Finn Voorhees <finnvoorhees@gmail.com>
Date:   Wed Jan 3 08:39:43 2024 -0500

    ggml : add error handling to graph_compute (whisper/1714)

ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-metal.h
ggml-metal.m

commit c1d7cb28d3fed97fbc95fc3c43f0c5e2113e546c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 5 15:18:21 2024 +0200

    ggml : do not sched_yield when calling BLAS (#4761)
    
    * ggml : do not sched_yield when calling BLAS
    
    ggml-ci
    
    * ggml : fix do_yield logic
    
    ggml-ci
    
    * ggml : simplify do_yield logic
    
    ggml-ci

ggml.c

commit 3681f22443d917e7328456b69c276d6927dafeec
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jan 5 15:11:10 2024 +0200

    examples : add few-shot translation example (#4783)

examples/base-translate.sh

commit b3a7c20b5c035250257d2b62851c379b159c899a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Jan 4 20:45:37 2024 +0100

    finetune : remove unused includes (#4756)
    
    This commit removes unused includes from finetune.cpp.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/finetune/finetune.cpp

commit 012cf349aec8ffb47c9def5dc018240fa3721e8b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 4 19:56:33 2024 +0200

    server : send token probs for "stream == false" (#4714)

examples/server/server.cpp

commit a91928014fcf51fe297823fcff0788de4f14206e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jan 4 09:43:23 2024 +0100

    Print backend name on test-backend-ops failure (#4751)

tests/test-backend-ops.cpp

commit 3c0b585561d74a56977cf3a3844535ecc9e37972
Author: singularity <12184989+singularity-s0@users.noreply.github.com>
Date:   Thu Jan 4 16:22:38 2024 +0800

    llama.swiftui : support loading custom model from file picker (#4767)
    
    * swiftui: support load model from file picker
    
    * swiftui: remove trailing whitespace

examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj
examples/llama.swiftui/llama.swiftui/UI/ContentView.swift
examples/llama.swiftui/llama.swiftui/UI/LoadCustomButton.swift

commit e5804313a1edaf00726ed0b96ecced07accbf50c
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Thu Jan 4 03:17:09 2024 -0500

    server : fix options in README.md (#4765)
    
    * fix examples/server/README.md
    
    * minor : fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/README.md

commit dc891b7f7a23158d54f9383790b92c79cc5906c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jan 4 10:12:26 2024 +0200

    ggml : include stdlib.h before intrin.h (#4736)

ggml-impl.h

commit 46cea79e1f32499bb24b9fab12123cd386e96728
Author: singularity <12184989+singularity-s0@users.noreply.github.com>
Date:   Thu Jan 4 15:58:16 2024 +0800

    llama.swiftui : fix build of ggml.metallib (#4754)
    
    * metal: fix metal backend init failure in swiftui
    
    * metal: build ggml.metallib instead of copy src
    
    * llama.swift : remove debug flags from metallib build
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj

commit cb1e2818e0e12ec99f7236ec5d4f3ffd8bcc2f4a
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Jan 3 18:53:40 2024 +0100

    train : fix typo in overlapping-samples help msg (#4758)
    
    This commit fixes a typo in the help message for the
    --overlapping-samples option.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

common/train.cpp

commit ece9a45e8ffb73ad461c792720c2fec28b0137bc
Author: Ashraful Islam <ashraful.meche@gmail.com>
Date:   Wed Jan 3 11:30:02 2024 -0600

    swift : update Package.swift to use ggml as dependency (#4691)
    
    * updates the package.swift to use ggml as dependency
    
    * changes the ggml package url src to ggerganov

Package.swift

commit 7bed7eba359b0fa8e575345dc5467a46b4ba509f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 14:18:46 2024 +0200

    cuda : simplify expression
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu
scripts/sync-ggml.last

commit d55356d3baa58a6c3a9171cb67a67094b9aa9dff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 13:01:44 2024 +0200

    cuda : mark I16 and I32 ops as unsupported
    
    ggml-ci

ggml-cuda.cu

commit 75e3fd85814c367b55aea11e7bb38cb7b82c6aa0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 11:37:44 2024 +0200

    sync : ggml
    
    ggml-ci

scripts/sync-ggml.last

commit 289313716ff7ccf6aee284f686a0fe8cbc7714af
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 11:35:46 2024 +0200

    metal : add kernel_get_rows_i32
    
    ggml-ci

ggml-metal.m
ggml-metal.metal

commit ab62fc3e5520f5a143c36cb23c269f11aa4dafd6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jan 3 11:25:54 2024 +0200

    scripts : fix sync order + metal sed

scripts/sync-ggml-am.sh

commit 5f66ebca9c41a17385341da4b553a8eb5f07edee
Author: Guillaume Wenzek <gwenzek@users.noreply.github.com>
Date:   Fri Dec 29 18:07:03 2023 +0100

    ggml : extend ggml_get_rows, ggml_repeat, ggml_concat (ggml/639)
    
    * add more int ops
    
    * ggml_compute_forward_dup_bytes
    
    * add tests
    
    * PR comments
    
    * tests : minor indentations
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c
tests/test-backend-ops.cpp

commit f2eb19bd8bc9f5730d6e05d7a52a9e19bf5ac099
Author: Justin Parker <jparkerweb@gmail.com>
Date:   Wed Jan 3 03:43:19 2024 -0500

    server : throw an error when `slot unavailable` (#4741)

examples/server/public/completion.js

commit f3f62f0d835d559e80714bbeb05d03125574e3dd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 2 21:07:47 2024 +0200

    metal : optimize ggml_mul_mat_id (faster Mixtral PP) (#4725)
    
    * ggml : disable fast-math for Metal (cmake build only)
    
    ggml-ci
    
    * metal : fix Metal API debug warnings
    
    * cmake : add -fno-inline for Metal build (#4545)
    
    * metal : fix API debug warnings
    
    * metal : fix compile warnings
    
    * metal : use uint64_t for strides
    
    * cmake : rename option to LLAMA_METAL_SHADER_DEBUG
    
    * metal : fix mat-vec Q8_0 kernel for BS > 1
    
    * metal : normalize mat-vec kernel signatures
    
    * cmake : respect LLAMA_QKK_64 option
    
    * metal : fix mat-vec Q4_K kernel for QK_K == 64
    
    * metal : optimizing ggml_mul_mat_id (wip)
    
    * metal : minor fix
    
    * metal : opt mul_mm_id

ggml-metal.m
ggml-metal.metal

commit 0ef3ca2ac62016c0c545de1c89dc2e3e130f4a99
Author: Phil H <5756783+phiharri@users.noreply.github.com>
Date:   Tue Jan 2 15:48:49 2024 +0000

    server : add token counts to html footer (#4738)
    
    * server: add token counts to stats
    
    * server: generate hpp
    
    ---------
    
    Co-authored-by: phiharri <ph@got-root.co.uk>

examples/server/completion.js.hpp
examples/server/index.html.hpp
examples/server/index.js.hpp
examples/server/public/index.html

commit 540938f8904707dd74cb3be18495f853b312e72f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 2 16:26:45 2024 +0200

    llama : llama_model_desc print number of experts

llama.cpp

commit 0040d42eeb237197054cc7790df5776eacfa608e
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Tue Jan 2 06:15:16 2024 -0800

    llama : replace all API facing `int`'s with `int32_t` (#4577)
    
    * replaced all API facing `int`'s with `int32_t`
    
    * formatting and missed `int` in `llama_token_to_piece`

llama.cpp
llama.h

commit 83e633c27efdf0eb0ba54249e784b0ea760b1007
Author: postmasters <namnguyen@google.com>
Date:   Tue Jan 2 03:51:28 2024 -0800

    llama : differentiate the KV dims in the attention (#4657)
    
    * Add n_key_dim and n_value_dim
    
    Some models use values that are not derived from `n_embd`.
    Also remove `n_embd_head` and `n_embd_gqa` because it is not clear
    which "head" is referred to (key or value).
    
    Fix issue #4648.
    
    * Fix `llm_build_kqv` to use `n_value_gqa`
    
    * Rebase
    
    * Rename variables
    
    * Fix llm_build_kqv to be more generic wrt n_embd_head_k
    
    * Update default values for n_embd_head_k and n_embd_head_v
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Fix llm_load_tensors: the asserts were not backcompat
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
llama.cpp

commit 32866c5edde402f42ff4233bb89dcfcede34fd22
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 2 13:28:15 2024 +0200

    editorconfig : fix whitespace and indentation #4710

examples/server/server.cpp

commit 5d7002d4372ebf107cfaf46fcd90df27b204f330
Author: minarchist <minarchist@users.noreply.github.com>
Date:   Tue Jan 2 04:38:15 2024 -0600

    server : add --override-kv parameter (#4710)
    
    * Changes to server to allow metadata override
    
    * documentation
    
    * flake.nix: expose full scope in legacyPackages
    
    * flake.nix: rocm not yet supported on aarch64, so hide the output
    
    * flake.nix: expose checks
    
    * workflows: nix-ci: init; build flake outputs
    
    * workflows: nix-ci: add a job for eval
    
    * workflows: weekly `nix flake update`
    
    * workflows: nix-flakestry: drop tag filters
    
    ...and add a job for flakehub.com
    
    * workflows: nix-ci: add a qemu job for jetsons
    
    * flake.nix: suggest the binary caches
    
    * flake.lock: update
    
    to a commit recently cached by nixpkgs-cuda-ci
    
    ---------
    
    Co-authored-by: John <john@jLap.lan>
    Co-authored-by: Someone Serge <sergei.kozlukov@aalto.fi>

examples/server/server.cpp

commit 26f3071d714f0b27ad7f021a46a66a1085480258
Author: Nam D. Tran <42194884+namtranase@users.noreply.github.com>
Date:   Tue Jan 2 16:23:38 2024 +0700

    py : re-enable mmap in convert hf (#4732)
    
    * update: awq support llama-7b model
    
    * update: change order
    
    * update: benchmark results for llama2-7b
    
    * update: mistral 7b v1 benchmark
    
    * update: support 4 models
    
    * fix: Readme
    
    * update: ready for PR
    
    * update: readme
    
    * fix: readme
    
    * update: change order import
    
    * black
    
    * format code
    
    * update: work for bot mpt and awqmpt
    
    * update: readme
    
    * Rename to llm_build_ffn_mpt_awq
    
    * Formatted other files
    
    * Fixed params count
    
    * fix: remove code
    
    * update: more detail for mpt
    
    * fix: readme
    
    * fix: readme
    
    * update: change folder architecture
    
    * fix: common.cpp
    
    * fix: readme
    
    * fix: remove ggml_repeat
    
    * update: cicd
    
    * update: cicd
    
    * uppdate: remove use_awq arg
    
    * update: readme
    
    * llama : adapt plamo to new ffn
    
    ggml-ci
    
    * fix: update torch version
    
    ---------
    
    Co-authored-by: Trần Đức Nam <v.namtd12@vinai.io>
    Co-authored-by: Le Hoang Anh <v.anhlh33@vinai.io>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

awq-py/requirements.txt
convert-hf-to-gguf.py

commit 775ac8712a7b42cfead2585f42cec0dfd56644ab
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Tue Jan 2 10:16:55 2024 +0100

    finetune: fix typo in README.md (#4733)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/finetune/README.md

commit 58ba655af054715c0516ee270ad028ad9e74f357
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jan 2 10:57:44 2024 +0200

    metal : enable shader debugging (cmake option) (#4705)
    
    * ggml : disable fast-math for Metal (cmake build only)
    
    ggml-ci
    
    * metal : fix Metal API debug warnings
    
    * cmake : add -fno-inline for Metal build (#4545)
    
    * metal : fix API debug warnings
    
    * metal : fix compile warnings
    
    * metal : use uint64_t for strides
    
    * cmake : rename option to LLAMA_METAL_SHADER_DEBUG
    
    * metal : fix mat-vec Q8_0 kernel for BS > 1
    
    * metal : normalize mat-vec kernel signatures
    
    * cmake : respect LLAMA_QKK_64 option
    
    * metal : fix mat-vec Q4_K kernel for QK_K == 64
    
    ggml-ci

CMakeLists.txt
ci/run.sh
ggml-metal.m
ggml-metal.metal
tests/test-backend-ops.cpp

commit edd1ab7bc34c10a780ee7f9a4499f7689cdad36d
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sun Dec 31 17:42:22 2023 +0000

    flake.lock: update
    
    to a commit recently cached by nixpkgs-cuda-ci

flake.lock

commit 198ed7ebfc89b8f2b35a8b1655d57bfb57530c1a
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 18:25:25 2023 +0000

    flake.nix: suggest the binary caches

flake.nix

commit d8361747317c5cb2e00e7fb3b59ff4dce5a176a5
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 18:01:07 2023 +0000

    workflows: nix-ci: add a qemu job for jetsons

.github/workflows/nix-ci.yml

commit 06f2a5d1909a1385b1a16dab4ade68377e121bdd
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 17:36:08 2023 +0000

    workflows: nix-flakestry: drop tag filters
    
    ...and add a job for flakehub.com

.github/workflows/nix-flakestry.yml
.github/workflows/nix-publish-flake.yml

commit c5239944bab0ff71915df8f2dc7e42fc2c138ff6
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 16:38:36 2023 +0000

    workflows: weekly `nix flake update`

.github/workflows/nix-flake-update.yml

commit 1e9ae54cf24d27afe3900d1250634a2a33423db1
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Sat Dec 30 17:19:11 2023 +0000

    workflows: nix-ci: add a job for eval

.github/workflows/nix-ci.yml

commit 7adedecbe39bd552bc14142f496246d55a43ac4e
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Tue Dec 26 19:17:26 2023 +0000

    workflows: nix-ci: init; build flake outputs

.github/workflows/build.yml
.github/workflows/nix-ci.yml

commit 356ea17e0f92bfbbf28a4f69261bed48eff68d9c
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Fri Dec 29 16:21:50 2023 +0000

    flake.nix: expose checks

flake.nix

commit a5c088d8c698299b973d2709153e5d95295606d9
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Tue Dec 26 23:34:40 2023 +0000

    flake.nix: rocm not yet supported on aarch64, so hide the output

flake.nix

commit 1e3900ebacb3a0b385271389686403c97ad76d88
Author: Someone Serge <sergei.kozlukov@aalto.fi>
Date:   Fri Dec 29 16:15:37 2023 +0000

    flake.nix: expose full scope in legacyPackages

.devops/nix/jetson-support.nix
flake.nix

commit e39106c0554cbd0e9310e08fb3b2a577ea4b6273
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 31 11:43:31 2023 +0200

    ggml : add ggml_vdotq_s32 alias (#4715)
    
    ggml-ci

ggml-quants.c

commit 9fbda719de18a9400a064c28759c39d55d687d3e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Dec 30 23:24:42 2023 +0200

    clip : refactor + bug fixes (#4696)
    
    * clip : refactor + bug fixes
    
    ggml-ci
    
    * server : add log message

examples/llava/clip.cpp
examples/llava/clip.h
examples/llava/llava.cpp
examples/server/server.cpp

commit 39d8bc71edcb8b6f99d46fa4216af7a15232e218
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Dec 30 13:52:01 2023 +0100

    CUDA: fixed tensor cores not being used on RDNA3 (#4697)

ggml-cuda.cu

commit 24a447e20af425fa44cf10feaa632b6bb596c80f
Author: automaticcat <daogiatuank54@gmail.com>
Date:   Sat Dec 30 15:07:48 2023 +0700

    ggml : add ggml_cpu_has_avx_vnni() (#4589)
    
    * feat: add avx_vnni based on intel documents
    
    * ggml: add avx vnni based on intel document
    
    * llama: add avx vnni information display
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * docs: add more details about using oneMKL and oneAPI for intel processors
    
    * Update ggml.c
    
    Fix indentation upgate
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
common/common.cpp
ggml.c
ggml.h
llama.cpp

commit a20f3c7465d6d1b33767757c2760643b799a81bf
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Dec 29 23:12:53 2023 +0100

    CUDA: fix tensor core logic for Pascal and HIP (#4682)

ggml-cuda.cu

commit 0235b9b571f3cc7d2b8836409a5404b41ce1379c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 29 18:53:34 2023 +0200

    clip : use ggml_backend_buffer_is_host (#4205)

examples/llava/clip.cpp

commit ce18d727a47f2473ca863a6f78bf3ad480008f72
Author: Steward Garcia <57494570+FSSRepo@users.noreply.github.com>
Date:   Fri Dec 29 11:52:15 2023 -0500

    clip : enable gpu backend (#4205)
    
    * clip: enable CUDA backend
    
    * add missing kernels
    
    * add enough padding for alignment
    
    * remove ggml_repeat of clip.cpp
    
    * add metal backend
    
    * llava : fixes
    
    - avoid ggml_repeat
    - use GGML_USE_ instead of CLIP_USE_ macros
    - remove unused vars
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/llava/CMakeLists.txt
examples/llava/clip.cpp

commit 91bb39cec7e4dfb9e2293509ef60298a67f0b1b7
Author: hydai <z54981220@gmail.com>
Date:   Sat Dec 30 00:31:19 2023 +0800

    cuda: fix vmm oom issue on NVIDIA AGX Orin (#4687)
    
    Signed-off-by: hydai <hydai@secondstate.io>

ggml-cuda.cu

commit 04ac0607e913ab91234dfb240e12a76509e30982
Author: crasm <crasm@git.vczf.us>
Date:   Fri Dec 29 09:50:29 2023 -0500

    python : add check-requirements.sh and GitHub workflow (#4585)
    
    * python: add check-requirements.sh and GitHub workflow
    
    This script and workflow forces package versions to remain compatible
    across all convert*.py scripts, while allowing secondary convert scripts
    to import dependencies not wanted in convert.py.
    
    * Move requirements into ./requirements
    
    * Fail on "==" being used for package requirements (but can be suppressed)
    
    * Enforce "compatible release" syntax instead of ==
    
    * Update workflow
    
    * Add upper version bound for transformers and protobuf
    
    * improve check-requirements.sh
    
    * small syntax change
    
    * don't remove venvs if nocleanup is passed
    
    * See if this fixes docker workflow
    
    * Move check-requirements.sh into ./scripts/
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

.devops/full-cuda.Dockerfile
.devops/full-rocm.Dockerfile
.devops/full.Dockerfile
.devops/main-rocm.Dockerfile
.github/workflows/python-check-requirements.yml
convert-hf-to-gguf.py
convert-lora-to-ggml.py
convert-persimmon-to-gguf.py
requirements-hf-to-gguf.txt
requirements.txt
requirements/requirements-convert-hf-to-gguf.txt
requirements/requirements-convert-llama-ggml-to-gguf.txt
requirements/requirements-convert-lora-to-ggml.txt
requirements/requirements-convert-persimmon-to-gguf.txt
requirements/requirements-convert.txt
scripts/check-requirements.sh

commit 68eccbdc5b56f2a2450f9a8463f9934388cafabf
Author: Philip Taron <philip.taron@gmail.com>
Date:   Fri Dec 29 06:42:26 2023 -0800

    flake.nix : rewrite (#4605)
    
    * flake.lock: update to hotfix CUDA::cuda_driver
    
    Required to support https://github.com/ggerganov/llama.cpp/pull/4606
    
    * flake.nix: rewrite
    
    1. Split into separate files per output.
    
    2. Added overlays, so that this flake can be integrated into others.
       The names in the overlay are `llama-cpp`, `llama-cpp-opencl`,
       `llama-cpp-cuda`, and `llama-cpp-rocm` so that they fit into the
       broader set of Nix packages from [nixpkgs](https://github.com/nixos/nixpkgs).
    
    3. Use [callPackage](https://summer.nixos.org/blog/callpackage-a-tool-for-the-lazy/)
       rather than `with pkgs;` so that there's dependency injection rather
       than dependency lookup.
    
    4. Add a description and meta information for each package.
       The description includes a bit about what's trying to accelerate each one.
    
    5. Use specific CUDA packages instead of cudatoolkit on the advice of SomeoneSerge.
    
    6. Format with `serokell/nixfmt` for a consistent style.
    
    7. Update `flake.lock` with the latest goods.
    
    * flake.nix: use finalPackage instead of passing it manually
    
    * nix: unclutter darwin support
    
    * nix: pass most darwin frameworks unconditionally
    
    ...for simplicity
    
    * *.nix: nixfmt
    
    nix shell github:piegamesde/nixfmt/rfc101-style --command \
        nixfmt flake.nix .devops/nix/*.nix
    
    * flake.nix: add maintainers
    
    * nix: move meta down to follow Nixpkgs style more closely
    
    * nix: add missing meta attributes
    
    nix: clarify the interpretation of meta.maintainers
    
    nix: clarify the meaning of "broken" and "badPlatforms"
    
    nix: passthru: expose the use* flags for inspection
    
    E.g.:
    
    ```
    ❯ nix eval .#cuda.useCuda
    true
    ```
    
    * flake.nix: avoid re-evaluating nixpkgs too many times
    
    * flake.nix: use flake-parts
    
    * nix: migrate to pname+version
    
    * flake.nix: overlay: expose both the namespace and the default attribute
    
    * ci: add the (Nix) flakestry workflow
    
    * nix: cmakeFlags: explicit OFF bools
    
    * nix: cuda: reduce runtime closure
    
    * nix: fewer rebuilds
    
    * nix: respect config.cudaCapabilities
    
    * nix: add the impure driver's location to the DT_RUNPATHs
    
    * nix: clean sources more thoroughly
    
    ...this way outPaths change less frequently,
    and so there are fewer rebuilds
    
    * nix: explicit mpi support
    
    * nix: explicit jetson support
    
    * flake.nix: darwin: only expose the default
    
    ---------
    
    Co-authored-by: Someone Serge <sergei.kozlukov@aalto.fi>

.devops/nix/apps.nix
.devops/nix/devshells.nix
.devops/nix/jetson-support.nix
.devops/nix/nixpkgs-instances.nix
.devops/nix/package.nix
.devops/nix/scope.nix
.github/workflows/nix-flakestry.yml
flake.lock
flake.nix

commit 97bbca6e8522d18041fcde6c3d0907a52ce36446
Author: Cuong Trinh Manh <nguoithichkhampha@gmail.com>
Date:   Fri Dec 29 21:39:15 2023 +0700

    cmake : fix ld warning duplicate libraries libllama.a (#4671)
    
    * fix "ld: warning: ignoring duplicate libraries: '../libllama.a'"
    
    * fix warning in example.

common/CMakeLists.txt
examples/llava/CMakeLists.txt
examples/server/CMakeLists.txt
tests/CMakeLists.txt

commit 4af4801566bc262a38fb77f51edf278ac323c2bd
Author: Justine Tunney <jtunney@gmail.com>
Date:   Fri Dec 29 06:38:38 2023 -0800

    llava-cli : refactor to use sampling library (#4669)
    
    This change makes it possible to use flags like `--grammar` when using
    the `llava-cli` program. The rest is just code cleanup deleting a long
    standing TODO comment.
    
    This change also ensures that logging information is emitted to stderr
    which helps the `llava-cli` command be more friendly to shell scripts.
    
    See Mozilla-Ocho/llamafile@1cd334f

examples/llava/llava-cli.cpp

commit db49ff8ed7f0bb201176703441cc02911b08ef2a
Author: Justine Tunney <jtunney@gmail.com>
Date:   Fri Dec 29 06:24:12 2023 -0800

    server : replace sleep with condition variables (#4673)
    
    The server currently schedules tasks using a sleep(5ms) busy loop. This
    adds unnecessary latency since most sleep implementations do a round up
    to the system scheduling quantum (usually 10ms). Other libc sleep impls
    spin for smaller time intervals which results in the server's busy loop
    consuming all available cpu. Having the explicit notify() / wait() code
    also helps aid in the readability of the server code.
    
    See mozilla-Ocho/llamafile@711344b

examples/server/server.cpp

commit 60f55e888c29cbd87c4238dd19e85d0eef87245d
Author: SakuraUmi <yukinon244@gmail.com>
Date:   Fri Dec 29 22:22:44 2023 +0800

    server : fix OpenAI server sampling w.r.t. penalty. (#4675)

examples/server/server.cpp

commit b93edd22f55d3e5268263c3edcdae1818505c078
Author: Karthik Sethuraman <k.seth1993@gmail.com>
Date:   Fri Dec 29 06:22:10 2023 -0800

    server : allow to generate multimodal embeddings (#4681)

examples/server/README.md
examples/server/server.cpp

commit 82d6eab224862a7044069fb9211dc4b29124264b
Author: andrijdavid <david@geek.mg>
Date:   Fri Dec 29 15:18:20 2023 +0100

    main-cmake-pkg : fix build issue (#4665)
    
    * Fix main-cmake-pkg compilation
    
    * Use glob to load common files
    
    * cmake : fix trailing whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/main-cmake-pkg/CMakeLists.txt

commit afd997ab6011dfefe9e917425b04ef4d83614841
Author: Peter Sugihara <peter@campsh.com>
Date:   Fri Dec 29 05:58:56 2023 -0800

    llama.swiftui : fix infinite loop, ouput timings, buff UI (#4674)
    
    * fix infinite loop
    
    * slight UI simplification, clearer UX
    
    * clearer UI text, add timings to completion log

examples/llama.swiftui/llama.cpp.swift/LibLlama.swift
examples/llama.swiftui/llama.swiftui/Models/LlamaState.swift
examples/llama.swiftui/llama.swiftui/UI/ContentView.swift
examples/llama.swiftui/llama.swiftui/UI/DownloadButton.swift

commit c8255f8a6b2a3b3ebc6cb340cc2487f39fc95ffc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 29 15:12:35 2023 +0200

    scripts : print list of sync commits

scripts/sync-ggml-am.sh
scripts/sync-ggml.last

commit 441f51dca004debf8b275f1bdc08e0f1af7fd8f8
Author: Tamotsu Takahashi <ttakah+github@gmail.com>
Date:   Fri Dec 29 19:23:27 2023 +0900

    ci : build with CLBlast + ggml-opencl use GGML_API (whisper/1576)
    
    * Build with CLBlast
    
    * Declare GGML_API
    
    After rebasing, examples/talk-llama failed:
    
    "D:\a\whisper.cpp\whisper.cpp\build\ALL_BUILD.vcxproj" (build target) (1) ->
    "D:\a\whisper.cpp\whisper.cpp\build\examples\talk-llama\talk-llama.vcxproj" (default target) (14) ->
    (Link target) ->
      llama.obj : error LNK2019: unresolved external symbol ggml_cl_free_data referenced in function "public: __cdecl llama_model::~llama_model(void)" (??1llama_model@@QEAA@XZ) [D:\a\whisper.cpp\whisper.cpp\build\examples\talk-llama\talk-llama.vcxproj]
      llama.obj : error LNK2019: unresolved external symbol ggml_cl_transform_tensor referenced in function "public: void __cdecl llama_model_loader::load_all_data(struct ggml_context *,void (__cdecl*)(float,void *),void *,struct llama_mlock *)" (?load_all_data@llama_model_loader@@QEAAXPEAUggml_context@@P6AXMPEAX@Z1PEAUllama_mlock@@@Z) [D:\a\whisper.cpp\whisper.cpp\build\examples\talk-llama\talk-llama.vcxproj]
      D:\a\whisper.cpp\whisper.cpp\build\bin\Release\talk-llama.exe : fatal error LNK1120: 2 unresolved externals [D:\a\whisper.cpp\whisper.cpp\build\examples\talk-llama\talk-llama.vcxproj]

ggml-opencl.h

commit 38b3de4658292582a8941a2be5c77b40ce6ac0f2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 29 14:56:41 2023 +0200

    sync : ggml

scripts/sync-ggml.last

commit afc8c192919f04613a92d40391bff4c8cd99856b
Author: bssrdf <merlintiger@hotmail.com>
Date:   Fri Dec 29 03:32:31 2023 -0500

    ggml : fix some mul mat cases + add tests for src1 F16 (ggml/669)
    
    * fixed mul-mat error for old GPUs
    
    * style fixes
    
    * add mul mat src1 f16 test cases, fix more cases
    
    ggml-ci
    
    ---------
    
    Co-authored-by: bssrdf <bssrdf@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-backend.c
ggml-cuda.cu
ggml.c
tests/test-backend-ops.cpp

commit ca38b8d334baa724bd6c9402470931d26427466f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 29 14:41:36 2023 +0200

    scripts : do not sync commits from this repo

scripts/sync-ggml-am.sh

commit 65e5f6dadbba4b496bba27f573e473c66b446496
Author: Justine Tunney <jtunney@gmail.com>
Date:   Thu Dec 28 11:20:00 2023 -0800

    Fix OpenAI server sampling w.r.t. temp and seed (#4668)
    
    The default values for tfs_z and typical_p were being set to zero, which
    caused the token candidates array to get shrunk down to one element thus
    preventing any sampling. Note this only applies to OpenAI API compatible
    HTTP server requests.
    
    The solution is to use the default values that OpenAI documents, as well
    as ensuring we use the llama.cpp defaults for the rest. I've tested this
    change still ensures deterministic output by default. If a "temperature"
    greater than 0 is explicitly passed, then output is unique each time. If
    "seed" is specified in addition to "temperature" then the output becomes
    deterministic once more.
    
    See mozilla-Ocho/llamafile#117
    See mozilla-Ocho/llamafile@9e4bf29

examples/server/server.cpp

commit ea5497df5d138c83b2b0ca70aefdc4b1175c1001
Author: manikbhandari <mbbhandarimanik2@gmail.com>
Date:   Thu Dec 28 09:03:57 2023 -0500

    gpt2 : Add gpt2 architecture integration (#4555)

README.md
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp
models/ggml-vocab-gpt2.gguf
tests/CMakeLists.txt

commit f6793491b5af6da75edad34d6f503ef86d31b09f
Author: Nam D. Tran <42194884+namtranase@users.noreply.github.com>
Date:   Wed Dec 27 22:39:45 2023 +0700

    llama : add AWQ for llama, llama2, mpt, and mistral models (#4593)
    
    * update: awq support llama-7b model
    
    * update: change order
    
    * update: benchmark results for llama2-7b
    
    * update: mistral 7b v1 benchmark
    
    * update: support 4 models
    
    * fix: Readme
    
    * update: ready for PR
    
    * update: readme
    
    * fix: readme
    
    * update: change order import
    
    * black
    
    * format code
    
    * update: work for bot mpt and awqmpt
    
    * update: readme
    
    * Rename to llm_build_ffn_mpt_awq
    
    * Formatted other files
    
    * Fixed params count
    
    * fix: remove code
    
    * update: more detail for mpt
    
    * fix: readme
    
    * fix: readme
    
    * update: change folder architecture
    
    * fix: common.cpp
    
    * fix: readme
    
    * fix: remove ggml_repeat
    
    * update: cicd
    
    * update: cicd
    
    * uppdate: remove use_awq arg
    
    * update: readme
    
    * llama : adapt plamo to new ffn
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Trần Đức Nam <v.namtd12@vinai.io>
    Co-authored-by: Le Hoang Anh <v.anhlh33@vinai.io>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

awq-py/README.md
awq-py/awq/apply_awq.py
awq-py/requirements.txt
convert-hf-to-gguf.py
convert.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 879b690a9e1eb1ab0a29b58236fc76978fb4d902
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Wed Dec 27 15:16:55 2023 +0100

    finetune : fix output formatting in print_params (#4653)
    
    This commit fixes the output formatting in the print_params function
    which currently looks like this:
    ```console
    print_params: n_vocab:   32000
    print_params: n_ctx:     128
    print_params: n_embd:    4096
    print_params: n_ff:      11008
    print_params: n_head:    32
    print_params: n_head_kv: 32
    print_params: n_layer:   32
    print_params: norm_rms_eps          : 0.000010
    print_params: rope_freq_base        : 10000.000000
    print_params: rope_freq_scale       : 1.000000
    ```
    With this comit the output will look like this:
    ```console
    print_params: n_vocab               : 32000
    print_params: n_ctx                 : 128
    print_params: n_embd                : 4096
    print_params: n_ff                  : 11008
    print_params: n_head                : 32
    print_params: n_head_kv             : 32
    print_params: n_layer               : 32
    print_params: norm_rms_eps          : 0.000010
    print_params: rope_freq_base        : 10000.000000
    print_params: rope_freq_scale       : 1.000000
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/finetune/finetune.cpp

commit b47879b0dda43f2d26415e88b6840295817e552a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 27 11:15:31 2023 +0200

    scripts : add sync-ggml-am.sh

scripts/sync-ggml-am.sh
scripts/sync-ggml.last

commit 951010fa53a0ffe81b7d2e87c4349e0d3cb3d19d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 27 11:02:13 2023 +0200

    ggml : fix dot product for ARM (#4630)
    
    ggml-ci

ggml-quants.c

commit f56d6077d0c37e6606ac0a4fa3169de70593acfe
Author: wonjun Jang <strutive07@gmail.com>
Date:   Wed Dec 27 17:37:25 2023 +0900

    Add byte token type when tokenizer.model is not exists (#4641)
    
    * Add byte token type to hf format
    
    * remove unused variable

convert.py

commit dc68f0054cd279cddddb0cae0c9ef4f9cbaa512a
Author: slaren <slarengh@gmail.com>
Date:   Tue Dec 26 21:23:59 2023 +0100

    cuda : fix vmm pool with multi GPU (#4620)
    
    * cuda : fix vmm pool with multi GPU
    
    * hip
    
    * use recommended granularity instead of minimum
    
    * better error checking
    
    * fix mixtral
    
    * use cudaMemcpy3DPeerAsync
    
    * use cuda_pool_alloc in ggml_cuda_op_mul_mat
    
    * consolidate error checking in ggml_cuda_set_device
    
    * remove unnecessary inlines
    
    ggml-ci
    
    * style fixes
    
    * only use vmm for the main device
    
    * fix scratch buffer size, re-enable vmm pool for all devices
    
    * remove unnecessary check id != g_main_device

ggml-cuda.cu
ggml.c
llama.cpp

commit de8e496437c59e7d1cc84109e3e49a3478aee25a
Author: WillCorticesAI <150854901+WillCorticesAI@users.noreply.github.com>
Date:   Tue Dec 26 05:42:08 2023 -0500

    Update comment for AdamW implementation reference. (#4604)
    
    Co-authored-by: Will Findley <findley@gmail.com>

ggml.c

commit 77465dad48d7c945c367ab46b6f2ea98ae9b7b15
Author: FantasyGmm <16450052+FantasyGmm@users.noreply.github.com>
Date:   Tue Dec 26 18:38:36 2023 +0800

    Fix new CUDA10 compilation errors (#4635)

ggml-cuda.cu

commit a206137f927daef1752753cf5e281220b449a468
Author: Paul Tsochantaris <ptsochantaris@icloud.com>
Date:   Mon Dec 25 16:09:53 2023 +0000

    Adding Emeltal reference to UI list (#4629)

README.md

commit b9f47952ffae4e0d3420905526003c23333f6c98
Author: slaren <slarengh@gmail.com>
Date:   Sun Dec 24 21:01:12 2023 +0100

    simplify bug issue template (#4623)

.github/ISSUE_TEMPLATE/bug.md

commit 753be377b69bda2d65a7e089f2b7f0c53ef3495e
Author: Shintarou Okada <kokuzen@gmail.com>
Date:   Sun Dec 24 22:35:49 2023 +0900

    llama : add PLaMo model (#3557)
    
    * add plamo mock
    
    * add tensor loading
    
    * plamo convert
    
    * update norm
    
    * able to compile
    
    * fix norm_rms_eps hparam
    
    * runnable
    
    * use inp_pos
    
    * seems ok
    
    * update kqv code
    
    * remove develop code
    
    * update README
    
    * shuffle attn_q.weight and attn_output.weight for broadcasting
    
    * remove plamo_llm_build_kqv and use llm_build_kqv
    
    * fix style
    
    * update
    
    * llama : remove obsolete KQ_scale
    
    * plamo : fix tensor names for correct GPU offload
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp

commit 5bf3953d7e9831ea22b0bc017ce97409b801ccf1
Author: slaren <slarengh@gmail.com>
Date:   Sun Dec 24 14:34:22 2023 +0100

    cuda : improve cuda pool efficiency using virtual memory (#4606)
    
    * cuda : improve cuda pool efficiency using virtual memory
    
    * fix mixtral
    
    * fix cmake build
    
    * check for vmm support, disable for hip
    
    ggml-ci
    
    * fix hip build
    
    * clarify granularity
    
    * move all caps to g_device_caps
    
    * refactor error checking
    
    * add cuda_pool_alloc, refactor most pool allocations
    
    ggml-ci
    
    * fix hip build
    
    * CUBLAS_TF32_TENSOR_OP_MATH is not a macro
    
    * more hip crap
    
    * llama : fix msvc warnings
    
    * ggml : fix msvc warnings
    
    * minor
    
    * minor
    
    * cuda : fallback to CPU on host buffer alloc fail
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * ensure allocations are always aligned
    
    * act_size -> actual_size
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

CMakeLists.txt
Makefile
ggml-backend.c
ggml-cuda.cu
ggml.c
ggml.h
llama.cpp
tests/test-grad0.cpp

commit 708e179e8562c2604240df95a2241dea17fd808b
Author: slaren <slarengh@gmail.com>
Date:   Sat Dec 23 16:10:51 2023 +0100

    fallback to CPU buffer if host buffer alloc fails (#4610)

ggml-cuda.cu
llama.cpp

commit 925e5584a058afb612f9c20bc472c130f5d0f891
Author: Samuel Maynard <samwmaynard@gmail.com>
Date:   Sat Dec 23 11:35:55 2023 +0200

    ci(docker): fix tags in "Build and push docker image (tagged)" (#4603)

.github/workflows/docker.yml

commit 6123979952385847d8348e295d77d6e01da8aa84
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sat Dec 23 09:31:49 2023 +0000

    server : allow to specify custom prompt for penalty calculation (#3727)

common/sampling.cpp
common/sampling.h
examples/server/README.md
examples/server/server.cpp

commit b9ec82d262cb20d7f0a8a1157bfa9aace40e2625
Author: kalomaze <66376113+kalomaze@users.noreply.github.com>
Date:   Sat Dec 23 03:27:07 2023 -0600

    grammar : check the full vocab only if necessary (opt) (#4306)
    
    * Check the full vocab for grammar only if necessary
    
    * Fix missing logit restoration step (?)
    
    Does this matter, actually?
    
    * Fix whitespace / formatting
    
    * Adjust comment
    
    * Didn't mean to push test gbnf
    
    * Split sampling into the helper function (?)
    
    And also revert the changes made to the header
    
    * common : fix final newline
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/sampling.cpp

commit e0a4002273907b2c414b6b5442d99e08bfe2df35
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Dec 23 09:16:33 2023 +0100

    CUDA: fixed row rounding for 0 tensor splits (#4594)

ggml-cuda.cu

commit 7082d24cec35e9ce9147535a2224dfc67ee0a78c
Author: LeonEricsson <70749762+LeonEricsson@users.noreply.github.com>
Date:   Fri Dec 22 17:05:56 2023 +0100

    lookup : add prompt lookup decoding example (#4484)
    
    * initial commit, going through initializations
    
    * main loop finished, starting to debug
    
    * BUG: generates gibberish/repeating tokens after a while
    
    * kv_cache management
    
    * Added colors to distinguish drafted tokens (--color). Updated README
    
    * lookup : fix token positions in the draft batch
    
    * lookup : use n_draft from CLI params
    
    * lookup : final touches
    
    ---------
    
    Co-authored-by: Leon Ericsson <leon.ericsson@icloud.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile
common/common.h
examples/CMakeLists.txt
examples/lookup/CMakeLists.txt
examples/lookup/README.md
examples/lookup/lookup.cpp

commit ba661751322a7c201fd3bef71af077c5aebfaa2a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 22 17:53:43 2023 +0200

    sync : ggml (fix im2col) (#4591)
    
    * cuda : fix im2col_f32_f16 (ggml/#658)
    
    ggml-ci
    
    * ggml-alloc : fix ggml_tallocr_is_own
    
    ---------
    
    Co-authored-by: leejet <leejet714@gmail.com>

ggml-alloc.c
ggml-cuda.cu

commit a55876955b1a83464171de8d578d3ab062a7b62d
Author: FantasyGmm <16450052+FantasyGmm@users.noreply.github.com>
Date:   Fri Dec 22 23:11:12 2023 +0800

    cuda : fix jetson compile error (#4560)
    
    * fix old jetson compile error
    
    * Update Makefile
    
    * update jetson detect and cuda version detect
    
    * update cuda marco define
    
    * update makefile and cuda,fix some issue
    
    * Update README.md
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update Makefile
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

Makefile
README.md
ggml-cuda.cu
ggml-quants.c

commit 6724ef16573ec7ecce620be56cbbff145856b2fb
Author: Henrik Forstén <henrik.forsten@gmail.com>
Date:   Fri Dec 22 15:34:05 2023 +0200

    Fix CudaMemcpy direction (#4599)

ggml-cuda.cu

commit 48b7ff193e64c97ab174280ba0eb8d14b47c49ba
Author: slaren <slarengh@gmail.com>
Date:   Fri Dec 22 12:12:53 2023 +0100

    llama : fix platforms without mmap (#4578)
    
    * llama : fix platforms without mmap
    
    * win32 : limit prefetch size to the file size
    
    * fix win32 error clobber, unnecessary std::string in std::runtime_error

ggml-cuda.cu
ggml.c
llama.cpp

commit 48b24b170e3b4f9dc28200306840cb07d1c123df
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri Dec 22 09:26:49 2023 +0000

    ggml : add comment about backward GGML_OP_DIAG_MASK_INF (#4203)

ggml.c

commit 28cb35a0ecb9852adc3494aa51dde60141939d64
Author: Michael Kesper <mkesper@schokokeks.org>
Date:   Fri Dec 22 09:03:25 2023 +0100

    make : add LLAMA_HIP_UMA option (#4587)
    
    NB: LLAMA_HIP_UMA=1 (or any value) adds MK_CPPFLAG -DGGML_HIP_UMA

Makefile
README.md

commit f31b98489824a86c937fa62ccf5dfd4bb0327b86
Author: rhuddleston <ryan.huddleston@percona.com>
Date:   Thu Dec 21 23:56:34 2023 -0700

    ci : tag docker image with build number (#4584)

.github/workflows/docker.yml

commit 2bb98279c5a087d62949972b35cf63ff974ffe6a
Author: Deins <deinsegle@gmail.com>
Date:   Fri Dec 22 08:49:54 2023 +0200

    readme : add zig bindings (#4581)

README.md

commit 0137ef88ea9f8fd837a065700814329d24adeec3
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Fri Dec 22 06:47:01 2023 +0000

    ggml : extend `enum ggml_log_level` with `GGML_LOG_LEVEL_DEBUG` (#4579)

ggml.h

commit c7e9701f86564088350209d2f9d71c96ea00527f
Author: crasm <crasm@git.vczf.us>
Date:   Fri Dec 22 01:19:36 2023 -0500

    llama : add ability to cancel model loading (#4462)
    
    * llama : Add ability to cancel model load
    
    Updated llama_progress_callback so that if it returns false, the model
    loading is aborted.
    
    * llama : Add test for model load cancellation
    
    * Fix bool return in llama_model_load, remove std::ignore use
    
    * Update llama.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Fail test if model file is missing
    
    * Revert "Fail test if model file is missing"
    
    This reverts commit 32ebd525bf7e5a87ee8a3dbaab3d92ce79fbf23d.
    
    * Add test-model-load-cancel to Makefile
    
    * Revert "Revert "Fail test if model file is missing""
    
    This reverts commit 2796953257ee5383fa7c8fe8fa8fc888c048fb0b.
    
    * Simplify .gitignore for tests, clang-tidy fixes
    
    * Label all ctest tests
    
    * ci : ctest uses -L main
    
    * Attempt at writing ctest_with_model
    
    * ci : get ci/run.sh working with test-model-load-cancel
    
    * ci : restrict .github/workflows/build.yml ctest to -L main
    
    * update requirements.txt
    
    * Disable test-model-load-cancel in make
    
    * Remove venv before creation
    
    * Restructure requirements.txt
    
    Top-level now imports the specific additional requirements for each
    python file. Using `pip install -r requirements.txt` will fail if
    versions become mismatched in the per-file requirements.
    
    * Make per-python-script requirements work alone
    
    This doesn't break the main requirements.txt.
    
    * Add comment
    
    * Add convert-persimmon-to-gguf.py to new requirements.txt scheme
    
    * Add check-requirements.sh script and GitHub workflow
    
    * Remove shellcheck installation step from workflow
    
    * Add nocleanup special arg
    
    * Fix merge
    
    see: https://github.com/ggerganov/llama.cpp/pull/4462#discussion_r1434593573
    
    * reset to upstream/master
    
    * Redo changes for cancelling model load
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

llama.cpp
llama.h

commit afefa319f1f59b002dfa0d1ef407a2c74bd9770b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 21 23:20:49 2023 +0200

    ggml : change ggml_scale to take a float instead of tensor (#4573)
    
    * ggml : change ggml_scale to take a float instead of tensor
    
    * ggml : fix CPU implementation
    
    * tests : fix test-grad0
    
    ggml-ci

examples/baby-llama/baby-llama.cpp
examples/export-lora/export-lora.cpp
examples/finetune/finetune.cpp
examples/llava/clip.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-cuda.cu
ggml-metal.m
ggml.c
ggml.h
llama.cpp
tests/test-backend-ops.cpp
tests/test-grad0.cpp

commit 769a7bc85eaa44e3d7eadf39abfeff7bb0b9cc2f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 21 23:20:36 2023 +0200

    gguf-py : fix broken link

gguf-py/README.md

commit 32259b2dade6f6856739bf7ba0a4ff7b474dc760
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 21 23:07:58 2023 +0200

    gguf : simplify example dependencies

Makefile
examples/gguf/CMakeLists.txt
examples/gguf/gguf.cpp

commit 4a5f9d629ecfd0a53afdddbaf54a4fa02d9a9ce9
Author: Samuel Maynard <samwmaynard@gmail.com>
Date:   Thu Dec 21 22:36:26 2023 +0200

    ci : add `jlumbroso/free-disk-space` to docker workflow (#4150)
    
    * [github][workflows][docker]: removes hardcoded `ggerganov` from `ghcr` repo
    
    * [github][workflows][docker]: adds `jlumbroso/free-disk-space`

.github/workflows/docker.yml

commit d232aca5a73b290e218a2e48b91023d5e994203f
Author: slaren <slarengh@gmail.com>
Date:   Thu Dec 21 21:07:46 2023 +0100

    llama : initial ggml-backend integration (#4520)
    
    * llama : initial ggml-backend integration
    
    * add ggml-metal
    
    * cuda backend can be used though ggml-backend with LLAMA_GGML_BACKEND_CUDA_TEST
    access all tensor data with ggml_backend_tensor_get/set
    
    * add ggml_backend_buffer_clear
    zero-init KV cache buffer
    
    * add ggml_backend_buffer_is_hos, used to avoid copies if possible when accesing tensor data
    
    * disable gpu backends with ngl 0
    
    * more accurate mlock
    
    * unmap offloaded part of the model
    
    * use posix_fadvise64(.., POSIX_FADV_SEQUENTIAL) to improve performance with mmap
    
    * update quantize and lora
    
    * update session copy/set to use ggml-backend
    
    ggml-ci
    
    * use posix_fadvise instead of posix_fadvise64
    
    * ggml_backend_alloc_ctx_tensors_from_buft : remove old print
    
    * llama_mmap::align_offset : use pointers instead of references for out parameters
    
    * restore progress_callback behavior
    
    * move final progress_callback call to load_all_data
    
    * cuda : fix fprintf format string (minor)
    
    * do not offload scales
    
    * llama_mmap : avoid unmapping the same fragments again in the destructor
    
    * remove unnecessary unmap
    
    * metal : add default log function that prints to stderr, cleanup code
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

Makefile
ggml-alloc.c
ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-metal.h
ggml-metal.m
ggml.c
ggml.h
llama.cpp

commit 31f27758faf4a4bd08101a57c7ec3a473f771f86
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Thu Dec 21 11:57:48 2023 -0800

    llama : allow getting n_batch from llama_context in c api (#4540)
    
    * allowed getting n_batch from llama_context in c api
    
    * changed to use `uint32_t` instead of `int`
    
    * changed to use `uint32_t` instead of `int` in `llama_n_ctx`
    
    * Update llama.h
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp
llama.h

commit 56fa50819f7a3ca2128f63b81c17c08a4454479e
Author: Finn Voorhees <finnvoorhees@gmail.com>
Date:   Thu Dec 21 14:55:02 2023 -0500

    metal : fix `ggml_metal_log` vargs (#4373)

commit 0f630fbc924aaabeea6eaf466bb4b47d13015c3e
Author: Erik Garrison <erik.garrison@gmail.com>
Date:   Thu Dec 21 13:45:32 2023 -0600

    cuda : ROCm AMD Unified Memory Architecture (UMA) handling (#4449)
    
    * AMD ROCm: handle UMA memory VRAM expansions
    
    This resolves #2797 by allowing ROCm AMD GPU users with a UMA to
    dynamically expand the VRAM allocated to the GPU.
    
    Without this, AMD ROCm users with shared CPU/GPU memory usually are
    stuck with the BIOS-set (or fixed) framebuffer VRAM, making it
    impossible to load more than 1-2 layers.
    
    Note that the model is duplicated in RAM because it's loaded once for
    the CPU and then copied into a second set of allocations that are
    managed by the HIP UMA system. We can fix this later.
    
    * clarify build process for ROCm on linux with cmake
    
    * avoid using deprecated ROCm hipMallocHost
    
    * keep simplifying the change required for UMA
    
    * cmake: enable UMA-compatible allocation when LLAMA_HIP_UMA=ON

CMakeLists.txt
README.md
ggml-cuda.cu

commit 562cf222b5129e40b312877e928eac3a02e4ec33
Author: arlo-phoenix <140345165+arlo-phoenix@users.noreply.github.com>
Date:   Thu Dec 21 20:13:25 2023 +0100

    ggml-cuda: Fix HIP build by adding define for __trap (#4569)
    
    Regression of 139882392258671ffe5acdfcadc0bc08572d6eef
    HIP doesn't have trap, only abort

ggml-cuda.cu

commit 8fe03ffddaaa0ab5d48feaafe398151c9f22d4f6
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Dec 21 12:55:34 2023 -0500

    common : remove incorrect --model-draft default (#4568)

common/common.cpp

commit 9154494808dc865475c59022c29060b4947a803b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Dec 21 18:42:59 2023 +0100

    CUDA: mul_mat_id always on GPU for batches >= 32 (#4553)

ggml-cuda.cu

commit c083718c895b7c8c7fb2a4660643fb78d0c64dfd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 21 19:27:14 2023 +0200

    readme : update coding guidelines

README.md

commit 880e352277fc017df4d5794f0c21c44e1eae2b84
Author: howlger <github@voormann.de>
Date:   Thu Dec 21 18:07:34 2023 +0100

    py : open merges file as 'utf-8' (#4566)
    
    Otherwise, on Windows converting bling-phi-2-v0 (<https://huggingface.co/llmware/bling-phi-2-v0>) via convert-hf-to-gguf.py will fail with the following error:
    
    ```
    Traceback (most recent call last):
      File "C:\Users\User\git\gguf\convert-hf-to-gguf.py", line 1061, in <module>
        model_instance.set_vocab()
      File "C:\Users\User\git\gguf\convert-hf-to-gguf.py", line 52, in set_vocab
        self._set_vocab_gpt2()
      File "C:\Users\User\git\gguf\convert-hf-to-gguf.py", line 264, in _set_vocab_gpt2
        special_vocab = gguf.SpecialVocab(dir_model, load_merges=True)
      File "C:\Users\User\git\gguf\gguf\vocab.py", line 33, in __init__
        self._load(Path(path))
      File "C:\Users\User\git\gguf\gguf\vocab.py", line 81, in _load
        self._try_load_merges_txt(path)
      File "C:\Users\User\git\gguf\gguf\vocab.py", line 95, in _try_load_merges_txt
        for line in fp:
      File "C:\Users\User\miniconda3\envs\gguf\lib\encodings\cp1252.py", line 23, in decode
        return codecs.charmap_decode(input,self.errors,decoding_table)[0]
    UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1415: character maps to <undefined>
    ```

gguf-py/gguf/vocab.py

commit 66f35a2f48e1965a13835a523e677223dbf148be
Author: bobqianic <129547291+bobqianic@users.noreply.github.com>
Date:   Thu Dec 21 17:06:44 2023 +0000

    cuda : better error message for ggml_get_rows (#4561)
    
    * Update ggml-cuda.cu
    
    * Update ggml-cuda.cu
    
    * Update ggml-cuda.cu
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-cuda.cu

commit 139882392258671ffe5acdfcadc0bc08572d6eef
Author: slaren <slarengh@gmail.com>
Date:   Thu Dec 21 18:02:30 2023 +0100

    cuda : replace asserts in wrong architecture checks with __trap (#4556)
    
    * cuda : replace asserts in wrong architecture checks with __trap
    
    * make bad_arch noreturn, remove returns

ggml-cuda.cu

commit d3223afdad0ed2821a8ddf739c291cd410c92a11
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Dec 21 17:34:17 2023 +0100

    llama : disable per-tensor info prints on model load (#4562)

llama.cpp

commit 1d7a1912cea2227f9a1a449758ed622c560542f9
Author: LoganDark <github@logandark.mozmail.com>
Date:   Thu Dec 21 01:59:27 2023 -0800

    Fix access violation in ggml_cuda_free_data if tensor->extra is NULL (#4554)

ggml-cuda.cu

commit 799fc2268989482054944c902874cca76337580f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Dec 20 15:41:22 2023 +0100

    CUDA: Faster Mixtral prompt processing (#4538)
    
    * CUDA: make MoE tensors contiguous for batch size>1
    
    * Update ggml-cuda.cu
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu

commit 328b83de23b33240e28f4e74900d1d06726f5eb1
Author: Eric Sommerlade <es0m@users.noreply.github.com>
Date:   Tue Dec 19 16:17:01 2023 +0000

    ggml : fixed check for _MSC_VER (#4535)
    
    Co-authored-by: Eric Sommerlade <ersomme@microsoft.com>

ggml.h

commit a7aee47b98e45539d491071b25778b833b77e387
Author: arlo-phoenix <140345165+arlo-phoenix@users.noreply.github.com>
Date:   Mon Dec 18 22:33:45 2023 +0100

    ggml-cuda: Fix HIP build (#4528)
    
    regression of #4490
    Adds defines for two new datatypes
    cublasComputeType_t, cudaDataType_t.
    
    Currently using deprecated hipblasDatatype_t since newer ones very recent.

ggml-cuda.cu

commit 0e18b2e7d0b5c0a509ea40098def234b8d4a938a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Dec 18 20:17:43 2023 +0200

    llama.swiftui : add tinyllama 1.1B F16

examples/llama.swiftui/llama.swiftui/UI/ContentView.swift

commit 6ff39b129d0281d045f83d515e51b7197b44b253
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Dec 18 20:05:12 2023 +0200

    llama.swiftui : add more models

examples/llama.swiftui/llama.cpp.swift/LibLlama.swift
examples/llama.swiftui/llama.swiftui/UI/ContentView.swift

commit b9e74f9bca5fdf7d0a22ed25e7a9626335fdfa48
Author: Ebey Abraham <ebey97@gmail.com>
Date:   Mon Dec 18 17:27:47 2023 +0000

    llama : add phi-2 + fix NeoX rope + ggml_mul_mat_set_prec (#4490)
    
    * phi2 implementation
    
    * fix breaking change
    
    * phi-2 : various fixes
    
    * phi-2 : use layer norm eps
    
    * py : whitespaces
    
    * llama : fix meta KV override bug
    
    * convert : phi don't add BOS token
    
    * convert : revert "added_tokens_decoder" change
    
    * phi-2 : scale Q instead of KQ for better precision
    
    * ggml : fix NeoX rope to rotate just first n_dims
    
    * cuda : less diff in the rope_neox kernel
    
    * ggml : add ggml_mul_mat_set_prec
    
    ggml-ci
    
    * Update ggml-cuda.cu
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * cuda : ggml_cuda_op_mul_mat_cublas support F32 precision
    
    * cuda : remove oboslete comment
    
    ---------
    
    Co-authored-by: Ebey Abraham <ebeyabraham@microsoft.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

convert-hf-to-gguf.py
ggml-cuda.cu
ggml-metal.metal
ggml.c
ggml.h
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp
tests/test-backend-ops.cpp

commit 3c04bf6da89eaf4c7d317e0518f0687dfcbf2de7
Author: hankcs <cnhankmc@gmail.com>
Date:   Mon Dec 18 05:14:58 2023 -0800

    llama : fix try_override for bool_value which always return true (#4519)

llama.cpp

commit 2994f0c5a2e8c96955b422dedc93ec2595d16b82
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sun Dec 17 19:39:02 2023 -0500

    decode : fix logits_valid for legacy API (#4516)

llama.cpp

commit b1306c439490c7fa4ec33594500d980d1e9e15e6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 17 20:16:23 2023 +0200

    readme : update hot topics

README.md

commit 800a489e4a8be199122259a995b1ee9dd7fae320
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 17 19:38:41 2023 +0200

    llama.swiftui : add bench functionality (#4483)
    
    * llama.swiftui : add bench button
    
    * llama.swiftui : initial bench functionality
    
    * force to use n_gpu_layers on simulator
    
    * add download buttons & expose llamaState.loadModel
    
    * update project.pbxproj
    
    * comment #Preview & fix editorconfig check
    
    * gitignore : xcode stuff
    
    * llama.swiftui : UX improvements
    
    * llama.swiftui : avoid data copy via "downloadTask"
    
    * llama.swiftui : remove model from project
    
    * llama : remove "mostly" from model infos
    
    * llama.swiftui : improve bench
    
    ---------
    
    Co-authored-by: jhen <developer@jhen.me>

.editorconfig
examples/llama.swiftui/.gitignore
examples/llama.swiftui/llama.cpp.swift/LibLlama.swift
examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj
examples/llama.swiftui/llama.swiftui/Models/LlamaState.swift
examples/llama.swiftui/llama.swiftui/UI/ContentView.swift
examples/llama.swiftui/llama.swiftui/UI/DownloadButton.swift
llama.cpp

commit f7f468a97dceec2f8fe8b1ed7a2091083446ebc7
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sun Dec 17 10:45:46 2023 -0500

    gguf-py : fail fast on nonsensical special token IDs (#4489)

gguf-py/gguf/vocab.py

commit 919c40660fd27157b391b5832d2a577d5afef4cb
Author: Matheus Gabriel Alves Silva <matheusgasource@gmail.com>
Date:   Sun Dec 17 12:23:33 2023 -0300

    build : Check the ROCm installation location (#4485)
    
    * build : Check the ROCm installation location
    
    * more generic approach
    
    * fixup! It was returning the path instead of the command output
    
    * fixup! Trailing whitespace

Makefile

commit 45668633fdb522a925c3dafc1ecf426f539efb27
Author: slaren <slarengh@gmail.com>
Date:   Sun Dec 17 16:05:56 2023 +0100

    finetune : keep allocs alive until all allocations are done (#4486)

examples/finetune/finetune.cpp

commit 0ffc92d2d23a789625f018840469af045be1e3c0
Author: olexiyb <olexiyb@gmail.com>
Date:   Sun Dec 17 17:02:16 2023 +0200

    server : disable llm logs if SERVER_VERBOSE is off (#3792)

examples/server/server.cpp

commit 8edd2b40fdbcafbf630f2cf29306b29d5cb48c42
Author: AdithyanI <adithyan.i4internet@gmail.com>
Date:   Sun Dec 17 15:57:56 2023 +0100

    server : fix grammar being ignored (#4494)
    
    Fix bug in identifying the grammar.

examples/server/server.cpp

commit eb16dae7e70ca97396190698b29c0f9ee3388e88
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sun Dec 17 14:56:09 2023 +0000

    server : fix possible ambiguity in content type charset (#4501)

examples/server/server.cpp

commit 62bd52b7bf90819e75f427a95a484cd5eee0b3c7
Author: mzcu <milos.cubrilo@gmail.com>
Date:   Sun Dec 17 15:54:37 2023 +0100

    server : allow requests larger than 8K (#4500)

examples/server/server.cpp

commit 5daa5f54fdcd2b5228add1a4c43a1897b2168f35
Author: Bach Le <bach@bullno1.com>
Date:   Sun Dec 17 18:57:33 2023 +0800

    Link to cublas dynamically on Windows even with LLAMA_STATIC (#4506)

CMakeLists.txt

commit c6c4fc081c1df1c60a9bfe3e6a3fd086f1a29ec7
Author: slaren <slarengh@gmail.com>
Date:   Sat Dec 16 18:58:46 2023 +0100

    lora : add support for non-llama models (#3333)
    
    * lora : add support for non-llama models
    
    ggml-ci
    
    * avoid leaking ggml_context on failure
    cleanup
    
    ggml-ci
    
    * lora : allow 1d tensors
    
    * lora : include embd and output layers in size calculation
    
    * fix style

convert-lora-to-ggml.py
llama.cpp
llama.h

commit 8a5be3bd5885d79ad84aadf32bb8c1a67bd43c19
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Fri Dec 15 22:16:15 2023 -0500

    llama : sanity checks for access to logits (#4274)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp

commit 88ae8952b65cbf32eb1f5703681ea592e510e570
Author: ShadovvBeast <ShadovvBeast@gmail.com>
Date:   Fri Dec 15 13:49:01 2023 +0200

    server : add optional API Key Authentication example (#4441)
    
    * Add API key authentication for enhanced server-client security
    
    * server : to snake_case
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/public/completion.js
examples/server/public/index.html
examples/server/server.cpp

commit ee4725a686643669a8587142fa068cbf29de3ce2
Author: slaren <slarengh@gmail.com>
Date:   Fri Dec 15 12:45:50 2023 +0100

    ggml : group mul_mat_id rows by matrix (cpu only) (#4480)
    
    * ggml : group mul_mat_id rows by matrix (cpu only)
    
    * remove mmid parameters from mm forward
    
    * store row groups in wdata and calculate only once in GGML_TASK_INIT
    
    ggml-ci

ggml.c

commit 6744dbe924a317e3e2a5a2a4a2037061b2223449
Author: slaren <slarengh@gmail.com>
Date:   Thu Dec 14 20:05:21 2023 +0100

    ggml : use ggml_row_size where possible (#4472)
    
    * ggml : use ggml_row_size where possible
    
    ggml-ci
    
    * ggml : move ggml_nbytes_split to ggml-cuda.cu

ggml-cuda.cu
ggml.c
ggml.h
tests/test-backend-ops.cpp
tests/test-quantize-perf.cpp

commit cafcd4f89500b8afef722cdb08088eceb8a22572
Author: slaren <slarengh@gmail.com>
Date:   Thu Dec 14 16:52:08 2023 +0100

    ggml : remove n_dims from ggml_tensor (#4469)
    
    ggml-ci

common/train.cpp
examples/baby-llama/baby-llama.cpp
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
examples/finetune/finetune.cpp
examples/gguf/gguf.cpp
examples/llava/clip.cpp
ggml.c
ggml.h
llama.cpp

commit c50e40016394f124b97ce39da48148b1f6c01833
Author: wonjun Jang <strutive07@gmail.com>
Date:   Thu Dec 14 21:44:49 2023 +0900

    py : add protobuf dependency (#4466)

requirements.txt

commit 20a68a7030ee06e8eb7eb8e24ae4ac52dc17803f
Author: LostRuins <39025047+LostRuins@users.noreply.github.com>
Date:   Thu Dec 14 20:13:33 2023 +0800

    ggml : add ggml_row_size() (fixes llama out of space) (#4461)
    
    * Fixes "Not enough space in the context's memory pool" encountered on certain models, which seems to be caused by some imprecision related to the automatic casting of floating point values
    
    * do not cast to size_t, instead just use doubles
    
    * ggml : add ggml_row_size(), deprecate ggml_type_sizef()
    
    * ggml : fix row size compute to avoid overflows
    
    * tests : fix sizey -> sizez
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/benchmark/benchmark-matmult.cpp
ggml.c
ggml.h
llama.cpp

commit 55e87c3749cb4985c3b316984d40e00e4df4a5d0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 14 10:35:29 2023 +0200

    ggml : fix OpenCL broadcast requirement for ggml_mul (close #4453)

ggml.c

commit 873637afc7924f435ac44c067630a28e82eefa7b
Author: wonjun Jang <strutive07@gmail.com>
Date:   Thu Dec 14 17:09:34 2023 +0900

    convert : support loading vocab from fast tokenizer config (#3633)
    
    * Add HFVocab into convert.py
    
    * Update convert.py
    
    * Update convert.py
    
    * add bytes_to_unicode function
    
    * change add_meta_vocab fucntion
    
    * remove debug code
    
    * remove byte_encoder
    
    * Add newline between classes
    
    * Check tokenizer.json when tokenizer.model is not exist.
    
    * Move transformers dependency to local code
    
    * Add error context with 'raise from'
    
    * Add fast tokenizer option to BpeVocab
    
    * Update convert.py
    
    * Add VocabLoader and remove *Vocab class
    
    * Add transformers dependency
    
    * remove added tokens and check newline token to decide spm or bpe
    
    * Update convert.py
    
    * Add special token type
    
    * Update convert.py
    
    * Update convert.py
    
    * Update convert.py
    
    * Fix typo in convert.py
    
    * Fix when params.n_vocab < tokenizer vocab size
    
    * update vocab class
    
    * change funtion name
    
    * Remove unused variable/functions, add types to class variable and methods, delete blank liens
    
    * fix flake8 warnings
    
    * code style cleanup
    
    * make mypy happy
    
    * change exception
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

convert.py
requirements.txt

commit 0353a1840134b24b07ab61fd4490192f28c4db6b
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Thu Dec 14 02:38:49 2023 -0500

    readme : update supported model list (#4457)

README.md

commit 948ff137ec37f1ec74c02905917fa0afc9b97514
Author: shibe2 <shibe@tuta.io>
Date:   Wed Dec 13 23:57:15 2023 +0400

    server : fix handling of characters that span multiple tokens when streaming (#4446)

examples/server/server.cpp

commit 4d98d9a65665eee3838cef936641f640e3f5b649
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 13 21:54:54 2023 +0200

    sync : ggml (SD ops, tests, kernels) (#4444)
    
    * sync : ggml (SD ops, tests, kernels)
    
    ggml-ci
    
    * cuda : restore im2col
    
    ggml-ci
    
    * metal : fix accuracy of dequantization kernels
    
    ggml-ci
    
    * cuda : restore correct im2col
    
    ggml-ci
    
    * metal : try to fix moe test by reducing expert size
    
    ggml-ci
    
    * cuda : fix bin bcast when src1 and dst have different types
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
tests/test-backend-ops.cpp

commit 70f806b821f603cafb6f634c93a6729dc21bb354
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Dec 13 12:10:10 2023 -0500

    build : detect host compiler and cuda compiler separately (#4414)

.editorconfig
CMakeLists.txt
Makefile
scripts/get-flags.mk

commit 9fb13f95840c722ad419f390dc8a9c86080a3700
Author: Siwen Yu <yusiwen@gmail.com>
Date:   Wed Dec 13 20:50:14 2023 +0800

    common : add `--version` option to show build info in CLI (#4433)

common/common.cpp

commit 113f9942fc73a262c85e9dcf7c2ea7336250bba0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 13 14:05:38 2023 +0200

    readme : update hot topics

README.md

commit 799a1cb13b0b1b560ab0ceff485caed68faa8f1f
Author: slaren <slarengh@gmail.com>
Date:   Wed Dec 13 13:04:25 2023 +0100

    llama : add Mixtral support (#4406)
    
    * convert : support Mixtral as LLAMA arch
    
    * convert : fix n_ff typo
    
    * llama : model loading
    
    * ggml : sync latest ggml_mul_mat_id
    
    * llama : update graph to support MoE
    
    * llama : fix cur -> cur_expert
    
    * llama : first working version
    
    * llama : fix expert weighting in the FFN
    
    * ggml : ggml_get_rows support 2D indexing [n_tokens, n_experts] (cpu only)
    
    * ggml : add n_as argument to ggml_mul_mat_id
    
    * ggml : fix ggml_get_rows to take into account ne02 / ne11
    
    * metal : add more general support for ggml_get_rows + tests
    
    * llama : add basic support for offloading moe with CUDA
    
    * metal : add/mul/div use general kernel when src1 not cont
    
    * metal : reduce the kernel launches for ggml_mul_mat_id
    
    * ggml : get_rows : support non-contiguos tensors with gaps, generalize up to 3D
    
    * ggml : update get_rows f16 and q
    
    * cuda : support non-contiguous src1 in get_rows
    
    * llama : offload missing ffn_moe_silu
    
    * metal : fix ggml_get_rows to work with non-cont src1
    
    * metal : add indirect mat-vec kernels for all quantization types
    
    * llama : do not quantize expert gating tensors
    
    * llama : add n_expert and n_expert_used to hparams + change quants
    
    * test-backend-ops : add moe test
    
    * cuda : fix get_rows when ncols is odd
    
    * convert : determine n_ctx correctly
    
    * metal : fix ggml_mul_mat_id for F32
    
    * test-backend-ops : make experts more evenly probable (test_moe)
    
    * test-backend-ops : cleanup, add moe test for batches
    
    * test-backend-ops : add cpy from f32 -> all types test
    
    * test-backend-ops : fix dequantize block offset
    
    * llama : fix hard-coded number of experts
    
    * test-backend-ops : simplify and disable slow tests to avoid CI timeout
    
    * test-backend-ops : disable MOE test with thread sanitizer
    
    * cuda : fix mul_mat_id with multi gpu
    
    * convert : use 1e6 rope_freq_base for mixtral
    
    * convert : fix style
    
    * convert : support safetensors format
    
    * gguf-py : bump version
    
    * metal : add cpy f16 -> f32 kernel
    
    * metal : fix binary ops for ne10 % 4 != 0
    
    * test-backend-ops : add one more sum_rows test
    
    * ggml : do not use BLAS with ggml_mul_mat_id
    
    * convert-hf : support for mixtral-instruct (#4428)
    
    * convert : typo fix, add additional hyperparameters, use LLaMA arch for Mixtral-instruct
    
    * convert : use sentencepiece tokenizer for Mixtral-instruct
    
    * convert : make flake8 happy
    
    * metal : fix soft_max kernels
    
    ref: https://github.com/ggerganov/ggml/pull/621/commits/1914017863d2f9ab8ecc0281cc2a56d683668b92
    
    * metal : limit kernels to not use more than the allowed threads
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: Radek Pilar <github@mrkva.eu>

Makefile
convert-hf-to-gguf.py
convert.py
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/tensor_mapping.py
gguf-py/pyproject.toml
llama.cpp
tests/test-backend-ops.cpp

commit fecac45658a99eddc4d6e36ba0310ca8f87a77f0
Author: kalomaze <66376113+kalomaze@users.noreply.github.com>
Date:   Tue Dec 12 04:12:35 2023 -0600

    server : tweak default sampling parameters (#4367)
    
    * Set a more typical Top P setting as the default
    
    * Update temp max

examples/server/public/index.html

commit 9494d7c4774ab745490b5a19570ff7747a194143
Author: Richard Kiss <him@richardkiss.com>
Date:   Tue Dec 12 01:53:36 2023 -0800

    english : use `typos` to fix comments and logs (#4354)

common/log.h
convert.py
examples/llava/clip.cpp
examples/llava/convert-image-encoder-to-gguf.py
examples/lookahead/README.md
examples/server/json.hpp
examples/server/public/completion.js
examples/server/public/index.html
examples/speculative/README.md
examples/speculative/speculative.cpp
ggml-alloc.h
ggml-quants.c
ggml.c
gguf-py/README.md
llama.cpp
tests/test-grad0.cpp
tests/test-quantize-perf.cpp

commit 6138963fb232cbae70c9d181db0ba125708f473d
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Dec 12 04:27:26 2023 -0500

    build : target Windows 8 for standard mingw-w64 (#4405)
    
    * build : target Windows 8 for standard mingw-w64
    
    * make : fix missing console.o deps
    
    This was causing a link error with `make all` on Windows.

CMakeLists.txt
Makefile

commit 6391817cd19a4507c6c941a1fd08756268662b2d
Author: crasm <crasm@git.vczf.us>
Date:   Tue Dec 12 04:25:57 2023 -0500

    llama : document logits_all deprecation (#4418)
    
    llama_context_params.logits_all is a parameter for controlling
    llama_eval. This documents that logits_all should not be used with
    llama_decode and llama_batch.

llama.h

commit d9d4cfef64ea416dd66632173787d03ffb180cc7
Author: Vladimir Zorin <vladimir@deviant.guru>
Date:   Tue Dec 12 11:25:29 2023 +0200

    server : fix local model name in server (#4420)

examples/server/server.cpp

commit 41a11aaf99feff4901e4c8dc48ad00766c5da4e9
Author: Taikono-Himazin <kazu@po.harenet.ne.jp>
Date:   Tue Dec 12 18:24:32 2023 +0900

    ggml : increased GGML_MAX_PARAMS to allow finetuning of 70b models (#4424)

ggml.h

commit 8a7b2fa528f130631a5f43648481596ab320ed5a
Author: Yueh-Po Peng <94939112+y10ab1@users.noreply.github.com>
Date:   Mon Dec 11 06:27:38 2023 +0800

    Update README.md (#4388)
    
    Fix small typo.

examples/server/README.md

commit e18f7345a300920e234f732077bda660cc6cda9c
Author: Xiang (Kevin) Li <kevinli020508@gmail.com>
Date:   Sat Dec 9 16:29:27 2023 -0500

    grammar : revert the replacement of llama_token_to_piece with id_to_token (#4396)

llama.cpp

commit fe680e3d1080a765e5d3150ffd7bab189742898d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 7 22:26:54 2023 +0200

    sync : ggml (new ops, tests, backend, etc.) (#4359)
    
    * sync : ggml (part 1)
    
    * sync : ggml (part 2, CUDA)
    
    * sync : ggml (part 3, Metal)
    
    * ggml : build fixes
    
    ggml-ci
    
    * cuda : restore lost changes
    
    * cuda : restore lost changes (StableLM rope)
    
    * cmake : enable separable compilation for CUDA
    
    ggml-ci
    
    * ggml-cuda : remove device side dequantize
    
    * Revert "cmake : enable separable compilation for CUDA"
    
    This reverts commit 09e35d04b1c4ca67f9685690160b35bc885a89ac.
    
    * cuda : remove assert for rope
    
    * tests : add test-backend-ops
    
    * ggml : fix bug in ggml_concat
    
    * ggml : restore `ggml_get_n_tasks()` logic in `ggml_graph_plan()`
    
    * ci : try to fix macOS
    
    * ggml-backend : remove backend self-registration
    
    * ci : disable Metal for macOS cmake build
    
    ggml-ci
    
    * metal : fix "supports family" call
    
    * metal : fix assert
    
    * metal : print resource path
    
    ggml-ci
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

.github/workflows/build.yml
.gitignore
CMakeLists.txt
Makefile
ggml-alloc.c
ggml-alloc.h
ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-cuda.h
ggml-impl.h
ggml-metal.h
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
scripts/sync-ggml.sh
tests/CMakeLists.txt
tests/test-backend-ops.cpp

commit bcc0eb4591bec5ec02fad3f2bdcb1b265052ea56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Dec 7 13:03:17 2023 +0200

    llama : per-layer KV cache + quantum K cache (#4309)
    
    * per-layer KV
    
    * remove unnecessary copies
    
    * less code duplication, offload k and v separately
    
    * llama : offload KV cache per-layer
    
    * llama : offload K shift tensors
    
    * llama : offload for rest of the model arches
    
    * llama : enable offload debug temporarily
    
    * llama : keep the KV related layers on the device
    
    * llama : remove mirrors, perform Device -> Host when partial offload
    
    * common : add command-line arg to disable KV cache offloading
    
    * llama : update session save/load
    
    * llama : support quantum K cache (#4312)
    
    * llama : support quantum K cache (wip)
    
    * metal : add F32 -> Q8_0 copy kernel
    
    * cuda : add F32 -> Q8_0 copy kernel
    
    ggml-ci
    
    * cuda : use mmv kernel for quantum cache ops
    
    * llama : pass KV cache type through API
    
    * llama : fix build
    
    ggml-ci
    
    * metal : add F32 -> Q4_0 copy kernel
    
    * metal : add F32 -> Q4_1 copy kernel
    
    * cuda : wip
    
    * cuda : add F32 -> Q4_0 and F32 -> Q4_1 copy kernels
    
    * llama-bench : support type_k/type_v
    
    * metal : use mm kernel only for quantum KV cache
    
    * cuda : add comment
    
    * llama : remove memory_f16 and kv_f16 flags
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * readme : add API change notice
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

README.md
common/common.cpp
common/common.h
examples/llama-bench/llama-bench.cpp
examples/quantize-stats/quantize-stats.cpp
examples/server/server.cpp
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
llama.cpp
llama.h

commit 81bc9214a389362010f7a57f4cbc30e5f83a2d28
Author: Hongyu Ouyang <96765450+casavaca@users.noreply.github.com>
Date:   Thu Dec 7 02:25:22 2023 -0800

    train : fix #4227 (double free in examples/train-text-from-scratch/train-text-from-scratch.cpp) (#4351)
    
    On commit b1108 (44c117f4) xaedes added
    
        ggml_allocr * alloc = NULL;
    
        ... (many lines in between)
    
        if (alloc) {
            ggml_allocr_free(alloc);
        }
    
    Which is correct, but it's easy to lose context after many lines in between.
    
    On commit b1287 (0e76a899) xaedes made a big change. From here on, alloc is freed eagerly.
    
        alloc = ggml_allocr_new(...)
        ... (short lines of code)
        ggml_allocr_free(alloc)
    
    This happens a few times, but alloc is never set to NULL, and many lines below,
    we still have
    
        if (alloc) {
            ggml_allocr_free(alloc);
        }
    
    which causes a double-free.

examples/train-text-from-scratch/train-text-from-scratch.cpp

commit 05cd6e5036d72d0930de4d8f6be7bce09e8dda24
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 6 20:21:59 2023 +0200

    server : recognize cache_prompt parameter in OAI API (#4347)

examples/server/server.cpp

commit caa9249217c5fd524b900add5ddcbeaa20cbcb12
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Dec 6 10:41:03 2023 +0200

    common : fix compile warning

common/sampling.cpp

commit da5eaef1f34d0a1f584cd4a092e7691ea46a9d91
Author: stduhpf <stephduh@live.fr>
Date:   Wed Dec 6 09:08:17 2023 +0100

    speculative : support `--color` (#4343)
    
    * speculative: add some colors
    
    * minor : add braces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/speculative/speculative.cpp

commit 5f6e0c0dff1e7a89331e6b25eca9a9fd71324069
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Tue Dec 5 10:55:12 2023 -1000

    grammar : pre-computed pieces + reserve mem + less string copies (#4330)
    
    * reserve space for codepoints
    
    * improvement for the appended 0
    
    * used precomputed token text for grammar sample
    
    * reserve canidates_decoded
    
    * reserve canidates_grammar
    
    * remove candidates_decoded
    
    * Revert "remove candidates_decoded"
    
    This reverts commit 3773328080e6a139ee83198329a13cf4ff61d707.
    
    * changed decode_utf8 to take src by ref

llama.cpp

commit 5aa365d88fdb8fdd430ef3fc141c7a5fd37c3502
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Tue Dec 5 10:19:18 2023 -0700

    llama : allow overriding GGUF metadata when loading model (#4092)
    
    * feat: Allow overriding GGUF metadata when loading model
    
    * Fix the one time GCC is stricter than clang about something
    
    * Step1
    
    * Refactor... basically everything!
    
    * Nuke obsolete GetArrayLen struct
    
    * simplify std::string specialization
    
    * Various cleanups
    
    Add informational output when overrides are applied
    
    Warn user when an override with the wrong type is specified
    
    * Fix broken logic for parsing bool KV overrides
    Fix issue where overrides didn't apply when key missing in GGUF metadata
    Resolve merge changes
    
    * llama : rearrange model params
    
    * Update new GET_KEY call
    
    Add note that metadata KV overrides aren't reflected in initial metadata KV info dump
    
    ---------
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
llama.cpp
llama.h

commit 52c8bc3cf312e1caf02d37bfb9d9d865cbe33594
Author: MaggotHATE <clay1326@gmail.com>
Date:   Tue Dec 5 15:05:51 2023 +0500

    sampling : custom samplers order (#4285)
    
    * Samplers sequence order w parameter
    
    * Cleaned commented code
    
    * Fixed formatting
    
    * Rewrote with unordered_map
    
    * Revert and rewrite, too many problems and safeguards would be needed
    
    * Fixed code style
    
    * Code style fixes according to review
    
    * More readable samplers input string, fixed help
    
    * Style fix in sampler_queue
    
    * Formatting fixes
    
    * Fixing whitespaces

common/common.cpp
common/common.h
common/sampling.cpp
common/sampling.h
examples/main/main.cpp

commit e4b76bbe316ee50fb17d9ac29e654c0edf830eba
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Mon Dec 4 23:29:46 2023 -0800

    swift : revert compiler checks for swift package (#4332)

Package.swift

commit 23b5e12eb5a76489b4c3ee22213a081da68b1809
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Mon Dec 4 17:04:21 2023 +0100

    simple : update error message for KV cache check (#4324)
    
    This commit updates the error message that is printed when the
    KV cache is not big enough to hold all the prompt and generated
    tokens. Specifically it removes the reference to n_parallel and
    replaces it with n_len.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/simple/simple.cpp

commit d208995c6da66f252d4054c1c5a90eb8ccb7a2f7
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Tue Dec 5 01:03:49 2023 +0900

    swift : fix concatenation method to avoid invalid UTF8 stringfication (#4325)

examples/llama.swiftui/llama.cpp.swift/LibLlama.swift

commit 5c9f90cba1cc6b0a2a7d19ee5dcb73cad6331d30
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Mon Dec 4 22:43:45 2023 +0900

    swift : fix prompt tokenization logic (#4321)

examples/batched.swift/Sources/main.swift
examples/llama.swiftui/llama.cpp.swift/LibLlama.swift

commit 4fa44e84adb4c78e1885694cc3513982d4af2b08
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Mon Dec 4 16:57:35 2023 +0900

    grammar-parser : fix typo (#4318)
    
    preceeding -> preceding

common/grammar-parser.cpp

commit fbbc42827b2949b95bcde23ce47bb47d006c895d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 3 15:56:35 2023 +0200

    ggml : reuse ggml_get_n_tasks() in ggml_graph_plan() (#4308)
    
    * ggml : fix soft max out-of-bounds access
    
    ggml-ci
    
    * ggml : reuse ggml_get_n_tasks() in ggml_graph_plan()
    
    ggml-ci

ggml.c

commit adf3de4f69ff7e44131222f05f9c7447ac0be3cb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 3 15:56:22 2023 +0200

    ggml : fix soft max out-of-bounds access (#4307)
    
    ggml-ci

ggml.c

commit 33e171d1e9fc4903f9314b490d77fb8d58331b63
Author: Ed Lee <edilee@mozilla.com>
Date:   Sun Dec 3 01:10:43 2023 -0800

    server : fix OpenAI API `stop` field to be optional (#4299)
    
    (cherry picked from commit Mozilla-Ocho/llamafile@e8c92bcb84ae3bcbf0d617b7ee6a5413bcbd58af)

examples/server/server.cpp

commit 6949b50df56ee58a2d76d45487942cb211c08629
Author: Rickard Edén <rickardeden@gmail.com>
Date:   Sun Dec 3 10:03:25 2023 +0100

    py : add grammar to oai like api (#4294)

examples/server/api_like_OAI.py

commit d7b800b8bc490a221acbd83c575206a907f2f6e2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Dec 3 10:58:16 2023 +0200

    llama : pad KV cache size (#4280)
    
    * llama : pad KV cache size to 32
    
    * metal : try to improve batched decoding

ggml-metal.m
llama.cpp

commit 5a7d3125e7c24f223659b7f0b7aa7736986e92c0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 1 20:39:12 2023 +0200

    llama : avoid using "optional" keyword (#4283)

llama.cpp

commit d5a1cbde60531d02ac74da27ea355182e3a4d516
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 1 20:35:03 2023 +0200

    llama : support optional tensors (#4283)

examples/server/server.cpp
llama.cpp

commit b220222a64ce760bfbec9c770f11db3ec6a6abb6
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Sat Dec 2 03:19:45 2023 +0900

    swift : fix token_to_piece implementation (#4278)
    
    * Fix token_to_piece implementation in Swift
    
    * Fix errors

examples/batched.swift/Sources/main.swift
examples/llama.swiftui/llama.cpp.swift/LibLlama.swift

commit 511f52c334e37033f9c9de07b98fca4abc9470bd
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Fri Dec 1 13:18:35 2023 -0500

    build : enable libstdc++ assertions for debug builds (#4275)

CMakeLists.txt
Makefile

commit 03562f3a86d6706eea9f4fc09b532946c191b34e
Author: CausalLM <148736309+CausalLM@users.noreply.github.com>
Date:   Sat Dec 2 02:17:06 2023 +0800

    llama : support attention bias on LLaMA architecture (#4283)
    
    * Support attention_bias on LLaMA architecture
    
    QKVO bias, should fix InternLM (https://github.com/ggerganov/llama.cpp/issues/3133) and works for LLaMAfied Qwen models (https://github.com/ggerganov/llama.cpp/pull/3743#issuecomment-1825923608).
    
    * check existence of qkvo bias while loading llama models
    
    Tested on LLaMA2, CUDA and CPU.
    
    * Update llama.cpp

llama.cpp

commit 37c746d687d877bc11803e96b4dc5f378b83c0a0
Author: Shijie <821898965@qq.com>
Date:   Sat Dec 2 02:16:31 2023 +0800

    llama : add Qwen support (#4281)
    
    * enable qwen to llama.cpp
    
    * llama : do not GPU split bias tensors
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-hf-to-gguf.py
gguf-py/gguf/constants.py
gguf-py/gguf/tensor_mapping.py
llama.cpp
prompts/chat-with-qwen.txt

commit 880f57973b8e0091d0f9f50eb5ab4cd4e31582ca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 1 18:42:11 2023 +0200

    llama : fix integer overflow during quantization (#4284)
    
    happens with multi-threaded quantization of Qwen-72B
    
    ggml-ci

llama.cpp

commit 8d6d9f033b8101f929e445cf45b39e1557ca7934
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Dec 1 10:41:56 2023 +0100

    py : add requirements file for convert-hf-to-gguf.py (#4277)
    
    This commit adds a requirements file for the convert-hf-to-gguf.py
    script, and also add the torch and transformers packages to it.
    
    The motivation for this is that currently running convert-hf-to-gguf.py
    will produce the following error:
    ```console
    $ python3 -m venv venv
    $ source venv/bin/activate
    (venv) $ pip install -r requirements.txt
    Collecting numpy==1.24.4
    Collecting sentencepiece==0.1.98
    Collecting gguf>=0.1.0
    Installing collected packages: sentencepiece, numpy, gguf
    Successfully installed gguf-0.5.1 numpy-1.24.4 sentencepiece-0.1.98
    
    (venv) $ python convert-hf-to-gguf.py --help
    Traceback (most recent call last):
      File "llama.cpp/convert-hf-to-gguf.py", line 16, in <module>
        import torch
    ModuleNotFoundError: No module named 'torch'
    ```
    With this commit, and using requirements-hf-to-gguf.txt instead of
    requirements.txt, the script can be run and shows the help output.
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

requirements-hf-to-gguf.txt

commit ef47ec18da469423c276b683dd9b5741cee7023e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Dec 1 10:51:24 2023 +0200

    ggml : add ggml_soft_max_ext (#4256)
    
    * metal : implement soft_max_ext
    
    * cuda : implement soft_max_ext
    
    * ggml : implement soft_max_ext (CPU)
    
    * batched-bench : print threads
    
    ggml-ci
    
    * metal : simplify soft_max encoding
    
    ggml-ci
    
    * cuda : use 512 threads for soft_max instead of 32
    
    * ggml : update soft max cpu
    
    * cuda : do warp-based block reduce
    
    * cuda : increase max block size to 1024
    
    * cuda : fix warp reduction initialization of shared mem
    
    * metal : warp-based reduction for soft max kernel
    
    * metal : warp-based reduce for rms_norm
    
    * metal : simplify soft max kernel
    
    ggml-ci
    
    * alloc : fix build with debug

examples/batched-bench/batched-bench.cpp
ggml-alloc.c
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
llama.cpp

commit 1d144112c0fbbb4ecc07dbcf4f05a380148bd6de
Author: Ziad Ben Hadj-Alouane <zied.benhadjalouane@gmail.com>
Date:   Thu Nov 30 17:25:49 2023 -0500

    server : add --log-disable to disable logging to file (#4260)
    
    * * add --log-disable to disable logging to file in the server example
    
    * * typo fix

examples/server/server.cpp

commit f43f09366dfd018e4568e23a232aaa8c4f7cfc78
Author: Ziad Ben Hadj-Alouane <zied.benhadjalouane@gmail.com>
Date:   Thu Nov 30 17:25:04 2023 -0500

    server : add single-client multi-prompt support (#4232)
    
    * * add multiprompt support
    
    * * cleanup
    
    * * more cleanup
    
    * * remove atomicity of id_gen, and change lock_guard to unique_lock on completion requests
    
    * * remove all references to mutex_multitasks
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update examples/server/server.cpp
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * * change to set
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

examples/server/server.cpp

commit d2809a3ba2780e00fce5a6149a7eda09f1c0e906
Author: WillCorticesAI <150854901+WillCorticesAI@users.noreply.github.com>
Date:   Thu Nov 30 17:23:44 2023 -0500

    make : fix Apple clang determination bug (#4272)
    
    Co-authored-by: Will Findley <findley@gmail.com>

Makefile

commit 15f5d96037e597523b721aa39c874d69de2acf85
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Thu Nov 30 17:23:08 2023 -0500

    build : fix build info generation and cleanup Makefile (#3920)
    
    * cmake : fix joining of REAL_GIT_DIR
    
    * fix includes with help from include-what-you-use
    
    * make : remove unneeded deps and add test-rope target
    
    * fix C includes in C++ source files
    
    * Revert "fix includes with help from include-what-you-use"
    
    This reverts commit 635e9fadfd516d4604a0fecf4a854bfb25ad17ae.

.gitignore
Makefile
common/CMakeLists.txt
ggml-opencl.cpp
llama.cpp

commit 33c9892af58b7b161f2a532935dcccff8c8048c6
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Thu Nov 30 23:11:14 2023 +0100

    llava : ShareGPT4V compatibility (vision encoder only loading) (#4172)
    
    * ShareGPT4 compatibility (vision encoder only loading)
    
    Load only a CLIP vision encoder (as supplied by ShareGPT finetunes)
    Corrects the argument parsing for --img_mean and --img_std (which were previously not parsed but attempted to access)
    Defines defaults for img_mean and img_std which are equal to the llava 1.5 CLIP encoder, so you do not have to provide them
    
    * Update convert-image-encoder-to-gguf.py

examples/llava/convert-image-encoder-to-gguf.py

commit 8efa0f6ebed53c9453e6721da86fb294e5015909
Author: Andrew Godfrey <AndrewGodfrey@users.noreply.github.com>
Date:   Thu Nov 30 13:56:19 2023 -0800

    main : pass LOG_TEE callback to llama.cpp log (#4033)
    
    * main : Call llama_log_set to use LOG_TEE
    
    * tabs to spaces

examples/main/main.cpp

commit 524907aa768a26cbf83d8e2eb30547e2ee1d1b1a
Author: vodkaslime <646329483@qq.com>
Date:   Fri Dec 1 05:49:21 2023 +0800

    readme : fix (#4135)
    
    * fix: readme
    
    * chore: resolve comments
    
    * chore: resolve comments

README.md

commit 3bd2c7ce1b752973cf937482a0333e85d1681e2b
Author: Juraj Bednar <juraj@bednar.io>
Date:   Thu Nov 30 22:46:01 2023 +0100

    docker : add finetune option (#4211)

.devops/tools.sh

commit bde629bb53b85886ee0fe83524c1efe2689bc618
Author: Miwa / Ensan <63481257+ensan-hcl@users.noreply.github.com>
Date:   Fri Dec 1 06:45:17 2023 +0900

    batched.swift : update README.md (#4214)
    
    docs: update how to run

examples/batched.swift/README.md

commit f7f9e06212d44530b3200033286049dbdf84b3d3
Author: Li Tan <tanliboy@gmail.com>
Date:   Thu Nov 30 13:44:11 2023 -0800

    cmake : fix the metal file foder path (#4217)

CMakeLists.txt

commit 74daabae6927b99e7333d6126dee35193c418457
Author: Dawid Wysocki <62249621+TortillaZHawaii@users.noreply.github.com>
Date:   Thu Nov 30 22:43:32 2023 +0100

    readme : fix typo (#4253)
    
    llama.cpp uses GitHub Actions, not Gitlab Actions.

README.md

commit b18c66ca6eee4fe0465cff5042daf05005dc9ab2
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Nov 30 22:43:08 2023 +0100

    llama : fix alignment of general.name in print meta (#4254)
    
    * llama: fix alignment of general.name in print meta
    
    This commit fixes the alignment of the general.name field in the
    llm_load_print_meta function.
    
    Currently the output looks like this:
    ```console
    llm_load_print_meta: model ftype      = mostly Q4_0
    llm_load_print_meta: model params     = 13.02 B
    llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW)
    llm_load_print_meta: general.name   = LLaMA v2
    ```
    And with this commit it looks like this:
    ```console
    llm_load_print_meta: model ftype      = mostly Q4_0
    llm_load_print_meta: model params     = 13.02 B
    llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW)
    llm_load_print_meta: general.name     = LLaMA v2
    ```
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    * llama: fix alignment of special tokens
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>
    
    ---------
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

llama.cpp

commit f4d973cecb7368c985720ba9100ae6abba14806d
Author: slaren <slarengh@gmail.com>
Date:   Thu Nov 30 22:42:23 2023 +0100

    convert.py : fix llama/llama2 conversion due to vocab_size=-1 (#4258)

convert.py

commit 954e22858c5cea1dc03e9172d3879402af2b5990
Author: tarcey <cey.tarik@gmail.com>
Date:   Thu Nov 30 22:40:23 2023 +0100

    llama : fix typical sampling (#4261)
    
    Typical sampling was broken because after copying new_candidates into canditates, the "sorted" bool is left at "true", but the new data is no longer sorted according to probability. Patch to set "sorted" to false.
    
    Test: Generating with temp=0.0001 (approx. argmax)  should generate the same sequence at typical>=1.0 and typical=0.9999 (approx. disabled, but enters the typical sampling codepath).

llama.cpp

commit e2bd725f4b39bc5c6234858d158e01248f5ab5bd
Author: rhjdvsgsgks <26178113+rhjdvsgsgks@users.noreply.github.com>
Date:   Thu Nov 30 20:50:40 2023 +0000

    py : fix oai proxy (#3972)
    
    * fix oai proxy
    
    fix generation not stoped while bot stop talking in chat mode
    
    fix possible `slot_id` not exist
    
    response for cors (and pre flight)
    
    * oai proxy: workaround for some client (such as Chatbox)
    
    * use stop as separator to replace hardcoded `\n`

examples/server/api_like_OAI.py

commit 1f5cd83275fabb43f2ae92c30033b384a3eb37b4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 29 11:00:17 2023 +0200

    examples : add readme files

examples/lookahead/README.md
examples/speculative/README.md

commit 4fea3420ee3918d125d74c94d962a6ea82875351
Author: Peter Sugihara <peter@campsh.com>
Date:   Tue Nov 28 23:16:34 2023 -0800

    readme : add FreeChat (#4248)

README.md

commit 64e64aa2557d97490b2fe1262b313e2f4a1607e3
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Tue Nov 28 04:51:11 2023 -0500

    ggml : restore abort() in GGML_ASSERT (#4242)

ggml.h

commit 8406b0924bf323f37d536dee8b8165c1f3d9d11d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Nov 28 10:32:03 2023 +0200

    ggml : re-enable BLAS for CPU when src0 != F32 + remove redundant full offload checks in llama.cpp (#4240)
    
    * ggml : use blas even if src0 is not F32
    
    * llama : use n_threads_batch only when n_tokens >= 32
    
    ggml-ci
    
    * llama : revert n_threads_batch logic
    
    ggml-ci

ggml.c
llama.cpp

commit b38a16dfcff88d547f78f52d1bea31b84a05aff7
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Mon Nov 27 15:25:42 2023 -0400

    cmake : fix issue with version info not getting baked into LlamaConfig.cmake (#3970)
    
    * Split CPP generation from build-info query
    
    * Remove blank lines
    
    * Add BUILD_SHARED_LIBS option

CMakeLists.txt
common/CMakeLists.txt
scripts/build-info.cmake
scripts/gen-build-info-cpp.cmake

commit 0dab8cd7cca7e1bc3550dcb4797b9062cdbb1ebd
Author: Kasumi <90275229+kasumi-1@users.noreply.github.com>
Date:   Tue Nov 28 01:39:42 2023 +0800

    readme : add Amica to UI list (#4230)

README.md

commit bb03290c17540768a16000a2b01ee4f22440aba1
Author: Bailey Chittle <39804642+bachittle@users.noreply.github.com>
Date:   Mon Nov 27 09:56:52 2023 -0500

    examples : iOS example with swift ui (#4159)
    
    * copy to llama.cpp as subdir
    
    * attempt enabling metal, fails
    
    * ggml metal compiles!
    
    * Update README.md
    
    * initial conversion to new format, utf8 errors?
    
    * bug fixes, but now has an invalid memory access :(
    
    * added O3, now has insufficient memory access
    
    * begin sync with master
    
    * update to match latest code, new errors
    
    * fixed it!
    
    * fix for loop conditionals, increase result size
    
    * fix current workflow errors
    
    * attempt a llama.swiftui workflow
    
    * Update .github/workflows/build.yml
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
examples/llama.swiftui/.gitignore
examples/llama.swiftui/README.md
examples/llama.swiftui/llama.cpp.swift/LibLlama.swift
examples/llama.swiftui/llama.cpp.swift/bridging-header.h
examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj
examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/contents.xcworkspacedata
examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist
examples/llama.swiftui/llama.swiftui/Assets.xcassets/AccentColor.colorset/Contents.json
examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset/Contents.json
examples/llama.swiftui/llama.swiftui/Assets.xcassets/Contents.json
examples/llama.swiftui/llama.swiftui/Models/LlamaState.swift
examples/llama.swiftui/llama.swiftui/Preview Content/Preview Assets.xcassets/Contents.json
examples/llama.swiftui/llama.swiftui/Resources/models/.gitignore
examples/llama.swiftui/llama.swiftui/UI/ContentView.swift
examples/llama.swiftui/llama.swiftui/llama_swiftuiApp.swift

commit f3b269813f6147c5b5cda082e6b45cf04a932e0d
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Sun Nov 26 22:58:43 2023 -0500

    ggml : fix -Warray-bounds warning with gcc (#4231)

ggml.c

commit 3e73d31d9cc0232882ce61c64742aff3ecfec416
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 26 21:51:46 2023 +0200

    lookahead : support `-n -1` infinite generation

examples/lookahead/lookahead.cpp

commit 9656026b53236ed7328458269c4c798dd50ac8d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 26 20:42:51 2023 +0200

    readme : update hot topics

README.md

commit 922754a8d60080e956891f6cee1fb03aa48d57c6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 26 20:33:07 2023 +0200

    lookahead : add example for lookahead decoding (#4207)
    
    * lookahead : init
    
    * lookahead : generate and store n-grams
    
    * lookahead : use loop instead recursion to generate n-grams
    
    * lookahead : initial working implementation
    
    * lookahead : filter repeating n-grams
    
    * lookahead : use deterministic init
    
    * lookahead : add to Makefile
    
    * lookahead : fix a bug in the seq_id of the lookahead tokens
    
    * lookahead : add comments
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

.gitignore
Makefile
examples/CMakeLists.txt
examples/lookahead/CMakeLists.txt
examples/lookahead/lookahead.cpp

commit 22da05536ff4ad963080773bef1fb839fdab95d3
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Sun Nov 26 02:30:02 2023 -0600

    metal : fix yarn (#4220)
    
    get the correct n_orig_ctx in metal

ggml-metal.m

commit 1ddb52ec38f9931925a587f45a23b1c37152c028
Author: Galunid <karolek1231456@gmail.com>
Date:   Sat Nov 25 22:45:02 2023 +0100

    scripts : Use mmap in torch load (#4202)
    
    * Use mmap in torch load, prefer .bin files when loading
    
    * Revert .bin > .safetensors preference

convert-hf-to-gguf.py

commit f837c3a992b2b6146936cb120871a8cf9d0e3857
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Sat Nov 25 08:58:23 2023 -0800

    llama : grammar `reserve` space in `decode_utf8` (#4210)
    
    * reserve space for codepoints
    
    * improvement for the appended 0

llama.cpp

commit 3014b5415d08e3dff961da6eea835b9760a701b8
Author: crasm <crasm@git.vczf.us>
Date:   Sat Nov 25 10:47:07 2023 -0500

    Update docs for yarn_ext_factor <0.0 as unspecified instead of NaN (#4189)

convert.py
llama.h

commit 04814e718edb13bdf8cca861dc2e5ab4e1995c30
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 25 12:02:13 2023 +0200

    readme : update hot topics

README.md

commit af19d3573481d409b3c4e55494810eb1f65a9aae
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Nov 25 11:29:06 2023 +0200

    server : OAI API compatibility (#4198)
    
    * Add openai-compatible POST /v1/chat/completions API endpoint to server example
    
    * fix code style
    
    * Update server README.md
    
    * Improve server README.md
    
    * Fix server.cpp code style according to review
    
    * server : some style changes
    
    * server : indentation
    
    * server : enable special tokens during tokenization by default
    
    * server : minor code style
    
    * server : change random string generator
    
    * straightforward /v1/models endpoint
    
    ---------
    
    Co-authored-by: kir-gadjello <111190790+kir-gadjello@users.noreply.github.com>
    Co-authored-by: Tobi Lütke <tobi@Tobis-MacBook-Pro.local>

examples/server/README.md
examples/server/server.cpp

commit e9c13ff78114af6fc6a4f27cc8dcdda0f3d389fb
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 24 18:10:01 2023 +0100

    llama : set metal log callback correctly (#4204)

llama.cpp

commit 8a052c131ed3525313cdb84e5ae4e2b6cf8d2e24
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 24 18:04:31 2023 +0100

    ggml-cuda : support stablelm rope (#4156)
    
    * ggml-cuda : support stablelm rope
    
    * remove unused freq_base kernel parameter
    
    * add n_dims parameter to llm_build_k_shift, default to n_rot via overload
    
    * llama : fix llm_build_k_shift args
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-cuda.cu
llama.cpp

commit 189d68446e7ef21e8f3af3c0a3d91c35a39aec89
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri Nov 24 15:02:49 2023 +0100

    convert : fix tensors using grad in some models (#4173)

convert-hf-to-gguf.py

commit 2568a4bf548d7392e9c78c008b33b4c11d53fe95
Author: eastriver <lee@eastriver.dev>
Date:   Fri Nov 24 18:25:10 2023 +0900

    main.swift : fix eos checking (#4197)
    
    llama_token_eos(const struct llama_model *) is currently getting struct llama_context type variable context as a parameter.

examples/batched.swift/Sources/main.swift

commit b35f3d0def3efde92ed465d92a267430d957e87d
Author: Aaryaman Vasishta <aaryaman.vasishta@amd.com>
Date:   Fri Nov 24 16:52:39 2023 +0900

    readme : use PATH for Windows ROCm (#4195)
    
    * Update README.md to use PATH for Windows ROCm
    
    * Update README.md
    
    * Update README.md

README.md

commit 55978ce09b69d3987d17d08d92d8cc27193e0773
Author: Haohui Mai <ricetons@gmail.com>
Date:   Thu Nov 23 13:56:53 2023 -0800

    Fix incorrect format strings and uninitialized variables. (#4133)
    
    * Fix incorrect format strings and uninitialized variables.
    
    * Address comments
    
    * Add the missing include statement

examples/server/server.cpp
ggml-cuda.cu

commit 6b0a7420d03b9d13cb0e9439a01ce8476d8bf093
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 23 19:07:56 2023 +0200

    llama : KV cache view API + better KV cache management (#4170)
    
    * llama : keep track of used KV cells + better KV cache management
    
    * llama : zero KV cache used upon clear
    
    ggml-ci
    
    * llama : allow exporting a view of the KV cache (#4180)
    
    * Allow exporting a view of the KV cache
    
    * Allow dumping the sequences per cell in common
    
    * Track max contiguous cells value and position as well
    
    * Fix max contiguous empty cells index calculation
    
    Make dump functions deal with lengths or sequences counts > 10 better
    
    * Fix off by one error in dump_kv_cache_view
    
    * Add doc comments for KV cache view functions
    
    Eliminate cell sequence struct; use llama_seq_id directly
    
    Minor cleanups
    
    * common : add -dkvc arg for enabling kv cache dumps
    
    ---------
    
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>

common/common.cpp
common/common.h
examples/parallel/parallel.cpp
llama.cpp
llama.h

commit d103d935c0e75769a6a597f7a64cab72c6cc3e79
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 23 13:51:22 2023 +0200

    readme : update hot topics

README.md

commit 9d5949f04b1f8b76184818abbd99938c2020803f
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Thu Nov 23 12:34:20 2023 +0100

    examples : fix typo in parallel example doc comment (#4181)
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

examples/parallel/parallel.cpp

commit ff8238f71d56245f4a6dbea693f4ebd81263464d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 23 11:35:04 2023 +0200

    docs : add llama-star arch idea

docs/llama-star/idea-arch.key
docs/llama-star/idea-arch.pdf

commit 8e672efe632bb6a7333964a255c4b96f018b9a65
Author: Galunid <karolek1231456@gmail.com>
Date:   Tue Nov 21 16:22:30 2023 +0100

    stablelm : simplify + speedup generation (#4153)

llama.cpp

commit 0b871f1a04ef60e114bbe43004fd9c21114e802d
Author: Galunid <karolek1231456@gmail.com>
Date:   Mon Nov 20 19:30:00 2023 +0100

    finetune - update readme to mention llama support only (#4148)

examples/finetune/README.md

commit dfc7cd48b1cc31d759c093e917a18c0efe03d0e8
Author: Aaryaman Vasishta <aaryaman.vasishta@amd.com>
Date:   Tue Nov 21 00:02:46 2023 +0900

    readme : update ROCm Windows instructions (#4122)
    
    * Update README.md
    
    * Update README.md
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

README.md

commit 881800d1f083c39431cef288347082be516d1c80
Author: Seb C <47074056+Sebby37@users.noreply.github.com>
Date:   Tue Nov 21 00:26:59 2023 +1030

    main : Add ChatML functionality to main example (#4046)
    
    Co-authored-by: Sebastian Cramond <sebby37@users.noreply.github.com>

common/common.cpp
common/common.h
examples/infill/infill.cpp
examples/main/main.cpp

commit f23c0359a32871947169a044eb1dc4dbffd0f405
Author: Galunid <karolek1231456@gmail.com>
Date:   Mon Nov 20 11:35:47 2023 +0100

    ci : add flake8 to github actions (python linting) (#4129)
    
    Disabled rules:
    
    * E203 Whitespace before ':' - disabled because we often use 'C' Style where values are aligned
    
    * E211 Whitespace before '(' (E211) - disabled because we often use 'C' Style where values are aligned
    
    * E221 Multiple spaces before operator - disabled because we often use 'C' Style where values are aligned
    
    * E225 Missing whitespace around operator - disabled because it's broken so often it seems like a standard
    
    * E231 Missing whitespace after ',', ';', or ':' - disabled because we often use 'C' Style where values are aligned
    
    * E241 Multiple spaces after ',' - disabled because we often use 'C' Style where values are aligned
    
    * E251 Unexpected spaces around keyword / parameter equals - disabled because it's broken so often it seems like a standard
    
    * E261 At least two spaces before inline comment - disabled because it's broken so often it seems like a standard
    
    * E266 Too many leading '#' for block comment - sometimes used as "section" separator
    
    * E501 Line too long - disabled because it's broken so often it seems like a standard
    
    * E701 Multiple statements on one line (colon) - broken only in convert.py when defining abstract methods (we can use# noqa instead)
    
    * E704 Multiple statements on one line - broken only in convert.py when defining abstract methods (we can use# noqa instead)

.github/workflows/python-lint.yml
convert-hf-to-gguf.py
convert-llama-ggml-to-gguf.py
convert-persimmon-to-gguf.py
convert.py
gguf-py/gguf/gguf_writer.py
tests/test-tokenizer-0-falcon.py
tests/test-tokenizer-0-llama.py

commit 40a34fe8d034bd484efd79ccbb95059ca6308dcb
Author: Branden Butler <bwtbutler@hotmail.com>
Date:   Mon Nov 20 03:50:04 2023 -0600

    speculative : fix prompt tokenization in speculative example (#4025)
    
    * Support special tokens and not adding BOS to prompt in speculative
    
    * Adapt to new should_add_bos function
    
    * Ensure tgt and dft have same add_bos setting

examples/speculative/speculative.cpp

commit dae06c06e5c6232ae2be4d567dd5101e1e96c814
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 19 19:16:07 2023 +0200

    Revert "finetune : add --n-gpu-layers flag info to --help (#4128)"
    
    This reverts commit 05e8301e4593e2a67b4bae24f093dd12ce5cc7c2.

examples/finetune/finetune.cpp

commit 05e8301e4593e2a67b4bae24f093dd12ce5cc7c2
Author: Clark Saben <76020733+csaben@users.noreply.github.com>
Date:   Sun Nov 19 11:56:38 2023 -0500

    finetune : add --n-gpu-layers flag info to --help (#4128)

examples/finetune/finetune.cpp

commit 936c79b2275a8f15f3512e63de615c676904d650
Author: SoftwareRenderer <138734813+SoftwareRenderer@users.noreply.github.com>
Date:   Sun Nov 19 11:54:10 2023 -0500

    server : relay error messages (#4131)

examples/server/public/completion.js
examples/server/server.cpp

commit 262005ad9ded375e9c544a02c18ef6254fe185a2
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Sun Nov 19 08:52:57 2023 -0800

    common : comma should be semicolon (#4137)

common/common.cpp

commit 35985acffaab1eb4ffcbc22c86063bc630f24a89
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Nov 19 18:50:49 2023 +0200

    gitignore : tokenize

.gitignore

commit e937066420b79a757bf80e9836eb12b88420a218
Author: slaren <slarengh@gmail.com>
Date:   Sun Nov 19 11:10:52 2023 +0100

    gguf-py : export chat templates (#4125)
    
    * gguf-py : export chat templates
    
    * llama.cpp : escape new lines in gguf kv info prints
    
    * gguf-py : bump version
    
    * gguf-py : check chat_template type
    
    * gguf-py : initialize chat_template

gguf-py/gguf/constants.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/vocab.py
gguf-py/pyproject.toml
llama.cpp

commit 28a2e6e7d476717881be6eb9e2d3331342cec57b
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Nov 18 14:48:17 2023 -0700

    tokenize example: Respect normal add BOS token behavior (#4126)
    
    Allow building with Makefile

Makefile
examples/tokenize/tokenize.cpp

commit 0b5c3b04572a05f80163a365070fb377a837ac27
Author: Galunid <karolek1231456@gmail.com>
Date:   Sat Nov 18 21:08:33 2023 +0100

    scripts : Remove missed baichuan convert script (#4127)

convert-baichuan-hf-to-gguf.py

commit 2923f17f6fec049a71186636c3c4d96408856194
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Nov 18 08:11:18 2023 -0700

    Clean up ggml-cuda.cu warnings when compiling with clang (for ROCM) (#4124)
    
    * ggml-cuda.cu: Clean up warnings when compiling with clang
    
    * ggml-cuda.cu: Move static items into anonymous namespace
    
    * ggml-cuda.cu: Fix use of namespace start macro
    
    * Revert "ggml-cuda.cu: Fix use of namespace start macro"
    
    This reverts commit 26c11490266c096e3e5731e05270a8f73a5b2874.
    
    * Revert "ggml-cuda.cu: Move static items into anonymous namespace"
    
    This reverts commit e29757e0f7535d1ac314300f0324684cc785e06c.

ggml-cuda.cu

commit bbecf3f415797f812893947998bda4f866fa900e
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 17 20:39:11 2023 +0100

    llama : increase max nodes (#4115)

llama.cpp

commit 8e9361089dd31ae9ae59452a8ee409fd51a16371
Author: Roger Meier <r.meier@siemens.com>
Date:   Fri Nov 17 17:11:23 2023 +0100

    build : support ppc64le build for make and CMake (#3963)
    
    * build: support ppc64le build for make and CMake
    
    * build: keep __POWER9_VECTOR__ ifdef and extend with __powerpc64__
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
Makefile
ggml-quants.c

commit 5ad387e994dde77a47ec547a4a65f7611dc325f4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 17 18:01:38 2023 +0200

    tokenize : fix trailing whitespace

examples/tokenize/tokenize.cpp

commit 2fa02b4b3d86182381311c98b75065ee1b7c2930
Author: zakkor <edward.partenie@gmail.com>
Date:   Fri Nov 17 17:36:44 2023 +0200

    examples : add tokenize (#4039)

examples/CMakeLists.txt
examples/tokenize/CMakeLists.txt
examples/tokenize/tokenize.cpp

commit 2ab0707acbf3a3ca9e5bc5959c7920c22eba2257
Author: Don Mahurin <dmahurin@users.noreply.github.com>
Date:   Fri Nov 17 07:32:34 2023 -0800

    convert : use 'model' value if it exists. This allows karpathy/tinyllamas to load (#4089)
    
    Co-authored-by: Don Mahurin <@>

convert.py

commit 11173c92d6eaa2bd1308c2389f44f838480836ac
Author: John <78893154+cmp-nct@users.noreply.github.com>
Date:   Fri Nov 17 16:24:30 2023 +0100

    py : Falcon HF compatibility (#4104)
    
    Falcon HF compatibility

convert-hf-to-gguf.py

commit 9e87ef60e18d69338c5efea314aa7e718bf2040a
Author: Jannis Schönleber <joennlae@gmail.com>
Date:   Fri Nov 17 16:24:07 2023 +0100

    common : improve yaml log escaping (#4080)
    
    * logging: improve escaping in yaml output
    
    * logging: include review feedback

common/common.cpp

commit c7cce1246e248124117ae5bc058923e3ade95f11
Author: Huawei Lin <huaweilin.cs@gmail.com>
Date:   Fri Nov 17 10:22:56 2023 -0500

    llava : fix compilation warning that fread return value is not used (#4069)

examples/llava/llava.cpp

commit f7d5e975424ff0eea55ca5a9181ac8e15553c1fc
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Fri Nov 17 16:20:53 2023 +0100

    py : remove superfluous import statements (#4076)
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>
    Co-authored-by: Jiri Podivin <jpodivin@redhat.com>

convert-baichuan-hf-to-gguf.py
convert-llama-ggml-to-gguf.py
examples/finetune/convert-finetune-checkpoint-to-gguf.py
tests/test-tokenizer-0-falcon.py
tests/test-tokenizer-0-llama.py

commit ba4cf5c0bf37a729d29e899dadf14541cddd23d4
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Fri Nov 17 16:19:16 2023 +0100

    train : move number of gpu layers argument parsing to common/train.cpp (#4074)
    
    - introduces help entry for the argument
     - cuts '--gpu-layers' form in order to simplify usage and documentation.
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>
    Co-authored-by: Jiri Podivin <jpodivin@redhat.com>

common/train.cpp
examples/finetune/finetune.cpp

commit e85bb1a8e736228a1f0d965777de5f77f22834b8
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 17 16:17:37 2023 +0100

    llama : add functions to get the model's metadata (#4013)
    
    * llama : add functions to get the model's metadata
    
    * format -> std::to_string
    
    * better documentation

ggml.c
ggml.h
llama.cpp
llama.h

commit 3e916a07ac093045d88ef0c4fa78647ae0efc010
Author: gwjr <502526+gwjr@users.noreply.github.com>
Date:   Fri Nov 17 14:48:19 2023 +0000

    finetune : speed-up ggml_compute_forward_out_prod_f32 via BLAS (#4079)
    
    * Remove logically superfluous assertions and order by dimension
    
    * Use cblas_sgemm() to implement ggml_compute_forward_out_prod()
    
    * Remove ggml_compute_forward_out_prod_use_blas(), fix compiling errors on cmake/zig, remove trailing whitespace
    
    * Add openBLAS support for sgemm() in compute_forward_out_prod()

ggml.c

commit 947f64f1630bb8b0b363a3bb5e29e11425312d57
Author: Andrew Godfrey <AndrewGodfrey@users.noreply.github.com>
Date:   Fri Nov 17 02:23:11 2023 -0800

    finetune : zero the loraB initial vectors (#4082)
    
    * finetune : zero the loraB initial vectors
    
    Without this, the first iteration is starting out far from the base model, instead of exactly on it.
    Zeroing loraB is what the paper recommends. loralib also zeroes at least one of the init vector pairs
    (though it departs from the paper in using a different distribution for the other vector, in some cases).
    
    * tabs to spaces
    
    * Use ggml_set_zero instead of adding a new function

examples/finetune/finetune.cpp

commit b83e149ec6264d078e6a47412e7347bf5c2bfcc9
Author: Andrew Godfrey <AndrewGodfrey@users.noreply.github.com>
Date:   Fri Nov 17 00:01:15 2023 -0800

    cuda : get_row_rounding F32 (#4095)
    
    * Fix #4017
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

ggml-cuda.cu

commit 4f447a48339977073a1af4f33ae873465ff64994
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 17 10:00:15 2023 +0200

    llama : fix data units (#4101)
    
    * llama : fix data units
    
    ggml-ci
    
    * Revert "llama : fix data units"
    
    This reverts commit f5feac831fe225ed7f3db938d115732a49dccfc4.
    
    * llama : disambiguate data units
    
    ggml-ci

ggml-cuda.cu
ggml-metal.m
llama.cpp

commit 91f6499393d2d999331fbfdba47a7f8b9f913f0d
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Nov 16 19:14:37 2023 -0700

    Respect tokenizer.ggml.add_bos_token value when tokenizing (#4040)
    
    * gguf-py: gguf-dump: Respect --no-tensor flag in JSON mode.
    
    * Respect add_bos_token GGUF metadata value
    
    * gguf-py: Try to fix SpecialVocab giving up too easily for the Nth time

common/common.cpp
common/common.h
examples/infill/infill.cpp
examples/llava/llava-cli.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/server/server.cpp
gguf-py/gguf/vocab.py
gguf-py/pyproject.toml
gguf-py/scripts/gguf-dump.py
llama.cpp
llama.h

commit 8da46278e1a57107591653275f8e03a281de94f0
Author: texmex76 <40733439+texmex76@users.noreply.github.com>
Date:   Thu Nov 16 16:01:48 2023 +0100

    gguf : fix potential infinite loops while parsing (#4100)
    
    Co-authored-by: Bernhard Gstrein <gstrein@cs.uni-freiburg.de>

ggml.c

commit a6fc554e268634494f33b0de76f9dde650dd292f
Author: Jared Van Bortel <jared@nomic.ai>
Date:   Wed Nov 15 11:34:47 2023 -0500

    llama : restore prefix space in llama tokenizer (#4081)

llama.cpp

commit 1cf2850d52bb027aa215e039ed9a0c61beeef8d3
Author: slaren <slarengh@gmail.com>
Date:   Wed Nov 15 13:58:13 2023 +0100

    ggml-cuda : increase max graph size (#4084)

ggml-cuda.cu

commit 6bb4908a17150b49373b5f977685b2e180a04f6f
Author: Michael Potter <NanoTekGuy@Gmail.com>
Date:   Tue Nov 14 09:34:41 2023 -0800

    Fix MacOS Sonoma model quantization (#4052)
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
Makefile
ggml-quants.c

commit 36eed0c42c5b0bf74af81fb9243d262014f9382f
Author: Galunid <karolek1231456@gmail.com>
Date:   Tue Nov 14 11:17:12 2023 +0100

    stablelm : StableLM support (#3586)
    
    * Add support for stablelm-3b-4e1t
    * Supports GPU offloading of (n-1) layers

README.md
convert-hf-to-gguf.py
gguf-py/gguf/constants.py
llama.cpp
models/ggml-vocab-stablelm-3b-4e1t.gguf
tests/CMakeLists.txt

commit b46d12f86d56bef3dc8b596dfb3d22f3b08102be
Author: afrideva <95653597+afrideva@users.noreply.github.com>
Date:   Mon Nov 13 17:03:40 2023 -0800

    convert.py: also look for plain model.safetensors  (#4043)
    
    * add safetensors to convert.py help message
    
    * Check for single-file safetensors model
    
    * Update convert.py "model" option help message
    
    * revert convert.py help message change

convert.py

commit bd90eca237b498dd106d315dcb9ad3e6fae3906f
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Mon Nov 13 18:20:52 2023 +0300

    llava : fix regression for square images in #3613 (#4056)

examples/llava/clip.cpp

commit 3d68f364f15778dc326f5024f2e5af1ad6dfddef
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 13 16:55:52 2023 +0200

    ggml : sync (im2col, GPU conv, 32-bit arm compat) (#4060)
    
    ggml-ci

ggml-cuda.cu
ggml-impl.h
ggml-metal.h
ggml-metal.m
ggml-metal.metal
ggml-quants.c
ggml.c
ggml.h

commit c049b37d7baf558944501705b91ac89b26ee3e41
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 13 14:18:08 2023 +0200

    readme : update hot topics

README.md

commit 4760e7cc0b68570d58f55e8dda469805d1759d0d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Nov 13 14:16:23 2023 +0200

    sync : ggml (backend v2) (#3912)
    
    * sync : ggml (backend v2) (wip)
    
    * sync : migrate examples and llama.cpp to dynamic graphs (wip)
    
    * sync : update tests + fix max op params to 64
    
    ggml-ci
    
    * sync : ggml-cuda
    
    ggml-ci
    
    * llama : fix save/load state context size
    
    ggml-ci
    
    * sync : try to fix build on tvOS
    
    * sync : pass custom graph sizes in training examples
    
    * sync : update graph copies to new ggml API
    
    * sync : update sync-ggml.sh with new files
    
    * scripts : fix header in sync script
    
    * train : fix context size calculations
    
    * llama : increase inference graph size up to 4096 nodes
    
    * train : allocate grads for backward graphs
    
    * train : allocate grads for gb_tmp

common/train.cpp
common/train.h
examples/benchmark/benchmark-matmult.cpp
examples/export-lora/export-lora.cpp
examples/finetune/finetune.cpp
examples/llava/clip.cpp
examples/metal/metal.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-alloc.c
ggml-alloc.h
ggml-backend-impl.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-impl.h
ggml-metal.m
ggml.c
ggml.h
llama.cpp
scripts/sync-ggml.sh
tests/test-grad0.cpp
tests/test-opt.cpp

commit bb50a792ec2a49944470c82694fa364345e95170
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Mon Nov 13 01:58:15 2023 -0700

    Add ReLU and SQR CUDA ops to (partially) fix Persimmon offloading (#4041)
    
    * Add ReLU and SQR CUDA ops to fix Persimmon offloading
    
    * Persimmon loader: More helpful error on CUDA/ROCM when offloading too many layers

ggml-cuda.cu
llama.cpp

commit 21fd874c8d2a14dea2d56724e4357c0824aee6a8
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Nov 12 16:39:37 2023 -0700

    gguf-py: gguf_writer: Use bytearray to build metadata (#4051)
    
    * gguf-py: gguf_writer: Use BytesIO to build metadata
    
    * Use bytearray instead
    
    Bump gguf-py package version

gguf-py/gguf/gguf_writer.py
gguf-py/pyproject.toml

commit 532dd74e38c29e16ea1cfc4e7eedb4f2fab3f3cd
Author: Richard Kiss <him@richardkiss.com>
Date:   Sat Nov 11 22:04:58 2023 -0800

    Fix some documentation typos/grammar mistakes (#4032)
    
    * typos
    
    * Update examples/parallel/README.md
    
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>

README.md
docs/token_generation_performance_tips.md
examples/main/README.md
examples/parallel/README.md
grammars/README.md

commit e86fc56f7521ca4b18d1d9939e82abd40c2f1c01
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Sat Nov 11 18:35:31 2023 +0300

    Fix gguf-convert-endian script (#4037)
    
    * Fix gguf-convert-endian script
    
    * Bump version and update description

gguf-py/pyproject.toml
gguf-py/scripts/gguf-convert-endian.py

commit d96ca7ded77df764db797b68b4a29e34c5b56285
Author: Alexey Parfenov <zxed@alkatrazstudio.net>
Date:   Sat Nov 11 05:48:21 2023 +0000

    server : fix crash when prompt exceeds context size (#3996)

examples/server/server.cpp

commit 34b0a082074b073eb14c2bd93c0c070e20ddcd16
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Nov 10 22:04:50 2023 -0700

    gguf-py: Refactor and allow reading/modifying existing GGUF files (#3981)
    
    * gguf-py: Refactor and add file reading support
    
    * Replay changes from #3871
    
    Credit to @cebtenzzre for that pull
    
    * Various type annotation fixes.
    
    * sort imports with isort (again)
    
    * Fix missing return statement in add_tensor
    
    * style cleanup with flake8
    
    * fix NamedTuple and Enum usage
    
    * Fix an issue with state init in GGUFReader
    
    Move examples to an examples/ directory
    
    Clean up examples
    
    Add an example of modifying keys in a GGUF file
    
    Update documentation with info on examples
    
    Try to support people importing gguf/gguf.py directly
    
    * Damagage is not a word.
    
    * Clean up gguf-py/examples/modify_gguf.py whitespace
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update gguf-py/examples/modify_gguf.py formatting
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Update gguf-py/gguf/gguf_reader.py type hint
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * Make examples executable, formatting changes
    
    * Add more information to GGUFReader and examples comments
    
    * Include a gguf Python package version bump
    
    * Add convert-gguf-endian.py script
    
    * cleanup
    
    * gguf-py : bump minor version
    
    * Reorganize scripts
    
    * Make GGUFReader endian detection less arbitrary
    
    * Add JSON dumping support to gguf-dump.py
    
    Which I kind of regret now
    
    * A few for gguf-dump.py cleanups
    
    * Murder accidental tuple in gguf-py/scripts/gguf-dump.py
    
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>
    
    * cleanup
    
    * constants : remove unneeded type annotations
    
    * fix python 3.8 compat
    
    * Set up gguf- scripts in pyproject.toml
    
    * And include scripts/__init__.py, derp
    
    * convert.py: We can't currently support Q8_0 on big endian.
    
    * gguf-py: SpecialVocab: Always try available sources for special token ids
    
    gguf-py: SpecialVocab: Try to load merges from merges.txt if not in tokenizer.json
    
    gguf-py: SpecialVocab: Add 'add_bos_token' type bools to GGUF metadata
    u
    
    * cleanup
    
    * Promote add_X_token to GGUF metadata for BOS and EOS
    
    ---------
    
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>
    Co-authored-by: Jared Van Bortel <cebtenzzre@gmail.com>

convert-baichuan-hf-to-gguf.py
convert-llama-ggml-to-gguf.py
convert-persimmon-to-gguf.py
convert.py
examples/train-text-from-scratch/convert-train-checkpoint-to-gguf.py
gguf-py/README.md
gguf-py/examples/writer.py
gguf-py/gguf/__init__.py
gguf-py/gguf/constants.py
gguf-py/gguf/gguf.py
gguf-py/gguf/gguf_reader.py
gguf-py/gguf/gguf_writer.py
gguf-py/gguf/tensor_mapping.py
gguf-py/gguf/vocab.py
gguf-py/pyproject.toml
gguf-py/scripts/__init__.py
gguf-py/scripts/gguf-convert-endian.py
gguf-py/scripts/gguf-dump.py
gguf-py/scripts/gguf-set-metadata.py
gguf-py/tests/test_gguf.py

commit 4a4fd3eefad5bd17ab6bcd8e2181b4f62eae76cf
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Nov 11 06:49:33 2023 +0800

    server : allow continue edit on completion mode (#3950)
    
    * server : allow continue edit on completion mode
    
    * server : handle abort case in runCompletion
    
    * server : style improvement

examples/server/index.html.hpp
examples/server/public/index.html

commit df9d1293defe783f42bc83af732d3c670552c541
Author: Galunid <karolek1231456@gmail.com>
Date:   Fri Nov 10 14:24:54 2023 +0100

    Unbreak persimmon after #3837 (#4010)

llama.cpp

commit a75fa576abba9d37f463580c379e4bbf1e1ad03c
Author: Galunid <karolek1231456@gmail.com>
Date:   Thu Nov 9 11:09:29 2023 +0100

    scripts: Generalize convert scripts (#3838)
    
    * Replace convert-*-hf-to-gguf.py files with convert-hf-to-gguf.py

convert-bloom-hf-to-gguf.py
convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-hf-to-gguf.py
convert-mpt-hf-to-gguf.py
convert-refact-hf-to-gguf.py
convert-starcoder-hf-to-gguf.py
convert.py
mypy.ini

commit 57ad015dc3011b046ed5a23186c86ea55f987c54
Author: Mihai <mihai.chirculescu@yahoo.com>
Date:   Thu Nov 9 04:00:34 2023 +0200

    server : add min_p param (#3877)
    
    * Update server.cpp with min_p after it was introduced in https://github.com/ggerganov/llama.cpp/pull/3841
    
    * Use spaces instead of tabs
    
    * Update index.html.hpp after running deps.sh
    
    * Fix test - fix line ending

examples/server/README.md
examples/server/index.html.hpp
examples/server/public/index.html
examples/server/server.cpp

commit 875fb42871a0f5a88fbe31a0b5edd697b84038e4
Author: slaren <slarengh@gmail.com>
Date:   Wed Nov 8 13:15:14 2023 +0100

    ggml-alloc : fix backend assignments of views (#3982)

ggml-alloc.c

commit 0a7c980b6f94a049cb804573df2d8092a34df8e4
Author: Jared Van Bortel <cebtenzzre@gmail.com>
Date:   Tue Nov 7 12:43:04 2023 -0500

    gguf : track writer state, free unneeded tensors, cleanup (#3871)

gguf-py/gguf/gguf.py
gguf-py/pyproject.toml

commit 413503d4b92500d82b002d03c580a71a54747138
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Nov 7 19:25:32 2023 +0200

    make : do not add linker flags when compiling static llava lib (#3977)

Makefile

commit e9c1cecb9d7d743d30b4a29ecd56a411437def0a
Author: xaedes <xaedes@gmail.com>
Date:   Tue Nov 7 09:04:51 2023 +0100

    ggml : fix backward rope after YaRN (#3974)
    
    * fix backward process of rope
    
    rope backward process was broken after YaRN RoPE (#2268) implementation, due to missing changes in backward functions.
    
    the code for the backward process is nearly identically to the forward process:
    the only difference is the sign of the sin-values.
    
    to avoid future regressions remove the near-duplicate backward functions and reuse the forward code:
    
    for this a new function argument `bool forward` was added to `ggml_compute_forward_rope_f32` and `ggml_compute_forward_rope_f16`.
    the sin-values will be negated when forward is false.
    
    * fix finetune rope call to use correct default attn_factor of 1.0f
    
    * remove unused `ggml_rope_xpos_back`
    
    it is better to have only one `ggml_rope_back` function that accepts all rope parameters, so that `ggml_compute_backward` can propagate all parameters without having to switch between different rope_back variants.
    
    * fix comments explaining the sinus sign in ggml_forward_rope
    
    * add missing function arguments in declaration
    
    * fix function argument type in declaration

examples/finetune/finetune.cpp
ggml.c
ggml.h

commit 54b4df8886103b436a4bb3b60f4d84824f9e8868
Author: Matthew Tejo <matthew.tejo@gmail.com>
Date:   Mon Nov 6 23:43:59 2023 -0800

    Use params when loading models in llava-cli (#3976)
    
    llava-cli was loading models with default params and ignoring settings
    from the cli. This switches to a generic function to load the params
    from the cli options.

examples/llava/llava-cli.cpp

commit 46876d2a2c92e60579dc732cdb8cbd243b06f317
Author: Meng Zhang <meng@tabbyml.com>
Date:   Mon Nov 6 22:49:08 2023 -0800

    cuda : supports running on CPU for GGML_USE_CUBLAS=ON build (#3946)
    
    * protyping the idea that supports running on CPU for a GGML_USE_CUBLAS=on build
    
    * doc: add comments to ggml_cublas_loaded()
    
    * fix defined(...)

ggml-cuda.cu
ggml-cuda.h
llama.cpp

commit 381efbf480959bb6d1e247a8b0c2328f22e350f8
Author: Damian Stewart <d@damianstewart.com>
Date:   Mon Nov 6 22:36:23 2023 +0100

    llava : expose as a shared library for downstream projects (#3613)
    
    * wip llava python bindings compatibility
    
    * add external llava API
    
    * add base64 in-prompt image support
    
    * wip refactor image loading
    
    * refactor image load out of llava init
    
    * cleanup
    
    * further cleanup; move llava-cli into its own file and rename
    
    * move base64.hpp into common/
    
    * collapse clip and llava libraries
    
    * move llava into its own subdir
    
    * wip
    
    * fix bug where base64 string was not removed from the prompt
    
    * get libllava to output in the right place
    
    * expose llava methods in libllama.dylib
    
    * cleanup memory usage around clip_image_*
    
    * cleanup and refactor *again*
    
    * update headerdoc
    
    * build with cmake, not tested (WIP)
    
    * Editorconfig
    
    * Editorconfig
    
    * Build with make
    
    * Build with make
    
    * Fix cyclical depts on Windows
    
    * attempt to fix build on Windows
    
    * attempt to fix build on Windows
    
    * Upd TODOs
    
    * attempt to fix build on Windows+CUDA
    
    * Revert changes in cmake
    
    * Fix according to review comments
    
    * Support building as a shared library
    
    * address review comments
    
    ---------
    
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
    Co-authored-by: Jared Van Bortel <jared@nomic.ai>

.gitignore
Makefile
common/CMakeLists.txt
common/base64.hpp
examples/llava/CMakeLists.txt
examples/llava/README.md
examples/llava/clip.cpp
examples/llava/clip.h
examples/llava/llava-cli.cpp
examples/llava/llava-utils.h
examples/llava/llava.cpp
examples/llava/llava.h
examples/server/CMakeLists.txt

commit 2833a6f63c1b87c7f4ac574bcf7a15a2f3bf3ede
Author: slaren <slarengh@gmail.com>
Date:   Sun Nov 5 18:45:16 2023 +0100

    ggml-cuda : fix f16 mul mat (#3961)
    
    * ggml-cuda : fix f16 mul mat
    
    ggml-ci
    
    * silence common.cpp warning (bonus)

common/common.cpp
ggml-cuda.cu

commit d9ccce2e339ca0396560d18b8637f2c848d72a08
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Nov 5 10:06:06 2023 -0700

    Allow common process_escapes to handle \x sequences (#3928)
    
    * Allow common process_escapes to handle \x sequences
    
    * Fix edge case when second hex digit is NUL

common/common.cpp

commit bb60fd0bf6bb270744d86dd45b3a95af01b7de45
Author: Thái Hoàng Tâm <75922889+RoyalHeart@users.noreply.github.com>
Date:   Sun Nov 5 23:15:27 2023 +0700

    server : fix typo for --alias shortcut from -m to -a (#3958)

examples/server/README.md

commit 132d25b8a62ea084447e0014a0112c1b371fb3f8
Author: Jared Van Bortel <cebtenzzre@gmail.com>
Date:   Sun Nov 5 10:08:57 2023 -0500

    cuda : fix disabling device with --tensor-split 1,0 (#3951)
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-cuda.cu

commit 3d48f42efcd05381221654376e9f6f69d76af739
Author: Meng Zhang <meng@tabbyml.com>
Date:   Sun Nov 5 04:40:08 2023 -0800

    llama : mark LLM_ARCH_STARCODER as full offload supported (#3945)
    
    as done in https://github.com/ggerganov/llama.cpp/pull/3827

llama.cpp

commit c41ea36eaa3548776de4cb3d5d49b925cd3fc0f2
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Sun Nov 5 08:03:09 2023 +0000

    cmake : MSVC instruction detection (fixed up #809) (#3923)
    
    * Add detection code for avx
    
    * Only check hardware when option is ON
    
    * Modify per code review sugguestions
    
    * Build locally will detect CPU
    
    * Fixes CMake style to use lowercase like everywhere else
    
    * cleanup
    
    * fix merge
    
    * linux/gcc version for testing
    
    * msvc combines avx2 and fma into /arch:AVX2 so check for both
    
    * cleanup
    
    * msvc only version
    
    * style
    
    * Update FindSIMD.cmake
    
    ---------
    
    Co-authored-by: Howard Su <howard0su@gmail.com>
    Co-authored-by: Jeremy Dunn <jeremydunn123@gmail.com>

CMakeLists.txt
cmake/FindSIMD.cmake

commit a7fac013cf1cc7bbc0160a226aa2412e9f22e78a
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Sun Nov 5 07:46:44 2023 +0000

    ci : use intel sde when ci cpu doesn't support avx512 (#3949)

.github/workflows/build.yml

commit 48ade94538fa509465d71023e49d07aab0ec8cd5
Author: slaren <slarengh@gmail.com>
Date:   Sun Nov 5 08:12:13 2023 +0100

    cuda : revert CUDA pool stuff (#3944)
    
    * Revert "cuda : add ROCM aliases for CUDA pool stuff (#3918)"
    
    This reverts commit 629f917cd6b96ba1274c49a8aab163b1b189229d.
    
    * Revert "cuda : use CUDA memory pool with async memory allocation/deallocation when available (#3903)"
    
    This reverts commit d6069051de7165a4e06662c89257f5d2905bb156.
    
    ggml-ci

ggml-cuda.cu

commit f28af0d81aa1010afa5de74cf627dcb04bea3157
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Nov 4 16:20:34 2023 -0600

    gguf-py: Support 01.AI Yi models (#3943)

gguf-py/gguf/gguf.py

commit d9b33fe95bd257b36c84ee5769cc048230067d6f
Author: Peter Sugihara <peter@campsh.com>
Date:   Fri Nov 3 12:18:18 2023 -0700

    metal : round up to 16 to fix MTLDebugComputeCommandEncoder assertion (#3938)

ggml-metal.m

commit 5ba37461711095c0284233dbd14f0d9010cdbf56
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Fri Nov 3 13:00:31 2023 -0500

    ggml-metal: fix yarn rope (#3937)

ggml-metal.m

commit abb77e7319aabc0b5cfb7c22da690a692489b6b7
Author: slaren <slarengh@gmail.com>
Date:   Fri Nov 3 12:13:09 2023 +0100

    ggml-cuda : move row numbers to x grid dim in mmv kernels (#3921)

ggml-cuda.cu

commit 8f961abdc4e134c83bf8c2ad618ab256b4cae0f9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 3 09:41:17 2023 +0200

    speculative : change default p_accept to 0.5 + CLI args (#3919)
    
    ggml-ci

common/common.cpp
common/common.h
examples/speculative/speculative.cpp

commit 05816027d649f977468fc804cdb54e99eac246d1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Nov 3 09:24:00 2023 +0200

    common : YAYF (yet another YARN fix) (#3925)
    
    ggml-ci

common/common.h
llama.h

commit 3fdbe6b66b7b5c6ad3b2f245cbad1517c27ff776
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Nov 3 02:31:58 2023 -0400

    llama : change yarn_ext_factor placeholder to -1 (#3922)

llama.cpp

commit 629f917cd6b96ba1274c49a8aab163b1b189229d
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Nov 2 13:58:22 2023 -0600

    cuda : add ROCM aliases for CUDA pool stuff (#3918)

ggml-cuda.cu

commit 51b2fc11f7f605fff49725a4540e9a6ef7b51b70
Author: Andrei <abetlen@gmail.com>
Date:   Thu Nov 2 15:40:31 2023 -0400

    cmake : fix relative path to git submodule index (#3915)

common/CMakeLists.txt

commit 224e7d5b14cbabab7ae45c64db2cfde979c8455d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 20:44:12 2023 +0200

    readme : add notice about #3912

README.md

commit c7743fe1c1cbda5a886362aa371480360580fdf0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 20:32:11 2023 +0200

    cuda : fix const ptrs warning causing ROCm build issues (#3913)

ggml-cuda.cu

commit d6069051de7165a4e06662c89257f5d2905bb156
Author: Oleksii Maryshchenko <oleksii.maryshchenko@gmail.com>
Date:   Thu Nov 2 18:10:39 2023 +0100

    cuda : use CUDA memory pool with async memory allocation/deallocation when available (#3903)
    
    * Using cuda memory pools for async alloc/dealloc.
    
    * If cuda device doesnt support memory pool than use old implementation.
    
    * Removed redundant cublasSetStream
    
    ---------
    
    Co-authored-by: Oleksii Maryshchenko <omaryshchenko@dtis.com>

ggml-cuda.cu

commit 4ff1046d75e64f0e556d8dcd930ea25c23eb8b18
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 16:22:30 2023 +0200

    gguf : print error for GGUFv1 files (#3908)

ggml.c

commit 21958bb393a654591ed26f339791b752d58f5c8b
Author: slaren <slarengh@gmail.com>
Date:   Thu Nov 2 13:10:33 2023 +0100

    cmake : disable LLAMA_NATIVE by default (#3906)

CMakeLists.txt

commit 2756c4fbffab097736d5116007872d86456a544a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 11:20:21 2023 +0200

    gguf : remove special-case code for GGUFv1 (#3901)
    
    ggml-ci

ggml.c
models/ggml-vocab-llama.gguf

commit 1efae9b7dca2a5cc5aa21c1997b538022964ea19
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 09:54:18 2023 +0200

    llm : prevent from 1-D tensors being GPU split (#3697)

llama.cpp

commit b12fa0d1c13596869c512f49a526b979c94787cc
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Nov 2 02:50:16 2023 -0400

    build : link against build info instead of compiling against it (#3879)
    
    * cmake : fix build when .git does not exist
    
    * cmake : simplify BUILD_INFO target
    
    * cmake : add missing dependencies on BUILD_INFO
    
    * build : link against build info instead of compiling against it
    
    * zig : make build info a .cpp source instead of a header
    
    Co-authored-by: Matheus C. França <matheus-catarino@hotmail.com>
    
    * cmake : revert change to CMP0115
    
    ---------
    
    Co-authored-by: Matheus C. França <matheus-catarino@hotmail.com>

.gitignore
CMakeLists.txt
Makefile
build.zig
common/CMakeLists.txt
common/build-info.cpp.in
common/common.cpp
common/common.h
examples/benchmark/CMakeLists.txt
examples/benchmark/benchmark-matmult.cpp
examples/embedding/CMakeLists.txt
examples/embedding/embedding.cpp
examples/infill/CMakeLists.txt
examples/infill/infill.cpp
examples/llama-bench/CMakeLists.txt
examples/llama-bench/llama-bench.cpp
examples/llava/CMakeLists.txt
examples/main/CMakeLists.txt
examples/main/main.cpp
examples/parallel/CMakeLists.txt
examples/parallel/parallel.cpp
examples/perplexity/CMakeLists.txt
examples/perplexity/perplexity.cpp
examples/quantize-stats/CMakeLists.txt
examples/quantize-stats/quantize-stats.cpp
examples/quantize/CMakeLists.txt
examples/quantize/quantize.cpp
examples/save-load-state/CMakeLists.txt
examples/save-load-state/save-load-state.cpp
examples/server/CMakeLists.txt
examples/server/server.cpp
examples/speculative/CMakeLists.txt
examples/speculative/speculative.cpp
scripts/build-info.cmake
scripts/build-info.h.in
scripts/build-info.sh

commit 4d719a6d4e74b9a98e75f826f865f3153717d54b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 08:35:10 2023 +0200

    cuda : check if this fixes Pascal card regression (#3882)

ggml-cuda.cu

commit 183b3fac6c28e65d23ac0230c1dd6fb84bf0154d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Nov 2 08:33:37 2023 +0200

    metal : fix build errors and kernel sig after #2268 (#3898)

ggml-metal.m
ggml-metal.metal

commit 2fffa0d61fa10e4b466e78cabcc6a4e16717b580
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Nov 2 01:49:44 2023 -0400

    cuda : fix RoPE after #2268 (#3897)

ggml-cuda.cu

commit 0eb332a10f3f14a3746c391bf80ff5e7bdf29d5d
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Nov 1 19:29:14 2023 -0400

    llama : fix llama_context_default_params after #2268 (#3893)

llama.cpp

commit d02e98cde035d91ed8032ab943d1d504fe9da394
Author: slaren <slarengh@gmail.com>
Date:   Wed Nov 1 23:10:09 2023 +0100

    ggml-cuda : compute ptrs for cublasGemmBatchedEx in a kernel (#3891)
    
    * ggml-cuda : compute ptrs for cublasGemmBatchedEx in a kernel
    
    * fix warnings

ggml-cuda.cu

commit 898aeca90a9bb992f506234cf3b8b7f7fa28a1df
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Nov 1 18:04:33 2023 -0400

    llama : implement YaRN RoPE scaling (#2268)
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    Co-authored-by: Jeffrey Quesnelle <jquesnelle@gmail.com>

common/common.cpp
common/common.h
convert-baichuan-hf-to-gguf.py
convert.py
examples/finetune/finetune.cpp
examples/server/server.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
gguf-py/gguf/gguf.py
llama.cpp
llama.h

commit c43c2da8afacaddfe51c09b21dbd9922cd0ea46b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 23:08:30 2023 +0200

    llm : fix llm_build_kqv taking unused tensor (benign, #3837)

llama.cpp

commit 523e49b11174368cd73460fa5eae7b39d856f300
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 23:00:50 2023 +0200

    llm : fix falcon norm after refactoring (#3837)

llama.cpp

commit e16b9fa4baa8a09c6619b116159830e898050942
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 21:25:00 2023 +0200

    metal : multi-simd softmax (#3710)
    
    ggml-ci

ggml-metal.m
ggml-metal.metal

commit ff8f9a88da0018972dfdf6fe64b5c8992caabd9c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 21:15:55 2023 +0200

    common : minor (#3715)

common/common.cpp

commit 50337961a678fce4081554b24e56e86b67660163
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 20:11:02 2023 +0200

    llm : add llm_build_context (#3881)
    
    * llm : add llm_build_context
    
    * llm : deduce norm eps based on type + explict max_alibi_bias, clamp_kqv
    
    * llm : restore the non-graph llm_build_ functional API
    
    ggml-ci
    
    * llm : cleanup + comments

llama.cpp

commit 0e40806c1cb3bdf9955ed807ffbe212be85b4c67
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Wed Nov 1 14:42:01 2023 -0300

    common : allow caller to handle help/argument exceptions (#3715)
    
    * Allow caller to handle help/argument exceptions
    
    * Prepend newline to usage output
    
    * Add new gpt_params_parse_ex function to hide arg-parse impl
    
    * Fix issue blocking success case
    
    * exit instead of returning false
    
    * Update common/common.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update common/common.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h

commit a2758d08e44ce3624d233af4d23c6843e2e735b5
Author: staviq <staviq@gmail.com>
Date:   Wed Nov 1 15:18:27 2023 +0100

    log : make generating separate log files optional (#3787)
    
    * impl --log-new, --log-append
    
    * Update common/log.h
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    
    * Update common/log.h
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    
    * Apply suggestions from code review
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>
    
    ---------
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>

common/log.h

commit e75dfdd31b6a3dfa0627ba4ac3bb4b36e9db588e
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Wed Nov 1 21:40:43 2023 +0800

    sampling : null grammar field after reset (#3885)

common/sampling.cpp

commit 9a3b4f6c86503c9cfc049d4d0fdeafef12806f5e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 13:50:45 2023 +0200

    ggml : fix UNUSED macro (#3762)

ggml-quants.c

commit 73bdcb395ef9a997d9c02950c7cd4249546162cd
Author: Andrew Godfrey <AndrewGodfrey@users.noreply.github.com>
Date:   Wed Nov 1 04:49:04 2023 -0700

    finetune : add -ngl parameter (#3762)
    
    * Add '-ngl' support to finetune.cpp
    
    * Add fprintf in ggml_cuda_op_add
    
    When I tried CUDA offloading during finetuning following the readme, I got an assert here.
    This probably isn't an important case because inference later gives a warning saying you should use f16 or f32 instead when using lora
    
    * Add 'finetune.sh', which currently fails when using GPU
    
    "error: operator (): Finetuning on tensors with type 'f16' is not yet supported"
    
    * tweak finetune.sh
    
    * Suppress some warnings in ggml.c
    
    * Add f16 implementation to ggml_compute_forward_add_f16_f32
    
    * Add an f16 case to ggml_add_cast_impl and llama_build_lora_finetune_graphs
    
    * finetune.sh: Edit comments
    
    * Add "add_f16_f32_f32_cuda"
    
    * Tweak an error message
    
    * finetune.sh: Add an optional LLAMA_MODEL_DIR variable
    
    * finetune.sh: Add an optional LLAMA_TRAINING_DIR variable
    
    * train : minor
    
    * tabs to spaces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>

common/train.cpp
common/train.h
examples/finetune/finetune.cpp
examples/finetune/finetune.sh
ggml-cuda.cu
ggml-quants.c
ggml.c
llama.cpp

commit f0e209324a7f663225791897877bf610f1af152d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 11:29:07 2023 +0200

    scripts : add server-llm.sh (#3868)
    
    * scripts : add deploy-server.sh
    
    * scripts : rename to server-llm.sh
    
    * scripts : working curl pipe

scripts/server-llm.sh

commit ca190bca8e844d171020d6147687e71472d71734
Author: Adrian Hesketh <a-h@users.noreply.github.com>
Date:   Wed Nov 1 09:28:28 2023 +0000

    server : re-enable completion and embedded at the same time (#3876)

.gitignore
examples/server/server.cpp

commit 71e3718abdb2771b50c9606d3a7569623a0b0afe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Nov 1 08:04:02 2023 +0200

    llama : refactor graph build code (#3837)
    
    * llama : factor out ggml-alloc from graph graph build functions
    
    ggml-ci
    
    * metal : disable kernel load log
    
    * llama : factor out tensor offloading outside the build call (wip)
    
    ggml-ci
    
    * llama : offload rest of the models
    
    ggml-ci
    
    * llama : update offload log messages to print node index
    
    * llama : comments
    
    * llama : support offloading result_norm + comments
    
    * llama : factor graph input into a function
    
    * llama : do tensor offload only with CUDA
    
    * llama : fix res_norm offloading
    
    * llama : try to optimize offloading code
    
    * llama : fix non-CUDA build
    
    * llama : try to fix build
    
    * llama : move refact in correct place + optimize graph input
    
    * llama : refactor tensor offloading as callback
    
    * llama : add layer index to all tensor names
    
    * llama : add functional header
    
    * llama : comment
    
    ggml-ci
    
    * llama : remove obsolete map for layer counting
    
    * llama : add llm_build helper functions (#3848)
    
    * llama : add llm_build_norm helper function
    
    ggml-ci
    
    * llama : add llm_build_ffn helper function (#3849)
    
    ggml-ci
    
    * llama : add llm_build_k_shift helper
    
    ggml-ci
    
    * llama : fix offloading after recent changes
    
    * llama : add llm_build_kv_store helper
    
    ggml-ci
    
    * llama : remove obsolete offload names
    
    * llama : fix llm_build_k_shift to use n_head_kv instead of n_head
    
    * llama : simplify falcon Q, K, V computation
    
    * llama : remove obsolete comments in build graphs
    
    * llama : add llm_build_kqv helper
    
    ggml-ci
    
    * llama : minor
    
    * llama : add LLAMA_OFFLOAD_DEBUG + fix starcoder offloading
    
    * llama : fix input allocation logic
    
    * llama : update offload functions for KQ tensors
    
    * llama : normalize tensor names
    
    ggml-ci
    
    * llama : enable warning about not offloaded tensors
    
    * llama : remove extra ; + deduplicate gate_b logic
    
    * llama : add llm_build_inp_embd helper

ggml-metal.m
ggml.h
llama.cpp

commit 238657db2364cfb728c694470a4a81702afea760
Author: kalomaze <66376113+kalomaze@users.noreply.github.com>
Date:   Tue Oct 31 14:44:49 2023 -0500

    samplers : Min-P sampler implementation [alternative to Top P/Top K] (#3841)
    
    * Introduce the new Min-P sampler by @kalomaze
       The Min-P sampling method was designed as an alternative to Top-P, and aims to ensure a balance of quality and variety. The parameter *p* represents the minimum probability for a token to be considered, relative to the probability of the most likely token.
    
    * Min-P enabled and set to 0.05 default
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>

common/common.cpp
common/sampling.cpp
common/sampling.h
examples/main/README.md
llama.cpp
llama.h

commit 07178c98e1b61a5e2af39d347add12e7eb9e08e1
Author: Tungsten842 <886724vf@anonaddy.me>
Date:   Tue Oct 31 18:24:03 2023 +0100

    flake.nix: fix for rocm 5.7 (#3853)

flake.lock
flake.nix

commit 207b51900e15cc7f89763a3bb1c565fe11cbb45d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 30 19:19:15 2023 +0200

    ggml : move FP16 <-> FP32 code to ggml-impl.h (#3861)
    
    * ggml : move FP16 <-> FP32 stuff to ggml-impl.h
    
    ggml-ci
    
    * tests : fix ARM build
    
    * ggml : explicitly initialize deprecated type traits
    
    * ggml : add math.h to ggml-impl.h
    
    * ggml : remove duplicate static assert macros
    
    * ggml : prefix lookup tables with ggml_
    
    ggml-ci
    
    * ggml-impl : move extern "C" to start of file

ggml-impl.h
ggml-quants.c
ggml-quants.h
ggml.c
llama.cpp
tests/test-double-float.cpp
tests/test-quantize-fns.cpp

commit 6e08281e588bbba1a5d180290a94a43f167f3a1a
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Oct 29 11:31:40 2023 -0600

    Extend llama_kv_cache_seq_rm to allow matching any sequence (#3843)
    
    * Extend llama_kv_cache_seq_rm to allow matichng any sequence
    
    * Replace llama_kv_cache_tokens_rm with llama_kv_cache_clear
    
    Use llama_kv_cache_clear for cache clearing
    
    Change calls to llama_kv_cache_tokens_rm that want to delete by position to use llama_kv_cache_seq_rm functionality

common/common.cpp
examples/batched-bench/batched-bench.cpp
examples/llama-bench/llama-bench.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/server/server.cpp
llama.cpp
llama.h

commit 2046eb4345e62c4575b3cdc0115a51db89f3fb70
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Sun Oct 29 12:33:47 2023 -0400

    make : remove unnecessary dependency on build-info.h (#3842)

Makefile

commit 71a09da301705b9c5ad4ca3cf3fbd966dd3f1ec5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 29 18:32:51 2023 +0200

    llama : fix kv shift bug (#3835)
    
    ggml-ci

llama.cpp

commit d69d777c02b9ac405a95f3cbfba219a990caefff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 29 18:32:28 2023 +0200

    ggml : quantization refactoring (#3833)
    
    * ggml : factor all quantization code in ggml-quants
    
    ggml-ci
    
    * ggml-quants : fix Zig and Swift builds + quantize tool
    
    ggml-ci
    
    * quantize : --pure option for disabling k-quant mixtures
    
    ---------
    
    Co-authored-by: cebtenzzre <cebtenzzre@gmail.com>

CMakeLists.txt
Makefile
Package.swift
build.zig
examples/quantize/quantize.cpp
ggml-quants.c
ggml-quants.h
ggml.c
ggml.h
llama.cpp
llama.h

commit ff3bad83e29e3009010cbc923bebd769055eaa7f
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Sat Oct 28 16:41:07 2023 +0200

    flake : update flake.lock for newer transformers version + provide extra dev shell (#3797)
    
    * flake : update flake.lock for newer transformers version + provide extra dev shell with torch and transformers (for most convert-xxx.py scripts)

flake.lock
flake.nix

commit 82a6646e0221216c41edcdf99f5a44bb051391f5
Author: Aarni Koskela <akx@iki.fi>
Date:   Sat Oct 28 15:43:01 2023 +0300

    metal : try cwd for ggml-metal.metal if bundle lookup fails (#3793)
    
    * Try cwd for ggml-metal if bundle lookup fails
    
    When building with `-DBUILD_SHARED_LIBS=ON -DLLAMA_METAL=ON -DLLAMA_BUILD_SERVER=ON`,
    `server` would fail to load `ggml-metal.metal` because `[bundle pathForResource:...]`
    returns `nil`.  In that case, fall back to `ggml-metal.metal` in the cwd instead of
    passing `null` as a path.
    
    Follows up on #1782
    
    * Update ggml-metal.m
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.m

commit ba231e8a6dd8ad82acfe0e4d492ff7cef6b3f0a1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 28 15:25:33 2023 +0300

    issues : change label from bug to bug-unconfirmed (#3748)

.github/ISSUE_TEMPLATE/bug.md

commit 8a2f2fea2914aaa3f4b2f82800c7de15f15bdb09
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 28 15:25:15 2023 +0300

    convert : ignore tokens if their IDs are within [0, vocab_size) (#3831)

convert.py

commit bd6d9e205982b34e0ba2c3b22bbf31a1ef1a1bb5
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Oct 28 05:54:24 2023 -0600

    llama : allow quantizing k-quants to fall back when tensor size incompatible (#3747)
    
    * Allow quantizing k-quants to fall back when tensor size incompatible
    
    * quantizing: Add warning when tensors were incompatible with k-quants
    
    Clean up k-quants state passing a bit

llama.cpp

commit ee1a0ec9cb367ba41d138134795cbbbe93d2bf1c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 28 14:23:11 2023 +0300

    llama : add option for greedy sampling with probs (#3813)
    
    * llama : add option for greedy sampling with probs
    
    * llama : add comment about llama_sample_token_greedy() missing probs
    
    * sampling : temp == 0.0 -> no probs, temp < 0.0 -> probs

common/common.cpp
common/sampling.cpp
examples/speculative/speculative.cpp
llama.h

commit 177461104b454163473dced2a5038f4e016cdb7e
Author: Henk Poley <HenkPoley@gmail.com>
Date:   Sat Oct 28 12:16:33 2023 +0200

    common : print that one line of the syntax help *also* to standard output (#3823)

common/common.cpp

commit fdee152e4eebb78c191df0b074857111d7f2aba7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Oct 28 12:06:08 2023 +0300

    starcoder : add GPU offloading (#3827)
    
    * starcoder : do not GPU split 1D bias tensors
    
    * starcoder : offload layers to GPU
    
    ggml-ci

llama.cpp

commit 41aee4df821854f37d90a45281f03b6db8d27de2
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Oct 27 15:40:07 2023 -0600

    speculative : ensure draft and target model vocab matches (#3812)
    
    * speculative: Ensure draft and target model vocab matches
    
    * Tolerate small differences when checking dft vs tgt vocab

examples/speculative/speculative.cpp

commit 6d459cbfbe5a011dfca94f9550527a504b6f9aa1
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Oct 27 17:33:53 2023 -0400

    llama : correctly report GGUFv3 format (#3818)

llama.cpp

commit c8d6a1f34ab6f1b6bd468d256e535a61f98f114c
Author: Thibault Terrasson <thibault.terrasson@gmail.com>
Date:   Fri Oct 27 16:37:41 2023 +0200

    simple : fix batch handling (#3803)

examples/simple/simple.cpp

commit 2f9ec7e271220a78fe27c9e6ccbcc0dda31cda0f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 27 17:01:23 2023 +0300

    cuda : improve text-generation and batched decoding performance (#3776)
    
    * cuda : prints wip
    
    * cuda : new cublas gemm branch for multi-batch quantized src0
    
    * cuda : add F32 sgemm branch
    
    * cuda : fine-tune >= VOLTA params + use MMQ only for small batches
    
    * cuda : remove duplicated cuBLAS GEMM code
    
    * cuda : add CUDA_USE_TENSOR_CORES and GGML_CUDA_FORCE_MMQ macros
    
    * build : add compile option to force use of MMQ kernels

CMakeLists.txt
Makefile
ggml-cuda.cu
llama.cpp
llama.h

commit 34b2a5e1ee4fe6295fb4420eb91131d743694c65
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 26 22:53:37 2023 +0300

    server : do not release slot on image input (#3798)

examples/server/server.cpp

commit 6961c4bd0b5176e10ab03b35394f1e9eab761792
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 25 10:26:27 2023 +0300

    batched-bench : print params at start

examples/batched-bench/batched-bench.cpp
ggml-cuda.cu

commit cc448774866e6479c750bd7c135cd8f92cedee67
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 25 10:09:16 2023 +0300

    log : disable pid in log filenames

common/log.h

commit ad939626577cd25b462e8026cc543efb71528472
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Oct 24 16:10:43 2023 -0400

    server : add parameter -tb N, --threads-batch N (#3584) (#3768)
    
    Co-authored-by: Michael Coppola <m18coppola@gmail.com>
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

examples/server/server.cpp

commit 1717521cdb976a2219888b0e5cba36e210eee9df
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 24 23:08:20 2023 +0300

    server : do not block system prompt update (#3767)
    
    * server : do not block system prompt update
    
    * server : update state machine logic to process system prompts
    
    * server : minor

examples/server/server.cpp

commit b2f7e04bd312eaf97eee0523aa09d950d585626b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 24 21:51:20 2023 +0300

    sync : ggml (conv ops + cuda MSVC fixes) (#3765)
    
    ggml-ci

ggml-cuda.cu
ggml.c
ggml.h

commit abd21fc99f1d35e2081e4c01dc09c71a86bf3c5a
Author: John Smith <67539080+kingsidelee@users.noreply.github.com>
Date:   Wed Oct 25 01:48:45 2023 +0800

    cmake : add missed dependencies (#3763)

examples/main-cmake-pkg/CMakeLists.txt

commit 2b4ea35e56792064598e922e46d081e02bc96b94
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 24 16:48:37 2023 +0300

    cuda : add batched cuBLAS GEMM for faster attention (#3749)
    
    * cmake : add helper for faster CUDA builds
    
    * batched : add NGL arg
    
    * ggml : skip nops in compute_forward
    
    * cuda : minor indentation
    
    * cuda : batched cuBLAS GEMMs for src0 F16 and src1 F32 (attention ops)
    
    * Apply suggestions from code review
    
    These changes plus:
    
    ```c++
    #define cublasGemmBatchedEx hipblasGemmBatchedEx
    ```
    
    are needed to compile with ROCM. I haven't done performance testing, but it seems to work.
    
    I couldn't figure out how to propose a change for lines outside what the pull changed, also this is the first time trying to create a multi-part review so please forgive me if I mess something up.
    
    * cuda : add ROCm / hipBLAS cublasGemmBatchedEx define
    
    * cuda : add cublasGemmStridedBatchedEx for non-broadcasted cases
    
    * cuda : reduce mallocs in cublasGemmBatchedEx branch
    
    * cuda : add TODO for calling cublas from kernel + using mem pool
    
    ---------
    
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>

CMakeLists.txt
examples/batched/batched.cpp
ggml-cuda.cu
ggml.c

commit daab3d7f45832e10773c99f3484b0d5b14d86c0c
Author: Galunid <karolek1231456@gmail.com>
Date:   Tue Oct 24 09:17:17 2023 +0200

    Add more tokenizer tests (#3742)
    
    * Add more tokenizer tests
    
    * Add starcoder
    
    * Update test vocab files
    
    * Restrict bpe tokenizer tests to unicode planes
    
    * Update comment
    
    * Comment cosmetics
    
    * Remove bloom vocab/test

models/ggml-vocab-baichuan.gguf
models/ggml-vocab-gpt-neox.gguf
models/ggml-vocab-refact.gguf
models/ggml-vocab-starcoder.gguf
tests/CMakeLists.txt
tests/test-tokenizer-1-bpe.cpp

commit 469c9addef75893e6be12edda852d12e840bf064
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 24 09:46:50 2023 +0300

    metal : handle ggml_scale for n%4 != 0 (close #3754)
    
    ggml-ci

ggml-metal.m
ggml-metal.metal

commit e3932593d46c30145301a13097895f9376cba509
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 23 23:46:05 2023 +0300

    Revert "make : add optional CUDA_NATIVE_ARCH (#2482)"
    
    This reverts commit 96981f37b1e3f450d9e63e571514217bf60f0a7f.
    
    See:
    
    https://github.com/ggerganov/llama.cpp/pull/2482#issuecomment-1775975866

Makefile

commit 9d02956443e5c1ded29b7b5ed8a21bc01ba6f563
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Mon Oct 23 22:57:16 2023 +0300

    issues : separate bug and enhancement template + no default title (#3748)

.github/ISSUE_TEMPLATE/bug.md
.github/ISSUE_TEMPLATE/enhancement.md

commit 69a6735087c3634963c642fd69f0851ac479cd78
Author: Galunid <karolek1231456@gmail.com>
Date:   Mon Oct 23 21:46:00 2023 +0200

    Update special token handling in conversion scripts for gpt2 derived tokenizers (#3746)
    
    We still have the heads up in `README.md` regarding `bpe` tokenizers and this patch is needed for
    
    - a couple of tokenizer tests
    - some more `special` and `non-special` added tokens handling (as far as I understand it)
    
    * Update special token handling
    
    * Add mpt

convert-bloom-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-mpt-hf-to-gguf.py
convert-refact-hf-to-gguf.py
convert-starcoder-hf-to-gguf.py

commit 5be6c803fa5378f62a1590f3ad8c6b64c7c0c2ce
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Mon Oct 23 12:40:03 2023 -0700

    llama : remove token functions with `context` args in favor of `model` (#3720)
    
    * added `llama_model_token_*` variants to all the `llama_token_*` functions.
    
    * added `LLAMA_API`
    
    * formatting
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * removed old `llama_token` functions
    
    * changed 3 more functions to take in model
    
    - `llama_token_get_text`
    - `llama_token_get_score`
    - `llama_token_get_type`
    
    * added back docs
    
    * fixed main.cpp
    
    * changed token functions to use new model variants
    
    * changed token functions to use new model variants
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/sampling.cpp
common/train.cpp
examples/batched/batched.cpp
examples/beam-search/beam-search.cpp
examples/infill/infill.cpp
examples/llama-bench/llama-bench.cpp
examples/llava/llava-utils.h
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/perplexity/perplexity.cpp
examples/server/server.cpp
examples/simple/simple.cpp
examples/speculative/speculative.cpp
llama.cpp
llama.h

commit 6336701c9378c23c85d1c0e464b663ca2bbb8e60
Author: Galunid <karolek1231456@gmail.com>
Date:   Mon Oct 23 17:47:03 2023 +0200

    Fix baichuan convert script not detecing model (#3739)
    
    It seems nobody objects.

convert-baichuan-hf-to-gguf.py

commit 96981f37b1e3f450d9e63e571514217bf60f0a7f
Author: Alex <awhill19@icloud.com>
Date:   Sun Oct 22 15:56:53 2023 -0400

    make : add optional CUDA_NATIVE_ARCH (#2482)
    
    Use the environment variable `CUDA_NATIVE_ARCH` if present to set NVCC arch. Otherwise, use `native`.

Makefile

commit 438c2ca83045a00ef244093d27e9ed41a8cb4ea9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 22 22:53:08 2023 +0300

    server : parallel decoding and multimodal (#3677)
    
    * implementing parallel decoding in server example
    
    * crash fixed
    
    * save dev progress
    
    * refactored sampling function
    
    * completion endpoint working
    
    * multiple client support
    
    * grammar + no stream completion
    
    * cached prompt support
    
    * chat.mjs support cached prompt + some fixes
    
    * server ui now support multiple clients
    
    * unused change reverted
    
    * fixed timings per slot
    
    * add context swap
    
    * add changes to README.md
    
    * llava multimodal integration
    
    * fixed tokens probs
    
    * add multimodal input - alfa
    
    * refactor code + remove unused comments + improved README.md
    
    * fix compilation errors with llvm
    
    * notify the user from server ui that multimodality is unavialable
    
    * some ci fixes
    
    * fix ci make build undefined ref errors
    
    * fix long prompt than ctx proposed in #3639
    
    * fixed premature end due stop word
    
    * context shift fixed
    
    * fix llava implementation
    
    * sync README.md changes
    
    * readme change
    
    * update api like OpenAI
    
    * multimodal support enabled by default
    
    * fix make bui;d errors
    
    * fix multiple clients
    
    * fix zig build
    
    * new sampling API
    
    * latest changes of sampling API
    
    * server : coding-style normalization
    
    * server : coding-style normalization (part 2)
    
    * server : remove beam-search functionality
    
    * server : bug fix in ingest_images
    
    n_tokens is incremented internally by llama_batch_add
    
    * server : use refs + use llama_batch_clear()
    
    * server : snake case
    
    * server : minor sync
    
    * added thread safe pipeline
    
    * server : bach has to be allocated for n_parallel sequences
    
    * server : no need for atomic int - already using mutex
    
    * server : logs + minor code style
    
    * server : fix multibyte handle in partial response (#3706)
    
    * fix image load + view image in chat
    
    * make : silence stb warnings
    
    * clip : link to ggml, not to llama
    
    * server : fix switch fallthrough
    
    * server : fix crash in Debug on macOS (I have no idea why this fixes it!?)
    
    * server : refactor ctx_sampling init + n_ctx + names
    
    * server : bug fix for prompt caching
    
    * Do not save/load image_data to localStorage
    
    * editorconfig : new line in index.html
    
    * server : completion requests remember slot_id
    
    * Update readme to document multimodal in server
    
    * server : minor style
    
    * Update readme to document multimodal in server
    
    * server : hide ctx_sampling->prev behind API (#3696)
    
    * server : apply fix from #3722
    
    * server : fix slot reuse
    
    * server : add comment about changing slot_state to bool
    
    ---------
    
    Co-authored-by: FSSRepo <go778sgt@gmail.com>
    Co-authored-by: Damian Stewart <d@damianstewart.com>
    Co-authored-by: Steward Garcia <57494570+FSSRepo@users.noreply.github.com>
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>

.gitignore
Makefile
build.zig
examples/llava/CMakeLists.txt
examples/llava/clip.cpp
examples/server/CMakeLists.txt
examples/server/README.md
examples/server/api_like_OAI.py
examples/server/chat.mjs
examples/server/index.html.hpp
examples/server/public/index.html
examples/server/server.cpp

commit 9e70cc03229df19ca2d28ce23cc817198f897278
Author: goerch <jhr.walter@t-online.de>
Date:   Sun Oct 22 21:21:42 2023 +0200

    Add test for MPT tokenization (#3728)
    
    * Add test for MPT tokenization
    
    * Revert code motion
    
    * Remove unnecessary restriction in test case
    
    * Clarify logic in conversion

convert-mpt-hf-to-gguf.py
llama.cpp
models/ggml-vocab-mpt.gguf
tests/CMakeLists.txt

commit 5a42a5f8e8a86da9ac88008d748cf232a83aa0e1
Author: Ian Scrivener <github@zilogy.asia>
Date:   Mon Oct 23 05:16:43 2023 +1100

    readme : remove unsupported node.js library (#3703)
    
    - https://github.com/Atome-FE/llama-node is quite out of date
    - doesn't support recent/current llama.cpp functionality

README.md

commit a5e7dbd6141128bfa3c40a19c2945a181df625d3
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Oct 22 12:14:56 2023 -0600

    llama : validate special token ids are in range when loading GGUF model (#3635)
    
    * Add validation for special token ids to llama.cpp
    
    Small optimization for llama_byte_to_token SPM mode
    
    * Fix BPE newline check, only I could break something so simple
    
    * Killll meeeeee
    
    * Account for GGUF_KEY_KEY only setting when the key exists
    
    * Minor code cleanups.
    
    * Fix convert.py error msg when added tokens are out of range
    
    * Make gguf SpecialVocab vocab size-aware
    
    Update conversion scripts accordingly
    
    * Avoid a string copy
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-baichuan-hf-to-gguf.py
convert-bloom-hf-to-gguf.py
convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-llama-ggml-to-gguf.py
convert-mpt-hf-to-gguf.py
convert-refact-hf-to-gguf.py
convert-starcoder-hf-to-gguf.py
convert.py
gguf-py/gguf/gguf.py
llama.cpp

commit d3956aea53369455008159cc405ed4c496976692
Author: vvhg1 <94630311+vvhg1@users.noreply.github.com>
Date:   Sun Oct 22 20:09:51 2023 +0200

    main : escape prompt for cfg_negative_prompt and consecutive inputs in main with interactive (#3623)
    
    * infill tokens correction
    
    * serverinfill tokens correction
    
    * removing any leading whitespace from infill suffix and removing leeading space token from suffix when params.escape
    
    * removing any leading whitespace from infill suffix and removing leeading space token from suffix when params.escape
    
    * only rm when params.escape, rm space if possible which is added back or rm added space token
    
    * only rm when params.escape, rm space if possible which is added back or rm added space token
    
    * Revert "only rm when params.escape, rm space if possible which is added back or rm added space token"
    
    This reverts commit 63ba0b621f21077c0e3bc6ba6a327534123cb738.
    
    * fix interactive prompt escaping and fix server infill leading space handling
    
    * rm unnecessary bool check
    
    * process escapes for neg prompt and interactive consec prompts
    
    * removed unneccessary static string escape

common/common.cpp
examples/main/main.cpp

commit 22c69a27945e7acf9690dd3210d316f22182751c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 22 08:37:20 2023 +0300

    batched : add len CLI argument

examples/batched/batched.cpp

commit 465219b9143ac01db0990bbcb0a081ef72ec2008
Author: shibe2 <shibe@tuta.io>
Date:   Thu Oct 12 16:01:23 2023 +0400

    CLBlast: Add outer loops over src0 for broadcasting in mulmat
    
    Reduce repeated dequantization of the same data.

ggml-opencl.cpp

commit d1031cf49c3b958b915fd558e23453471c29ac33
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 20 21:07:23 2023 +0300

    sampling : refactor init to use llama_sampling_params (#3696)
    
    * sampling : refactor init to use llama_sampling_params
    
    * llama : combine repetition, frequency and presence penalties in 1 call
    
    * examples : remove embd-input and gptneox-wip
    
    * sampling : rename penalty params + reduce size of "prev" vector
    
    * sampling : add llama_sampling_print helper
    
    * sampling : hide prev behind API and apply #3661
    
    ggml-ci

Makefile
README.md
common/common.cpp
common/common.h
common/sampling.cpp
common/sampling.h
examples/CMakeLists.txt
examples/embd-input/.gitignore
examples/embd-input/CMakeLists.txt
examples/embd-input/README.md
examples/embd-input/embd-input-lib.cpp
examples/embd-input/embd-input-test.cpp
examples/embd-input/embd-input.h
examples/embd-input/embd_input.py
examples/embd-input/llava.py
examples/embd-input/minigpt4.py
examples/embd-input/panda_gpt.py
examples/gptneox-wip/cmpnct_gpt2bpe.hpp
examples/gptneox-wip/falcon-main.cpp
examples/gptneox-wip/gptneox-main.cpp
examples/infill/CMakeLists.txt
examples/infill/infill.cpp
examples/llava/llava-utils.h
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/server/server.cpp
examples/speculative/speculative.cpp
llama.cpp
llama.h
tests/test-sampling.cpp

commit 8cf19d60dc93809db8e51fedc811595eed9134c5
Author: Qin Yue Chen <71813199+chenqiny@users.noreply.github.com>
Date:   Fri Oct 20 06:19:40 2023 -0500

    gguf : support big endian platform (#3552)
    
    * check whether platform is 390x if yes->do not import immintrin.h
    
    * support s390x big endian
    
    * support --bigendian option for s390x
    1. verified with baichuan7b-chat with float 16 on s390x
    2. verified with baichuan7b-chat
    3. verified with chinese-alpaca-2-13b-f16
    
    * update format based on editor-config checker result
    
    * Update convert-baichuan-hf-to-gguf.py
    
    * 1. check in ggml.c if endianess is not match
    2. update GGUF version
    3. change get_pack_prefix to property
    4. update information log
    
    * always use "GGUF" as beginng of GGUF file
    
    * Compare "GGUF" with file header char by char
    1.  Set GGUF_MAGIC to "GGUF" string instead of int value
    2. Compare "GGUF" char by char to ensure its byte order
    3. Move bytes swap code from convert.py to gguf.py write_tensor_data
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-baichuan-hf-to-gguf.py
convert.py
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
ggml.c
ggml.h
gguf-py/gguf/gguf.py
gguf-py/pyproject.toml
k_quants.c
tests/test-double-float.cpp

commit a0edf73bda31c7c4e649e6f07c6fd30a729929cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 20 13:06:10 2023 +0300

    server : fix uninitialized sampling context (close #3685)

examples/server/server.cpp

commit f439e506e8ae8b01df2ae2156380f8156d7553e3
Author: Herman Semenov <GermanAizek@yandex.ru>
Date:   Fri Oct 20 10:02:12 2023 +0000

    ggml : fix rope + llama minor optimizations (#3560)
    
    * Minor fixes and fixed memleak
    
    * Using const auto references in range-based loop C++17

common/grammar-parser.cpp
common/train.cpp
ggml.c
llama.cpp

commit e78f3ef24af4ca74e77e725644b41ae8ca3b10a5
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Oct 20 01:32:08 2023 -0400

    convert : restore compat with old Falcon models (#3680)

convert-falcon-hf-to-gguf.py

commit f3b25e40438b3c8383caabf4e7b89863145a9f0e
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Thu Oct 19 19:40:41 2023 +0300

    multimodal : add BakLLaVA conversion support (#3682)

examples/llava/llava-surgery.py

commit 60abea9798f47b918a9f38c66edfd88c526d20f6
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Thu Oct 19 16:59:11 2023 +0300

    llava : avoid segfault in case of non-existent mmproj file (#3674)

examples/llava/clip.cpp

commit 004797f6ac135383f8c1d1f5bd415ddee2f79318
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 18 21:44:43 2023 +0300

    readme : update hot topics

README.md

commit 4e82b2ea3fa6482915d147bc9f46e70b9ada7700
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 18 18:49:40 2023 +0300

    speculative : bug fixes

examples/speculative/speculative.cpp

commit 0e89203b517c95ec6675eda75d200a60d1e8921d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 18 16:21:57 2023 +0300

    speculative : add tree-based sampling example (#3624)
    
    * sampling : one sequence per sampling context
    
    ggml-ci
    
    * speculative : add tree-based sampling support
    
    ggml-ci
    
    * speculative : reuse the n_parallel CLI param
    
    * speculative : refactor sampling
    
    * examples : fix build after sampling refactoring
    
    ggml-ci
    
    * batched : fix n_seq_id
    
    * sampling : fix malloc
    
    ggml-ci
    
    * swift : fix build
    
    ggml-ci
    
    * swift : try to fix build
    
    ggml-ci
    
    * prompts : add assistant.txt
    
    * common : add llama_batch_add() and llama_batch_clear() helpers
    
    * speculative : minor refactor
    
    ggml-ci
    
    * minor : comments + rename
    
    ggml-ci
    
    * speculative : fix off-by-one for n_drafted
    
    * speculative : fix the n_drafted fix + p constants

Makefile
common/common.cpp
common/common.h
common/log.h
common/sampling.cpp
common/sampling.h
examples/batched-bench/batched-bench.cpp
examples/batched.swift/Sources/main.swift
examples/batched/batched.cpp
examples/embd-input/embd-input-lib.cpp
examples/infill/infill.cpp
examples/llava/llava-utils.h
examples/llava/llava.cpp
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/server/server.cpp
examples/simple/simple.cpp
examples/speculative/speculative.cpp
llama.cpp
llama.h
prompts/assistant.txt

commit c67fe68e417f766970fb1feaf2e66458aa24116a
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Wed Oct 18 07:21:48 2023 -0500

    metal : implement q5_0 and q5_1 kernels (#3648)
    
    * metal : implement dequantize_q5_0
    
    * metal : block_q_n_dot_y for block_q5_0 (broken)
    
    * metal : revert unnecessary change
    
    * metal : implement dequantize_q5_1
    
    * metal : block_q_n_dot_y for q5_1 (broken)
    
    * metal : fix block_q_n_dot_y
    
    * minor : spaces / formatting
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 1117d06607d2d885640ac501f05f0aae5494e2c5
Author: shibe2 <shibe@tuta.io>
Date:   Wed Oct 18 16:09:22 2023 +0400

    opencl : fix element-wise multiplication (#3656)

ggml-opencl.cpp

commit cb33f43a2a9f5a5a5f8d290dd97c625d9ba97a2f
Author: slaren <slarengh@gmail.com>
Date:   Tue Oct 17 22:24:50 2023 +0200

    fix embeddings when using CUDA (#3657)

llama.cpp

commit e1675d133c31e1c8de2f06be7164e12c0ba6cf2c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 17 22:34:26 2023 +0300

    llama : avoid fprintf in favor of LLAMA_LOG (#3538)

examples/main/main.cpp
llama.cpp

commit 8402566a7c436bfbde8e7b0461faee50298106a0
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Tue Oct 17 14:13:21 2023 -0400

    readme : update hot-topics & models, detail windows release in usage (#3615)
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md
    
    * move "Running on Windows" section below "Prepare data and run"
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md

commit 40e5ce054f4c4fa555e4510ea5f760bb29185332
Author: shibe2 <shibe@tuta.io>
Date:   Wed Oct 11 21:30:06 2023 +0400

    CLBlast: Fix temporary buffer size for f16 conversion (wsize)
    
    Fix buffer overflow.
    Reduce the size to fit just one 2D slice.
    Assert sufficient size.

ggml-opencl.cpp

commit a5e8c1d8c71f01d98ae2ec63a57c118664f9764d
Author: slaren <slarengh@gmail.com>
Date:   Tue Oct 17 19:00:58 2023 +0200

    train-text-from-scratch : fix assert failure in ggml-alloc (#3618)

examples/train-text-from-scratch/train-text-from-scratch.cpp

commit e74c705e15cd228ad696c4a3cdea6d6fb4ff434c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 17 19:52:53 2023 +0300

    editorconfig : remove trailing spaces

examples/server/README.md

commit 3ad1e3f1a10c1f66b4f1cd7510e0977fadbc0dfd
Author: coezbek <c.oezbek@gmail.com>
Date:   Tue Oct 17 18:51:02 2023 +0200

    server : documentation of JSON return value of /completion endpoint (#3632)
    
    * Added documentation of JSON return value of /completion endpoint
    
    * Update examples/server/README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/README.md

commit 1142013da40e98946a109b141dd858f0ed996051
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 17 19:12:46 2023 +0300

    save-load-state : fix example + add ci test (#3655)
    
    * save-load-state : fix example (close #3606)
    
    * ci : add test for save-load-state example
    
    ggml-ci

ci/run.sh
examples/save-load-state/save-load-state.cpp

commit 5fe268a4d9ce09f3a6c77239af583d3a8e49d54c
Author: ldwang <ftgreat@163.com>
Date:   Tue Oct 17 23:52:33 2023 +0800

    readme : add Aquila2 links (#3610)
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    Co-authored-by: ldwang <ftgreat@gmail.com>

README.md

commit 1a159553f921a9209fed8c714494e57b3649f232
Author: staviq <staviq@gmail.com>
Date:   Tue Oct 17 17:11:01 2023 +0200

    tokenizer : special token handling (#3538)
    
    * Rewrite special token handling from #1931
    
    * shorten param name, add st verification by type
    
    * use offsets instead of copy by substr
    
    * formatting, remove copying iterator on delete
    
    * llama : normalize code-style
    
    * swift fix
    
    * print pfx/sfx if verb, main: split pfx input sfx
    
    * dont add space when using special tokens
    
    * minor : comment + spacing
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
common/train.cpp
examples/batched.swift/Sources/main.swift
examples/main/main.cpp
llama.cpp
llama.h

commit 281ef73c258cc1eebec8a64264240432d5878c4b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 17 09:19:28 2023 +0300

    k-quants : fix quantization ranges (#3646)

k_quants.c

commit 940efa95fec0b8a98c226a889d2ad839dfeeae0d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 16 23:58:00 2023 +0300

    llava : fix tokenization to not add bos between image embeddings and user prompt (#3645)
    
    * llava : fix tokenization to not add bos after system prompt
    
    * set seed
    
    ---------
    
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>

examples/llava/llava-utils.h
examples/llava/llava.cpp

commit 11bff290458f12f020b588792707f76ec658a27a
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Sun Oct 15 02:32:06 2023 -0400

    MPT : support GQA for replit-code-v1.5 (#3627)

convert-mpt-hf-to-gguf.py
llama.cpp

commit 11dc1091f64b24ca6d643acc6d0051117ba60161
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Sat Oct 14 13:52:44 2023 +0300

    Honor -ngl option for Cuda offloading in llava (#3621)

examples/llava/llava.cpp

commit 2a4bcbacead886996f175f33479d1d874a3e577f
Author: Daniel Bevenius <daniel.bevenius@gmail.com>
Date:   Fri Oct 13 12:33:16 2023 +0200

    llama : remove n_threads from llama_decode_internal (#3614)
    
    This commit removes `n_threads` from the `llama_decode_internal`
    functions doc comment as it does not exist anymore.
    
    It looks like this parameter was removed in
    Commit 16bc66d9479edd5ee12ec734973554d4493c5dfa ("llama.cpp : split
    llama_context_params into model and context params").
    
    Signed-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>

llama.cpp

commit 424b6381c4daeed62e6600e0402e72f39845b58d
Author: slaren <slarengh@gmail.com>
Date:   Fri Oct 13 12:23:10 2023 +0200

    ggml : add context enumeration functions (#3605)
    
    finetune : fix assert failure in ggml-alloc

examples/finetune/finetune.cpp
ggml.c
ggml.h

commit 1e0e873c373c33989beb6bc64d83cd572ab7fe2b
Author: shibe2 <shibe@tuta.io>
Date:   Thu Oct 12 23:59:47 2023 +0400

    CLBlast: Fix matrix-vector multiplication (#3544)

ggml-opencl.cpp

commit 370359e5baf619f3a8d461023143d1494b1e8fde
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Thu Oct 12 18:23:18 2023 +0300

    examples: support LLaVA v1.5 (multimodal model) (#3436)
    
    * WIP: start implementing LLaVA
    
    * rm scratch buf for now, will revert after cleanup
    
    * LLaVA image encoder is working. will combine with llama
    
    * Add llava inference code, but it's buggy. debugging
    
    * LLaVA is working e2e, needs to optimize memory allocation + cleanup
    
    * Use ggml_allocr + rm unnecessary code
    
    * fix: crlf -> lf
    
    * fix: new line at EoF
    
    * fix: trailing whitespace
    
    * Add readme
    
    * Update readme
    
    * Some cleanup
    
    * Are you happy editorconfig?
    
    * rm unused batch image preprocessing
    
    * rm unused import
    
    * fix: rm designated initializers
    
    * introduce pad-to-square mode for non-square images
    
    * are you happy editorconfig?
    
    * gitignore /llava
    
    * Handle cases where image file does not exist
    
    * add llava target to Makefile
    
    * add support for 13b model variant
    
    * Maybe seed is unlucky?
    
    * Check if apples are compared to apples
    
    * are you happy editorconfig?
    
    * Use temperature = 0.1 by default
    
    * command line: use gpt_params_parse()
    
    * minor
    
    * handle default n_predict
    
    * fix typo
    
    * llava : code formatting, rename files, fix compile warnings
    
    * do not use Wno-cast-qual for MSVC
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile
common/common.cpp
common/common.h
common/stb_image.h
examples/CMakeLists.txt
examples/llava/CMakeLists.txt
examples/llava/README.md
examples/llava/clip.cpp
examples/llava/clip.h
examples/llava/convert-image-encoder-to-gguf.py
examples/llava/llava-surgery.py
examples/llava/llava-utils.h
examples/llava/llava.cpp
ggml.c

commit 9e24cc6e2e589d405bd1720c400f5b0b9d0ca3ee
Author: uint256_t <maekawatoshiki1017@gmail.com>
Date:   Thu Oct 12 22:36:16 2023 +0900

    docs : fix typo GOMP_CPU_AFFINITY (#3597)

docs/BLIS.md

commit d28e572c0270eb72649dbcc3a347e36c05c2934a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 12 14:31:05 2023 +0300

    cmake : fix add_compile_options on macOS

CMakeLists.txt

commit f3040beaab5228b1a9dfe5675a200379478f7204
Author: Ian Scrivener <github@zilogy.asia>
Date:   Thu Oct 12 22:10:50 2023 +1100

    typo : it is `--n-gpu-layers` not `--gpu-layers` (#3592)
    
    fixed a typo in the MacOS Metal run doco

README.md

commit 1a8c8795d64b04df96c28f29faac2d6e256f53bc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 12 13:44:56 2023 +0300

    ci : check if there is enough VRAM (#3596)
    
    ggml-ci

ci/run.sh

commit b016596d903641f8825cd94bb6742e1de0c21017
Author: Aarni Koskela <akx@iki.fi>
Date:   Thu Oct 12 15:51:53 2023 +0900

    server : add completion mode (no chat) (#3582)

examples/server/index.html.hpp
examples/server/public/index.html

commit 6b3ae4da92485f979a0f45774fcf68597634db0b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 12 09:35:19 2023 +0300

    prompts : add mnemonics.txt

prompts/mnemonics.txt

commit 57dd55e2c742bfc50e0f5c6fb95c14118cff44f6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Oct 12 09:29:04 2023 +0300

    server : fix kv cache management (#3588)

examples/server/server.cpp

commit b8fe4b5cc9cb237ca98e5bc51b5d189e3c446d13
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 11 23:55:08 2023 +0300

    main : fix session loading bug (#3400)

examples/main/main.cpp

commit a8bdd65525ae86dea905e9866ad369b53e30ac14
Author: Michael Coppola <m18coppola@gmail.com>
Date:   Wed Oct 11 15:42:22 2023 -0400

    server : add parameter -tb N, --threads-batch N (#3584)
    
    Co-authored-by: Michael Coppola <info@michaeljcoppola.com>

examples/server/server.cpp

commit 70c29da118cdb02bfcbd0376c32b5b2236e48e48
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Wed Oct 11 13:35:46 2023 -0600

    common : fix mirostat state when using multiple sequences (#3543)
    
    * Fix mirostat state when using multiple sequences
    
    * Fix mirostat by completely refactoring sampling!
    
    * Try to fix zig build.
    
    * Export function to fetch/create default sampler states
    
    Code formatting cleanups and add some comments
    
    Silence a warning about id not being used when logging is disabled
    
    * Apply some renaming suggestions.
    
    Fix comments that were out of sync with the pull.
    
    * Use more consistant naming convention for sampling contexts

Makefile
build.zig
common/CMakeLists.txt
common/common.cpp
common/common.h
common/sampling.cpp
common/sampling.h
examples/embd-input/embd-input-lib.cpp
examples/infill/infill.cpp
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/save-load-state/save-load-state.cpp
examples/server/server.cpp
examples/speculative/speculative.cpp

commit 8c70a5ff25964f0a81e20d142a2f5ac5baff22fc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 11 21:25:33 2023 +0300

    batched : add bench tool (#3545)
    
    * batched : add bench tool
    
    * batched : minor fix table
    
    * batched-bench : add readme + n_kv_max is now configurable
    
    * batched-bench : init warm-up batch
    
    * batched-bench : pass custom set of PP, TG and PL
    
    * batched-bench : add mmq CLI arg

.gitignore
Makefile
examples/CMakeLists.txt
examples/batched-bench/CMakeLists.txt
examples/batched-bench/README.md
examples/batched-bench/batched-bench.cpp
examples/batched/batched.cpp

commit 24ba3d829e31a6eda3fa1723f692608c2fa3adda
Author: Zane Shannon <z@zcs.me>
Date:   Wed Oct 11 04:14:05 2023 -0700

    examples : add batched.swift + improve CI for swift (#3562)

.github/workflows/build.yml
Makefile
examples/batched.swift/.gitignore
examples/batched.swift/Makefile
examples/batched.swift/Package.swift
examples/batched.swift/README.md
examples/batched.swift/Sources/main.swift

commit 9f6ede19f3cfa50d4a51a5babb056c3f8a450b80
Author: Galunid <karolek1231456@gmail.com>
Date:   Wed Oct 11 01:02:49 2023 +0200

    Add MPT model to supported models in README.md (#3574)

README.md

commit 233fc1c69f6f415f35363e18a755f9610e89161b
Author: goerch <jhr.walter@t-online.de>
Date:   Tue Oct 10 18:59:52 2023 +0200

    Minor improvements in GPT2 tokenizer (#3567)
    
    * Fixing minor bugs in bpe_gpt2_preprocess
    
    * Don't add bos token in test

llama.cpp
tests/test-tokenizer-0-falcon.cpp
tests/test-tokenizer-0-falcon.py
tests/test-tokenizer-0-llama.cpp
tests/test-tokenizer-0-llama.py

commit c5b49360d0d9e49f32e05a9116e90bd0b39a282d
Author: Xingchen Song(宋星辰) <xingchensong1996@163.com>
Date:   Wed Oct 11 00:28:50 2023 +0800

    readme : add bloom (#3570)

README.md

commit 02d2875deff28599c6c2c6e1886fab002ffe43b1
Author: Xingchen Song(宋星辰) <xingchensong1996@163.com>
Date:   Tue Oct 10 22:48:21 2023 +0800

    llm : add bloom models (#3553)
    
    * feat: Support bloom models
    
    * fix(bloom): fix model size
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-bloom-hf-to-gguf.py
gguf-py/gguf/gguf.py
llama.cpp

commit 0aa6595ae02f97f2e5ffd74bf57a8b21ac83b272
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Tue Oct 10 06:31:13 2023 -0500

    swift : improvements and fixes (#3564)
    
    * swift : use macOS 12 as minimum requirement
    
    * swift : add missing ggml-backend.c source
    
    * swift : add -O3 -DNDEBUG unsafe flags

Package.swift

commit f5f9121de140eff558f13b5c5e78a3a3b6b94377
Author: Jan Ploski <jpl@plosquare.com>
Date:   Tue Oct 10 09:50:23 2023 +0200

    llm : add MPT support (#3417)
    
    * CUDA: added support for ggml_clamp (see also: https://github.com/ggerganov/ggml/issues/545)
    
    * mpt : added an implementation based (mostly) on falcon integration, modified with deltas from ggml/examples/mpt
    
    * mpt : protect against "clip_qkv": null in mpt-7b
    
    * mpt : quick fix to avoid "Strange model" warning when quantizing MPT models
    
    * mpt : addendum to changeset:84e30e8 - leave parameter clamp_kqv out from metadata rather than use 0.0 to indicate "no clamping" (more compliant with the current GGUF spec?)
    
    * mpt : standardized all tensor names to follow GGUF spec
    
    * mpt : addendum to changeset:1be89c40 - use "req" parameter of GGUF_GET_KEY macro instead of duplicate code
    
    * mpt : fixed comment s/gptneox/mpt/
    
    * mpt : remove tabs, trailing whitespace
    
    * mpt : removed ne01 + n_past == ne00 assertion from alibi (cuda/f32) and rope_shift from build_mpt
    
    * mpt : updated convert-mpt-hf-to-gguf.py to reflect changes made to convert-gptneox-hf-to-gguf.py in pr:3252
    
    * comment out n_past instead of marking it unused
    
    * mpt : removed hardcoded +178 from convert script in favor of utilizing hparams["vocab_size"]
    
    * mpt : remove unused tokenizer_json in convert script
    
    * ggml : remove obsolete n_past assert in ggml_alibi
    
    * llama : print clam_kqv and max_alibi_bias hparams
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-mpt-hf-to-gguf.py
ggml-cuda.cu
ggml-metal.m
ggml.c
llama.cpp

commit 11ea5c7d96f2c28e1c99659e08ec0a44574056e2
Author: vvhg1 <94630311+vvhg1@users.noreply.github.com>
Date:   Tue Oct 10 09:31:21 2023 +0200

    infill. : fix tokenization (#3508)
    
    * infill tokens correction
    
    * serverinfill tokens correction
    
    * removing any leading whitespace from infill suffix and removing leeading space token from suffix when params.escape
    
    * removing any leading whitespace from infill suffix and removing leeading space token from suffix when params.escape
    
    * only rm when params.escape, rm space if possible which is added back or rm added space token
    
    * only rm when params.escape, rm space if possible which is added back or rm added space token
    
    * Revert "only rm when params.escape, rm space if possible which is added back or rm added space token"
    
    This reverts commit 63ba0b621f21077c0e3bc6ba6a327534123cb738.
    
    * fix interactive prompt escaping and fix server infill leading space handling
    
    * rm unnecessary bool check

examples/infill/infill.cpp
examples/server/server.cpp

commit 95bd60a0a69f57e9a2ff1269667ea484a1a9bb40
Author: slaren <slarengh@gmail.com>
Date:   Mon Oct 9 14:44:58 2023 +0200

    ggml-alloc : fix assert in debug builds (#3555)

ggml-alloc.c

commit fcca0a700487999d52a525c96d6661e9f6a8703a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 9 14:32:17 2023 +0300

    refact : fix convert script + zero out KV cache to avoid nans (#3523)
    
    * refact : fix convert script + zero out KV cache to avoid nans
    
    * ggml : silu(-inf) should never happen
    
    * metal : assert various kernel requirements

convert-refact-hf-to-gguf.py
examples/parallel/parallel.cpp
ggml-metal.m
ggml-metal.metal
ggml.c
llama.cpp

commit dcc09d25961c5d0626bc148e558ee841141748f7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Oct 9 14:28:27 2023 +0300

    metal : do not use mul_mm kernels when ne00 < 64 (#3542)

ggml-metal.m

commit db3abcc114d5d1790ba814aa1a80ac673d4ccc3e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 8 20:19:14 2023 +0300

    sync : ggml (ggml-backend) (#3548)
    
    * sync : ggml (ggml-backend)
    
    ggml-ci
    
    * zig : add ggml-backend to the build

CMakeLists.txt
Makefile
build.zig
ggml-alloc.c
ggml-alloc.h
ggml-backend.c
ggml-backend.h
ggml-cuda.cu
ggml-cuda.h
ggml-metal.h
ggml-metal.m
ggml.c
ggml.h
llama.cpp
scripts/sync-ggml.sh

commit eee42c670e6fa6df9cf17e7ffc319f74cbd81354
Author: Matheus C. França <matheus-catarino@hotmail.com>
Date:   Sun Oct 8 10:59:20 2023 -0300

    ci : add Zig CI/CD and fix build (#2996)
    
    * zig CI/CD and fix build
    
    Signed-off-by: Matheus Catarino França <matheus-catarino@hotmail.com>
    
    * fix build_compiler
    
    * ci : remove trailing whitespace
    
    ---------
    
    Signed-off-by: Matheus Catarino França <matheus-catarino@hotmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/zig-build.yml
build.zig

commit 8e6716a102e390e930594d51302730184dac83cc
Author: Ryder Wishart <ryderwishart@gmail.com>
Date:   Sun Oct 8 03:55:58 2023 -0700

    api_like_OAI.py : compat with Microsoft Guidance (#2746)
    
    Check for None in addition to empty string check in all request params
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/api_like_OAI.py

commit 9c38d181d40b9d27f8f42152c18e7c70bfffcf37
Author: arcrank <arcrank@gmail.com>
Date:   Sun Oct 8 06:52:57 2023 -0400

    api_like_OAI.py : simplify function (#2796)
    
    Simplify function

examples/server/api_like_OAI.py

commit a1202a31ed8c35705bd09fe91c3e7410c619bd70
Author: Johannes Rudolph <johannes.rudolph@gmail.com>
Date:   Sun Oct 8 12:21:19 2023 +0200

    k-quants : fix comments about block sizing (#3499)

k_quants.h

commit 94e502dfb79430870b42b8e8ee132b4aaa93e4a8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 8 11:24:50 2023 +0300

    ci : enable on obj-c changes + fix metal build (#3540)

.github/workflows/build.yml
ggml-metal.m

commit 7d8b24932fe788a4cda76459a0c5df3e0073cb98
Author: Luo Tian <lt@basecity.com>
Date:   Sun Oct 8 16:24:01 2023 +0800

    zig : fix build by introducing train.cpp (#3539)

build.zig

commit b0ec5218c3d24755786b80ecce9cf4ffc07583f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Oct 8 10:01:53 2023 +0300

    metal : support MTLGPUFamily < Apple7, formatting, style (#3524)
    
    * metal : improve decoding speed for batches of 2-16
    
    * metal : rename kernels mul_mat_ to mul_mv_
    
    * metal : indentations
    
    * minor
    
    * metal : print more GPU info + disable mul_mm for MTLGPUFamiliy < Apple7

ggml-metal.m
ggml-metal.metal

commit 63d3b06a4318329f92b078e8aa0be7ab6e9f871f
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Oct 7 23:22:17 2023 -0600

    llama : fix missing break in Persimmon arch case statements (#3535)

llama.cpp

commit a16e89cec83b4bd5f6af8f1ce1400f94c12356f9
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Oct 7 15:31:41 2023 -0600

    Fix trying to strip newline from empty prompt and cfg prompt file content (#3534)

common/common.cpp

commit 4d0383321184aadf91968d9e3c6a45286ed2473b
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Sat Oct 7 22:14:10 2023 +0300

    gguf.py : fix CI for publishing GGUF package (#3532)
    
    * Fix CI for publishing GGUF package
    
    * Bump version
    
    * fix
    
    * bump version
    
    * bump version
    
    * bump version

.github/workflows/gguf-publish.yml
gguf-py/README.md
gguf-py/pyproject.toml

commit c47066d833c6c112e0d23342aa62c3250dd33c81
Author: Tom C <tom.corelis@gmail.com>
Date:   Sat Oct 7 02:56:15 2023 -0700

    py : change version of numpy requirement to 1.24.4 (#3515)
    
    Co-authored-by: Lyjia <me@lyjia.us>

requirements.txt

commit f1782c68de13b64bb5283fc2038f584e47be9fd2
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Sat Oct 7 04:41:52 2023 -0400

    quantize : fail fast on write errors (#3521)

llama.cpp

commit c26765a0a148b47e5b541df32438c3ad2a0a8314
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Oct 7 03:40:27 2023 -0500

    metal : support default.metallib load & reuse code for swift package (#3522)
    
    * metal : support load default.metallib & reuse code for swift package
    
    * metal : use SWIFT_PACKAGE def instead of define GGML_SWIFT

.gitignore
Package.swift
ggml-metal.m

commit 0e797c2fc571b866090f7d60ac7d39d8533593f2
Author: Phillip Kravtsov <phillip@kravtsov.net>
Date:   Sat Oct 7 00:12:43 2023 -0700

    llm : support Adept Persimmon 8B (#3410)
    
    * Produces garbage output
    
    * wip: correct tensors up to RoPE
    
    * correct tensors thru RoPE
    
    * Correct outputs through masked & softmax'd KQ
    
    * fp32 works
    
    * Rename adept->persimmon
    
    * Produces correct outputs
    
    * clean up convert scripts
    
    * remove printing logic from ggml.c
    
    * remove prints from llama.cpp & fix merge
    
    * trivial cleanups
    
    * Add offload funcs
    
    * update conversion script to directly take adept artifacts rather than .saftensors file
    
    * Fix norm eps bug
    
    * Support sqr and concat on metal, persimmon-8b-q4 runs correctly
    
    * Small changes from review
    
    * Formatting changes
    
    * Minor changes to conversion script
    
    * Remove old script
    
    * Fix editorconfig formatting
    
    * Fix build
    
    * add overlooked offload code ggml-ci

convert-persimmon-to-gguf.py
ggml-metal.m
ggml-metal.metal
gguf-py/gguf/gguf.py
llama.cpp

commit 3a716b4dae545c3db307594fbc509a95d3e21b6e
Author: goerch <jhr.walter@t-online.de>
Date:   Sat Oct 7 06:57:01 2023 +0200

    Fix for #3454 (#3455)
    
    Fix: `sentencepiece` tokenizers with added tokens failed with an incorrect assertion

llama.cpp

commit 1faaae8c2bdc4a21302e367e0754c3fe74a8113e
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Fri Oct 6 15:13:36 2023 -0400

    readme : update models, cuda + ppl instructions (#3510)

README.md

commit cb13d73a720c42d1958bff79b6869d77b26b8cea
Author: Mihai <mihai.chirculescu@yahoo.com>
Date:   Fri Oct 6 21:39:33 2023 +0300

    server : docs fix default values and add n_probs (#3506)

examples/server/README.md

commit 9ca79d5cbbc8d43f2bff951404b6a40ff1ee3788
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Oct 6 10:10:13 2023 -0600

    kv cache slot search improvements (#3493)
    
    * kv cache slot search improvements
    
    * Use n_ctx in kv find slot for consistency
    
    * Ensure kv cache head points to a valid slot in llama_decode internal
    
    * Add some comments to prevent dumb people (like me) from getting confused.

llama.cpp

commit 0c731ca4039ccff86ffab90eaae4ca98037c4496
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Oct 6 16:35:55 2023 +0300

    prompts : fix editorconfig checks after #3416

prompts/parallel-questions.txt

commit a8777ad84e00cda0399e827cdf971e2c3fab1da2
Author: pudepiedj <pudepiedj@gmail.com>
Date:   Fri Oct 6 14:16:38 2023 +0100

    parallel : add option to load external prompt file (#3416)
    
    * Enable external file and add datestamp
    
    * Add name of external file at end
    
    * Upload ToK2024
    
    * Delete ToK2024.txt
    
    * Experiments with jeopardy
    
    * Move ParallelQuestions to /proimpts and rename
    
    * Interim commit
    
    * Interim commit
    
    * Final revision
    
    * Remove trailing whitespace
    
    * remove cmake_all.sh
    
    * Remove cmake_all.sh
    
    * Changed .gitignore
    
    * Improved reporting and new question files.
    
    * Corrected typo
    
    * More LLM questions
    
    * Update LLM-questions.txt
    
    * Yet more LLM-questions
    
    * Remove jeopardy results file
    
    * Reinstate original jeopardy.sh
    
    * Update examples/parallel/parallel.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
common/common.h
examples/jeopardy/README.md
examples/parallel/parallel.cpp
llama.cpp
prompts/LLM-questions.txt
prompts/parallel-questions.txt

commit 97af49fa395df77e4c18af0e1655b2fee67c9686
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Fri Oct 6 07:44:24 2023 -0500

    server : reuse llama_sample_token common util (#3494)
    
    * server : reuse llama_sample_token common function
    
    * common : use n_probs for temperature sampling

common/common.cpp
examples/server/server.cpp

commit 16820a5a0d885113f21021ce934f0b0027b9d69a
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Fri Oct 6 18:47:59 2023 +0800

    llama : correct hparams comparison (#3446)
    
    * fixed floating point comparison issues
    
    * updated implementation for hparam comparison to handle inf and NaN
    
    * fixed code review comments
    
    * minor simplification
    
    * rename is_float_eq -> is_float_close
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

llama.cpp

commit 04b2f4386eda0264287156104cbf9d1b87895422
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Fri Oct 6 05:36:43 2023 -0500

    ci : fix xcodebuild destinations (#3491)
    
    * ci : fix xcodebuild destinations
    
    * ci : add .swift to paths

.github/workflows/build.yml

commit 48edda30ee545fdac2e7a33d505382888f748bbf
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Oct 5 15:00:34 2023 -0400

    convert : update Falcon script for new HF config (#3448)
    
    Also adds Falcon-180B support.
    Closes #3049
    
    Co-authored-by: jb <jonathan.t.barnard@gmail.com>

convert-falcon-hf-to-gguf.py

commit 45eba9369fbcbd7f677eba9a2d3e4ffcfdc81824
Author: Kenvix ⭐ <kenvixzure@live.com>
Date:   Fri Oct 6 01:16:39 2023 +0800

    build : use std::make_tuple() for compatibility with older GCC versions (#3488)

common/common.cpp
examples/server/server.cpp

commit acec9eaaa93315711c11d15afa8d245d164b7cff
Author: staviq <staviq@gmail.com>
Date:   Thu Oct 5 18:17:29 2023 +0200

    common : process escape sequences in reverse prompts (#3461)

common/common.cpp

commit e2583cbc29cd7d6d1403f338842c07dfc0467e6c
Author: shibe2 <shibe@tuta.io>
Date:   Thu Oct 5 15:57:03 2023 +0400

    CLBlast: Fix handling of on-device tensor data
    
    Fix uploading tensor data to device, including 3D, 4D, and non-contiguous tensors.
    Use correct offsets into data that is already in VRAM.
    Correct handling of OpenCL events when multiple commands are queued.

ggml-opencl.cpp

commit e8b8d32e8663ffc55a02c9721af3a5190382cbb0
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Thu Oct 5 09:02:55 2023 -0500

    server : fix incorrect num_tokens_predicted (#3480)

examples/server/server.cpp

commit 8f3a642ec1d878b2d0a0d15e3a4277f522790d4c
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Thu Oct 5 09:00:07 2023 -0500

    swift : disable ACCELERATE_NEW_LAPACK (#3481)

Package.swift

commit 0745384449fe8d89d6d99c93153569079e853247
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Thu Oct 5 08:56:21 2023 -0500

    ci : add swift build via xcodebuild (#3482)

.github/workflows/build.yml

commit 019ba1dcd0c7775a5ac0f7442634a330eb0173cc
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Wed Oct 4 08:20:28 2023 -0600

    convert : fix Baichuan2 models by using vocab size in config.json (#3299)
    
    Use local GGUF package when possible in Baichuan converter

convert-baichuan-hf-to-gguf.py

commit beabc8cfb0145b48aad68fefc573d316fe9c3a8a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 4 16:50:44 2023 +0300

    readme : add project status link

README.md

commit 0d152b37fecd5a4838330d47bb034cebf1681779
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 4 16:25:41 2023 +0300

    ggml : fix build after #3329

ggml.c

commit f8c90cdbaa729e64493164c1aba7ea80da7b716f
Author: ds5t5 <145942675+ds5t5@users.noreply.github.com>
Date:   Wed Oct 4 06:23:39 2023 -0700

    llm : add Refact model (#3329)
    
    * add refact model
    
    * resolve comments
    
    * rebase to the latest
    
    * solve alibi cpu error
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-refact-hf-to-gguf.py
ggml.c
gguf-py/gguf/gguf.py
llama.cpp

commit f93af02488179b9c52d0d391b08ae4c4d891b8d3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Oct 4 15:29:58 2023 +0300

    sync : ggml (conv 1d + 2d updates, UB fixes) (#3468)
    
    * sync : ggml (conv 1d + 2d updates)
    
    ggml-ci
    
    * ggml : fix UB in q5_0 and q5_1 quantize code
    
    ggml.c:1033:39: runtime error: left shift of 1 by 31 places cannot be represented in type 'int'
    SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior
    
    ggml.c:1081:39: runtime error: left shift of 1 by 31 places cannot be represented in type 'int'
    SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior
    
    ggml-ci
    
    * tests : fix UB in test-quantize-perf

ggml.c
ggml.h
k_quants.c
tests/test-grad0.cpp
tests/test-opt.cpp
tests/test-quantize-perf.cpp

commit f72f8f22c9cb60465b2e79df2767e4ba9604e576
Author: Merrick Christensen <merrick.christensen@gmail.com>
Date:   Wed Oct 4 00:33:13 2023 -0600

    finetune : readme fix typo (#3465)
    
    Fix small typo

examples/finetune/README.md

commit 79f34abddb72ac5ddbf118f3d87520b611a10a7d
Author: Tameem <113388789+AhmadTameem@users.noreply.github.com>
Date:   Tue Oct 3 23:38:19 2023 +0500

    ggml : add RISC-V Vector Support for K-Quants and improved the existing intrinsics (#3453)
    
    * Added RVV intrinsics support for Q8 quantize row and also improved the existing dot product function for risc-v.
    
    The RVV intrinsics is added for the following quantize row functions
       quantize_row_q8_0
       quantize_row_q8_1
    
    The following dot product functions have also been optimized by using LMUL = 1/2 instead of LMUL = 1
       ggml_vec_dot_q4_0_q8_0
       ggml_vec_dot_q4_1_q8_1
       ggml_vec_dot_q5_0_q8_0
       ggml_vec_dot_q5_1_q8_1
    
    And vector initialization in Q5 by temporary array is also replaced by the vid intrinsics
    
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>
    
    * Added RVV intrinsics support for k_quants
    
    This adds RISC-V Vector intrinsics support for the following K_quants functions for both QKK = 256 and QKK = 64
       ggml_vec_dot_q2_K_q8_K
       ggml_vec_dot_q3_K_q8_K
       ggml_vec_dot_q4_K_q8_K
       ggml_vec_dot_q5_K_q8_K
       ggml_vec_dot_q6_K_q8_K
    
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>
    
    ---------
    
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>

ggml.c
k_quants.c

commit 8186242b6d67cf87ae179fb1a62f52fdf0e5c5eb
Author: h-h-h-h <13482553+h-h-h-h@users.noreply.github.com>
Date:   Tue Oct 3 20:16:15 2023 +0200

    main : consistent prefix/suffix coloring (#3425)
    
    * Typo
    
    * No `--in-prefix` coloring
    
    The `--in-prefix` text was inconsistently colored. Now, it's never colored, just like the `--in-suffix` text.

examples/main/main.cpp

commit ac2219fef34eb5b713c286c34c6e4162c39c8f3b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Oct 3 21:04:01 2023 +0300

    llama : fix session saving/loading (#3400)
    
    * llama : fix session saving/loading
    
    * llama : temp fix for clearing "future" tokens from the KV cache
    
    * llama : fix handling of "future" tokens when loading sessions
    
    * llama : fix comments for llama_kv_cache API

examples/chat-persistent.sh
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/server/server.cpp
examples/speculative/speculative.cpp
llama.cpp
llama.h

commit 48be797ffbd80b062f55778e09e97180eb25d2ab
Author: Alex Klinkhamer <git@grencez.dev>
Date:   Tue Oct 3 10:09:28 2023 -0700

    llama : expose model's rope_freq_scale in the API (#3418)
    
    so it can be scaled further before creating a context.

llama.cpp
llama.h

commit f56e1baec361b5381e32ee6b6e56e4f00e002dfe
Author: Jiahao Li <liplus17@163.com>
Date:   Wed Oct 4 00:55:21 2023 +0800

    metal : alibi for arbitrary number of heads (#3426)

ggml-metal.m
ggml-metal.metal

commit 017efe899d8fa76118aef88e963210d48da01172
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Tue Oct 3 16:53:15 2023 +0000

    cmake : make LLAMA_NATIVE flag actually use the instructions supported by the processor (#3273)
    
    * fix LLAMA_NATIVE
    
    * syntax
    
    * alternate implementation
    
    * my eyes must be getting bad...
    
    * set cmake LLAMA_NATIVE=ON by default
    
    * march=native doesn't work for ios/tvos, so disable for those targets. also see what happens if we use it on msvc
    
    * revert 8283237 and only allow LLAMA_NATIVE on x86 like the Makefile
    
    * remove -DLLAMA_MPI=ON
    
    ---------
    
    Co-authored-by: netrunnereve <netrunnereve@users.noreply.github.com>

.github/workflows/build.yml
CMakeLists.txt
flake.nix

commit ff5a3f0c09dfa0a8e0bf76d1748df5c6dee0e8ff
Author: goerch <jhr.walter@t-online.de>
Date:   Tue Oct 3 09:16:26 2023 +0200

    Work on the BPE tokenizer (#3252)
    
    * Work on the BPE tokenizer
    
    Tokenizer tests work for Falcon-7B
    
    * Try to fix build problem
    
    * Fix debug assertion failure
    
    * Fix MSVC Unicode BOM problem
    
    * Cleanup and an improvement
    
    * Fix compiler warning
    
    * Cleanup
    
    * Test doesn't work over the full range of Unicodes
    
    * Update .gitignore and Makefile
    
    * Another Makefile rule
    
    * Testing Aquila
    
    * Moving byte decoding back to `token_to_piece` ...
    
    ... because everyone is using it.
    
    * Guarding some unusable code pathes
    
    * Streamlining code and adding some more assertions
    
    Important change: I'm classifying added tokens as control tokens now for BPE.
    
    * Adding a comment
    
    * Adding another assertion
    
    * Fixed vocabulary guarding assertions
    
    * Fix PR for recent change
    
    * Fix PR for recent change
    
    * Fix for compiler warning
    
    * Fix PR for recent change
    
    * Fix PR for recent change
    
    * Fix PR for recent change
    
    * Fix for compiler warning
    
    * Fixes for more compiler warnings
    
    * Remove unused code
    
    * Fix initialization of static maps
    
    * Add scores and token types back, adapt gptneox
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update unicode.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update unicode.h
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Ported Starcoder and added some assertions
    
    * Fix coding style
    
    * Apply @jploski 's fix for missing tokens
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile
common/common.cpp
convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-starcoder-hf-to-gguf.py
convert.py
llama.cpp
models/ggml-vocab-aquila.gguf
models/ggml-vocab-falcon.gguf
tests/CMakeLists.txt
tests/test-tokenizer-0-falcon.cpp
tests/test-tokenizer-1-bpe.cpp
tests/test-tokenizer-1-llama.cpp
unicode.h

commit 1c84003c08027f5d3a4cb876f51d6b6224a34d0e
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 18:07:24 2023 -0400

    convert : fix vocab size when not defined in hparams (#3421)

convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-starcoder-hf-to-gguf.py

commit e78f0b0d0572168f328dd0e2ed3175a53fe52acc
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 15:38:43 2023 -0400

    cmake : increase minimum version for add_link_options (#3444)

CMakeLists.txt

commit 665018c749101e81c816675198e731e47d6b1dbe
Author: shibe2 <shibe@tuta.io>
Date:   Mon Oct 2 23:26:15 2023 +0400

    CLBlast: Add broadcast support for matrix multiplication (#3402)
    
    Broadcast src0 into src1 across dimensions 2 and 3 when needed.
    This is required for models that use GQA.

ggml-opencl.cpp
ggml.c

commit 29a404a951fb0b3f9c3b6ab8c4c9c76ac50d2bb3
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 15:20:28 2023 -0400

    gguf : add BERT, MPT, and GPT-J arch info (#3408)

gguf-py/gguf/gguf.py

commit 0fe321031a5c670ab5fb5f49d69c4c91d783c93f
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 14:58:46 2023 -0400

    gguf : general usability improvements (#3409)

convert.py
examples/finetune/convert-finetune-checkpoint-to-gguf.py
examples/train-text-from-scratch/convert-train-checkpoint-to-gguf.py
gguf-py/gguf/gguf.py
gguf-py/pyproject.toml

commit 9476b012260a2fb6c67976582d64484ce7406ed9
Author: cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Oct 2 09:16:50 2023 -0400

    cmake : make CUDA flags more similar to the Makefile (#3420)
    
    * cmake : fix misuse of cxx_flags
    
    * cmake : make CUDA flags more similar to the Makefile
    
    * cmake : fix MSVC build

CMakeLists.txt

commit a03ce38455544121c5c00cf845def1443acd6ac8
Author: xaedes <xaedes@gmail.com>
Date:   Mon Oct 2 15:15:45 2023 +0200

    finetune : fix #3404 (#3437)
    
    the shapes for init model of gqa models was wrong

examples/finetune/finetune.cpp

commit a84767698495d72e44044f1f6db1c1cc721bfd15
Author: Adrian <smith.adriane@gmail.com>
Date:   Mon Oct 2 03:49:59 2023 -0700

    metal : set log callback before initializing (#3427)

llama.cpp

commit 095231dfd32679e32300f8ffaf1770b693ea64b0
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Mon Oct 2 06:51:49 2023 -0300

    cmake : fix transient definitions in find pkg (#3411)

CMakeLists.txt
examples/main-cmake-pkg/CMakeLists.txt
scripts/LlamaConfig.cmake.in

commit ea55295a745c084f588be20710f5a1a12abb1109
Author: Kevin Ji <1146876+kevinji@users.noreply.github.com>
Date:   Mon Oct 2 04:53:53 2023 -0400

    docker : ignore Git files (#3314)

.dockerignore

commit c97f01c362ac102c6994edb80008f8608539553a
Author: vvhg1 <94630311+vvhg1@users.noreply.github.com>
Date:   Mon Oct 2 09:42:02 2023 +0200

    infill : add new example + extend server API (#3296)
    
    * vvhg-code-infill (#1)
    
    * infill in separate example (#2)
    
    * reverted changes to main and added infill example
    
    * cleanup
    
    * naming improvement
    
    * make : add missing blank line
    
    * fix missing semicolon
    
    * brought infill up to current main code
    
    * cleanup
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

.gitignore
Makefile
common/common.cpp
common/common.h
examples/infill/CMakeLists.txt
examples/infill/README.md
examples/infill/infill.cpp
examples/server/README.md
examples/server/server.cpp
llama.cpp
llama.h

commit f5ef5cfb18148131fcf45bdd2331f0db5ab7c3d0
Author: slaren <slarengh@gmail.com>
Date:   Sat Sep 30 18:12:57 2023 +0200

    ggml-cuda : perform cublas mat mul of quantized types as f16 (#3412)
    
    * ggml-cuda : perform cublas matrix multiplication of quantized types as fp16
    
    * rename CC_TURING to CC_VOLTA
    
    * disable fp16 mat mul completely with multi GPU

ggml-cuda.cu

commit 40e07a60f9ce06e79f3ccd4c903eba300fb31b5e
Author: slaren <slarengh@gmail.com>
Date:   Fri Sep 29 18:42:32 2023 +0200

    llama.cpp : add documentation about rope_freq_base and scale values (#3401)
    
    * llama.cpp : add documentation about rope_freq_base and scale values
    
    * add notice to hot topics

README.md
llama.h

commit bc34dd4f5b5a7c10ae3ed85a265ce6f2ed2fab79
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 29 19:05:18 2023 +0300

    train : fix KQ_pos allocation (#3392)
    
    * train : fix KQ_pos allocation
    
    * make sure KQ_pos is not reallocated in finetune
    
    ---------
    
    Co-authored-by: xaedes <xaedes@gmail.com>

examples/finetune/finetune.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp

commit 2777a84be429401a2b7d33c2b6a4ada1f0776f1b
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 29 09:48:45 2023 -0400

    llama : quantize up to 31% faster on Linux and Windows with mmap (#3206)
    
    * llama : enable mmap in quantize on Linux -> 31% faster
    
    * also enable mmap on Windows
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp

commit 0a4a4a098261ddd26480371eaccfe90d1bf6488a
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Fri Sep 29 08:50:35 2023 -0400

    readme : update hot topics + model links (#3399)

README.md

commit 569550df20c1ede59ff195a6b6e900957ad84d16
Author: Andrew Duffy <a10y@users.noreply.github.com>
Date:   Fri Sep 29 07:15:57 2023 -0400

    readme : add link to grammars app (#3388)
    
    * Add link to grammars app per @ggernagov suggestion
    
    Adding a sentence in the Grammars section of README to point to grammar app, per https://github.com/ggerganov/llama.cpp/discussions/2494#discussioncomment-7138211
    
    * Update README.md

README.md

commit c71bf2c45c5140203184f50b259828107658e900
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Fri Sep 29 13:25:13 2023 +0800

    swift : fix build on xcode 15 (#3387)

Package.swift

commit bc39553c901a91cfcb757863586250838c83eeab
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 28 17:41:44 2023 -0400

    build : enable more non-default compiler warnings (#3200)

.gitignore
CMakeLists.txt
Makefile
common/common.cpp
common/log.h
examples/baby-llama/baby-llama.cpp
examples/llama-bench/llama-bench.cpp
examples/main/main.cpp
examples/quantize/quantize.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml.c
ggml.h
llama.cpp
pocs/vdot/q8dot.cpp
tests/test-grad0.cpp
tests/test-opt.cpp

commit 0ccfc62a96a6b59a8faa14d1b350493f4cd51ae2
Author: Hua Jiang <allenhjiang@outlook.com>
Date:   Thu Sep 28 13:06:18 2023 -0700

    ggml_tensor: update the structure comments. (#3283)
    
    * ggml_tensor: update the structure comments.
    
    * remove semicolon
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Update ggml.h
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>
    Co-authored-by: slaren <slarengh@gmail.com>

ggml.h

commit 7f1a0fe709ea1a861da2f3759f58a28bf8953c12
Author: Qu Zongfu <43257352+yancaoweidaode@users.noreply.github.com>
Date:   Fri Sep 29 03:51:52 2023 +0800

    ggml : release the requested thread pool resource (#3292)
    
    * Release the requested thread pool resource
    
    * Release the requested thread pool resource 2
    
    ---------
    
    Co-authored-by: Zongfu ZF3 Qu <quzf3@Lenovo.com>

ggml.c

commit 16bc66d9479edd5ee12ec734973554d4493c5dfa
Author: slaren <slarengh@gmail.com>
Date:   Thu Sep 28 21:42:38 2023 +0200

    llama.cpp : split llama_context_params into model and context params (#3301)
    
    * llama.cpp : split llama_context_params into model and context params
    
    ggml-ci
    
    * fix metal build
    
    * fix freq_base/scale default to model value
    
    * llama-bench : keep the same model between tests when possible
    
    * move n_threads to llama_context_params, add n_threads_batch
    
    * fix mpi build
    
    * remove kv_size(), cuda scratch fixes
    
    * remove low-vram option
    
    * add n_threads_batch to system info, refactor to get_system_info()
    
    * add documentation about --threads-batch to the READMEs
    
    * llama-bench fix
    
    * main : fix rope freq/scale warning
    
    * llama.cpp : add llama_get_model
    common : add llama_tokenize from model
    
    * remove duplicated ctx/model functions
    
    ggml-ci
    
    * cuda : print total VRAM used

common/common.cpp
common/common.h
common/train.cpp
examples/batched/batched.cpp
examples/beam-search/beam-search.cpp
examples/embd-input/embd-input-lib.cpp
examples/embd-input/embd-input-test.cpp
examples/embedding/embedding.cpp
examples/finetune/finetune.cpp
examples/llama-bench/llama-bench.cpp
examples/main/README.md
examples/main/main.cpp
examples/parallel/parallel.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
examples/save-load-state/save-load-state.cpp
examples/server/README.md
examples/server/server.cpp
examples/simple/simple.cpp
examples/speculative/speculative.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-cuda.cu
llama.cpp
llama.h
tests/test-tokenizer-0-falcon.cpp
tests/test-tokenizer-0-llama.cpp
tests/test-tokenizer-1-llama.cpp

commit 0512d66670de3f650c579519833c085014b0f200
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Thu Sep 28 19:31:04 2023 +0000

    ci : multithreaded builds (#3311)
    
    * mac and linux threads
    
    * windows
    
    * Update build.yml
    
    * Update build.yml
    
    * Update build.yml
    
    * automatically get thread count
    
    * windows syntax
    
    * try to fix freebsd
    
    * Update build.yml
    
    * Update build.yml
    
    * Update build.yml

.github/workflows/build.yml

commit 0e76a8992c8200237bbc6471a53fb8796b3872f7
Author: xaedes <xaedes@gmail.com>
Date:   Thu Sep 28 20:40:11 2023 +0200

    train : finetune LORA (#2632)
    
    * fix track_max_mem in forward_batch_wo_cache_flash_attn_train
    
    * remove unnecessary Adam(W) optimizer tensors.
    
    reduces optimizer memory overhead from 7*modelsize to 2*modelsize.
    
    additionally allows to optimize models with more than 2^31 parameters by replacing int with int64_t.
    
    bumps training checkpoint file version, but old checkpoints can still be read.
    new version with less tensors is saved.
    
    * add gradient clipping to AdamW
    
    * Fix reset of unused g->nodes and g->grads to NULL
    
    * implement gradient checkpointing for training
    
    reduces memory overhead from O(n_layer) to O(sqrt(n_layer))
    
    as explained in readme of https://github.com/cybertronai/gradient-checkpointing
    
    * remove unused compute buffer 3
    
    * add and use function ggml_build_backward_expand to avoid stack overflows with large maximum number of nodes
    
    GGML_API void ggml_build_backward_expand(struct ggml_context * ctx, struct ggml_cgraph * gf, struct ggml_cgraph * gb, bool keep);
    
    * change AdamW decay parameter to work like the torch AdamW decay parameter
    
    It is now relative to Adam learning rate `alpha*sched`.
    Before that it was relative to `sched` only.
    
    `alpha` being the maximum learning rate and `sched` being a scaling parameter in [0..1]
    
    * change default AdamW weight decay parameter used in training to 0.1 as used in nanoGPT
    
    * change default AdamW weight decay parameter defined in ggml to 0.0, making Adam default instead of AdamW
    
    btw: the default weight decay parameter for torch.optim.AdamW is 0.01
    
    * bug fixes for cross entropy loss
    
    ggml_cross_entropy_loss: sums where not correctly added in workload of each thread
    ggml_cross_entropy_loss_back: simplify backward process, reducing numerical issues
    
    guard usage of exp f16 lookup in cross entropy by #define GGML_CROSS_ENTROPY_EXP_FP16
    
    cross entropy loss is only used once during training, but it is quite sensitive to numerical errors introduced by exp-f16-lookup.
    so exp-f16-lookup for cross entropy loss is disabled by default, trading better gradients for very slightly worse runtime performance.
    
    * fix test-grad0 for cross_entropy_loss
    
    the second argument to cross_entropy_loss must sum up to 1 for each row
    
    * fix test-grad0 for soft_max
    
    dont use only sum as aggregation, because sum of softmax is always 1 -> finite differences should not work
    instead use sum(log(soft_max()*(1-eps)+eps)); use eps to avoid log(0)
    
    * improve finite differences of test-grad0 by using double instead of float
    
    * change cross_entropy_loss to output average over all rows
    
    this helps keeping the loss and gradients in a sane range
    
    * improve gradient checkpointing
    
    sqrt(n_layers) is only the best checkpoint step when mem size of checkpoints and mem size of layers are equal.
    since layers require more memory than the single-tensor-checkpoint we use, the optimal values are compute different:
    
    ```
      given: n, u, v
      objective: minimize(a*u+b*v) where a*b=n, a>0, b>0
      b=n/a
      minimize(a*u+v*n/a)
      diff(a*u+v*n/a, a) = u - (v*n/a)/a
      diff(a*u+v*n/a, a) == 0
      u - (v*n/a)/a == 0
      u == v*n/(a*a)
      u*a*a = v*n
      a*a = v*n/u
      a = sqrt(n*v/u)
    ```
    
    this change results in more checkpoints, requiring less layers to store between checkpoints, overall improving memory usage.
    
    * disable gradient checkpointing debug output
    
    * llama : fix rope usage in train-text-from-scratch after ChatGLM change
    
    * add more training parameters:
    
    --enable-restart N         Only for Adam optimizer. Enable restarts of cos-decay
    --disable-restart N        Only for Adam optimizer. Disable restarts of cos-decay
    --opt-past N               Number of optimization iterations to track for delta convergence test. Disabled when zero.
    --opt-delta N              Maximum delta for delta convergence test. Disabled when <= zero.
    --opt-max-no-improvement N Maximum number of optimization iterations with no improvement. Disabled when <= zero.
    --adam-epsf N              AdamW epsilon for convergence test. Disabled when <= zero.
    --adam-min-alpha N         Adam minimum learning rate alpha, usually 0.1 * alpha
    
    * replace memcpy with reshape operation so that the graph is not cut at the input
    
    this makes it possible to store other values into the input tensor and then simply recompute the graph without rebuilding it
    
    * remove unused function argument from get_example_targets_batch
    
    * measure and print total training time
    
    * add optimization callback to ggml_opt_resume_g
    
    this callback is called before each iteration with custom data and pointer to learning schedule parameter (only used in Adam(W)).
    
    can be used for dynamic learning schedule and setting input data for batches before each iteration
    
    * use optimization callback in training
    
    allows dynamic learning schedule and different batch data for each iteration without relying on low n_iter and high n_examples parameters
    
    reduces runtime by avoiding restart of optimization function and improves training convergence by providing a different batch for each iteration
    
    * add minimum number of tensor dimensions to apply weight decay (default 2)
    
    this allows to not apply weight decay to bias parameters
    
    * rename training parameter cos-decay-alpha to cos-decay-min and clarify that adam-min-alpha also applies to warmup
    
    * fix increase of model.train_samples and model.train_tokens
    
    now that each optimizer iteration gets its own batch we need to multiply by number of opt iterations
    
    * change sampling parameters for prediction after training to defaults of common.h
    
    and clarify what is context for prediction and what are generated tokens
    
    * tighten abs error bounds for cross_entropy_loss in test-grad0
    
    * add conditional compilation of using F16 exp in flash attention
    
    uncomment `// #define GGML_FLASH_ATTN_EXP_FP16` to enable usage of f16 exp in flash attention
    
    * tighten abs error bounds for flash_attn in test-grad0
    
    * tighten abs error bounds for sqrt in test-grad0
    
    * remove out-commented vectorized code of opt_adam
    
    the vectorized code might be bit faster for low number of parameters, but it had a big memory usage overhead
    
    * ggml : update ggml_rms_norm_back with configurable eps
    
    * llama training : fix ggml_rms_norm_back calls to pass configurable eps
    
    * remove trailing whitespace
    
    * add train function using automatic gradient checkpointing backward pass and allocator
    
    * in train function replace add_inplace by regular add
    
    because using add_inplace seems to result in different gradients
    
    * don't use allocate hash_map on context
    
    because the context has no_alloc=True when using memory allocator resulting in NULL data pointers
    
    * correctly clone reshape and permute operations by also cloning tensor->nb values
    
    * fix variable name and add missing type cast
    
    * terminate recursive tensor cloning when reaching tensor without src tensors
    
    * correctly clone view tensors by setting data pointers
    
    without this the checkpointing would only work when being used together with memory allocator
    
    * fix variable names
    
    * swap arguments to commutative ops to be the same as in `forward_batch_wo_cache_flash_attn`
    
    * add input tensors as checkpoints
    
    so that recursive tensor cloning of gradient checkpointing terminates on input tensors
    
    * fix variable name and add missing boolean negation
    
    * make sure some tensors are not reallocated by inserting new temporary nodes depending on them:
    
    output and parameter gradient tensors need to be available at the end of the graph execution
    
    parameter gradient tensors also need to be available before the graph execution because they are set to zero before each optimizer iteration
    
    checkpoint tensors are allocated all together to reduce memory allocator fragmentation
    
    afterwards, in addition to the temporary nodes, we also need to reset the temporary leafs
    
    * fix ASSERT to work with zero layers
    
    * add training options whether to use allocator and/or unified training function
    
    * integrate unified training function which may use memory allocator
    
    the unified training function also supports arguments whether to use flash attention and/or gradient checkpointing
    
    * format name of cloned tensors with " (clone)" suffix
    
    * set names for tensors in unified train function for easier debugging
    
    * allocate graph on context using ggml_new_graph
    
    * remove handwritten training functions
    
    * remove unused training parameters "use_scratch" and "use_unified"
    
    * remove trailing whitespace
    
    * remove unused train params: mem_compute1_gb & mem_compute2_gb
    
    mem_compute_gb is used for compute when automatic memory allocator is not enabled, otherwise it can be very small to only hold the tensor definitions
    mem_compute0_gb is used for automatic memory allocator (as long as measurement of max required size is not implemented)
    
    * remove unused forward_batch function
    
    * add debug asserts in ggml_allocr_alloc to some common pitfalls when using this function directly
    
    * only use ggml_allocr_alloc when tensor has NULL data and is no view
    
    * fix test when to create temporary backward graph
    
    temporary backward graph is only necessary when using checkpointing
    
    * fix memory "leak" in optimizers
    
    each iteration a new cplan with new memory for work data was allocated.
    now cplan creation only happens at the start of optimization, with each iteration reusing the cplan and its work data.
    
    * reverse order of for loop in ggml_build_backward_expand to save memory when using gradient checkpointing and allocator
    
    with this loop order gradient checkpointing with allocator on 16 layer model saves 13% memory; 2 layer memory it saves 2% memory.
    
    the computation results are the same
    
    * add API functions to access llama model tensors
    
    * add stub example for finetuning, based on train-text-from-scratch
    
    * move and remove code
    
    * add API functions to access remaining model parameters:
    
    mult, head and rot
    
    * first draft for LORA finetune training
    
    * remove const model and layer arguments in API functions for accessing model tensors
    
    * bug fixes to make finetune compile
    
    automatic allocator does not work yet
    
    * add debug prints for training memory improvements
    
    * fix names of lora tensors
    
    * avoid stack overflow resulting from big ggml_cgraph
    
    replace stack allocation and ggml_build_forward by ggml_new_graph in combination with ggml_build_forward_expand
    
    * replace llama API functions to get model tensors by one function to get model tensor by name
    
    LLAMA_API struct ggml_tensor * llama_get_model_tensor(struct llama_model * model, const char * name);
    
    * remove unused call to not existing llama_get_layer_from_model
    
    * implement ggml_compute_forward_out_prod_q_f32
    
    * remove trailing whitespace
    
    * add lora finetune support on quantized base model tensors
    
    * add ggml_add_cast API function
    
    this function works like ggml_add, but accepts a data type for the resulting tensor.
    only supported for quantized src0 input.
    
    * use ggml_add_cast in finetuning
    
    lora-applied weights will now have data type F32, which improves gradients when finetuning quantized base models
    
    * bug fix: actually use result type passed to ggml_add_cast
    
    * make sure base model tensors data cannot be used in viewable operations
    
    memory allocator would try to make lora application inplace on base model tensors.
    since those are memory mapped this will result in memory access violations
    
    * fix bug in ggml_out_prod which resulted in wrong n_dims of result tensors
    
    * avoid keeping in memory ALL of the gradients
    
    The problem here stems from ggml_graph_reset. This function is called in the optimization function, before each graph computation, to reset the gradients to zero. This required a unique memory slot for each gradient: allocating memory from a previosly freed memory location might lead to non-zero input gradients.
    
    During ggml_compute_backward the gradients are build stepwise by adding or substracting new values, starting from a OP_NONE tensor which needs to contain zero-values. This requires the graph reset.
    
    To avoid this I now remember in ggml_build_backward_expand the original OP_NONE gradient tensors in a hash table, which is passed to ggml_compute_backward. There instead of using add (or sub or similar) I test whether the existing gradient to be changed is a zero-valued-tensor by looking up its existence in the hash table. When it is such a zero-tensor it will not be modified, but replaced by the value to be added, otherwise the regular add (not inplace, allocator will take care of this) will be used. This way none of those zero-tensor values will be necessary in the final backward graph and more importantly they won't need a unique memory slot, just to make them zero.
    
    * remove trailing whitespace
    
    * remove debug prints and function to compute tensor data hash
    
    * improve optimization iteration prints
    
    * adjust maximal values to support finetuning 3B models
    
    * change default finetune params lora_r and lora_alpha to match the n_rank parameters of 4
    
    * bug fix: make sure finetune input gradient is allocated at begin and kept until end
    
    * remove unnecessary src tensor from ggml_get_rows_back
    
    we don't need data of src[2] for computation, only to setup the correct output shape.
    remove dependency on src[2], so that allocator can work more freely.
    
    the computational graph is still completely determined, because the output shape is naturally included.
    this is similar to how ggml_reshape does it.
    
    * remove unnecessary src tensor from ggml_repeat & ggml_repeat_back
    
    we don't need data of src[1] for computation, only to setup the correct output shape.
    remove dependency on src[1], so that allocator can work more freely.
    
    the computational graph is still completely determined, because the output shape is naturally included
    
    * resolve todo
    
    allocator will only make it inplace when they are of the same type
    
    * mixing multiple LORA adapters is now possible
    
    pass more than one '--lora FNAME' argument to apply more than one LORA.
    use '--lora-scaled FNAME S' when you want to specify a user-defined scale for an adapter.
    
    * add option to save finetune output every N iterations
    
    * also save latest finetune output with ITERATION="LATEST" and print where files are saved
    
    saving with LATEST makes it easier to resume training from the latest checkpoint
    the string "LATEST" can be configured with command line option "--fn-latest STR"
    
    * update checkpoint train stats before saving via "--save-every"
    
    * add command line option `--rank-wo N` for rank of wo tensor
    
    * update finetune README
    
    * fix dump_non_result_info_yaml to output multiple lora adapters
    
    * bug fix: replace GGML_TYPE_SIZE[t] by ggml_type_size(t)
    
    * replace llama_n_mult by llama_n_ff
    
    * finetune bug fixes to compile with merged in code from master
    
    * remove prediction related code to reduce duplicated code with main
    
    use main instead
    
    * reduce large memory overhead in train-text-from-scratch
    
    all gradients had to be pinned so that graph_reset works correctly.
    this is no longer necessary with the changes to ggml_compute_backward introduced in this PR.
    
    * add comment explaining why finetune checkpoints are allocated in one block
    
    * make default value of float member a float literal
    
    * handle rms_norm and rope parameters the same as in train-text-from-scratch
    
    * remove unused code
    
    * remove vocab related code as it is unnecessary
    
    * add LLM_KV_TRAINING_TYPE to train-text-from-scratch checkpoints
    
    so that they can be differentiated from lora finetune checkpoints
    
    * add gguf constants and load/save functions from train-text-from-scratch
    
    * add load & save lora finetune checkpoints via gguf
    
    * add python script to convert old finetune checkpoint files to gguf
    
    * remove old checkpoint save & load code
    
    * remove code to print data checksums which was used to verify correctness of new gguf code
    
    * omit tokenization when training is disabled, only save llama lora adapter
    
    training can be disabled by passing '-n 0' to finetune
    
    * remove trailing whitespace
    
    * update README.md
    
    * implement ggml_compute_forward_repeat_f16
    
    * avoid stack overflow of large cgraphs in test-grad0
    
    * add ggml API functions ggml_unravel_index, ggml_get_i32_nd and its analogs for set and for f32
    
    ggml_get_i32_1d, ggml_set_i32_1d, ggml_get_f32_1d, ggml_set_f32_1d now support non-contiguous tensors.
    in case of non-contiguous tensor, the 1d index is unraveled into a multi index using ggml_unravel_index to be passed to '_nd' function equivalent.
    
    this fixes a bug in test-grad0 which happens due to ggml_build_backward not building purely contiguous tensors anymore
    
    * increase test-grad0 context mem size to accommodate for bigger cgraph
    
    * add sanity check to ggml_compute_backward, asserting the correct shape of gradients
    
    * fix ggml_acc_or_set to return tensor of correct shape
    
    * remove unused 'inplace' argument from ggml_compute_backward function
    
    inplace operations to add gradients are no longer created by ggml_compute_backward
    use allocator to automatically make inplace operations
    
    * add missing argument 'int i0' to ggml_get_i32_nd & ggml_set_i32_nd header declarations
    
    * fix error message in ggml_allocr_alloc to display actual max_avail
    
    * fix check_gradient
    
    ggml_build_backward_expand was previously replaced by ggml_build_backward, but the assignment of forward graph to backward graph missing
    
    * use tensor->view_src instead of ggml_is_view and get_view_source
    
    * move gradient checkpointing code into ggml, new API function:
    
    // build gradient checkpointing backward graph gb for gf using provided checkpoints
    // gb_tmp will contain original backward graph with rewritten backward process nodes,
    // but without the second forward pass nodes.
    GGML_API void ggml_build_backward_gradient_checkpointing(
            struct ggml_context   * ctx,
            struct ggml_cgraph    * gf,
            struct ggml_cgraph    * gb,
            struct ggml_cgraph    * gb_tmp,
            struct ggml_tensor  * * checkpoints,
            int                     n_checkpoints);
    
    * replace custom data getters and setters by ggml functions
    
    * train-text-from-scratch can train (full finetune) gguf models
    
    just pass the gguf model via `--checkpoint-in FN`.
    after this, to continue training, pass the generated checkpoint instead of the original gguf model.
    
    tested with smaller models, bigger models may exceed available memory.
    use (LORA) finetune for those.
    
    * remove trailing whitespace
    
    * add option to save train-text-from-scratch output every N iterations
    
    * update README.md
    
    * fix warnings
    
    * fix warnings
    
    * remove finetune option to disable allocator
    
    the allocator should always be used.
    by making sure that it is always used it gets easier to implement automatic memory requirements computation
    
    * add tensor checkpoints only when gradient checkpointing is enabled
    
    * initialize opt ggml context if none was provided
    
    * add ggml-alloc API function 'ggml_allocr_max_size' to get max size of alloc
    
    GGML_API size_t ggml_allocr_max_size(struct ggml_allocr * alloc);
    
    * finetune: automatically allocate all memory and changes to command line options
    
    remove '--n_examples N' parameter, as it no longer makes sense to call optimization process multiple times in a loop.
    add '--only_write_lora' command line option: will skip tokenization and training, to only write a llama.cpp comptabile LORA adapter.
    remove memory buffer related command line options.
    improve iteration console output.
    
    * add finetune to Makefile
    
    * update README.md
    
    * print time per iteration and estimate remaining time
    
    * increase measured alloc size by tensor_alignment
    
    ggml_allocr_reset will reduce the given size by up to tensor_alignment-1
    
    * fix README.md
    
    * add some more allocator debug prints
    
    * bug fix, probably solves the 'ggml_allocr_alloc: not enough space in the buffer' issue
    
    * revert last commit
    
    "bug fix, probably solves the 'ggml_allocr_alloc: not enough space in the buffer' issue"
    
    "alloc was freeing an externally allocated tensor, because it calculated the end of allocator memory as alloc->data + alloc->max_size instead of alloc->data + alloc->size."
    
    This is intentional to reduce the risk of freeing external tensors when measuring. Unless max_size is not properly calculated, I don't see why this is an issue.
    
    * remove unnecessary "0x" before "%p" output
    
    * move measurement memory segment to upper region of the address space
    
    * update README.md
    
    * fix printf format warnings
    
    * add missing gguf_free in load_checkpoint_lora_file
    
    * load default rms_norm and rope parameters from base model
    
    * add gradient accumulation
    
    specify number accumulation steps with '--grad-acc N'.
    this will simulate a bigger batch size of grad_acc*batch.
    
    * fix tracking of train_samples and train_tokens
    
    * build : fix compile warnings
    
    * ggml : fix L-BFGS linesearch loop
    
    * improve finetune time measurement
    
    fix printf warnings on system where int64_t is (long int).
    change time datatypes to double because values get big with long training times.
    exclude file saving from time measurement.
    converge faster to actual time per iteration by removing very small first duration before first iteration was performed.
    fix bug in output of total training time, the reported value was 1000 times to small.
    
    * specify default lora rank with '--lora-r N'
    
    '--lora-r N' will specify default rank for all tensors
    '--rank-wq N', etc. will override this default rank for specific tensor types.
    
    * fix gradient accumulation bug where the same batch was used for each microstep
    
    * fix gradient accumulation bug where the same batch was used for each microstep
    
    * support grouped-query-attention in ggml_flash_attn and ggml_flash_attn_back
    
    k and v can now be repeated in q along ne[2]
    
    in forward pass just use modulo to compute k and v indices, like ik2 = iq2 % nek2.
    
    in backard pass this won't work as easy, because multiple threads will compete to accumulate to the same k->grad[:,ik1,ik2,ik3] and v->grad[:,iv1,iv2,iv3].
    so we change the parallelization over q rows to be over k rows. this ensures non-overlapping (ik2,ik3) across threads.
    in each thread we then iterate over the number of repetitions of k/v in q to compute iq2 as iq2 = ik2 + irep*nek2.
    
    since ne2 is not the same for q,k and v we also change how the gradients are concatenated into the result tensor.
    additionally the offsets of gradq, gradk and gradv in the result tensor are now memory aligned.
    
    we also simplify the compute_backward part of flash_attn to use ggml_reshape instead of switching over the number of dimensions.
    this needs a small change to ggml_reshape, removing the assertion of second argument to be contiguous.
    since only the shape (ne) of the second reshape argument is of relevance, its memory layout (nb) is irrelevant -> it can very well be non-contiguous.
    
    change test-grad0 to also test for repeated k/v in q.
    
    this changes the rng and now results in small gradient differences in softmax. these solely come from using f16 exp table lookup in forward softmax: when temporarily changing softmax to use actual exp function, the reported gradient differences go away. gradient differences coming solely from f16 table lookup are acceptable.
    added a note to explain this.
    
    * add llama API functions to get grouped-query-attention n_head parameter 'n_head_kv'.
    
    * fix finetune to support grouped-query-attention (using flash-attention)
    
    note: ggml changes to ggml_out_prod are necessary to support grouped-query-attention without flash-attention.
    
    * support broadcastable a in out_prod(a, b) and backward pass of broadcasting mul_mat(a, b)
    
    * test broadcasting mul_mat backward pass
    
    * decouple random number generator of each operation test
    
    when changing one test the rng of others tests is not influenced anymore
    
    * add comment briefly describing what ggml_repeat_back does
    
    * simplify broadcasting mul_mat backward using ggml_repeat_back
    
    * add cgraph evaluation order member and corresponding enum type
    
    this controls in which order ggml_build_forward visits source nodes.
    by default the nodes are visited left to right, i.e. src[0] first.
    in some cases it is beneficial for ggml-alloc to visit in a different order.
    two possible orders are supported: left-to-right (src[0] first) and right-to-left (src[0] last).
    
    * measure max compute size for each cgraph eval order and use best order
    
    this can bring huge memory savings:
    e.g. codellama-34b with n_ctx=64, n_batch=1 goes from 92927.8mb down to 4627.6 MB
    
    * remove unused command line options
    
    * add sample start patterns and options to force new or by default resume last shuffling
    
    * update shuffle rng state on reshuffle
    
    * exclude known zero values from computations in flash_attn_f32 & flash_attn_back_f32
    
    * remove probably unnecessary exception type flags from stringstream
    
    * pass correct max number of tokens to llama_tokenize
    
    * account for possible leading whitespace that will be added by tokenizer
    e.g. '\t' will be tokenized by llama spm tokenizer to [29871, 12]
    
    * use unrolled vec_mad in out_prod
    
    y is vec_mad result vec.
    x is vec_mad input vec.
    v is vec_mad input scalar.
    
    ggml_vec_mad_f32_unroll will internally loop over x and v with same y.
    
    GGML_VEC_MAD_UNROLL is by default defined to 32.
    
    This value is empirical optimized using performance test runs of out-prod in openllama-3b finetune with 256 context length and batch size 1. It gives 23% performance boost for out_prod.
    
    Full measurements of out-prod runtime in ms:
            unroll_xv       unroll_yv
    1       67014.643       87826.469
    2       77117.552       89077.656
    4       72091.311       109121.657
    8       61077.543       88678.334
    16      56914.67        79514.947
    24      59024.595       84350.254
    28      55952.446       83368.73
    32      51476.658       85177.745
    36      55973.792       84659.92
    40      55139.616       93844.738
    48      60736.392       93330.267
    64      99856.878       116994.99
    
    Second column is when unrollying yv instead of xv
    
    * set lora_alpha to value of lora_r if it is not set via command line
    
    otherwise only changing lora_r will change scaling of lora adapter used in prediction
    
    * reshuffle original sample order instead of the previous shuffled order
    
    otherwise resumed reshuffle will not result in same sample order
    
    * block tiling for out-prod inspired by mul-mat
    
    block sizes are empirically optimized
    
    roughly doubles the flops of out-prod
    
    * exclude some more known zero values from computations in flash_attn_f32 & flash_attn_back_f32
    
    * add static keywords
    
    * remove outcommented old code
    
    * update train-text-from-scratch with tokenization, sample selection and shuffling from finetune
    
    * remove lbfgs related train parameters
    
    * move common train functions into common/train.[h|cpp]
    
    * move train state into struct train_state
    
    * move train data saving code into callback to unify code of opt_callback
    
    train_params are still different in finetune and train-text-from-scratch, so it can't yet be moved to train.h|cpp
    
    * move common train params into common/train
    
    * move common opt_callback into common/train
    
    * fix consume_common_train_arg
    
    * save and load head_count_kv in lora checkpoints
    
    * increase train_samples by used_samples instead of number of batches
    
    on batch can contain more than one sample when option "fill_with_next_samples" is used
    
    * fix usage of llama_tokenize
    
    * remove static from process_escape since we need it exposed in header
    
    * fix code formating of long function declarations
    
    * fix condition in load_train_state_gguf
    
    * use die("msg") instead of replace GGML_ASSERT(!"msg") or throw std::runtime_error("msg")
    
    * fix saving and loading of training type
    
    * remove terminating '\0' from tokenization
    
    (llama_tokenize is now passed the string length instead of relying on terminating '\0')
    
    * fix compile warnings
    
    * fix compile warnings
    
    * use new/delete for train_state instead of malloc/free
    
    using malloc may result in seg faults when trying to assign string fields
    
    * assert that sample_count > 0, avoiding division by zero
    
    * fix frand to return value in interval [0,1)
    
    * add train option "--sample-random-offsets"
    
    Use samples beginning at random offsets.
    The offset is only applied to the first sample in each batch context window.
    Together with "--fill-with-next-samples" this may help for training endless text generation.
    
    For example given a dataset containing samples "abcd", "ABCD", "0123".
    With context size of 8 and options "--fill-with-next-samples", "--no-separate-with-eos", "--no-separate-with-bos",
    the context windows of batches could only be filled with "abcdABCD", "ABCDabcd", "0123abcd", etc.
    
    With "--sample-random-offsets" it can also be filled with "23abcdAB", "bcd0123A", etc.
    
    * deduplicate code into function
    
    * remove n_rot hparam, as it must always be hparam.n_embd_head()
    
    * align code
    
    * assert correct base model tensor shapes
    
    * move some params from lora hparams into model hparams and load model params from gguf
    
    this equalizes the model definition in finetune and text-from-scratch and removes the need for additional llama api functions to get model parameters
    
    * remove now unnecessary llama API functions to get model params that where added by this PR
    
    * train-text-from-scratch: automatically allocate model tensors, remove option '--mem-model N'
    
    * train-text-from-scratch: automatically allocate opt context
    
    * train-text-from-scratch: automatically allocate input tensors
    
    * train-text-from-scratch: automatically allocate compute memory
    
    * remove unused options and equalize train-text-from-scratch with finetune
    
    * initialize opt->loss_after with zero
    
    * add export-lora program
    
    * remove trailing whitespace
    
    * add export-lora build in Makefile
    
    * remove unused struct tensor_info from export-lora
    
    * add export-lora build dependency to llama
    
    because it depends on common, which depends on llama
    
    * update finetune README.md
    
    * cancel optimization when specified number of epochs is completed
    
    * improve handling of export-lora arguments
    
    print errors and warnings when files could not be read or created
    
    * Fix export-lora.cpp "not enough space in the context's memory pool" (#1)
    
    * Fix export-lora.cpp "not enough space in the context's memory pool"
    
    Without this patch, export-lora would sometimes error with "not enough space in the context's memory pool (needed 656784, available 656800)".
    
    * increase required context size by 5*GGML_MEM_ALIGN instead of plain 16
    
    ---------
    
    Co-authored-by: xaedes <xaedes@gmail.com>
    
    * improve handling of not yet supported tensor types
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: meatbag-18a <145869052+meatbag-18a@users.noreply.github.com>

.gitignore
Makefile
common/CMakeLists.txt
common/common.cpp
common/common.h
common/train.cpp
common/train.h
examples/CMakeLists.txt
examples/baby-llama/baby-llama.cpp
examples/export-lora/CMakeLists.txt
examples/export-lora/README.md
examples/export-lora/export-lora.cpp
examples/finetune/CMakeLists.txt
examples/finetune/README.md
examples/finetune/convert-finetune-checkpoint-to-gguf.py
examples/finetune/finetune.cpp
examples/server/server.cpp
examples/train-text-from-scratch/README.md
examples/train-text-from-scratch/convert-train-checkpoint-to-gguf.py
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-alloc.c
ggml-alloc.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-grad0.cpp

commit 2db94d98eda56982d80238840b0652b4137a2a84
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 28 14:30:31 2023 -0400

    gguf : basic type checking in gguf_get_* (#3346)

ggml.c
ggml.h

commit ecf90b1a5114034bc0939b3968f549fe4d63cf6d
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 28 14:30:15 2023 -0400

    gguf : make token scores and types optional (#3347)

convert-falcon-hf-to-gguf.py
convert-starcoder-hf-to-gguf.py
llama.cpp

commit 2619109ad57d7a75388a9cce51e5da645410d92e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Sep 28 19:36:36 2023 +0300

    ci : disable freeBSD builds due to lack of VMs (#3381)

.github/workflows/build.yml

commit ec893798b7a2a803466cc8f063051499ec3d96f7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Sep 28 19:04:36 2023 +0300

    llama : custom attention mask + parallel decoding + no context swaps (#3228)
    
    * tests : verify that RoPE is "additive"
    
    * llama : replace ggml_diag_mask_inf with ggml_add (custom -inf mask)
    
    * ggml : ggml_rope now takes a vector with positions instead of n_past
    
    * metal : add rope_f16 kernel + optimize cpy kernels
    
    * llama : unified KV cache + batch inference API
    
    * llama : add new llama_decode() API that works with llama_batch
    
    * llama : add cell_max heuristic for more efficient kv_cache
    
    * llama : extend llama_kv_cache API
    
    * llama : more robust cell_max heuristic + wip shift
    
    * metal : disable concurrency optimization
    
    * llama : add llama_kv_cache_shift_seq + no more context swaps
    
    * llama : apply K-cache roping for Falcon and Baichuan
    
    * speculative : fix KV cache management
    
    * parallel : example for serving multiple users in parallel
    
    * parallel : disable hot-plug to avoid cache fragmentation
    
    * fixes : speculative KV cache + llama worst-case graph
    
    * llama : extend batch API to select which logits to output
    
    * llama : fix worst case graph build
    
    * ggml-cuda : update rope implementation for parallel decoding (#3254)
    
    * ggml-cuda : update rope implementation for parallel decoding
    
    * better solution for p0 computation
    
    * fix rope
    
    * simpler rope implementation
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * make : add parallel to build + fix static functions in llama.cpp
    
    * simple : fix token counting
    
    * parallel : various improvements
    
    * llama : fix cell_max logic + rename functions
    
    * parallel : try smaller batches when the KV cache is fragmented
    
    * parallel : fix sequence termination criteria
    
    * llama : silence errors KV cache errors
    
    * parallel : remove new line from prompt
    
    * parallel : process system prompt once + configurable paramters + llama API
    
    * parallel : remove question with short answers
    
    * parallel : count cache misses
    
    * parallel : print misses on each request
    
    * parallel : minor
    
    * llama : fix n_kv to never become 0
    
    * parallel : rename hot-plug to continuous-batching
    
    * llama : improve llama_batch API + simplify parallel example
    
    * simple : add parallel decoding support
    
    * simple : improve comments + free batch
    
    * ggml-cuda : add rope f16, restore performance with parallel decoding (#3272)
    
    * ggml-cuda : add rope f16, restore performance
    
    * offload KQ_mask with all models
    
    * fix rope shift
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * llama : disable MPI for now
    
    ggml-ci
    
    * train : make KQ_pos memory buffer permanent via dummy scale op
    
    * ggml : revert change to ggml_cpy, add ggml_cont_Nd instead (#3275)
    
    ggml-ci
    
    * parallel : fix bug (extra BOS) + smaller token_prev array
    
    * parallel : fix cases where the input prompts can overflow the batch
    
    * parallel : add disabled experimental batch chunking in powers of two
    
    * llama : llama.h formatting + comments
    
    * simple : add README.md
    
    * llama : fix kv cache heuristic when context is less than 32
    
    * parallel : fix crash when `-n -1`
    
    * llama : simplify returns if/else branches
    
    * metal : use mm kernels for batch size > 2
    
    * examples : utilize new llama_get_logits_ith()
    
    * examples : add example for batched decoding
    
    * examples : do not eval prompt 2 times (close #3348)
    
    * server : clear the KV cache beyond n_past before llama_decode
    
    * server : avoid context swaps by shifting the KV cache
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

.gitignore
Makefile
common/common.cpp
common/common.h
examples/CMakeLists.txt
examples/baby-llama/baby-llama.cpp
examples/batched/CMakeLists.txt
examples/batched/README.md
examples/batched/batched.cpp
examples/beam-search/beam-search.cpp
examples/embd-input/embd-input-lib.cpp
examples/embedding/embedding.cpp
examples/llama-bench/llama-bench.cpp
examples/main/main.cpp
examples/parallel/CMakeLists.txt
examples/parallel/README.md
examples/parallel/parallel.cpp
examples/perplexity/perplexity.cpp
examples/save-load-state/save-load-state.cpp
examples/server/server.cpp
examples/simple/README.md
examples/simple/simple.cpp
examples/speculative/speculative.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-cuda.cu
ggml-cuda.h
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
llama.cpp
llama.h
tests/CMakeLists.txt
tests/test-grad0.cpp
tests/test-rope.cpp

commit 45855b3f1c7bdd0320aa632334d0b3e8965c26c4
Author: Kevin Ji <1146876+kevinji@users.noreply.github.com>
Date:   Thu Sep 28 09:11:32 2023 -0400

    docs : mark code as Bash (#3375)

docs/BLIS.md

commit 4aea3b846ec151cc6d08f93a8889eae13b286b06
Author: Pierre Alexandre SCHEMBRI <pa.schembri@gmail.com>
Date:   Thu Sep 28 14:13:37 2023 +0200

    readme : add Mistral AI release 0.1 (#3362)

README.md

commit da0400344be12074e67dcabc565140289cf7efaa
Author: slaren <slarengh@gmail.com>
Date:   Thu Sep 28 12:08:28 2023 +0200

    ggml-cuda : perform cublas fp16 matrix multiplication as fp16 (#3370)
    
    * ggml-cuda : perform cublas fp16 matrix multiplication as fp16
    
    * try to fix rocm build
    
    * restrict fp16 mat mul to volta and up

ggml-cuda.cu

commit e519621010cac02c6fec0f8f3b16cda0591042c0
Author: Zhang Peiyuan <a1286225768@gmail.com>
Date:   Thu Sep 28 02:45:20 2023 +0800

    convert : remove bug in convert.py permute function (#3364)

convert.py

commit ac43576124a75c2de6e333ac31a3444ff9eb9458
Author: Richard Roberson <richardr1126@gmail.com>
Date:   Wed Sep 27 10:25:12 2023 -0600

    make-ggml.py : compatibility with more models and GGUF (#3290)
    
    * Resync my fork with new llama.cpp commits
    
    * examples : rename to use dash instead of underscore
    
    * New model conversions
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/make-ggml.py

commit 20c7e1e804690f3db58bd33eb56f8c6aa4735c63
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Sep 27 12:18:07 2023 -0400

    gguf : fix a few general keys (#3341)

examples/gptneox-wip/falcon-main.cpp
examples/gptneox-wip/gptneox-main.cpp
gguf-py/gguf/gguf.py
llama.cpp

commit dc6897404e141c74cbbf8030ecfebd74e1815411
Author: Rickard Hallerbäck <rickard.hallerback@gmail.com>
Date:   Wed Sep 27 17:48:33 2023 +0200

    metal : reusing llama.cpp logging (#3152)
    
    * metal : reusing llama.cpp logging
    
    * cmake : build fix
    
    * metal : logging callback
    
    * metal : logging va_args memory fix
    
    * metal : minor cleanup
    
    * metal : setting function like logging macro to capital letters
    
    * llama.cpp : trailing whitespace fix
    
    * ggml : log level enum used by llama
    
    * Makefile : cleanup ggml-metal recipe
    
    * ggml : ggml_log_callback typedef
    
    * ggml : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/llama-bench/llama-bench.cpp
ggml-metal.h
ggml-metal.m
ggml.h
llama.cpp
llama.h

commit 527e57cfd8a9a26bf622c0510c21c2508a24be26
Author: Jag Chadha <jagtesh@gmail.com>
Date:   Wed Sep 27 11:34:32 2023 -0400

    build : add ACCELERATE_NEW_LAPACK to fix warning on macOS Sonoma (#3342)

CMakeLists.txt
Makefile
Package.swift

commit ffe88a36a913e5792aa383f0726bdbcf632e7191
Author: BarfingLemurs <128182951+BarfingLemurs@users.noreply.github.com>
Date:   Wed Sep 27 11:30:36 2023 -0400

    readme : add some recent perplexity and bpw measurements to READMES, link for k-quants (#3340)
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md with k-quants bpw measurements

README.md
examples/perplexity/README.md
examples/quantize/README.md

commit 99115f3fa654b593099c6719ad30e3f54ce231e1
Author: DAN™ <dranger003@gmail.com>
Date:   Mon Sep 25 18:45:33 2023 -0400

    cmake : fix build-info.h on MSVC (#3309)

CMakeLists.txt
scripts/build-info.cmake

commit 1726f9626f21f102d8e01df06c23a7f94add7990
Author: 2f38b454 <dxf@protonmail.com>
Date:   Tue Sep 26 02:24:52 2023 +0800

    docs: Fix typo CLBlast_DIR var. (#3330)

README.md

commit a98b1633d5a94d0aa84c7c16e1f8df5ac21fc850
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Mon Sep 25 13:48:30 2023 +0200

    nix : add cuda, use a symlinked toolkit for cmake (#3202)

flake.nix

commit c091cdfb24621710c617ea85c92fcd347d0bf340
Author: slaren <slarengh@gmail.com>
Date:   Sat Sep 23 21:48:24 2023 +0200

    llama-bench : add README (#3317)
    
    * llama-bench : add README
    
    * minor edit

examples/llama-bench/README.md

commit 51a7cf5c6e490b2f51c82daa76c4ca4f8d845826
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Sat Sep 23 05:28:50 2023 -0400

    examples : fix RoPE defaults to match PR #3240 (#3315)

common/common.h

commit bedb92b603886768ad51e629f81eda15ff6b86f5
Author: Kevin Ji <1146876+kevinji@users.noreply.github.com>
Date:   Fri Sep 22 23:52:23 2023 -0400

    scripts : use `/usr/bin/env` in shebang (#3313)

scripts/verify-checksum-models.py

commit bc9d3e3971e5607a10ff4c24e39568ce1ac87271
Author: Lee Drake <b.lee.drake@gmail.com>
Date:   Thu Sep 21 13:00:24 2023 -0600

    Update README.md (#3289)
    
    * Update README.md
    
    * Update README.md
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

README.md

commit 36b904e20003017f50108ae68359ef87a192dae2
Author: shibe2 <shibe@tuta.io>
Date:   Thu Sep 21 22:10:26 2023 +0400

    ggml-opencl.cpp: Make private functions static (#3300)

ggml-opencl.cpp

commit 324f3403d54ae4499a1d68623161015f7419fb76
Author: Edward Taylor <edeetee@gmail.com>
Date:   Thu Sep 21 21:08:20 2023 +1200

    zig : fix for updated c lib (#3259)

build.zig

commit f56c418ab0a635c020bcb5bf44b8f00cb3c9e514
Author: yuiseki <yuiseki@gmail.com>
Date:   Thu Sep 21 17:57:40 2023 +0900

    embedding : update README.md (#3224)

examples/embedding/README.md

commit 8185710a80531e9ee0c0cb99d3a9c9af1019ab67
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Sep 21 10:43:53 2023 +0200

    CUDA: use only 1 thread if fully offloaded (#2915)

llama.cpp

commit 7eb41179edc56083ef4eb2df7967ac9ff38b34fb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 20 20:48:22 2023 +0300

    readme : update hot topics

README.md

commit a5661d7e71d15b8dfc81bc0510ba912ebe85dfa3
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Sep 20 12:12:47 2023 -0400

    llama : allow gguf RoPE keys to be overridden with defaults (#3240)

common/common.cpp
examples/server/server.cpp
llama.cpp

commit 65c2c1c5ab7c5089dbc6d10bc49b9c58f0164317
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Sep 20 12:06:08 2023 -0400

    benchmark-matmult : do not use integer abs() on a float (#3277)

examples/benchmark/benchmark-matmult.cpp

commit 80834daecf4b9021770361a6d5e1b9c7a60e6854
Author: kang <tpdns9032100@gmail.com>
Date:   Wed Sep 20 22:48:22 2023 +0900

    flake : Restore default package's buildInputs (#3262)

flake.nix

commit a40f2b656fab364ce0aff98dbefe9bd9c3721cc9
Author: Alon <alonfaraj@gmail.com>
Date:   Wed Sep 20 15:06:36 2023 +0300

    CI: FreeBSD fix (#3258)
    
    * - freebsd ci: use qemu

.github/workflows/build.yml

commit d119c04c159d015a93567df7e73e0e45a22d0f1d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 20 10:02:39 2023 +0300

    examples : fix benchmark-matmult (#1554)
    
    The precision for Q4_0 has degraded since #1508

examples/benchmark/benchmark-matmult.cpp

commit 8781013ef654270cbead3e0011e33a6d690fb168
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Sep 18 10:03:53 2023 -0400

    make : restore build-info.h dependency for several targets (#3205)

Makefile
common/common.h
examples/benchmark/benchmark-matmult.cpp
examples/embd-input/embd-input-lib.cpp
examples/embedding/embedding.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
examples/quantize/quantize.cpp
examples/save-load-state/save-load-state.cpp

commit 7ddf185537b712ea0ccbc5f222ee92bed654914e
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Mon Sep 18 02:21:47 2023 +0200

    ci : switch cudatoolkit install on windows to networked (#3236)

.github/workflows/build.yml

commit ee66942d7ef7c259528158f9a3bd1c314984d32f
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Sep 17 23:35:20 2023 +0200

    CUDA: fix peer access logic (#3231)

ggml-cuda.cu

commit 111163e2463171891680feed94371eb9becd9817
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Sep 17 16:37:53 2023 +0200

    CUDA: enable peer access between devices (#2470)

CMakeLists.txt
Makefile
README.md
ggml-cuda.cu

commit 8b428c9bc84be6887d904600d1298b28baffd552
Author: slaren <slarengh@gmail.com>
Date:   Sun Sep 17 14:33:28 2023 +0200

    llama.cpp : show model size and BPW on load (#3223)

llama.cpp

commit 578d8c8f5cb72f354bc115ba230ee5b2d803eee7
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Sep 17 14:16:22 2023 +0200

    CUDA: fix scratch malloced on non-main device (#3220)

ggml-cuda.cu

commit b541b4f0b1d4d9871c831e47cd5ff661039d6101
Author: IsaacDynamo <61521674+IsaacDynamo@users.noreply.github.com>
Date:   Sat Sep 16 19:35:25 2023 +0200

    Enable BUILD_SHARED_LIBS=ON on all Windows builds (#3215)

.github/workflows/build.yml

commit 5dbc2b3213126a31d3be4ade8ca042cb93019682
Author: Vlad <spitfireage@gmail.com>
Date:   Sat Sep 16 17:55:43 2023 +0300

    Enable build with CUDA 11.0 (make) (#3132)
    
    * CUDA 11.0 fixes
    
    * Cleaner CUDA/host flags separation
    
    Also renamed GGML_ASSUME into GGML_CUDA_ASSUME

Makefile
ggml-cuda.cu

commit b08e75baea294e366628b898e85c0bd359b58115
Author: goerch <jhr.walter@t-online.de>
Date:   Sat Sep 16 13:41:33 2023 +0200

    Fixing the last deviations from sentencepiece indicated by test-tokenizer-1 (#3170)
    
    * Fix für #2721
    
    * Reenable tokenizer test for LLaMa
    
    * Add `console.cpp` dependency
    
    * Fix dependency to `common`
    
    * Fixing wrong fix.
    
    * Make console usage platform specific
    
    Work on compiler warnings.
    
    * Adapting makefile
    
    * Remove trailing whitespace
    
    * Adapting the other parts of the makefile
    
    * Fix typo.
    
    * Fixing the last deviations from sentencepiece indicated by test-tokenizer-1
    
    * Simplify logic
    
    * Add missing change...
    
    * Fix ugly compiler warning
    
    * llama_tokenize should accept strings containing NUL now
    
    * Adding huichen's test case

common/common.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
llama.cpp
llama.h
tests/test-tokenizer-0-llama.cpp
tests/test-tokenizer-1-llama.cpp

commit e6616cf0db2b63189fc34d0076f654af9adecdf8
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 16:59:49 2023 -0400

    examples : add compiler version and target to build info (#2998)

Makefile
common/common.h
examples/beam-search/CMakeLists.txt
examples/beam-search/beam-search.cpp
examples/benchmark/CMakeLists.txt
examples/benchmark/benchmark-matmult.cpp
examples/embd-input/embd-input-lib.cpp
examples/embd-input/embd-input.h
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/CMakeLists.txt
examples/quantize-stats/quantize-stats.cpp
examples/quantize/CMakeLists.txt
examples/quantize/quantize.cpp
examples/save-load-state/save-load-state.cpp
examples/simple/CMakeLists.txt
examples/simple/simple.cpp
scripts/build-info.cmake
scripts/build-info.h.in
scripts/build-info.sh

commit 3aefaab9e59335ebb07d5205dbc8633efd680e58
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 15:38:27 2023 -0400

    check C++ code with -Wmissing-declarations (#3184)

CMakeLists.txt
Makefile
common/common.cpp
common/console.cpp
common/grammar-parser.cpp
examples/baby-llama/baby-llama.cpp
examples/beam-search/beam-search.cpp
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
examples/gguf/gguf.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
examples/quantize/quantize.cpp
examples/server/server.cpp
llama.cpp
llama.h
pocs/vdot/vdot.cpp
tests/test-opt.cpp
tests/test-quantize-fns.cpp
tests/test-quantize-perf.cpp
tests/test-sampling.cpp
tests/test-tokenizer-1-llama.cpp

commit 69eb67e28275cd2d57693405f768754a7b2245ad
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 15:18:15 2023 -0400

    fix build numbers by setting fetch-depth=0 (#3197)

.github/workflows/build.yml

commit 4fe09dfe665c58a753dc9eb638dd4dca1cd35488
Author: Meng Zhang <meng@tabbyml.com>
Date:   Sat Sep 16 03:02:13 2023 +0800

    llama : add support for StarCoder model architectures (#3187)
    
    * add placeholder of starcoder in gguf / llama.cpp
    
    * support convert starcoder weights to gguf
    
    * convert MQA to MHA
    
    * fix ffn_down name
    
    * add LLM_ARCH_STARCODER to llama.cpp
    
    * set head_count_kv = 1
    
    * load starcoder weight
    
    * add max_position_embeddings
    
    * set n_positions to max_positioin_embeddings
    
    * properly load all starcoder params
    
    * fix head count kv
    
    * fix comments
    
    * fix vram calculation for starcoder
    
    * store mqa directly
    
    * add input embeddings handling
    
    * add TBD
    
    * working in cpu, metal buggy
    
    * cleanup useless code
    
    * metal : fix out-of-bounds access in soft_max kernels
    
    * llama : make starcoder graph build more consistent with others
    
    * refactor: cleanup comments a bit
    
    * add other starcoder models: 3B, 7B, 15B
    
    * support-mqa-directly
    
    * fix: remove max_position_embeddings, use n_train_ctx
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Update llama.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Apply suggestions from code review
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * fix: switch to space from tab
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-starcoder-hf-to-gguf.py
gguf-py/gguf/gguf.py
llama.cpp

commit 80291a1d02a07f7f66666fb576c5b1e75aa48b46
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 14:02:01 2023 -0400

    common : do not use GNU zero-length __VA_ARGS__ extension (#3195)

common/common.h

commit c6f1491da032238241e01021c8c58d7b540a043f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 15 20:17:24 2023 +0300

    metal : fix bug in soft_max kernels (out-of-bounds access) (#3194)

ggml-metal.metal

commit e3d87a6c36eadd84d58763143c6d56a0c771ca40
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 15 12:29:02 2023 -0400

    convert : make ftype optional in simple scripts (#3185)

convert-baichuan-hf-to-gguf.py
convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py

commit 8c00b7a6ff38e27fa1e471452b8a480913772c2a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 15 19:06:03 2023 +0300

    sync : ggml (Metal F32 support + reduce ggml-alloc size) (#3192)
    
    * sync : ggml (Metal F32 support + reduce ggml-alloc size)
    
    ggml-ci
    
    * llama-bench : fix ggml_cpu_has_metal() duplicate function
    
    ggml-ci

examples/llama-bench/llama-bench.cpp
ggml-alloc.c
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h

commit 7e50d34be68aae2cc766203703dd188e910e033a
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Fri Sep 15 06:24:30 2023 -0600

    cmake : fix building shared libs for clang (rocm) on windows (#3176)

CMakeLists.txt

commit 235f7c193b02dacfb56319e41a28684b3a2c6db0
Author: Evgeny Kurnevsky <kurnevsky@gmail.com>
Date:   Fri Sep 15 10:10:22 2023 +0200

    flake : use pkg-config instead of pkgconfig (#3188)
    
    pkgconfig is an alias, it got removed from nixpkgs:
    https://github.com/NixOS/nixpkgs/blob/295a5e1e2bacd6e246db8b2bb35d2a9415883224/pkgs/top-level/aliases.nix#L1408

flake.nix

commit a51b68765799e75e17c7622f376bfbeb66f1bd70
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 15 11:09:24 2023 +0300

    metal : relax conditions on fast matrix multiplication kernel (#3168)
    
    * metal : relax conditions on fast matrix multiplication kernel
    
    * metal : revert the concurrnecy change because it was wrong
    
    * llama : remove experimental stuff

ggml-metal.m
ggml-metal.metal
ggml.c
llama.cpp

commit 76164fe2e65c058e9ee2c3afd0ad6b182ca57e25
Author: Andrei <abetlen@gmail.com>
Date:   Fri Sep 15 04:07:40 2023 -0400

    cmake : fix llama.h location when built outside of root directory (#3179)

CMakeLists.txt

commit c2ab6fe661af9834ca6dd75f14e2439938cc22ac
Author: Ali Tariq <ali.tariq@10xengineers.ai>
Date:   Fri Sep 15 13:06:56 2023 +0500

    ci : Cloud-V for RISC-V builds (#3160)
    
    * Added Cloud-V File
    
    * Replaced Makefile with original one
    
    ---------
    
    Co-authored-by: moiz.hussain <moiz.hussain@10xengineers.ai>

.devops/cloud-v-pipeline

commit 2d770505a89a99ce78a5950cf14fc06d3176ffa4
Author: Roland <14355895+rbur0425@users.noreply.github.com>
Date:   Fri Sep 15 03:28:45 2023 -0400

    llama : remove mtest (#3177)
    
    * Remove mtest
    
    * remove from common/common.h and examples/main/main.cpp

common/common.cpp
common/common.h
examples/main/README.md
examples/main/main.cpp
run_with_preset.py

commit 98311c427739e3b06527c3ce6b5c021ab6692740
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 14 21:09:53 2023 -0400

    llama : make quantize example up to 2.7x faster (#3115)

llama.cpp

commit feea179e9f9921e96e8fb1b8855d4a8f83682455
Author: jneem <joeneeman@gmail.com>
Date:   Thu Sep 14 13:54:47 2023 -0500

    flake : allow $out/include to already exist (#3175)

flake.nix

commit 769266a543f68377a1d904ec2a8c27b38a4025ab
Author: Andrei <abetlen@gmail.com>
Date:   Thu Sep 14 13:38:16 2023 -0400

    cmake : compile ggml-rocm with -fpic when building shared library (#3158)

CMakeLists.txt

commit cf8238e7f43cb82a36426af392037e85cd2a3df6
Author: Asbjørn Olling <asbjornolling@gmail.com>
Date:   Thu Sep 14 19:25:00 2023 +0200

    flake : include llama.h in nix output (#3159)

flake.nix

commit 4b8560e72a936b5d536ebd1e7a5dd579984769f3
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 14 13:22:47 2023 -0400

    make : fix clang++ detection, move some definitions to CPPFLAGS (#3155)
    
    * make : fix clang++ detection
    
    * make : fix compiler definitions outside of CPPFLAGS

Makefile

commit 83a53b753a9499a2a3535c93975b430cb2c828a9
Author: Alon <alonfaraj@gmail.com>
Date:   Thu Sep 14 20:21:25 2023 +0300

    CI: add FreeBSD & simplify CUDA windows (#3053)
    
    * add freebsd to ci
    
    * bump actions/checkout to v3
    * bump cuda 12.1.0 -> 12.2.0
    * bump Jimver/cuda-toolkit version
    
    * unify and simplify "Copy and pack Cuda runtime"
    * install only necessary cuda sub packages

.github/workflows/build.yml
.github/workflows/gguf-publish.yml

commit 5c872dbca2c7979b1f6dafc97db0774b8bbf9372
Author: akawrykow <142945436+akawrykow@users.noreply.github.com>
Date:   Thu Sep 14 10:19:42 2023 -0700

    falcon : use stated vocab size (#2914)

convert-falcon-hf-to-gguf.py

commit 990a5e226a1a0ac858abe3aa7e5f3b000d4fa665
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Thu Sep 14 14:04:40 2023 -0300

    cmake : add relocatable Llama package (#2960)
    
    * Keep static libs and headers with install
    
    * Add logic to generate Config package
    
    * Use proper build info
    
    * Add llama as import library
    
    * Prefix target with package name
    
    * Add example project using CMake package
    
    * Update README
    
    * Update README
    
    * Remove trailing whitespace

CMakeLists.txt
examples/main-cmake-pkg/.gitignore
examples/main-cmake-pkg/CMakeLists.txt
examples/main-cmake-pkg/README.md
scripts/LlamaConfig.cmake.in

commit 980ab41afba96106cd29cdf3aa6f948c251cb71f
Author: dylan <canardleteer@users.noreply.github.com>
Date:   Thu Sep 14 09:47:00 2023 -0700

    docker : add gpu image CI builds (#3103)
    
    Enables the GPU enabled container images to be built and pushed
    alongside the CPU containers.
    
    Co-authored-by: canardleteer <eris.has.a.dad+github@gmail.com>

.github/workflows/docker.yml
README.md

commit e394084166baac09e8ee9a08a4686f907f7e5291
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Sep 14 10:32:26 2023 -0600

    gguf-py : support identity operation in TensorNameMap (#3095)
    
    Make try_suffixes keyword param optional.

gguf-py/gguf/gguf.py
gguf-py/pyproject.toml

commit 4c8643dd6ea1a163bc5979cb69c1e7ab0975bc93
Author: jameswu2014 <545426914@qq.com>
Date:   Fri Sep 15 00:32:10 2023 +0800

    feature : support Baichuan serial models (#3009)

convert-baichuan-hf-to-gguf.py
gguf-py/gguf/gguf.py
llama.cpp
prompts/chat-with-baichuan.txt

commit 35f73049af6c676a106a5a990a819ae0bc3fcd7d
Author: Leng Yue <lengyue@lengyue.me>
Date:   Thu Sep 14 09:14:44 2023 -0700

    speculative : add heuristic algorithm (#3006)
    
    * Add heuristic algo for speculative
    
    * Constrain minimum n_draft to 2
    
    * speculative : improve heuristic impl
    
    * speculative : be more rewarding upon guessing max drafted tokens
    
    * speculative : fix typos
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/speculative/speculative.cpp

commit 71ca2fad7d6c0ef95ef9944fb3a1a843e481f314
Author: goerch <jhr.walter@t-online.de>
Date:   Wed Sep 13 15:19:44 2023 +0200

    whisper : tokenizer fix + re-enable tokenizer test for LLaMa (#3096)
    
    * Fix für #2721
    
    * Reenable tokenizer test for LLaMa
    
    * Add `console.cpp` dependency
    
    * Fix dependency to `common`
    
    * Fixing wrong fix.
    
    * Make console usage platform specific
    
    Work on compiler warnings.
    
    * Adapting makefile
    
    * Remove trailing whitespace
    
    * Adapting the other parts of the makefile
    
    * Fix typo.

Makefile
llama.cpp
tests/CMakeLists.txt
tests/test-tokenizer-0-llama.cpp
tests/test-tokenizer-1-llama.cpp
tests/test-tokenizer-1.cpp

commit 1b6c650d16048d6427dd502a9627e72837265844
Author: Tristan Ross <rosscomputerguy@protonmail.com>
Date:   Wed Sep 13 06:08:52 2023 -0700

    cmake : add a compiler flag check for FP16 format (#3086)

CMakeLists.txt

commit 0a5eebb45d5697127b84418576dc479c400c4b3d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Sep 13 11:20:24 2023 +0200

    CUDA: mul_mat_q RDNA2 tunings (#2910)
    
    * CUDA: mul_mat_q RDNA2 tunings
    
    * Update ggml-cuda.cu
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    
    ---------
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>

CMakeLists.txt
Makefile
ggml-cuda.cu

commit 84e723653ca99d51a74b454984acf2c077468561
Author: FK <sozforex@gmail.com>
Date:   Wed Sep 13 08:50:46 2023 +0200

    speculative: add --n-gpu-layers-draft option (#3063)

common/common.cpp
common/common.h
examples/speculative/speculative.cpp

commit b52b29ab9d601bb298050bcd2261169bc917ba2c
Author: Eric Sommerlade <es0m@users.noreply.github.com>
Date:   Wed Sep 13 02:54:20 2023 +0100

    arm64 support for windows (#3007)
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

CMakeLists.txt
ggml.c
ggml.h
k_quants.c

commit 4f7cd6ba9c88d3ca9a207b6e04f8b2b1efd707b8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Sep 13 00:15:33 2023 +0200

    CUDA: fix LoRAs (#3130)

ggml-cuda.cu

commit 89e89599fd095172f8d67903b5e227467420f036
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Sep 11 22:58:41 2023 +0200

    CUDA: fix mul_mat_q not used for output tensor (#3127)

ggml-cuda.cu

commit d54a4027a6ebda98ab0fef7fa0c2247d0bef132a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Sep 11 19:55:51 2023 +0200

    CUDA: lower GPU latency + fix Windows performance (#3110)

ggml-cuda.cu

commit 1b0d09259e37898c519edb6c52d58f4d096f10bd
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Mon Sep 11 19:49:06 2023 +0800

    cmake : support build for iOS/tvOS (#3116)
    
    * cmake : support build for iOS/tvOS
    
    * ci : add iOS/tvOS build into macOS-latest-cmake
    
    * ci : split ios/tvos jobs

.github/workflows/build.yml
CMakeLists.txt

commit 8a4ca9af569853023ce87f047eb5165df13f2ff1
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Sep 11 13:00:24 2023 +0200

    CUDA: add device number to error messages (#3112)

ggml-cuda.cu

commit f31b6f4e2d6def3c0bd7c75f75c0c1e8698e0589
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Sep 11 09:30:11 2023 +0200

    metal : PP speedup (#3084)
    
    * Minor speed gains for all quantization types
    
    * metal: faster kernel_scale via float4
    
    * Various other speedups for "small" kernels
    
    * metal: faster soft_max vial float4
    
    * metal: faster diagonal infinity
    
    Although, to me it looks like one should simply
    fuse scale + diagnonal infinity + soft_max on the
    KQtensor.
    
    * Another faster f16 x f32 matrix multiply kernel
    
    * Reverting the diag infinity change
    
    It does work for PP, but somehow it fails for TG.
    Need to look more into it.
    
    * metal: add back faster diagonal infinity
    
    This time more carefully
    
    * metal : minor (readibility)
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 6eeb4d90839bac1e6085e5544654ab5c319ad09a
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Sun Sep 10 17:06:53 2023 +0200

    convert: remove most of the n_mult usage in convert.py (#3098)

convert.py

commit 21ac3a1503001020122db5dce6adf34b761675f5
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Sat Sep 9 02:12:10 2023 -0700

    metal : support for Swift (#3078)
    
    * Metal support for Swift
    
    * update
    
    * add a toggle for arm/arm64
    
    * set minimum versions for all platforms
    
    * update to use newLibraryWithURL
    
    * bump version
    
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>
    
    ---------
    
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>

Package.swift
ggml-metal.m

commit 4fd54779550e43e2a29f6840ebcf8f395a2f879e
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Sep 9 16:46:04 2023 +0800

    metal : support build for iOS/tvOS (#3089)

ggml-metal.m

commit ec2a24fedf1de8ebd5f170016953b09ff2806924
Author: takov751 <40316768+takov751@users.noreply.github.com>
Date:   Fri Sep 8 17:06:26 2023 +0100

    flake : add train-text-from-scratch to flake.nix (#3042)

flake.nix

commit 7d99aca759f2f8a1ff39f3bb02a840f69863428b
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Sat Sep 9 01:04:32 2023 +0900

    readme : fix typo (#3043)
    
    * readme : fix typo
    
    acceleation -> acceleration
    
    * Update README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md

commit ba7ffbb2517ff8cf4c689f94a9ad866f3ee71225
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Sep 8 18:01:04 2023 +0200

    metal : Q3_K speedup (#2995)
    
    * Slightly faster Q3_K and Q5_K on metal
    
    * Another Q3_K speedup on metal
    
    Combined with previous commit, we are now +9.6% for TG.
    PP is not affected as this happens via the matrix multiplication
    templates.
    
    * Slowly progressing on Q3_K on metal
    
    We are now 13% faster than master
    
    * nother small improvement for Q3_K on metal
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.metal

commit e64f5b55783e910d8287363895d652b4bea6527a
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 8 11:43:35 2023 -0400

    examples : make n_ctx warning work again (#3066)
    
    This was broken by commit e36ecdcc ("build : on Mac OS enable Metal by
    default (#2901)").

examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
llama.cpp
llama.h

commit 94f10b91ed69980f299441e49c8dbdb448f0ccc6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 8 18:18:04 2023 +0300

    readme : update hot tpoics

README.md

commit b3e9852e471d12cbbe5dad20c81c4766d969739a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 8 17:58:07 2023 +0300

    sync : ggml (CUDA GLM RoPE + POSIX) (#3082)
    
    ggml-ci

CMakeLists.txt
ggml-cuda.cu
ggml.c

commit cb6c44c5e045709b6bb5cc9bb8c9be107c771a78
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Fri Sep 8 14:09:21 2023 +0200

    build : do not use _GNU_SOURCE gratuitously (#2035)
    
    * Do not use _GNU_SOURCE gratuitously.
    
    What is needed to build llama.cpp and examples is availability of
    stuff defined in The Open Group Base Specifications Issue 6
    (https://pubs.opengroup.org/onlinepubs/009695399/) known also as
    Single Unix Specification v3 (SUSv3) or POSIX.1-2001 + XSI extensions,
    plus some stuff from BSD that is not specified in POSIX.1.
    
    Well, that was true until NUMA support was added recently,
    so enable GNU libc extensions for Linux builds to cover that.
    
    Not having feature test macros in source code gives greater flexibility
    to those wanting to reuse it in 3rd party app, as they can build it with
    FTMs set by Makefile here or other FTMs depending on their needs.
    
    It builds without issues in Alpine (musl libc), Ubuntu (glibc), MSYS2.
    
    * make : enable Darwin extensions for macOS to expose RLIMIT_MEMLOCK
    
    * make : enable BSD extensions for DragonFlyBSD to expose RLIMIT_MEMLOCK
    
    * make : use BSD-specific FTMs to enable alloca on BSDs
    
    * make : fix OpenBSD build by exposing newer POSIX definitions
    
    * cmake : follow recent FTM improvements from Makefile

CMakeLists.txt
Makefile
examples/beam-search/beam-search.cpp
examples/embd-input/embd-input-lib.cpp
examples/main/main.cpp
examples/simple/simple.cpp
examples/speculative/speculative.cpp
ggml-alloc.c
ggml.c
llama.cpp

commit a21baeb12202a9020b48c53beaaf4b355228e8ba
Author: hongbo.mo <352280764@qq.com>
Date:   Fri Sep 8 18:57:55 2023 +0800

    docker : add git to full-cuda.Dockerfile main-cuda.Dockerfile (#3044)

.devops/full-cuda.Dockerfile
.devops/main-cuda.Dockerfile

commit 6ff712a6d1a0c85d996e2f681df57a2554cfe5c1
Author: Yui <dev@sleepyyui.com>
Date:   Fri Sep 8 12:32:55 2023 +0200

    Update deprecated GGML TheBloke links to GGUF (#3079)

README.md

commit ebc96086af49fe70108cafcea6ab4bebd658a41a
Author: slaren <slarengh@gmail.com>
Date:   Fri Sep 8 04:04:56 2023 +0200

    ggml-alloc : correctly check mmap return value for errors (#3075)

ggml-alloc.c

commit 7f412dab9c8801f5d37904f7dce1faf4c2b43b42
Author: Kunshang Ji <kunshang.ji@intel.com>
Date:   Fri Sep 8 09:46:56 2023 +0800

    enable CPU HBM (#2603)
    
    * add cpu hbm support
    
    * add memalign 0 byte check
    
    * Update ggml.c
    
    * Update llama.cpp
    
    * ggml : allow ggml_init with 0 size
    
    * retrigger ci
    
    * fix code style
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
ggml.c
llama.cpp

commit 6336d834ec7bff3e93e24182c0f609d2f2bdce26
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 7 14:27:42 2023 -0400

    convert : fix F32 ftype not being saved (#3048)

convert.py

commit 00d62adb79bf914a95fb9a2e8f42f3029e76d62c
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 7 13:22:29 2023 -0400

    fix some warnings from gcc and clang-tidy (#3038)
    
    Co-authored-by: xaedes <xaedes@gmail.com>

.clang-tidy
CMakeLists.txt
Makefile
common/common.cpp
common/common.h
common/grammar-parser.cpp
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
examples/embd-input/embd-input-lib.cpp
examples/embedding/embedding.cpp
examples/gptneox-wip/falcon-main.cpp
examples/gptneox-wip/gptneox-main.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
examples/quantize/quantize.cpp
examples/save-load-state/save-load-state.cpp
examples/server/server.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-alloc.c
ggml.c
llama.cpp
tests/test-quantize-perf.cpp

commit 4fa2cc1750b861880de42515cb19c13b2d776ee2
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 7 10:15:01 2023 -0400

    make : improve test target (#3031)

Makefile

commit 5ffab089a54bc06ae4a9ab533893b558756a1e80
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Sep 7 10:13:50 2023 -0400

    make : fix CPPFLAGS (#3035)

Makefile

commit 15b67a66c2f2d6032415b28a699b5131962318f1
Author: slaren <slarengh@gmail.com>
Date:   Thu Sep 7 15:52:34 2023 +0200

    llama-bench : use two tokens in the warmup run for prompt evals (#3059)

examples/llama-bench/llama-bench.cpp

commit be8c9c245bd129ebabb80e0a7a8dd7daeb4d30af
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Sep 7 15:45:01 2023 +0200

    metal : parallel RoPE on Metal (#3024)
    
    * Parallel RoPE on metal
    
    * PR suggestion
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit be6beeb8d75294552c4918fce06d7b84eebf3d79
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Sep 7 15:42:42 2023 +0200

    metal : correct fix of kernel_norm (#3060)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.metal

commit c4f496648c1e32efeb714200e7eae7fc7cfbb223
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Sep 7 15:49:09 2023 +0300

    metal : fix kernel_norm (fixes Falcon on Metal) (#3057)
    
    * metal : fix kernel_norm
    
    ggml-ci
    
    * metal : put warning in kernel_norm to not combine the loops
    
    * metal : restore original F16 mat-vec multiplication
    
    It works after the norm fixes
    
    * common : don't do warm-up with more than n_batch tokens (close #3058)
    
    ggml-ci
    
    * metal : minor

common/common.cpp
ggml-metal.metal

commit fec2fb19e4229aac58c98171c46e77144b99f8a3
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Thu Sep 7 10:15:06 2023 +0200

    ggml : posixify madvise and pagesize (#3037)
    
    * llama : use posix_madvise() instead of madvise() derived from BSD
    
    sed -i 's,\<madvise\>,posix_&,g;s,\<MADV_,POSIX_&,g' llama.cpp
    
    * ggml : use sysconf(_SC_PAGESIZE) instead of getpagesize() derived from BSD
    
    sed -i 's,getpagesize(),sysconf(_SC_PAGESIZE),g' ggml.c
    
    * metal : use sysconf(_SC_PAGESIZE) instead of getpagesize() derived from BSD
    
    sed -i 's,getpagesize(),sysconf(_SC_PAGESIZE),g' ggml-metal.m

ggml-metal.m
ggml.c
llama.cpp

commit 178b1850ebd21b349cebbee887950e435c5aa2d3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Sep 6 12:40:57 2023 +0300

    k-quants : fix zero-weight guard in Q6_K (ref #3040)

k_quants.c

commit ea2c85d5d2a93d39d0172222917f3195f0e456ff
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Wed Sep 6 02:49:11 2023 -0600

    convert-llama-ggml-to-gguf: Try to handle files older than GGJTv3 (#3023)
    
    * convert-llama-ggmlv3-to-gguf: Try to handle files older than GGJTv3
    
    * Better error messages for files that cannot be converted
    
    * Add file type to GGUF output
    
    * Rename to convert-llama-ggml-to-gguf.py
    
    * Include original file type information in description
    
    * Improve some informational output

convert-llama-ggml-to-gguf.py

commit 9912b9efc8922321fe7202ab42ba913833cbe9cd
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Sep 5 18:21:10 2023 -0400

    build : add LLAMA_METAL_NDEBUG flag (#3033)

CMakeLists.txt
Makefile

commit 9e2023156e5b5acabaf8632e66c6ae68d3703c31
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Sep 5 15:12:00 2023 -0400

    make : use new flag variables for recent changes (#3019)

Makefile

commit de2fe892af92a5c7b5ef1beb7efbc0524343fbab
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Sep 5 15:10:27 2023 -0400

    examples : replace fprintf to stdout with printf (#3017)

common/common.cpp
common/log.h
examples/gguf/gguf.cpp
examples/gptneox-wip/falcon-main.cpp
examples/gptneox-wip/gptneox-main.cpp
examples/llama-bench/llama-bench.cpp
examples/server/server.cpp

commit c9c3220c485c7bea740a07cda7343677fb3beaae
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Tue Sep 5 19:41:00 2023 +0200

    convert: fix convert.py not working with int filename_stem (#3028)
    
    * fix implicit int to string conversion
    * convert : remove an obsolete pyright comment
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

convert.py

commit d59bd97065cd7ded6c4ecab54b1d5e0b1b11e318
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Sep 5 09:55:33 2023 +0200

    Guard against all weights in a super-block being zero (#3010)
    
    * Guard against all weights in a super-block being zero
    
    * Also guard against extremely small weights
    
    Closes #2982
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

k_quants.c

commit 35938ee3b0c16f1fbbf240dae21e0228864b938c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 5 10:46:39 2023 +0300

    llama : update logic for number of threads when using BLAS

llama.cpp

commit 921772104ba2219bfdc2b2980d05ebc0aa0c92a4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Sep 5 08:46:17 2023 +0300

    speculative : add grammar support (#2991)
    
    * speculative : add grammar support
    
    * grammars : add json_arr.gbnf
    
    * grammar : add comments to new grammar file
    
    * grammar : remove one nested level
    
    * common : warm-up with 2 tokens - seems to work better
    
    * speculative : print draft token pieces
    
    * speculative : reuse grammar parser + better logs and comments
    
    * speculative : avoid grammar_mem
    
    * make : fix speculative build

Makefile
common/common.cpp
examples/speculative/speculative.cpp
grammars/json_arr.gbnf
llama.cpp
llama.h

commit 2ba85c8609309a59d49c45ab43c31800b7ba141c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 4 22:50:50 2023 +0300

    py : minor

convert-falcon-hf-to-gguf.py

commit e36ecdccc8754783f93ad3ac8a09e540101f2ca0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Sep 4 22:26:24 2023 +0300

    build : on Mac OS enable Metal by default (#2901)
    
    * build : on Mac OS enable Metal by default
    
    * make : try to fix build on Linux
    
    * make : move targets back to the top
    
    * make : fix target clean
    
    * llama : enable GPU inference by default with Metal
    
    * llama : fix vocab_only logic when GPU is enabled
    
    * common : better `n_gpu_layers` assignment
    
    * readme : update Metal instructions
    
    * make : fix merge conflict remnants
    
    * gitignore : metal

.gitignore
CMakeLists.txt
Makefile
README.md
common/common.cpp
common/common.h
examples/main/main.cpp
examples/perplexity/perplexity.cpp
llama.cpp

commit bd33e5ab92e7f214205792fc1cd9ca28e810f897
Author: slaren <slarengh@gmail.com>
Date:   Mon Sep 4 14:59:52 2023 +0200

    ggml-opencl : store GPU buffer in ggml_tensor::extra (#2994)

ggml-opencl.cpp

commit 31035681445181fb414e0def7ec3f84462b3bd97
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Sep 4 06:40:18 2023 -0400

    llama-bench : make cpp file non-executable (#2999)

examples/llama-bench/llama-bench.cpp

commit 5b8530d88c489f9d0c0ef3d0886b369f655b792e
Author: Leng Yue <lengyue@lengyue.me>
Date:   Mon Sep 4 03:39:57 2023 -0700

    make : add speculative example (#3003)

.gitignore
Makefile

commit e4386f417faf894f6706eec005e24d142b577fcb
Author: Aarni Koskela <akx@iki.fi>
Date:   Mon Sep 4 10:28:55 2023 +0200

    server : add a subtle loading animation to the edit box (#2466)
    
    * editorconfig: add override for the server HTML (which already is 2-space indented)
    
    * server: add a subtle loading animation to the edit box

.editorconfig
examples/server/index.html.hpp
examples/server/public/index.html

commit 35195689cd835464779c247b1c22ab9247418fd1
Author: Jiahao Li <liplus17@163.com>
Date:   Mon Sep 4 14:53:30 2023 +0800

    2x faster (rms) norm cuda kernels (3.7% e2e improvement) (#2985)
    
    * 2x faster (rms) norm cuda kernels
    
    * Fix code style

ggml-cuda.cu

commit cf9b08485c4c2d4d945c6e74fe20f273a38b6104
Author: slaren <slarengh@gmail.com>
Date:   Sun Sep 3 20:34:09 2023 +0200

    ggml-alloc : use virtual memory for measurement (#2973)
    
    * ggml-alloc : use virtual memory for measurement
    
    * compatibility fixes for MAP_ANONYMOUS
    
    * fallback to fixed address for systems without virtual memory

ggml-alloc.c

commit 47068e517004d90f13c16352bb3b4cafd53a00cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 3 15:12:08 2023 +0300

    speculative : PoC for speeding-up inference via speculative sampling (#2926)
    
    * speculative : initial example
    
    * speculative : print encoding speed
    
    * speculative : add --draft CLI arg

common/common.cpp
common/common.h
examples/CMakeLists.txt
examples/main/main.cpp
examples/speculative/CMakeLists.txt
examples/speculative/speculative.cpp

commit 8f429fa5111901f9646cf998643ac5310846d487
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 3 13:42:56 2023 +0300

    perplexity : fix ETA by warming up the model with an empty run

common/common.cpp
examples/main/main.cpp

commit 6519e9c99cffbad19b31bcba86df48c500628c09
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Sep 3 04:38:43 2023 -0600

    gguf(python): Fix special vocab handling when id < 0 (#2984)

gguf-py/gguf/gguf.py
gguf-py/pyproject.toml

commit b7f2aa9e512c3be2e863d877cbb1056d7c4a03f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 3 13:23:33 2023 +0300

    metal : restore 363f0bf and fix reduce in F16_F32 kernels (#2986)

ggml-metal.metal

commit 73a12a6344d5da4d8e2eba5d12221b8bc6895931
Author: Alon <alonfaraj@gmail.com>
Date:   Sun Sep 3 13:19:01 2023 +0300

    cov : disable comment in PRs (#2989)

codecov.yml

commit 37301347767d555d0a66c043ce4ef6ead8e61c55
Author: opparco <parco.opaai@gmail.com>
Date:   Sun Sep 3 19:18:09 2023 +0900

    llama : fix bpe tokenize from byte (#2889)

llama.cpp

commit d9151e6f570eb20bfd54427bd8a337d9b1a08018
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Sep 3 12:40:56 2023 +0300

    metal : revert 6af0bab until we fix it
    
    This restores the generated text to be the same as before #2959

ggml-metal.metal

commit afc43d5f82588d2ed71ea104e8262f5e5da13980
Author: Alon <alonfaraj@gmail.com>
Date:   Sun Sep 3 11:48:49 2023 +0300

    cov : add Code Coverage and codecov.io integration (#2928)
    
    * update .gitignore
    
    * makefile: add coverage support (lcov, gcovr)
    
    * add code-coverage workflow
    
    * update code coverage workflow
    
    * wun on ubuntu 20.04
    
    * use gcc-8
    
    * check why the job hang
    
    * add env vars
    
    * add LLAMA_CODE_COVERAGE=1 again
    
    * - add CODECOV_TOKEN
    - add missing make lcov-report
    
    * install lcov
    
    * update make file -pb flag
    
    * remove unused  GGML_NITER from workflows
    
    * wrap coverage output files in COV_TARGETS

.github/workflows/build.yml
.github/workflows/code-coverage.yml
.gitignore
Makefile

commit 6460f758dbd472653296044d36bed8c4554988f5
Author: Wentai Zhang <rchardx@gmail.com>
Date:   Sun Sep 3 16:46:44 2023 +0800

    opencl : fix a bug in ggml_cl_pool_malloc() for ggml_cl_mul_mat_f32() (#2955)
    
    Co-authored-by: Wentai Zhang <wentaizhang@tencent.com>

ggml-opencl.cpp

commit ca82cf7bac0c91d03e3d320b3a865dd006f854ac
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Sep 3 11:06:22 2023 +0300

    metal : more optimizations (#2959)
    
    * Very minor speedup via simd-group synchronization in f16 x f32
    
    * Another very minor speedup on metal
    
    * Quite significant PP speedup on metal
    
    * Another attempt
    
    * Minor
    
    * Massive improvement for TG for fp16
    
    * ~4-5% improvement for Q8_0 TG on metal
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 6a31a3bd9806c85ed08266f6ab65181da0f30d03
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Sat Sep 2 23:21:05 2023 -0700

    swift : add support for k-quants (#2983)

Package.swift

commit cff7b0bf07cb46e1ad4fd199f6bdeb538925c8c4
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Sep 2 23:52:13 2023 -0600

    convert.py : BPE fixes (#2938)
    
    * convert.py: BPE fixes?
    
    * Remove unnecessary conditional in addl token error handling

convert.py

commit 340af42f09a80e32f4998857b4f0543e41124525
Author: Ido S <ido.pluto@gmail.com>
Date:   Sun Sep 3 08:50:51 2023 +0300

    docs : add `catai` to `README.md` (#2967)

README.md

commit c42f0ec6b344e14bd81c8612ab1445b3ff77358b
Author: momonga <115213907+mmnga@users.noreply.github.com>
Date:   Sun Sep 3 14:36:28 2023 +0900

    examples : fix gpt-neox (#2943)
    
    Co-authored-by: mmnga <mmnga1mmnga@gmail.com>

examples/gptneox-wip/gptneox-main.cpp
llama.cpp

commit 2753415afdaf22a18c49608bd9d93cfffc05d435
Author: kchro3 <62481661+kchro3@users.noreply.github.com>
Date:   Sat Sep 2 22:27:25 2023 -0700

    swift : add missing c file to Package.swift (#2978)

Package.swift

commit bc054af97ac68a4b726e972cb283eb9565253ed5
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Sun Sep 3 01:26:59 2023 -0400

    make : support overriding CFLAGS/CXXFLAGS/CPPFLAGS/LDFLAGS (#2886)
    
    * make : remove unused -DGGML_BIG_ENDIAN
    
    * make : put preprocessor stuff in CPPFLAGS
    
    * make : pass Raspberry Pi arch flags to g++ as well
    
    * make : support overriding CFLAGS/CXXFLAGS/CPPFLAGS/LDFLAGS
    
    * make : fix inverted conditional

Makefile

commit 3358c381f6251bf6e65855e1c93bfaa9ec82ddb3
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Sep 2 11:53:55 2023 -0600

    logging: Fix creating empty file even when disabled (#2966)
    
    * logging: Fix creating empty file even when disabled
    
    * Minor formatting fix
    
    Co-authored-by: staviq <staviq@gmail.com>
    
    ---------
    
    Co-authored-by: staviq <staviq@gmail.com>

common/log.h

commit 52315a421674ff64305dbf082f69e4ec77f0a3f3
Author: bandoti <141645996+bandoti@users.noreply.github.com>
Date:   Sat Sep 2 09:53:18 2023 -0300

    readme : update clblast instructions (#2903)
    
    * Update Windows CLBlast instructions
    
    * Update Windows CLBlast instructions
    
    * Remove trailing whitespace

README.md

commit 8b56b4f2c396eae1f4417e5a859557fed989e0ee
Author: Karsten Weiss <knweiss@gmail.com>
Date:   Sat Sep 2 14:29:09 2023 +0200

    metal : show all Metal device instances in the system (#2952)
    
    * ggml_metal_init: Show all Metal device instances in the system
    
    Also show the default Metal device that was picked.
    
    * Update ggml-metal.m
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.m

commit 21f3d1be867b4d7be07c26f5da6e4bc69bcf4d27
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Sep 2 20:23:45 2023 +0800

    k-quants : fix build on armv7 (android only) (#2920)
    
    * k-quants : fix build on armv7
    
    * ggml : cleanup unused arm32 specific impl
    
    * k-quants : avoid some unused vzero / mzero define
    
    * ggml-alloc : use 4g for MEASURE_MAX_SIZE in 32-bit arm

ggml-alloc.c
ggml.c
k_quants.c

commit 571083f508266c4eb5cb5457d836df5dd3c173ce
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Sep 2 08:31:46 2023 +0800

    server : avoid aniprompt in probabilities of final response (#2849)

examples/server/server.cpp

commit f04d0028444bc9b3d4225fba47e19d4c3aeb3741
Author: Engininja2 <139037756+Engininja2@users.noreply.github.com>
Date:   Fri Sep 1 15:33:19 2023 -0600

    cuda : vsubss4 for older versions of ROCm/clang (#2942)

ggml-cuda.cu

commit 69fdbb9abc8907dd2a9ffdd840cba92d678a660a
Author: ZHAOKAI WANG <sanxianwei@163.com>
Date:   Fri Sep 1 22:06:44 2023 +0800

    readme : quick start command fix (#2908)
    
    * quick start command fix
    
    * quick start win command fix

examples/main/README.md

commit 5d6f19f16b2173afe2d5c6aee2f5c9fc31038eba
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Sep 1 08:02:48 2023 -0600

    Allow quantize to only copy tensors, some other improvements (#2931)
    
    * Allow quantize tool to only copy tensors to allow repackaging models.
    
    * Slightly better logic when requantizing.
    
    * Change help message to go to `stdout`.

examples/quantize/quantize.cpp
llama.cpp
llama.h

commit 0d5893668625456c94bbadfddc53fc69cd51c223
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 1 17:00:40 2023 +0300

    llama2c : rename function

examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp

commit 6c9c23429bf4e4fcaaddbebadc4638558430a7f2
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 1 09:53:14 2023 -0400

    make : use unaligned vector moves on MinGW (#2945)
    
    Fixes #2922

Makefile

commit ee8654bcd0146708988a703e54406d5b553712ea
Author: m3ndax <adrian.goessl@outlook.com>
Date:   Fri Sep 1 15:47:27 2023 +0200

    minor : add const qualifiers (#2853)
    
    * made the methods const
    
    # Conflicts:
    #       examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
    
    * made method const
    
    * Update convert-llama2c-to-ggml.cpp
    
    removed write_raw and write_u32
    
    * llama2c : remove misleading const
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
llama.cpp

commit 49bb9cbe0f598bc43be539b0df8eafb2130cfad3
Author: Konstantin Herud <konstantin.herud@denkbares.com>
Date:   Fri Sep 1 15:36:14 2023 +0200

    docs : add java-llama.cpp to README.md (#2935)

README.md

commit ef156499721c67748cde01a5436cb6f0648bb4b4
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 1 09:34:50 2023 -0400

    build : fix most gcc and clang warnings (#2861)
    
    * fix most gcc and clang warnings
    
    * baby-llama : remove commented opt_params_adam
    
    * fix some MinGW warnings
    
    * fix more MinGW warnings

CMakeLists.txt
Makefile
common/common.cpp
common/console.cpp
examples/baby-llama/baby-llama.cpp
examples/beam-search/beam-search.cpp
examples/server/server.cpp
k_quants.c
llama.cpp

commit d8d6977f48f1fa402ade38ad32c5b5fb1358d059
Author: Ben Siraphob <bensiraphob@gmail.com>
Date:   Fri Sep 1 09:32:14 2023 -0400

    examples : add C grammar (#2357)

grammars/c.gbnf

commit 5aec2cfaac386eb09aebb75b805860828f00de91
Author: Tameem <113388789+AhmadTameem@users.noreply.github.com>
Date:   Fri Sep 1 18:27:40 2023 +0500

    ggml : add RISC-V vector intrinsics support (#2929)
    
    * added support for RISCV CFLAGS & native compile + cross compile options
    
    * Add RISC-V Vector Intrinsics Support
    
    Added RVV intrinsics for following
       ggml_vec_dot_q4_0_q8_0
       ggml_vec_dot_q4_1_q8_1
       ggml_vec_dot_q5_0_q8_0
       ggml_vec_dot_q5_1_q8_1
       ggml_vec_dot_q8_0_q8_0
    
    Co-authored-by: Sharafat <sharafat.hussain@10xengineers.ai>
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>
    
    ---------
    
    Signed-off-by: Ahmad Tameem <ahmad.tameem@10xengineers.ai>
    Co-authored-by: moiz.hussain <moiz.hussain@10xengineers.ai>
    Co-authored-by: Sharafat <sharafat.hussain@10xengineers.ai>

Makefile
ggml.c

commit 13268c533177a4dc76bce0b465645d74f0d51d55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Sep 1 13:42:41 2023 +0300

    metal : slight speed-up for add and mul kernels (#2917)

ggml-metal.m
ggml-metal.metal

commit 4dcd47d71df8ca4edcc31302744bd93f0c31298e
Author: staviq <staviq@gmail.com>
Date:   Fri Sep 1 11:07:06 2023 +0200

    logs : fix mingw-like builds (fixes #2898) (#2911)
    
    * fix mingw-like builds
    
    * formatting
    
    * make LOG_COMPAT easier to override and extend
    
    * simplify win detection
    
    * fix for #2940

Makefile
common/log.h

commit 18705a30ef3d6a89e1d7c6cb8cfe8633f760cb53
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Sep 1 05:03:49 2023 -0400

    llama2c : fix segfault and alloc-dealloc-mismatch (#2913)
    
    * llama2c : fix segfault if vocab is not found
    
    * llama2c : fix mismatch between new[] and delete
    
    * llama2c : fix basename on Windows
    
    * llama2c : use a destructor to prevent memory leaks

examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp

commit e8d91589258f9204397a7ac5f9b3c857835c98f8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Sep 1 11:15:57 2023 +0300

    metal: somewhat faster f16 x f32 matrix multiply kernel (#2951)
    
    * Somewhat faster f16 x f32 matrix multiply kernel
    
    * Better use 32 thread groups for f16 x f32
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit bce1fef328941499dc0acb76cc7fd7ac90449c2f
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Aug 31 22:13:51 2023 -0400

    convert : fix another python 3.8 issue (#2949)

convert.py

commit 528134dd0267838d9c0250cf1d9621631dff09b2
Author: slaren <slarengh@gmail.com>
Date:   Fri Sep 1 01:32:09 2023 +0200

    remove convert-llama-7b-pth-to-gguf.py and convert-llama-hf-to-gguf.py (#2906)

convert-llama-7b-pth-to-gguf.py
convert-llama-hf-to-gguf.py

commit aeefac4ff760acea5afe66fbfe8d7eca1937b79c
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Aug 31 16:49:24 2023 -0600

    scripts: Use local gguf package when running from repo (#2927)
    
    * scripts: Use local gguf when running from repo

convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-llama-ggmlv3-to-gguf.py
convert.py
examples/train-text-from-scratch/convert-train-checkpoint-to-gguf.py

commit e8422de39e4aa2f7e50574124b060a80607e654a
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu Aug 31 04:21:45 2023 -0700

    @vxiiduu's fix for PrefetchVirtualMemory (#2930)
    
    Reimplement fix for `PrefetchVirtualMemory`.
    Co-authored-by: vxiiduu <73044267+vxiiduu@users.noreply.github.com>

llama.cpp

commit 92d0b751a77a089e650983e9f1564ef4d31b32b9
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Thu Aug 31 01:02:23 2023 -0400

    convert : fix python 3.8 support, modernize type annotations (#2916)
    
    * convert : fix python 3.8 support
    
    * convert : sort imports
    
    * convert : fix required parameters in convert-llama-ggmlv3-to-gguf
    
    * convert : fix mypy errors in convert-llama-ggmlv3-to-gguf
    
    * convert : use PEP 585 generics and PEP 604 unions
    
    Now that we have `from __future__ import annotations`, we can use this
    modern syntax in Python 3.7 instead of restricting support to Python 3.9
    or 3.10 respectively.
    
    * gguf.py : a tuple is already a tuple
    
    * add mypy.ini
    
    * convert : add necessary `type: ignore` comments
    
    * gguf-py: bump version

convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-llama-7b-pth-to-gguf.py
convert-llama-ggmlv3-to-gguf.py
convert-llama-hf-to-gguf.py
convert-lora-to-ggml.py
convert.py
gguf-py/gguf/gguf.py
gguf-py/pyproject.toml
mypy.ini

commit 8afe2280009ecbfc9de2c93b8f41283dc810609a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Aug 30 21:46:19 2023 +0200

    CUDA: mul_mat_q=true llama_context_params default (#2912)

llama.cpp

commit 71d6975559acfd6c8407a4ef8275a9979c737765
Author: Henri Vasserman <henv@hot.ee>
Date:   Wed Aug 30 19:14:53 2023 +0300

    [Docker] fix tools.sh argument passing. (#2884)
    
    * [Docker] fix tools.sh argument passing.
    
    This should allow passing multiple arguments to containers with
    the full image that are using the tools.sh frontend.
    
    Fix from https://github.com/ggerganov/llama.cpp/issues/2535#issuecomment-1697091734

.devops/tools.sh

commit b532a69b2fd08067f34f32f37a2fd9b37678a34a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 30 13:29:40 2023 +0300

    convert.py : use dir name to name the llama

convert.py

commit c90d135eb433cf0d40fb95e46a48d1391d2352b5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 30 12:52:46 2023 +0300

    examples : fix underscore in beam-search + .gitignore (close #2900)

.gitignore
Makefile
examples/CMakeLists.txt
examples/beam-search/CMakeLists.txt
examples/beam-search/beam-search.cpp

commit 0d1c706181cd31e7f368dd14eeb16c1a2569e4df
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Wed Aug 30 12:47:40 2023 +0300

    gguf : add workflow for Pypi publishing (#2896)
    
    * gguf : add workflow for Pypi publishing
    
    * gguf : add workflow for Pypi publishing
    
    * fix trailing whitespace

.github/workflows/gguf-publish.yml
gguf-py/README.md

commit 950929442070874d45561d2a4b68b010457767de
Author: alonfaraj <alonfaraj@gmail.com>
Date:   Wed Aug 30 12:42:51 2023 +0300

    make : add test and update CI (#2897)
    
    * build ci: run make test
    
    * makefile:
    - add all
    - add test
    
    * enable tests/test-tokenizer-0-llama
    
    * fix path to model
    
    * remove gcc-8 from macos build test
    
    * Update Makefile
    
    * Update Makefile

.github/workflows/build.yml
Makefile

commit 35092fb54712d032860f3976a6fc1ae1f84a4a28
Author: Gilad S <giladgd@users.noreply.github.com>
Date:   Wed Aug 30 11:40:12 2023 +0300

    docs : add `node-llama-cpp` to `README.md` (#2885)

README.md

commit dc07dc492ef9640bbb82904d7c7679f7bdcf6d76
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Wed Aug 30 02:25:50 2023 -0600

    convert : various script cleanups/fixes + merges and special token handling (#2842)
    
    * convert: Fix permute calls and method/func definitions
    
    * Cleanups for gguf-py
    
    * Minor types cleanups.
    
    * Initial implementation of handling merges and special tokens
    
    * convert: Handle special tokens and merges in vocab only mode
    
    convert: Vocab only mode no longer requires loading model tensors
    
    * gguf: Refactor tensor name mapping
    
    * convert: Fix type hint for special_token_types in SpecialVocab
    
    * Use common special vocab handling in various conversion scripts
    
    * First pass at implementing suggested changes
    
    * Second pass
    
    * gguf: SpecialVocab: Fix issue with special token content not in a dict
    
    gguf: SpecialVocab: Allow skipping handling of merges
    
    * convert-falcon-hf-to-gguf: Support --vocab-only option, bail out if no tokenizer.json
    
    * convert-gptneox-hf-to-gguf and convert: Only handle merges for BPE tokenizer
    
    * gguf: SpecialVocab: Actually set load_merges in object
    
    * Uniform args parsing and vocab only mode for convert examples
    
    * convert.py: Set gpt2 as tokenizer model when using BPE
    
    * Squish last type warning in gguf.py - yay!

convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-llama-7b-pth-to-gguf.py
convert-llama-ggmlv3-to-gguf.py
convert-llama-hf-to-gguf.py
convert-lora-to-ggml.py
convert.py
gguf-py/gguf/gguf.py
gguf-py/gguf/py.typed
gguf-py/pyproject.toml

commit ad9ddcff6ef322db5cf13785bd7c856b610d242e
Author: chaihahaha <chai836275709@gmail.com>
Date:   Wed Aug 30 14:50:55 2023 +0800

    llm.vim : stop generation at multiple linebreaks, bind to <F2> (#2879)

examples/llm.vim

commit 8341a25957b319a03d4a811176cd5ad7f2b0fbd4
Author: staviq <staviq@gmail.com>
Date:   Wed Aug 30 08:29:32 2023 +0200

    main : log file (#2748)
    
    * initial, base LOG macro
    
    * add *.log to .gitignore
    
    * added basic log file handler
    
    * reverted log auto endline to better mimic printf
    
    * remove atomics and add dynamic log target
    
    * log_enable/disable, LOG_TEE, basic usage doc
    
    * update .gitignore
    
    * mv include to common, params, help msg
    
    * log tostring helpers, token vectors pretty prints
    
    * main: replaced fprintf/LOG_TEE, some trace logging
    
    * LOG_DISABLE_LOGS compile flag, wrapped f in macros
    
    * fix LOG_TEELN and configchecker
    
    * stub LOG_DUMP_CMDLINE for WIN32 for now
    
    * fix msvc
    
    * cleanup main.cpp:273
    
    * fix stray whitespace after master sync
    
    * log : fix compile warnings
    
    - do not use C++20 stuff
    - use PRIu64 to print uint64_t
    - avoid string copies by using const ref
    - fix ", ##__VA_ARGS__" warnings
    - compare strings with == and !=
    
    * log : do not append to existing log + disable file line func by default
    
    * log : try to fix Windows build
    
    * main : wip logs
    
    * main : add trace log
    
    * review: macro f lowercase, str append to sstream
    
    * review: simplify ifs and str comparisons
    
    * fix MSVC, formatting, FMT/VAL placeholders
    
    * review: if/else cleanup
    
    * review: if/else cleanup (2)
    
    * replace _ prefix with _impl suffix
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile
common/common.cpp
common/common.h
common/log.h
examples/chat.sh
examples/main/main.cpp

commit 849408957c687cde4ab32c147107f643fc55130b
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Aug 30 02:20:26 2023 -0400

    tests : add a C compliance test (#2848)
    
    * tests : add a C compliance test
    
    * make : build C compliance test by default
    
    * make : fix clean and make sure C test fails on clang
    
    * make : move -Werror=implicit-int to CFLAGS

CMakeLists.txt
Makefile
tests/CMakeLists.txt
tests/test-c.c

commit 06abf8eebabe086ca4003dee2754ab45032cd3fd
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 29 23:24:42 2023 +0200

    ggml : add view_src and view_offs to ggml_tensor for views (#2874)
    
    * ggml : add view_src and view_offs
    
    * update ggml-alloc to use view_src
    
    * update ggml_diag_mask to work correctly with automatic inplace
    
    * exclude other ops that set an inplace flag from automatic inplace

ggml-alloc.c
ggml.c
ggml.h

commit c03a243abf9f30889f31fefdfa94fe9d7034820c
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 29 23:17:34 2023 +0200

    remove outdated references to -eps and -gqa from README (#2881)

README.md

commit fa3582f509a2715e80a473e79f88dcd1ebff44c2
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Aug 29 23:55:45 2023 +0300

    Tell users attmepting to run perplexity with too few tokens to use more (#2882)
    
    Closes #2858
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp

commit e37e69dcc3d52f21222a63cafed2a71b3f6b53c6
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Aug 29 23:55:03 2023 +0300

    10X faster BPE tokenizer (#2876)
    
    * 10X faster BPE tokenizer
    
    * Remove comment that no longer applies
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit 53885d7256909ec3e2176cdc2477f3986c15ec69
Author: maddes8cht <55592906+maddes8cht@users.noreply.github.com>
Date:   Tue Aug 29 15:51:02 2023 +0200

    py : fix "usage" messages (#2873)
    
    convert-to-gguf python scripts

convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-llama-7b-pth-to-gguf.py
convert-llama-hf-to-gguf.py

commit bcce96ba4dd95482824700c4ce2455fe8c49055a
Author: jameswu2014 <545426914@qq.com>
Date:   Tue Aug 29 17:48:41 2023 +0800

    convert.py : fix baichuan7B support (#2870)
    
    * [Fix]: convert.py support baichuan7B
    
    * convert.py : fix trailing whitespaces
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert.py

commit 74e0caeb82fc9db77fa2cc93070bb919a9a935dd
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Tue Aug 29 17:30:10 2023 +0800

    readme : add react-native binding (#2869)

README.md

commit d4b5e16c32ba9c5fa6bbd035e80a99c113050cde
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Tue Aug 29 04:42:41 2023 -0400

    make : fix clang tests build, add missing examples (#2859)
    
    * make : do not pass headers to the compiler
    
    This fixes building tests with clang.
    
    * make : add missing examples
    
    * make : fix build-info.h dependencies

Makefile

commit 3a007648f230ea37d6cca5e63850f04ebb12d2cf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 29 11:33:46 2023 +0300

    metal : add option to disable debug logs (close #2764)

CMakeLists.txt
Makefile
ggml-metal.m

commit 611363ac791435497e66278dfe31ac8a4e11fa4f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 29 10:50:30 2023 +0300

    scripts : add pipefail

scripts/qnt-all.sh
scripts/run-all-perf.sh
scripts/run-all-ppl.sh

commit 95b6e5212f5e4e1419de1d833d7f8d788f9f2227
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Mon Aug 28 23:33:27 2023 -0700

    added `struct` to llama_dump_timing_info_yaml's `llama_context` (#2857)
    
    fixes C compat.

llama.h

commit 44c117f41ee01c5ac8fb86bba041f08d8b87b46d
Author: xaedes <xaedes@gmail.com>
Date:   Mon Aug 28 21:51:47 2023 +0200

    train : mem usage and other improvements  (#2439)
    
    * fix track_max_mem in forward_batch_wo_cache_flash_attn_train
    
    * remove unnecessary Adam(W) optimizer tensors.
    
    reduces optimizer memory overhead from 7*modelsize to 2*modelsize.
    
    additionally allows to optimize models with more than 2^31 parameters by replacing int with int64_t.
    
    bumps training checkpoint file version, but old checkpoints can still be read.
    new version with less tensors is saved.
    
    * add gradient clipping to AdamW
    
    * Fix reset of unused g->nodes and g->grads to NULL
    
    * implement gradient checkpointing for training
    
    reduces memory overhead from O(n_layer) to O(sqrt(n_layer))
    
    as explained in readme of https://github.com/cybertronai/gradient-checkpointing
    
    * remove unused compute buffer 3
    
    * add and use function ggml_build_backward_expand to avoid stack overflows with large maximum number of nodes
    
    GGML_API void ggml_build_backward_expand(struct ggml_context * ctx, struct ggml_cgraph * gf, struct ggml_cgraph * gb, bool keep);
    
    * change AdamW decay parameter to work like the torch AdamW decay parameter
    
    It is now relative to Adam learning rate `alpha*sched`.
    Before that it was relative to `sched` only.
    
    `alpha` being the maximum learning rate and `sched` being a scaling parameter in [0..1]
    
    * change default AdamW weight decay parameter used in training to 0.1 as used in nanoGPT
    
    * change default AdamW weight decay parameter defined in ggml to 0.0, making Adam default instead of AdamW
    
    btw: the default weight decay parameter for torch.optim.AdamW is 0.01
    
    * bug fixes for cross entropy loss
    
    ggml_cross_entropy_loss: sums where not correctly added in workload of each thread
    ggml_cross_entropy_loss_back: simplify backward process, reducing numerical issues
    
    guard usage of exp f16 lookup in cross entropy by #define GGML_CROSS_ENTROPY_EXP_FP16
    
    cross entropy loss is only used once during training, but it is quite sensitive to numerical errors introduced by exp-f16-lookup.
    so exp-f16-lookup for cross entropy loss is disabled by default, trading better gradients for very slightly worse runtime performance.
    
    * fix test-grad0 for cross_entropy_loss
    
    the second argument to cross_entropy_loss must sum up to 1 for each row
    
    * fix test-grad0 for soft_max
    
    dont use only sum as aggregation, because sum of softmax is always 1 -> finite differences should not work
    instead use sum(log(soft_max()*(1-eps)+eps)); use eps to avoid log(0)
    
    * improve finite differences of test-grad0 by using double instead of float
    
    * change cross_entropy_loss to output average over all rows
    
    this helps keeping the loss and gradients in a sane range
    
    * improve gradient checkpointing
    
    sqrt(n_layers) is only the best checkpoint step when mem size of checkpoints and mem size of layers are equal.
    since layers require more memory than the single-tensor-checkpoint we use, the optimal values are compute different:
    
    ```
      given: n, u, v
      objective: minimize(a*u+b*v) where a*b=n, a>0, b>0
      b=n/a
      minimize(a*u+v*n/a)
      diff(a*u+v*n/a, a) = u - (v*n/a)/a
      diff(a*u+v*n/a, a) == 0
      u - (v*n/a)/a == 0
      u == v*n/(a*a)
      u*a*a = v*n
      a*a = v*n/u
      a = sqrt(n*v/u)
    ```
    
    this change results in more checkpoints, requiring less layers to store between checkpoints, overall improving memory usage.
    
    * disable gradient checkpointing debug output
    
    * llama : fix rope usage in train-text-from-scratch after ChatGLM change
    
    * add more training parameters:
    
    --enable-restart N         Only for Adam optimizer. Enable restarts of cos-decay
    --disable-restart N        Only for Adam optimizer. Disable restarts of cos-decay
    --opt-past N               Number of optimization iterations to track for delta convergence test. Disabled when zero.
    --opt-delta N              Maximum delta for delta convergence test. Disabled when <= zero.
    --opt-max-no-improvement N Maximum number of optimization iterations with no improvement. Disabled when <= zero.
    --adam-epsf N              AdamW epsilon for convergence test. Disabled when <= zero.
    --adam-min-alpha N         Adam minimum learning rate alpha, usually 0.1 * alpha
    
    * replace memcpy with reshape operation so that the graph is not cut at the input
    
    this makes it possible to store other values into the input tensor and then simply recompute the graph without rebuilding it
    
    * remove unused function argument from get_example_targets_batch
    
    * measure and print total training time
    
    * add optimization callback to ggml_opt_resume_g
    
    this callback is called before each iteration with custom data and pointer to learning schedule parameter (only used in Adam(W)).
    
    can be used for dynamic learning schedule and setting input data for batches before each iteration
    
    * use optimization callback in training
    
    allows dynamic learning schedule and different batch data for each iteration without relying on low n_iter and high n_examples parameters
    
    reduces runtime by avoiding restart of optimization function and improves training convergence by providing a different batch for each iteration
    
    * add minimum number of tensor dimensions to apply weight decay (default 2)
    
    this allows to not apply weight decay to bias parameters
    
    * rename training parameter cos-decay-alpha to cos-decay-min and clarify that adam-min-alpha also applies to warmup
    
    * fix increase of model.train_samples and model.train_tokens
    
    now that each optimizer iteration gets its own batch we need to multiply by number of opt iterations
    
    * change sampling parameters for prediction after training to defaults of common.h
    
    and clarify what is context for prediction and what are generated tokens
    
    * tighten abs error bounds for cross_entropy_loss in test-grad0
    
    * add conditional compilation of using F16 exp in flash attention
    
    uncomment `// #define GGML_FLASH_ATTN_EXP_FP16` to enable usage of f16 exp in flash attention
    
    * tighten abs error bounds for flash_attn in test-grad0
    
    * tighten abs error bounds for sqrt in test-grad0
    
    * remove out-commented vectorized code of opt_adam
    
    the vectorized code might be bit faster for low number of parameters, but it had a big memory usage overhead
    
    * ggml : update ggml_rms_norm_back with configurable eps
    
    * llama training : fix ggml_rms_norm_back calls to pass configurable eps
    
    * remove trailing whitespace
    
    * add train function using automatic gradient checkpointing backward pass and allocator
    
    * in train function replace add_inplace by regular add
    
    because using add_inplace seems to result in different gradients
    
    * don't use allocate hash_map on context
    
    because the context has no_alloc=True when using memory allocator resulting in NULL data pointers
    
    * correctly clone reshape and permute operations by also cloning tensor->nb values
    
    * fix variable name and add missing type cast
    
    * terminate recursive tensor cloning when reaching tensor without src tensors
    
    * correctly clone view tensors by setting data pointers
    
    without this the checkpointing would only work when being used together with memory allocator
    
    * fix variable names
    
    * swap arguments to commutative ops to be the same as in `forward_batch_wo_cache_flash_attn`
    
    * add input tensors as checkpoints
    
    so that recursive tensor cloning of gradient checkpointing terminates on input tensors
    
    * fix variable name and add missing boolean negation
    
    * make sure some tensors are not reallocated by inserting new temporary nodes depending on them:
    
    output and parameter gradient tensors need to be available at the end of the graph execution
    
    parameter gradient tensors also need to be available before the graph execution because they are set to zero before each optimizer iteration
    
    checkpoint tensors are allocated all together to reduce memory allocator fragmentation
    
    afterwards, in addition to the temporary nodes, we also need to reset the temporary leafs
    
    * fix ASSERT to work with zero layers
    
    * add training options whether to use allocator and/or unified training function
    
    * integrate unified training function which may use memory allocator
    
    the unified training function also supports arguments whether to use flash attention and/or gradient checkpointing
    
    * format name of cloned tensors with " (clone)" suffix
    
    * set names for tensors in unified train function for easier debugging
    
    * allocate graph on context using ggml_new_graph
    
    * remove handwritten training functions
    
    * remove unused training parameters "use_scratch" and "use_unified"
    
    * remove trailing whitespace
    
    * remove unused train params: mem_compute1_gb & mem_compute2_gb
    
    mem_compute_gb is used for compute when automatic memory allocator is not enabled, otherwise it can be very small to only hold the tensor definitions
    mem_compute0_gb is used for automatic memory allocator (as long as measurement of max required size is not implemented)
    
    * remove unused forward_batch function
    
    * add debug asserts in ggml_allocr_alloc to some common pitfalls when using this function directly
    
    * only use ggml_allocr_alloc when tensor has NULL data and is no view
    
    * fix test when to create temporary backward graph
    
    temporary backward graph is only necessary when using checkpointing
    
    * fix memory "leak" in optimizers
    
    each iteration a new cplan with new memory for work data was allocated.
    now cplan creation only happens at the start of optimization, with each iteration reusing the cplan and its work data.
    
    * reverse order of for loop in ggml_build_backward_expand to save memory when using gradient checkpointing and allocator
    
    with this loop order gradient checkpointing with allocator on 16 layer model saves 13% memory; 2 layer memory it saves 2% memory.
    
    the computation results are the same
    
    * add missing lctx argument to get_example_targets_batch
    
    * implement llama model file saving using gguf
    
    checkpoint loading and saving disabled, to be replaced by loading and saving via gguf
    
    * implement loading/saving of checkpointing files using GGUF
    
    * bug fixes
    
    * add checkpoint file version for future compatibility
    
    * update readme with gguf filenames
    
    * save & load opt->just_initialized value
    
    * add first draft for checkpoint conversion script
    
    * add gguf arch and ftype
    
    * save opt parameter counter as uint64
    
    * add gguf key and tensor names for optimizer and training
    
    * add layer_norm_rms_eps to checkpoint convert script
    
    * use same GGUF_GET_KEY macro as in llama.cpp
    
    * use norm_rms_eps, and rope parameters and command line options to set them
    
    * fix memory corruption bug in gguf
    
    ctx->kv and ctx->infos was reallocated using not-aligned realloc, but freed with aligned free.
    to fix this a GGML_ALIGNED_REALLOC was added, but there is no posix_memalign_realloc function.
    so on non-windows and non-mingw32 platforms we fall back to aligned malloc, followed by copying
    and freeing the old data.
    
    * add gguf example cmake file
    
    * bug fixes in tokenize_file
    
    * bug fixes in load_llama_model_gguf
    
    * bug fix: init model when no checkpoint was loaded
    
    * bug fix in read_tensor_by_name
    
    * bug fix in load_opt_context_gguf
    
    * avoid printing lots of spaced on the unusual case that loss gets nan
    
    * set name of tensors with empty name from what was read from gguf
    
    * remove trailing whitespace
    
    * print data checksums before saving and after loading to verify correctness
    
    * bug fixes for convert-train-checkpoint-to-gguf
    
    * temporarily add code to write old checkpoint files
    
    used to verify that old checkpoint files are correctly converted to gguf
    
    * bug fixes for convert-train-checkpoint-to-gguf.py loading checkpoints with opt_version=0
    
    * remove code used to verify correctness of checkpoint file conversion
    
    * remove trailing whitespace
    
    * remove prediction related code
    
    use main for prediction, it is better optimized
    
    * update train-text-from-scratch README.md
    
    * fix non-windows GGML_ALIGNED_REALLOC
    
    * add missing blank line at end of file
    
    * remove GGML_ALIGNED_REALLOC and use normal malloc/realloc/free for gguf ctx->kv & ctx->infos
    
    * train : fix compile warnings
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

common/common.cpp
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
examples/gguf/CMakeLists.txt
examples/train-text-from-scratch/README.md
examples/train-text-from-scratch/convert-train-checkpoint-to-gguf.py
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-alloc.c
ggml.c
ggml.h
llama.cpp
tests/test-grad0.cpp

commit 43033b7bb4858da4f591715b3babdf906c9b7cbc
Author: slaren <slarengh@gmail.com>
Date:   Mon Aug 28 19:19:18 2023 +0200

    llama-bench : set locale to utf8 (#2832)

examples/llama-bench/llama-bench.cpp

commit 6b73ef120114beb5664ea94aab48d07ed248ee52
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Aug 28 17:59:39 2023 +0200

    YAML result logging + preset script (#2657)

common/common.cpp
common/common.h
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/server/server.cpp
llama.cpp
llama.h
run_with_preset.py

commit 75fafcbcccc280a5b3883bc76d0a2dabf474d094
Author: alonfaraj <alonfaraj@gmail.com>
Date:   Mon Aug 28 18:38:35 2023 +0300

    make : fix tests build (#2855)
    
    * makefile:
    - fix test name
    - add missing tests build
    
    * editorconfig : fixes
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile

commit be475f60af1a54e8de81466ccc907d080cf6df1a
Author: grahameth <96447521+grahameth@users.noreply.github.com>
Date:   Mon Aug 28 17:38:12 2023 +0200

    llama.cpp : fix wrong vsnprintf call in MS compiler (#2856)
    
    Co-authored-by: grahameth <->

llama.cpp

commit 3af6b86301ddfb11bb68e91dfc030b611b0d8426
Author: Ronny Brendel <ronnybrendel@gmail.com>
Date:   Mon Aug 28 14:51:08 2023 +0200

    ggml : tiny ggml_vec_dot_q4_K_q8_K AVX2 improvement (#2819)

k_quants.c

commit 35feac6560387cf0484371af3d9b12bff678e0b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 28 14:24:53 2023 +0300

    ggml : sync (mem align to header + conv_transpose_2d fixes + ggml_alloc) (#2852)
    
    * ggml : sync (mem align to header + conv_transpose_2d fixes)
    
    ggml-ci
    
    * ggml-alloc : minor fix
    
    * ggml-alloc : sync more fixes

ggml-alloc.c
ggml.c
ggml.h

commit 92b1bbd2ec43c82ec0530ba3c8758846c5790c75
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Aug 28 13:23:55 2023 +0200

    CUDA: fix RoPE asserts, block sizes (#2833)

ggml-cuda.cu

commit dd0dc366dab10e8df28d3924e7f313b5c695e908
Author: igarnier <igarnier@protonmail.com>
Date:   Mon Aug 28 10:19:59 2023 +0200

    llama.h : add missing struct keyword for C compat in callback type (#2847)

llama.h

commit f55538c3ccba9b926846ef862fa830cea08c433e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 28 10:59:08 2023 +0300

    metal : fix memory leak (#2762)
    
    * metal : fix memory leak
    
    * metal : fix encoders memory leak
    
    * metal : clean up more memory resources
    
    * metal : fix more leaks
    
    * metal : reuse dispatch queue + autoreleasepool
    
    * metal : reuse array for command buffers and encoders
    
    * ggml : assert for odd number of blocks on ARM
    
    15M tinyllama is an example

ggml-metal.h
ggml-metal.m
ggml.c

commit ebcee207b6058b7f695bb5c203ad87b1066a9790
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Mon Aug 28 02:32:25 2023 -0400

    quantize : make output filename optional again (#2823)
    
    * quantize : make output filename optional again
    
    * quantize : fix path parsing on Windows
    
    suggested by @slaren

examples/quantize/quantize.cpp

commit 3e8ff47af620a31e0810c58a41e4b089145982ef
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Mon Aug 28 07:31:24 2023 +0100

    devops : added systemd units and set versioning to use date. (#2835)
    
    * Corrections and systemd units
    
    * Missing dependency clblast

.devops/llama-cpp-clblast.srpm.spec
.devops/llama-cpp-cublas.srpm.spec
.devops/llama-cpp.srpm.spec

commit 103cfafc774f6feb3172b5d4d39681c965b17eba
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 21:50:22 2023 +0300

    gguf : fix strings to not be null-terminated (#2839)
    
    * gguf : fix strings to not be null-terminated
    
    ggml-ci
    
    * gguf : fix gguf_add_tensor name

ggml.c

commit c10704d01e21e3dbe4d6ca1026ebff85349dd239
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 18:55:41 2023 +0300

    llama : fix MPI threads (close #2827)

llama.cpp

commit 230d46c723edf5999752e4cb67fd94edb19ef9c7
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Sun Aug 27 15:13:31 2023 +0100

    examples : update llama2.c converter to read vocab and write models in GGUF format (#2751)
    
    * llama2.c: direct gguf output (WIP)
    
    * Simplify vector building logic
    
    * llama2.c gguf conversion: fix token types in converter
    
    * llama2.c: support copying vocab from a llama gguf model file
    
    * llama2.c: update default path for vocab model + readme
    
    * llama2.c: use defines for gguf keys
    
    * llama2.c: escape whitespaces w/ U+2581 in vocab converter the llama.cpp way
    
    * llama2.c converter: cleanups + take n_ff from config

examples/convert-llama2c-to-ggml/README.md
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp

commit 463173a6c0ff353055eb90665794884c888c790f
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Aug 27 16:50:33 2023 +0300

    llama : speedup tokenization (#2831)
    
    * Speedup tokenization
    
    On current master it takes ~3.2 seconds to tokenize
    Wikitext. With this change it becomes ~525 ms.
    
    * Fixit: it was missing the piece after the last found occurence
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp
llama.cpp

commit eaa13a48ff4136f01c1cdb79cacd61b67ec53095
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 16:40:48 2023 +0300

    falcon : fix CUDA inference by making K and Q contiguous (#2830)
    
    * falcon : fix CUDA inference by making K and Q contiguous
    
    ggml-ci
    
    * cuda : add assert to guard from non-cont ropes

ggml-cuda.cu
llama.cpp

commit da7455d0467b5f5cc2e45d0dcffaf098df13db63
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 15:52:34 2023 +0300

    readme : fix headings

README.md

commit 25423e9185b7c2a1881ed8f85cc752a12370be9d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 15:24:40 2023 +0300

    scripts : helper convert script

scripts/convert-gg.sh
scripts/qnt-all.sh
scripts/run-all-perf.sh
scripts/run-all-ppl.sh

commit a6d1189fdd4c1ab4ba23f9d777f8950901dcffb2
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Aug 27 15:19:59 2023 +0300

    k_quants tuning for Falcon-7b (#2816)
    
    * Make ggml-cuda.cu build with QK_K = 64
    
    Using LLAMA_CUDA_FORCE_DMMV = ON and -nommq it runs and produces
    a meaningful result.
    
    * k_quants tuning for Falcon-7b
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu
llama.cpp

commit c48c5bb0b06385f6c708339188d2aaf2bc278477
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 14:44:35 2023 +0300

    readme : update hot topics

README.md

commit d0cee0d36d5be95a0d9088b674dbb27354107221
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 14:19:54 2023 +0300

    gguf : add 64-bit support (GGUF v2) (#2821)
    
    * gguf : bump version to 2
    
    * gguf : add support for 64-bit (no backwards comp yet)
    
    * gguf : v1 backwards comp
    
    * gguf.py : bump GGUF version
    
    * gguf.py : uint64_t on all lengths, sizes and counts, enums still uint32_t
    
    * gguf.py : string lengths uint32_t
    
    * gguf : update all counts to 64-bit
    
    * gguf.py : string len uint64_t and n_dims uint32_t
    
    * gguf : fix typo
    
    * llama.cpp : print gguf version
    
    ---------
    
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>

examples/gguf/gguf.cpp
ggml.c
ggml.h
gguf-py/gguf/gguf.py
llama.cpp

commit edd4c1481708fcd788b0e423268304fd26e2b125
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Aug 27 14:19:19 2023 +0300

    llama : more tokenizer fixes (#2810)
    
    * tests : write a Python tokenizer test (wip)
    
    * llama : prefix input text for tokenization with whitespace
    
    * llama : distinguish pieces from decoded text + fix detokenization
    
    * common : add comments
    
    * examples : no longer manually add leading space when tokenizing
    
    * tests : use Python to generate tokenizer tests for C++
    
    * tests : add option to tokenize text files
    
    ggml-ci
    
    * tests : add test-tokenizer-1.py
    
    * llama.cpp : fix LF token
    
    * hellaswag : move the concat space for clarity
    
    * tests : add falcon tests (py + cpp, currently do not pass Unicode)
    
    ggml-ci
    
    * common : temporary separate llama_detokenize calls for SPM and BPE
    
    ---------
    
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>

common/common.cpp
common/common.h
examples/beam_search/beam_search.cpp
examples/embd-input/embd-input-lib.cpp
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/save-load-state/save-load-state.cpp
examples/server/server.cpp
examples/simple/simple.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
llama.cpp
llama.h
tests/CMakeLists.txt
tests/test-tokenizer-0-falcon.cpp
tests/test-tokenizer-0-falcon.py
tests/test-tokenizer-0-llama.cpp
tests/test-tokenizer-0-llama.py
tests/test-tokenizer-0.cpp
tests/test-tokenizer-1.cpp

commit 1591e2e590762011b43b10a9b6e04f13f98f2aa5
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Sun Aug 27 10:10:25 2023 +0200

    ggml : detect SSSE3 (#2825)
    
    * ggml : add ggml_cpu_has_ssse3
    
    * llama : show SSSE3 in system info

ggml.c
ggml.h
llama.cpp

commit 789c8c945a2814e1487e18e68823d9926e3b1454
Author: slaren <slarengh@gmail.com>
Date:   Sun Aug 27 09:03:27 2023 +0200

    ci : add LoRA test to CI (#2650)
    
    * ci : add lora test
    
    ggml-ci
    
    * move lora summary to the top, add lora logs
    
    ggml-ci
    
    * ci : decrease CPU ppl runs to 2 to avoide 20 min timeout
    
    ggml-ci
    
    * add 7b lora test
    
    use 1 thread for CUDA generation tests
    
    ggml-ci
    
    * add test with q8_0 (cpu only)
    
    ggml-ci
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ci/run.sh

commit c1ac54b77aaba10d029084d152be786102010eb2
Author: Bruce MacDonald <brucewmacdonald@gmail.com>
Date:   Sat Aug 26 16:11:45 2023 -0700

    server : add `/detokenize` endpoint (#2802)
    
    * Add a /detokenize endpoint to the example server
    
    * remove trailing white-space

examples/server/README.md
examples/server/server.cpp

commit 730d9c681e339b76407659344e5a2cd50af7d7d5
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Aug 26 14:13:36 2023 -0600

    convert.py : advanced option (#2753)
    
    * Allow convert.py to convert to q8_0
    
    Fix issue with bounded_parallel_map and greedy consuming iterator
    
    Display elapsed time during conversion
    
    * Add --concurrency option
    
    Minor improvements to help text
    
    Clean up bounded_parallel_map function a bit
    
    * Massive speed improvement thanks to Cebtenzzre
    
    * Refactor types

convert.py

commit c7d92e6dfec3f54849f3a0ba373054d29f321ea2
Author: Tim Miller <drasticactions@users.noreply.github.com>
Date:   Sun Aug 27 03:27:07 2023 +0900

    llama : use Unicode Escape Sequence to replace encoded characters (#2814)
    
    The use of special characters within source files can break compiling on some computers with different region and language settings. Using Unicode escape sequences should allow for the code to be compiled on all setups without needing to change your computers settings or switch regions.

llama.cpp

commit 61d1a2895eeca55e0c8b7018492f6ab9c90cff78
Author: Tungsten842 <quantmint@protonmail.com>
Date:   Sat Aug 26 20:19:44 2023 +0200

    flake.nix : add rocm support and cleanup (#2808)

flake.lock
flake.nix

commit 741ca7dd1cec0a0349494742b9083d6ef4cd73c5
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Sat Aug 26 14:17:51 2023 -0400

    llama : move #includes out of _GNU_SOURCE conditional (#2817)

llama.cpp

commit 72f895c923ba98b8f2af294440206f35915c0501
Author: Dr. Tom Murphy VII Ph.D <499244+tom7@users.noreply.github.com>
Date:   Sat Aug 26 14:12:56 2023 -0400

    main : fix bug (penalize_nl=false doesn't work) + suppress warning on mingw (#1528)
    
    * Fix bug in main.cpp where penalize_nl=false has no effect. It modifies the underlying logits array, but at this point we are already working on the candidates copy.
    
    * Suppress redefinition warning for NOMINMAX on mingw. In my installation, this macro is already defined by /usr/lib/gcc/x86_64-w64-mingw32/11/include/c++/x86_64-w64-mingw32/bits/os_defines.h:45.
    
    * main : fix indentation
    
    * main : pass ctx to llama_token_nl()
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/main/main.cpp

commit 50526f37eba0b28336700890242ff282b949cd83
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Sat Aug 26 12:53:52 2023 -0400

    llama : use std::abs in llama_sample_tail_free (#2800)
    
    Plain 'abs' casts the input to int.

llama.cpp

commit 04f4b1eb10f3e25750ca3e530265ce2841730e6b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Aug 26 17:37:35 2023 +0300

    k-quants : remove unnecessary tensor shape restrictions (#2811)

llama.cpp

commit 7592375403a0bd0456d5ec2cdf8350e591f04fb0
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Aug 26 17:27:49 2023 +0300

    Better perplexity for 2- and 3-bit quantization for LLaMA-v2-70B (#2807)
    
    * Better perplexity for 2- and 3-bit quantization for the 70B model
    
    * PR comment
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit 771551a793c9976ed9cdfe7b8c69536af32af9f9
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Aug 26 16:48:53 2023 +0300

    Fix HellaSwag (#2805)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp

commit f305bad11e10ad09e396faed2e37f4f845f5d566
Author: Volodymyr Vitvitskyi <72226+signalpillar@users.noreply.github.com>
Date:   Sat Aug 26 14:25:39 2023 +0100

    flake : build llama.cpp on Intel with nix (#2795)
    
    Problem
    -------
    `nix build` fails with missing `Accelerate.h`.
    
    Changes
    -------
    - Fix build of the llama.cpp with nix for Intel: add the same SDK frameworks as
    for ARM
    - Add `quantize` app to the output of nix flake
    - Extend nix devShell with llama-python so we can use convertScript
    
    Testing
    -------
    Testing the steps with nix:
    1. `nix build`
    Get the model and then
    2. `nix develop` and then `python convert.py models/llama-2-7b.ggmlv3.q4_0.bin`
    3. `nix run llama.cpp#quantize -- open_llama_7b/ggml-model-f16.gguf ./models/ggml-model-q4_0.bin 2`
    4. `nix run llama.cpp#llama -- -m models/ggml-model-q4_0.bin -p "What is nix?" -n 400 --temp 0.8 -e -t 8`
    
    Co-authored-by: Volodymyr Vitvitskyi <volodymyrvitvitskyi@SamsungPro.local>

flake.nix

commit a2ca4e9de9da45ed0bb1c34935d5ec80cebc22d5
Author: Nigel Bosch <pnigelb@gmail.com>
Date:   Sat Aug 26 07:11:17 2023 -0500

    Handle null rope scaling value (#2793)

convert.py

commit 2ba83c8685177faea3399db9564f9c52df75c366
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Sat Aug 26 13:45:53 2023 +0200

    Fix spm whitespaces (#2806)
    
    * llama.cpp : fix spm whitespace escaping + clean up
    
    * main.cpp : spm - add whitespace in front of prompt
    
    * test-tokenizer-0.cpp : spm - add whitespace in front of prompt

examples/main/main.cpp
llama.cpp
tests/test-tokenizer-0.cpp

commit bae5c5f679e043371bc2b4dffff8d4964d6cb953
Author: lon <114724657+longregen@users.noreply.github.com>
Date:   Sat Aug 26 10:07:43 2023 +0200

    examples : skip unnecessary external lib in server README.md how-to (#2804)

examples/server/README.md

commit 232caf3c1581a6cb023571780ff41dc2d66d1ca0
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Fri Aug 25 09:17:15 2023 -0700

    llama : fix struct decl (#2790)

llama.h

commit d046dcee081118c9071bbc63dacdb359a58c467a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Aug 25 19:05:02 2023 +0300

    Faster perplexity computation (#2786)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp

commit c82742ac9cd96fd34aa961978805c1d8a361d589
Author: Matt Pulver <matt.pulver@heavy.ai>
Date:   Fri Aug 25 11:18:48 2023 -0400

    llama : add llama_beam_search() (#2267)
    
    * Add llama_beam_search().
    
    * Add '// Beam search' heading to llama.{h,cpp} after llama_grammar_accept_token().
    
    * Add space around * pointers and & references.
    
    * Add spaces around comparison and assignment operators.
    
    * Prefer west const.
    
    * Use llama_ prefix for structs in global namespace.
    
    * Delete obsolete comment from an earlier revision.
    
    * Change eos to eob in llama_beam and llama_beam_view structs.

common/common.h
examples/CMakeLists.txt
examples/beam_search/CMakeLists.txt
examples/beam_search/beam_search.cpp
examples/server/server.cpp
llama.cpp
llama.h

commit 28b2c996ca0ab90a5669946084f13443ec98e241
Author: Nigel Bosch <pnigelb@gmail.com>
Date:   Fri Aug 25 09:41:52 2023 -0500

    convert.py : Get rope scale from HuggingFace models (#2772)
    
    * Get rope scale from HF models
    
    * Save rope scale only for linear scaling
    
    * Rewrite for clarity

convert.py

commit 154725c5436808e5c519685d0279e850596dbe62
Author: slaren <slarengh@gmail.com>
Date:   Fri Aug 25 15:16:19 2023 +0200

    llama-bench : add model sizes (#2771)
    
    * llama-bench : add model sizes
    
    * more compact markdown output
    
    * back to GiB
    
    * adjust column sizes

examples/llama-bench/llama-bench.cpp
llama.cpp
llama.h

commit 12e2e33a977af73e75885eeee91c5575a77f4e5f
Author: slaren <slarengh@gmail.com>
Date:   Fri Aug 25 14:08:53 2023 +0200

    convert.py : export rope freq_base when converting CodeLlama from an HF model (#2773)

convert.py

commit 29674ab4e847fcaba60cc6558f0d46d5f74ae279
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Fri Aug 25 18:32:45 2023 +0800

    server : display token probabilities in the UI (#2489)
    
    * server : add n_probs param in chat UI
    
    * server : keep message data array & show in probabilites component
    
    * server : add simple popover component
    
    * server : fix completion_probabilities undefined if not set n_probs
    
    * server : implement Probabilites
    
    * server : handle bytes
    
    * server : make n_probs max to 10 for easy scroll
    
    * server : adjust for dark/light mode
    
    * server : Fix regenerated prompt
    
    * server : update index.html.hpp
    
    * server : convert prob to percentage + show original value as div title
    
    * server : fix Probabilites not used if included empty str
    
    * server : skip byte pair in display probabilites
    
    * server : remove array check of completion_probabilities in messages
    
    * skip empty array or byte pair (> 1) in Probabilites
    
    * generate index.html.hpp
    
    * fix incorrect prob convert if the str is already a known token
    
    * use final response to show probabilities on stop
    
    * revert unnecessary change
    
    * correct probabilites usage
    
    * remove unused function
    
    * always send partial response for get correct probs of last to_send
    
    * fix typo
    
    * fix content of format_final_response
    
    * refactor probs render & make pColor transparent if not found
    
    * send empty string when got stop_pos in partial
    
    * avoid unnecessary empty data event & send rest of partial tokens on stop
    
    * use <br /> for new line
    
    * skip -1 tok in loop to avoid send '' on end
    
    * trim last new lines on stop
    
    * revert unnecessary change

examples/server/index.html.hpp
examples/server/public/index.html
examples/server/server.cpp

commit 5439a0ab57c16b556ffa91a0953df5e46b1e7fb4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 25 13:03:25 2023 +0300

    ci : pip install gguf in editable mode (#2782)
    
    ggml-ci

ci/run.sh

commit 8194cd8772c58b8a43aa07a2fc468f5366d7e320
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Fri Aug 25 12:43:41 2023 +0300

    gguf : export objects to user code (#2780)
    
    * gguf export more objects to user code
    
    * gguf export all objects to user code for now
    
    * gguf : bump version

gguf-py/gguf/__init__.py
gguf-py/pyproject.toml

commit 6bbc598a632560cb45dd2c51ad403bda8723b629
Author: Henri Vasserman <henv@hot.ee>
Date:   Fri Aug 25 12:09:42 2023 +0300

    ROCm Port (#1087)
    
    * use hipblas based on cublas
    * Update Makefile for the Cuda kernels
    * Expand arch list and make it overrideable
    * Fix multi GPU on multiple amd architectures with rocblas_initialize() (#5)
    * add hipBLAS to README
    * new build arg LLAMA_CUDA_MMQ_Y
    * fix half2 decomposition
    * Add intrinsics polyfills for AMD
    * AMD assembly optimized __dp4a
    * Allow overriding CC_TURING
    * use "ROCm" instead of "CUDA"
    * ignore all build dirs
    * Add Dockerfiles
    * fix llama-bench
    * fix -nommq help for non CUDA/HIP
    
    ---------
    
    Co-authored-by: YellowRoseCx <80486540+YellowRoseCx@users.noreply.github.com>
    Co-authored-by: ardfork <134447697+ardfork@users.noreply.github.com>
    Co-authored-by: funnbot <22226942+funnbot@users.noreply.github.com>
    Co-authored-by: Engininja2 <139037756+Engininja2@users.noreply.github.com>
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
    Co-authored-by: jammm <2500920+jammm@users.noreply.github.com>
    Co-authored-by: jdecourval <7315817+jdecourval@users.noreply.github.com>

.devops/full-rocm.Dockerfile
.devops/main-rocm.Dockerfile
.dockerignore
.gitignore
CMakeLists.txt
Makefile
README.md
common/common.cpp
examples/llama-bench/llama-bench.cpp
ggml-cuda.cu
ggml-cuda.h
llama.cpp

commit 3f460a2b723c8b936ac29ecfd02f244b3adeba55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 25 11:55:59 2023 +0300

    cuda : add RoPE kernel for mode == 2 (NeoX) (#2760)
    
    * cuda : add RoPE kernel for mode == 2 (NeoX)
    
    * falcon : do not offload the embeddings layer

ggml-cuda.cu
llama.cpp

commit 87e3733f24a85d894cc16e1cbdfa1ea1e81a76f3
Author: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
Date:   Fri Aug 25 09:26:05 2023 +0300

    gguf : make gguf pip-installable
    
    * gitignore : add dist and rm pyproject.toml
    
    * gguf: prepare as Pip package
    
    * gguf: prepare as Pip package
    
    * gguf : fix line endings
    
    * requirements : add gguf
    
    * gguf : update readme with build notes
    
    * gguf : update readme with build notes
    
    * gguf : add notes for tests

.gitignore
gguf-py/LICENSE
gguf-py/README.md
gguf-py/gguf/__init__.py
gguf-py/gguf/gguf.py
gguf-py/pyproject.toml
gguf-py/tests/test_gguf.py
requirements.txt

commit b91ad7f46134d0d051dc516eb59a76f402de55c2
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Fri Aug 25 01:58:00 2023 -0400

    ggml-alloc : enlarge size of parse_seq (#2776)
    
    Since we also store barriers in this array, we need to double its size.

ggml-alloc.c

commit 2e5f70a25fc4576e9ed78603fe493eb7702c37a3
Author: Marcus Dunn <51931484+MarcusDunn@users.noreply.github.com>
Date:   Thu Aug 24 14:49:30 2023 -0700

    Added `enum` to `llama_token_get_type` return type (#2774)

llama.h

commit d0f77b1353fc820d1ff1e6b87bc6bedde315938d
Author: slaren <slarengh@gmail.com>
Date:   Thu Aug 24 21:10:39 2023 +0200

    convert.py : try to determine n_ctx automatically for CodeLlama (#2770)

convert.py

commit 0d3094f0c742ce61f84feb6e4f0b59beee6194d7
Author: slaren <slarengh@gmail.com>
Date:   Thu Aug 24 20:04:05 2023 +0200

    gguf : add rope_freq_base parameter for CodeLlama (#2769)

convert.py
gguf.py
llama.cpp

commit 01f2224682b08185af609b28b1268b95c8b4cfa2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 24 19:58:30 2023 +0300

    falcon : write file type

convert-falcon-hf-to-gguf.py
scripts/run-all-ppl.sh

commit 38b16dfca6e5032e6cfb90c1653bf1ba4cf647b4
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Thu Aug 24 12:27:25 2023 -0400

    metal : bug-fix when enable ggml-alloc (#2757)
    
    * metal: better memory alloc w/ concurrency dispatch
    
    The ggml-alloc should only free tensors at memory barriers.
    
    * ggml-alloc: avoid return silently
    
    In certain cases, the allocate_node() function may silently return
    without performing any memory allocation.

ggml-alloc.c
llama.cpp

commit 8f8c28e89cb9531211783da697d6e7c445e2af1d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 24 19:26:19 2023 +0300

    convert : auto-determine model name based on dir + scripts update

convert.py
scripts/perf-run-all.sh
scripts/ppl-run-all.sh
scripts/qnt-all.sh
scripts/run-all-perf.sh
scripts/run-all-ppl.sh

commit 7694adda8d1111b3cf758ad6c91d754a0a4cacff
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Aug 24 10:11:13 2023 -0600

    Fix for main example getting stuck when -n -2 and --interactive (#2767)
    
    * Fix for main example getting stuck when -n -2 and --interactive
    
    * Add a comment so future generations may suffer less.

examples/main/main.cpp

commit fea95c682d0028fdd25853bea58035794a0c964d
Author: slaren <slarengh@gmail.com>
Date:   Thu Aug 24 17:44:11 2023 +0200

    fix convert.py for codellama, add llama 34B to the list of recognized models (#2768)

convert.py
llama.cpp

commit ef955fbd230c571cc1cda0d19baaeec347523175
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu Aug 24 06:58:02 2023 -0700

    Tag release with build number (#2732)
    
    * Modified build.yml to use build number for release
    
    * Add the short hash back into the tag
    
    * Prefix the build number with b

.github/workflows/build.yml

commit d67777c202c03bcb74372690599ef3c03affb3ba
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 24 16:19:57 2023 +0300

    metal : add Q8_0 support (#2763)
    
    * metal : add dequantize_q8_0 kernel
    
    * metal : add mul_mat_q8_0_f32 kernel
    
    * metal : add Q8_0 mul_mm kernel

ggml-metal.m
ggml-metal.metal

commit c3e53b421a9910548be0345f85712c535f467a98
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 24 12:26:01 2023 +0300

    llama : escape all U+2581 in a string (#2750)

llama.cpp

commit 6e91a1b0706c2e0e52b9d9be7ee82d3c1e7a33c1
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Thu Aug 24 00:07:13 2023 -0400

    llama : fix grammar sometimes generating null char (#2756)

llama.cpp

commit 44d5462b5cddc1c5cbcd7647646f7b55b175b01f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 23:44:19 2023 +0300

    readme : fix link

README.md

commit c7868b075377c8c3fa916ea7c1aca600f44bed55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 23:43:00 2023 +0300

    minor : fix trailing whitespace

README.md

commit 79da24b58c1ea72340e64f799a4717d372207676
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 23:41:16 2023 +0300

    readme : update hot topics

README.md

commit cf658adc832badaaa2ca119fe86070e5a830f8f6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 23:08:04 2023 +0300

    llm : add Falcon support (#2717)
    
    * llama : refactor GGUF constants into static maps
    
    * llama : check if model architecture is known
    
    * llama : refactor llama_model_load_internal()
    
    * gguf : add KV constant maps
    
    * llm : read arch-specific KVs
    
    * convert : add dummy scores + types
    
    * falcon : load tensor data (CPU only)
    
    * llama : fix loading progress bar
    
    * llama : add arch member to llama_model
    
    * falcon : CPU inference working
    
    * falcon : support non-40B models
    
    * falcon : minor
    
    * llama : minor updates
    
    ggml-ci
    
    * convert-falcon-hf-to-gguf.py : fix special token mapping
    
    * llama.cpp : llama default UNK token = id 0
    
    * llama.cpp : fix bpe tokenizer
    
    * llama.cpp : fix the fix of bpe tokenizer
    
    * ggml : pass eps to ggml_norm
    
    * metal : implement RoPE (mode = 2) + avoid ggml_repeat
    
    * ggml : ggml_repeat always creates new tensor
    
    * falcon : copy-paste self-attention from LLaMA
    
    * metal : print extra compute pipeline info
    
    * falcon : minor changes (still chasing the Metal problem)
    
    * llama.cpp : fix linefeed token
    
    * metal : fix GELU kernel numerical stability by using precise::tanh
    
    * metal : temporary workaround for the concurrency optimization bug
    
    * falcon : add CUDA offloading (#2739)
    
    * llama : better model naming and size reporting
    
    * llama : prep new tokenizer support
    
    * llama : advanced BPE tokenizer based on ggllm.cpp imlpementation
    
    * llama : remove oboslete comment
    
    ggml-ci
    
    * common : remove obsolete BPE API + disable test-tokenizer-1
    
    * llama : revert BPE special-case in llama_byte_to_token()
    
    * cuda : add TODOs for RoPE NeoX implementation
    
    * llama : default special tokens based on vocab type
    
    * perplexity : add log for start of tokenization
    
    ---------
    
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>
    Co-authored-by: slaren <slarengh@gmail.com>

common/common.cpp
common/common.h
convert-falcon-hf-to-gguf.py
convert.py
examples/main/main.cpp
examples/perplexity/perplexity.cpp
ggml-alloc.c
ggml-alloc.h
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
gguf.py
llama.cpp
llama.h
tests/CMakeLists.txt
tests/test-tokenizer-1.cpp

commit a192860cfec89a38d59a943623bf595b1fe4495b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Aug 23 22:37:39 2023 +0300

    minor : fix trailing whitespace

examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp

commit 95385241a91a616788a3bb76d12c9b7b2379ca2d
Author: Olivier Chafik <ochafik@users.noreply.github.com>
Date:   Wed Aug 23 20:33:05 2023 +0100

    examples : restore the functionality to import llama2.c models (#2685)
    
    * Fix import of llama2.c models that don't share weights between embedding layers
    
    * llama2c: reinstate ggmlv3 conversion output + update readme w/ gguf conv
    
    * llama2.c: comment out legacy "load from ggml model" logic
    
    * llama2.c: convert special-cased "<0xXX>" single byte tokens from tokenizer.bin

examples/convert-llama2c-to-ggml/README.md
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp

commit 335acd2ffd7b04501c6d8773ab9fcee6e7bf8639
Author: slaren <slarengh@gmail.com>
Date:   Wed Aug 23 16:46:54 2023 +0200

    fix convert-lora-to-ggml.py (#2738)

convert-lora-to-ggml.py

commit 5290c38e6e9b66ee2b543e560e301c1a1a90929c
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Wed Aug 23 16:46:03 2023 +0200

    main : insert bos if no tokens (#2727)
    
    * main.cpp : insert bos if no tokens
    
    * Update examples/main/main.cpp
    
    * Update examples/main/main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/main/main.cpp

commit cc34dbda9681418a2b18382446b90cdcec398d82
Author: akawrykow <142945436+akawrykow@users.noreply.github.com>
Date:   Wed Aug 23 07:31:34 2023 -0700

    gitignore : fix for windows (#2729)

.gitignore

commit 7c2227a1972a4add4b5c118e4914c086513d0382
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Aug 23 10:29:09 2023 -0400

    chmod : make scripts executable (#2675)

ci/run.sh
convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-llama-7b-pth-to-gguf.py
convert-llama-ggmlv3-to-gguf.py
convert-llama-hf-to-gguf.py
convert-lora-to-ggml.py
convert.py
examples/embd-input/embd_input.py
examples/embd-input/llava.py
examples/embd-input/minigpt4.py
examples/embd-input/panda_gpt.py
examples/jeopardy/graph.py
examples/jeopardy/jeopardy.sh
examples/json-schema-to-grammar.py
examples/make-ggml.py
examples/reason-act.sh
examples/server-llama2-13B.sh
examples/server/api_like_OAI.py
examples/server/chat-llama2.sh
examples/server/chat.sh
gguf.py
scripts/get-wikitext-2.sh

commit f19dca04ea5fbf9a0b2753091d93464585d5c73b
Author: JohnnyB <jboero@users.noreply.github.com>
Date:   Wed Aug 23 15:28:22 2023 +0100

    devops : RPM Specs (#2723)
    
    * Create llama-cpp.srpm
    
    * Rename llama-cpp.srpm to llama-cpp.srpm.spec
    
    Correcting extension.
    
    * Tested spec success.
    
    * Update llama-cpp.srpm.spec
    
    * Create lamma-cpp-cublas.srpm.spec
    
    * Create lamma-cpp-clblast.srpm.spec
    
    * Update lamma-cpp-cublas.srpm.spec
    
    Added BuildRequires
    
    * Moved to devops dir

.devops/lamma-cpp-clblast.srpm.spec
.devops/lamma-cpp-cublas.srpm.spec
.devops/llama-cpp.srpm.spec

commit 8207214b6a37a46526cee9e72d4c9092b9d1872f
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Aug 23 12:57:12 2023 +0300

    Fix values shown in the quantize tool help (#2735)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/quantize/quantize.cpp

commit 62959e740e8759d246ac8d09036950efde09981c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Aug 23 12:56:42 2023 +0300

    Strided perplexity (#2714)
    
    * Implementing strided computation of perplexity
    
    * Alternative way to output PPL results
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

common/common.cpp
common/common.h
examples/perplexity/perplexity.cpp

commit 7f7ddd5002040804e33fcdbde44aa22f8635f57d
Author: IgnacioFDM <ignaciofdm@gmail.com>
Date:   Wed Aug 23 06:31:09 2023 -0300

    Fix ggml to gguf conversion on Windows (#2733)
    
    This fixes `RuntimeWarning: overflow encountered in long_scalars`
    
    Credit: anon (not mine)

convert-llama-ggmlv3-to-gguf.py

commit b8ad1b66b23f9b2e6e4531e9a62753323036a556
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Wed Aug 23 02:12:12 2023 -0500

    server : allow json array in prompt or content for direct token input (#2306)
    
    * server: allow json array in prompt or content
    
    We accept an array of strings and numbers representing tokens,
    in addition to the current string valued prompt or content.
    
    This allows direct token input, so that any special tokens
    can be processed and used at the frontend during the construction
    of the json data, before sending to the server. And the server
    does not need to know or parse special tokens from textual input.
    
    With this, we can use EOS and BOS used in llama-2-chat models.
    
    * server: use tokenizePrompt(json) and default "" if empty prompt
    
    * server: fix prompt check
    
    * server: tokenize endpoint no longer adds BOS

examples/server/README.md
examples/server/server.cpp

commit f5fe98d11bdf9e7797bcfb05c0c3601ffc4b9d26
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Tue Aug 22 21:01:57 2023 -0400

    docs : add grammar docs (#2701)
    
    * docs : add grammar docs
    
    * tweaks to grammar guide
    
    * rework GBNF example to be a commented grammar

README.md
examples/main/README.md
grammars/README.md

commit 777f42ba18b29f25c71ff8de3ecf97b8017304c0
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Tue Aug 22 17:39:39 2023 -0600

    Improve handling of special tokens in GGML to GGUF converter (#2725)
    
    * Improve UNK, BOS, EOS token handling when converting without metadata.
    
    * Allow importing as a module.
    
    * Remove some obsolete code and minor cleanups.
    
    * Set default UNK token mapping from -1 to 0 in llama.cpp
    
    * Try to handle overflow due to buggy Windows Python with a better error message

convert-llama-ggmlv3-to-gguf.py
llama.cpp

commit 46ef5b5fcf4c366e1fb27726b6394adbbf8fd0ea
Author: goerch <jhr.walter@t-online.de>
Date:   Tue Aug 22 23:10:42 2023 +0200

    llama : fix whitespace escaping in tokenizer (#2724)

llama.cpp
tests/test-tokenizer-0.cpp
tests/test-tokenizer-1.cpp

commit c63bb1d16a70c03440671b76954bb767513cead8
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Aug 22 22:47:05 2023 +0200

    CUDA: use mul_mat_q kernels by default (#2683)

common/common.cpp
common/common.h
examples/server/server.cpp
ggml-cuda.cu

commit 3b6cfe7c927df178ca3c11643c3ec93e143471c9
Author: Alex Petenchea <alex.petenchea@gmail.com>
Date:   Tue Aug 22 21:58:16 2023 +0300

    convert.py : clarifying error message (#2718)

convert.py

commit 800c9635b4a9390126f397870f3a825fc7455bd1
Author: Jiahao Li <liplus17@163.com>
Date:   Wed Aug 23 02:27:06 2023 +0800

    Fix CUDA softmax by subtracting max value before exp (#2665)

ggml-cuda.cu

commit deb7dfca4b9725cd295d1426db75fe8e0a6d5312
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 22 20:05:59 2023 +0300

    gguf : add ftype meta info to the model (#2710)
    
    * llama : add ftype meta info to the model
    
    ggml-ci
    
    * convert.py : add ftype when converting (does not work)
    
    * convert.py : fix Enum to IntEnum
    
    ggml-ci

convert.py
gguf.py
llama.cpp
llama.h

commit bac66994cf356cf488078c056831396eb4ce31d5
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Aug 22 19:14:09 2023 +0300

    Quantization imrovements for k_quants (#2707)
    
    * Improve LLaMA-2 2-, 3- and 4-bit quantization
    
    * Q3_K_S: use Q5_K for 1st 2 layers of attention.wv and feed_forward.w2
    * Q4_K_S: use Q6_K for 1st 2 layers of attention.wv and feed_forward.w2
    * Q2_K and Q3_K_M: use Q5_K instead of Q4_K for 1st 2 layers of
      attention.wv and feed_forward.w2
    
    This leads to a slight model sized increase as follows:
    Q2_K  : 2.684G vs 2.670G
    Q3_K_S: 2.775G vs 2.745G
    Q3_K_M: 3.071G vs 3.057G
    Q4_K_S: 3.592G vs 3.563G
    
    LLaMA-2 PPL for context 512 changes as follows:
    Q2_K  : 6.6691 vs 6.8201
    Q3_K_S: 6.2129 vs 6.2584
    Q3_K_M: 6.0387 vs 6.1371
    Q4_K_S: 5.9138 vs 6.0041
    
    There are improvements for LLaMA-1 as well, but they are
    way smaller than the above.
    
    * Minor 4-bit quantization improvement
    
    For the same model size as previus commit, we get
    PPL = 5.9069 vs 5.9138.
    
    * Some more fine tuning
    
    * Adding make_qkx2_quants
    
    With it, we get PPL = 5.8828 for L2-7B Q4_K_S.
    
    * Another minor improvement
    
    * Q2_K improvement
    
    Smaller model, lower perplexity.
     7B: file size = 2.632G, PPL = 6.3772 vs original 2.670G PPL = 6.8201
    12B: file size = 5.056G, PPL = 5.4577 vs original 5.130G PPL = 5.7178
    
    It is mostly Q3_K except for tok_embeddings, attention.wq, attention.wk,
    which are Q2_K
    
    * Iterating
    
    * Revert Q5_K back to make_qkx1_quants
    
    * Better Q6_K
    
    * make_qkx2_quants is better for Q5_K after all
    
    * Fix after rebasing on master
    
    * Fix for changed tensor names
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

k_quants.c
llama.cpp

commit 519c981f8b65ee6c87c2965539685ced0a17223b
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 22 16:03:12 2023 +0200

    embedding : evaluate prompt in batches (#2713)

examples/embedding/embedding.cpp

commit 1123f7fbdfb8012e46f05e903e6f675922916378
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 22 15:25:19 2023 +0200

    ggml-cuda : use graph allocator (#2684)
    
    use a different function for no_alloc to avoid breaking backwards compat, fixes lora
    
    remove 512 n_batch limit
    
    fixed 2048 batch size
    
    cleanup
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

common/common.cpp
ggml-cuda.cu
ggml-cuda.h
llama.cpp

commit ef3f333d3775600d1646a9fa249aca532d15fb89
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 22 14:22:08 2023 +0300

    ggml : sync latest (SAM + SD operators, CUDA alibi) (#2709)
    
    * ggml : sync latest (SAM + SD operators, CUDA alibi)
    
    ggml-ci
    
    * ggml : fix tabs

examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-alloc.c
ggml-cuda.cu
ggml.c
ggml.h
scripts/sync-ggml.sh

commit 8e4364f2af9cd5d57240f23e83c0e29bc068bc02
Author: slaren <slarengh@gmail.com>
Date:   Tue Aug 22 09:56:03 2023 +0200

    llama-bench : minor fixes (#2695)

examples/llama-bench/llama-bench.cpp

commit 1e3bc523d8053a77df3ac7126a84d0297ee97ef6
Author: Kylin <56434533+KyL0N@users.noreply.github.com>
Date:   Tue Aug 22 15:14:23 2023 +0800

    ggml : support CUDA's half type for aarch64(#1455) (#2670)
    
    * ggml: support CUDA's half type for aarch64(#1455)
    support CUDA's half type for aarch64 in ggml_fp16_t definition
    
    * ggml: use __CUDACC__ to recognise nvcc compiler

ggml.h

commit 14b1d7e6f720dee41ce5a826376df738096d9033
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Tue Aug 22 02:18:40 2023 -0400

    metal : add missing barriers for mul-mat (#2699)

ggml-metal.metal

commit 226255b44ef2c2794bfac48d101d35a9c2dbb965
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Tue Aug 22 08:32:00 2023 +0800

    server : fallback to default if client param is null (#2688)
    
    * server : fallback to default if client param is null
    
    * server : do not overwrite 404 if status is 500 from exception_handler

examples/server/server.cpp

commit 930523c8e1cbbee5449c055daa894917fac6805e
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Mon Aug 21 18:01:34 2023 -0600

    Fix convert-llama-ggmlv3-to-gguf.py vocab conversion (#2698)
    
    When converting without metadata, the hex value for bytes entries weren't 0 padded to 2 digits.

convert-llama-ggmlv3-to-gguf.py

commit c8dba409e6d6a754090f08e6a862c5ffdd52e421
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 21 23:40:22 2023 +0300

    py : remove obsolete script

convert-pth-to-ggml.py

commit 6381d4e110bd0ec02843a60bbeb8b6fc37a9ace9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 21 23:07:43 2023 +0300

    gguf : new file format with flexible meta data (beta) (#2398)
    
    * gguf : first API pass
    
    * gguf : read header + meta data
    
    * gguf : read tensor info
    
    * gguf : initial model loading - not tested
    
    * gguf : add gguf_get_tensor_name()
    
    * gguf : do not support passing existing ggml_context to gguf_init
    
    * gguf : simplify gguf_get_val
    
    * gguf : gguf.c is now part of ggml.c
    
    * gguf : read / write sample models
    
    * gguf : add comments
    
    * refactor : reduce code duplication and better API (#2415)
    
    * gguf : expose the gguf_type enum through the API for now
    
    * gguf : add array support
    
    * gguf.py : some code style changes
    
    * convert.py : start a new simplified implementation by removing old stuff
    
    * convert.py : remove GGML vocab + other obsolete stuff
    
    * GGUF : write tensor (#2426)
    
    * WIP: Write tensor
    
    * GGUF : Support writing tensors in Python
    
    * refactor : rm unused import and upd todos
    
    * fix : fix errors upd writing example
    
    * rm example.gguf
    
    * gitignore *.gguf
    
    * undo formatting
    
    * gguf : add gguf_find_key (#2438)
    
    * gguf.cpp : find key example
    
    * ggml.h : add gguf_find_key
    
    * ggml.c : add gguf_find_key
    
    * gguf : fix writing tensors
    
    * gguf : do not hardcode tensor names to read
    
    * gguf : write sample tensors to read
    
    * gguf : add tokenization constants
    
    * quick and dirty conversion example
    
    * gguf : fix writing gguf arrays
    
    * gguf : write tensors one by one and code reuse
    
    * gguf : fix writing gguf arrays
    
    * gguf : write tensors one by one
    
    * gguf : write tensors one by one
    
    * gguf : write tokenizer data
    
    * gguf : upd gguf conversion script
    
    * Update convert-llama-h5-to-gguf.py
    
    * gguf : handle already encoded string
    
    * ggml.h : get array str and f32
    
    * ggml.c : get arr str and f32
    
    * gguf.py : support any type
    
    * Update convert-llama-h5-to-gguf.py
    
    * gguf : fix set is not subscriptable
    
    * gguf : update convert-llama-h5-to-gguf.py
    
    * constants.py : add layer norm eps
    
    * gguf.py : add layer norm eps and merges
    
    * ggml.h : increase GGML_MAX_NAME to 64
    
    * ggml.c : add gguf_get_arr_n
    
    * Update convert-llama-h5-to-gguf.py
    
    * add gptneox gguf example
    
    * Makefile : add gptneox gguf example
    
    * Update convert-llama-h5-to-gguf.py
    
    * add gptneox gguf example
    
    * Update convert-llama-h5-to-gguf.py
    
    * Update convert-gptneox-h5-to-gguf.py
    
    * Update convert-gptneox-h5-to-gguf.py
    
    * Update convert-llama-h5-to-gguf.py
    
    * gguf : support custom alignment value
    
    * gguf : fix typo in function call
    
    * gguf : mmap tensor data example
    
    * fix : update convert-llama-h5-to-gguf.py
    
    * Update convert-llama-h5-to-gguf.py
    
    * convert-gptneox-h5-to-gguf.py : Special tokens
    
    * gptneox-main.cpp : special tokens
    
    * Update gptneox-main.cpp
    
    * constants.py : special tokens
    
    * gguf.py : accumulate kv and tensor info data + special tokens
    
    * convert-gptneox-h5-to-gguf.py : accumulate kv and ti + special tokens
    
    * gguf : gguf counterpart of llama-util.h
    
    * gguf-util.h : update note
    
    * convert-llama-h5-to-gguf.py : accumulate kv / ti + special tokens
    
    * convert-llama-h5-to-gguf.py : special tokens
    
    * Delete gptneox-common.cpp
    
    * Delete gptneox-common.h
    
    * convert-gptneox-h5-to-gguf.py : gpt2bpe tokenizer
    
    * gptneox-main.cpp : gpt2 bpe tokenizer
    
    * gpt2 bpe tokenizer (handles merges and unicode)
    
    * Makefile : remove gptneox-common
    
    * gguf.py : bytesarray for gpt2bpe tokenizer
    
    * cmpnct_gpt2bpe.hpp : comments
    
    * gguf.py : use custom alignment if present
    
    * gguf : minor stuff
    
    * Update gptneox-main.cpp
    
    * map tensor names
    
    * convert-gptneox-h5-to-gguf.py : map tensor names
    
    * convert-llama-h5-to-gguf.py : map tensor names
    
    * gptneox-main.cpp : map tensor names
    
    * gguf : start implementing libllama in GGUF (WIP)
    
    * gguf : start implementing libllama in GGUF (WIP)
    
    * rm binary commited by mistake
    
    * upd .gitignore
    
    * gguf : calculate n_mult
    
    * gguf :  inference with 7B model working (WIP)
    
    * gguf : rm deprecated function
    
    * gguf : start implementing gguf_file_saver (WIP)
    
    * gguf : start implementing gguf_file_saver (WIP)
    
    * gguf : start implementing gguf_file_saver (WIP)
    
    * gguf : add gguf_get_kv_type
    
    * gguf : add gguf_get_kv_type
    
    * gguf : write metadata in gguf_file_saver (WIP)
    
    * gguf : write metadata in gguf_file_saver (WIP)
    
    * gguf : write metadata in gguf_file_saver
    
    * gguf : rm references to old file formats
    
    * gguf : shorter name for member variable
    
    * gguf : rm redundant method
    
    * gguf : get rid of n_mult, read n_ff from file
    
    * Update gguf_tensor_map.py
    
    * Update gptneox-main.cpp
    
    * gguf : rm references to old file magics
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : start implementing quantization (WIP)
    
    * gguf : quantization is working
    
    * gguf : roper closing of file
    
    * gguf.py : no need to convert tensors twice
    
    * convert-gptneox-h5-to-gguf.py : no need to convert tensors twice
    
    * convert-llama-h5-to-gguf.py : no need to convert tensors twice
    
    * convert-gptneox-h5-to-gguf.py : simplify nbytes
    
    * convert-llama-h5-to-gguf.py : simplify nbytes
    
    * gptneox-main.cpp : n_layer --> n_block
    
    * constants.py : n_layer --> n_block
    
    * gguf.py : n_layer --> n_block
    
    * convert-gptneox-h5-to-gguf.py : n_layer --> n_block
    
    * convert-llama-h5-to-gguf.py : n_layer --> n_block
    
    * gptneox-main.cpp : n_layer --> n_block
    
    * Update gguf_tensor_map.py
    
    * convert-gptneox-h5-to-gguf.py : load model in parts to save memory
    
    * convert-llama-h5-to-gguf.py : load model in parts to save memory
    
    * convert : write more metadata for LLaMA
    
    * convert : rm quantization version
    
    * convert-gptneox-h5-to-gguf.py : add file_type key
    
    * gptneox-main.cpp : add file_type key
    
    * fix conflicts
    
    * gguf : add todos and comments
    
    * convert-gptneox-h5-to-gguf.py : tensor name map changes
    
    * Create gguf_namemap.py : tensor name map changes
    
    * Delete gguf_tensor_map.py
    
    * gptneox-main.cpp : tensor name map changes
    
    * convert-llama-h5-to-gguf.py : fixes
    
    * gguf.py : dont add empty strings
    
    * simple : minor style changes
    
    * gguf : use UNIX line ending
    
    * Create convert-llama-7b-pth-to-gguf.py
    
    * llama : sync gguf-llama.cpp with latest llama.cpp (#2608)
    
    * llama : sync gguf-llama.cpp with latest llama.cpp
    
    * minor : indentation + assert
    
    * llama : refactor gguf_buffer and gguf_ctx_buffer
    
    * llama : minor
    
    * gitignore : add gptneox-main
    
    * llama : tokenizer fixes (#2549)
    
    * Merge tokenizer fixes into the gguf branch.
    
    * Add test vocabularies
    
    * convert : update convert-new.py with tokenizer fixes (#2614)
    
    * Merge tokenizer fixes into the gguf branch.
    
    * Add test vocabularies
    
    * Adapt convert-new.py (and fix a clang-cl compiler error on windows)
    
    * llama : sync gguf-llama with llama (#2613)
    
    * llama : sync gguf-llama with llama
    
    * tests : fix build + warnings (test-tokenizer-1 still fails)
    
    * tests : fix wstring_convert
    
    * convert : fix layer names
    
    * llama : sync gguf-llama.cpp
    
    * convert : update HF converter to new tokenizer voodoo magics
    
    * llama : update tokenizer style
    
    * convert-llama-h5-to-gguf.py : add token types
    
    * constants.py : add token types
    
    * gguf.py : add token types
    
    * convert-llama-7b-pth-to-gguf.py : add token types
    
    * gguf-llama.cpp :  fix n_head_kv
    
    * convert-llama-h5-to-gguf.py : add 70b gqa support
    
    * gguf.py : add tensor data layout
    
    * convert-llama-h5-to-gguf.py : add tensor data layout
    
    * convert-llama-7b-pth-to-gguf.py : add tensor data layout
    
    * gptneox-main.cpp : add tensor data layout
    
    * convert-llama-h5-to-gguf.py : clarify the reverse permute
    
    * llama : refactor model loading code (#2620)
    
    * llama : style formatting + remove helper methods
    
    * llama : fix quantization using gguf tool
    
    * llama : simplify gguf_file_saver
    
    * llama : fix method names
    
    * llama : simplify write_header()
    
    * llama : no need to pass full file loader to the file saver
    
    just gguf_ctx
    
    * llama : gguf_file_saver write I32
    
    * llama : refactor tensor names (#2622)
    
    * gguf: update tensor names searched in quantization
    
    * gguf : define tensor names as constants
    
    * gguf : initial write API (not tested yet)
    
    * gguf : write to file API (not tested)
    
    * gguf : initial write API ready + example
    
    * gguf : fix header write
    
    * gguf : fixes + simplify example + add ggml_nbytes_pad()
    
    * gguf : minor
    
    * llama : replace gguf_file_saver with new gguf write API
    
    * gguf : streaming support when writing files
    
    * gguf : remove oboslete write methods
    
    * gguf : remove obosolete gguf_get_arr_xxx API
    
    * llama : simplify gguf_file_loader
    
    * llama : move hparams and vocab from gguf_file_loader to llama_model_loader
    
    * llama : merge gguf-util.h in llama.cpp
    
    * llama : reorder definitions in .cpp to match .h
    
    * llama : minor simplifications
    
    * llama : refactor llama_model_loader (WIP)
    
    wip : remove ggml_ctx from llama_model_loader
    
    wip : merge gguf_file_loader in llama_model_loader
    
    * llama : fix shape prints
    
    * llama : fix Windows build + fix norm_rms_eps key
    
    * llama : throw error on missing KV paris in model meta data
    
    * llama : improve printing + log meta data
    
    * llama : switch print order of meta data
    
    ---------
    
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
    
    * gguf : deduplicate (#2629)
    
    * gguf : better type names
    
    * dedup : CPU + Metal is working
    
    * ggml : fix warnings about unused results
    
    * llama.cpp : fix line feed and compiler warning
    
    * llama : fix strncpy warning + note token_to_str does not write null
    
    * llama : restore the original load/save session implementation
    
    Will migrate this to GGUF in the future
    
    * convert-llama-h5-to-gguf.py : support alt ctx param name
    
    * ggml : assert when using ggml_mul with non-F32 src1
    
    * examples : dedup simple
    
    ---------
    
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>
    
    * gguf.py : merge all files in gguf.py
    
    * convert-new.py : pick #2427 for HF 70B support
    
    * examples/gguf : no need to keep q option for quantization any more
    
    * llama.cpp : print actual model size
    
    * llama.cpp : use ggml_elements()
    
    * convert-new.py : output gguf (#2635)
    
    * convert-new.py : output gguf (WIP)
    
    * convert-new.py : add gguf key-value pairs
    
    * llama : add hparams.ctx_train + no longer print ftype
    
    * convert-new.py : minor fixes
    
    * convert-new.py : vocab-only option should work now
    
    * llama : fix tokenizer to use llama_char_to_byte
    
    * tests : add new ggml-vocab-llama.gguf
    
    * convert-new.py : tensor name mapping
    
    * convert-new.py : add map for skipping tensor serialization
    
    * convert-new.py : convert script now works
    
    * gguf.py : pick some of the refactoring from #2644
    
    * convert-new.py : minor fixes
    
    * convert.py : update to support GGUF output
    
    * Revert "ci : disable CI temporary to not waste energy"
    
    This reverts commit 7e82d25f40386540c2c15226300ad998ecd871ea.
    
    * convert.py : n_head_kv optional and .gguf file extension
    
    * convert.py : better always have n_head_kv and default it to n_head
    
    * llama : sync with recent PRs on master
    
    * editorconfig : ignore models folder
    
    ggml-ci
    
    * ci : update ".bin" to ".gguf" extension
    
    ggml-ci
    
    * llama : fix llama_model_loader memory leak
    
    * gptneox : move as a WIP example
    
    * llama : fix lambda capture
    
    ggml-ci
    
    * ggml : fix bug in gguf_set_kv
    
    ggml-ci
    
    * common.h : .bin --> .gguf
    
    * quantize-stats.cpp : .bin --> .gguf
    
    * convert.py : fix HF tensor permuting / unpacking
    
    ggml-ci
    
    * llama.cpp : typo
    
    * llama : throw error if gguf fails to init from file
    
    ggml-ci
    
    * llama : fix tensor name grepping during quantization
    
    ggml-ci
    
    * gguf.py : write tensors in a single pass (#2644)
    
    * gguf : single pass for writing tensors + refactoring writer
    
    * gguf : single pass for writing tensors + refactoring writer
    
    * gguf : single pass for writing tensors + refactoring writer
    
    * gguf : style fixes in simple conversion script
    
    * gguf : refactor gptneox conversion script
    
    * gguf : rename h5 to hf (for HuggingFace)
    
    * gguf : refactor pth to gguf conversion script
    
    * gguf : rm file_type key and method
    
    * gguf.py : fix vertical alignment
    
    * gguf.py : indentation
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * convert-gptneox-hf-to-gguf.py : fixes
    
    * gguf.py : gptneox mapping
    
    * convert-llama-hf-to-gguf.py : fixes
    
    * convert-llama-7b-pth-to-gguf.py : fixes
    
    * ggml.h : reverse GGUF_MAGIC
    
    * gguf.py : reverse GGUF_MAGIC
    
    * test-tokenizer-0.cpp : fix warning
    
    * llama.cpp : print kv general.name
    
    * llama.cpp : get special token kv and linefeed token id
    
    * llama : print number of tensors per type + print arch + style
    
    * tests : update vocab file with new magic
    
    * editorconfig : fix whitespaces
    
    * llama : re-order functions
    
    * llama : remove C++ API + reorganize common source in /common dir
    
    * llama : minor API updates
    
    * llama : avoid hardcoded special tokens
    
    * llama : fix MPI build
    
    ggml-ci
    
    * llama : introduce enum llama_vocab_type + remove hardcoded string constants
    
    * convert-falcon-hf-to-gguf.py : falcon HF --> gguf conversion, not tested
    
    * falcon-main.cpp : falcon inference example
    
    * convert-falcon-hf-to-gguf.py : remove extra kv
    
    * convert-gptneox-hf-to-gguf.py : remove extra kv
    
    * convert-llama-7b-pth-to-gguf.py : remove extra kv
    
    * convert-llama-hf-to-gguf.py : remove extra kv
    
    * gguf.py : fix for falcon 40b
    
    * falcon-main.cpp : fix for falcon 40b
    
    * convert-falcon-hf-to-gguf.py : update ref
    
    * convert-falcon-hf-to-gguf.py : add tensor data layout
    
    * cmpnct_gpt2bpe.hpp : fixes
    
    * falcon-main.cpp : fixes
    
    * gptneox-main.cpp : fixes
    
    * cmpnct_gpt2bpe.hpp : remove non-general stuff
    
    * Update examples/server/README.md
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * cmpnct_gpt2bpe.hpp : cleanup
    
    * convert-llama-hf-to-gguf.py : special tokens
    
    * convert-llama-7b-pth-to-gguf.py : special tokens
    
    * convert-permute-debug.py : permute debug print
    
    * convert-permute-debug-master.py : permute debug for master
    
    * convert-permute-debug.py : change permute type of attn_q
    
    * convert.py : 70b model working (change attn_q permute)
    
    * Delete convert-permute-debug-master.py
    
    * Delete convert-permute-debug.py
    
    * convert-llama-hf-to-gguf.py : fix attn_q permute
    
    * gguf.py : fix rope scale kv
    
    * convert-llama-hf-to-gguf.py : rope scale and added tokens
    
    * convert-llama-7b-pth-to-gguf.py : rope scale and added tokens
    
    * llama.cpp : use rope scale kv
    
    * convert-llama-7b-pth-to-gguf.py : rope scale fix
    
    * convert-llama-hf-to-gguf.py : rope scale fix
    
    * py : fix whitespace
    
    * gguf : add Python script to convert GGMLv3 LLaMA models to GGUF (#2682)
    
    * First pass at converting GGMLv3 LLaMA models to GGUF
    
    * Cleanups, better output during conversion
    
    * Fix vocab space conversion logic
    
    * More vocab conversion fixes
    
    * Add description to converted GGUF files
    
    * Improve help text, expand warning
    
    * Allow specifying name and description for output GGUF
    
    * Allow overriding vocab and hyperparams from original model metadata
    
    * Use correct params override var name
    
    * Fix wrong type size for Q8_K
    
    Better handling of original style metadata
    
    * Set default value for gguf add_tensor raw_shape KW arg
    
    * llama : improve token type support (#2668)
    
    * Merge tokenizer fixes into the gguf branch.
    
    * Add test vocabularies
    
    * Adapt convert-new.py (and fix a clang-cl compiler error on windows)
    
    * Improved tokenizer test
    
    But does it work on MacOS?
    
    * Improve token type support
    
    - Added @klosax code to convert.py
    - Improved token type support in vocabulary
    
    * Exclude platform dependent tests
    
    * More sentencepiece compatibility by eliminating magic numbers
    
    * Restored accidentally removed comment
    
    * llama : add API for token type
    
    ggml-ci
    
    * tests : use new tokenizer type API (#2692)
    
    * Merge tokenizer fixes into the gguf branch.
    
    * Add test vocabularies
    
    * Adapt convert-new.py (and fix a clang-cl compiler error on windows)
    
    * Improved tokenizer test
    
    But does it work on MacOS?
    
    * Improve token type support
    
    - Added @klosax code to convert.py
    - Improved token type support in vocabulary
    
    * Exclude platform dependent tests
    
    * More sentencepiece compatibility by eliminating magic numbers
    
    * Restored accidentally removed comment
    
    * Improve commentary
    
    * Use token type API in test-tokenizer-1.cpp
    
    * py : cosmetics
    
    * readme : add notice about new file format
    
    ggml-ci
    
    ---------
    
    Co-authored-by: M. Yusuf Sarıgöz <yusufsarigoz@gmail.com>
    Co-authored-by: klosax <131523366+klosax@users.noreply.github.com>
    Co-authored-by: goerch <jhr.walter@t-online.de>
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>

.gitignore
CMakeLists.txt
Makefile
README.md
ci/run.sh
common/CMakeLists.txt
common/common.cpp
common/common.h
common/console.cpp
common/console.h
common/grammar-parser.cpp
common/grammar-parser.h
convert-falcon-hf-to-gguf.py
convert-gptneox-hf-to-gguf.py
convert-llama-7b-pth-to-gguf.py
convert-llama-ggmlv3-to-gguf.py
convert-llama-hf-to-gguf.py
convert.py
docs/token_generation_performance_tips.md
examples/CMakeLists.txt
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp
examples/embd-input/embd-input-lib.cpp
examples/embedding/embedding.cpp
examples/gguf/gguf.cpp
examples/gptneox-wip/cmpnct_gpt2bpe.hpp
examples/gptneox-wip/falcon-main.cpp
examples/gptneox-wip/gptneox-main.cpp
examples/llama-bench/llama-bench.cpp
examples/main/main.cpp
examples/metal/metal.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
examples/quantize/quantize.cpp
examples/save-load-state/save-load-state.cpp
examples/server/README.md
examples/server/server.cpp
examples/simple/simple.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-metal.h
ggml-metal.m
ggml.c
ggml.h
gguf.py
llama-util.h
llama.cpp
llama.h
models/.editorconfig
models/ggml-vocab-llama.gguf
models/ggml-vocab.bin
tests/CMakeLists.txt
tests/test-grammar-parser.cpp
tests/test-llama-grammar.cpp
tests/test-tokenizer-0.cpp
tests/test-tokenizer-1.cpp

commit dadbed99e65252d79f81101a392d0d6497b86caa
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Mon Aug 21 06:59:29 2023 -0400

    metal : fix synchronization in new matrix multiplication kernel (#2686)

ggml-metal.metal

commit cb1c0727bd59803b439b6a3af121c99e6393ff3d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Aug 21 11:11:31 2023 +0300

    HellaSwag: split token evaluation into batches if needed (#2681)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp

commit 9e232f0234073358e7031c1b8d7aa45020469a3b
Author: slaren <slarengh@gmail.com>
Date:   Sun Aug 20 22:17:53 2023 +0200

    ggml : move all type info to ggml_type_traits (#2663)

ggml.c
ggml.h

commit 5e9ff54a675d163d9f42aad1b5b3e734f17b2701
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Aug 20 16:44:46 2023 +0300

    More efficient Hellaswag implementation (#2677)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/perplexity/perplexity.cpp

commit 1f0bccb27929e261744c979bc75114955da49e98
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Aug 19 00:45:36 2023 +0300

    server : better default prompt (#2646)

examples/server/public/index.html

commit f63564adfaa157ca387071d6b9a06cfaef0ef576
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Sat Aug 19 05:41:32 2023 +0800

    server : update xxd usage for older versions compatibility (#2649)
    
    * server : update xxd usage for older versions compatibility
    
    * remove unused $func

examples/server/deps.sh

commit 2d8b76a110d76ff6b5728ff0af8477531e4db60e
Author: Adrian <smith.adriane@gmail.com>
Date:   Fri Aug 18 12:39:22 2023 -0700

    Add link to clojure bindings to Readme. (#2659)

README.md

commit 7af633aec339367e36c867ae709088d6a801aa75
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 18 17:48:31 2023 +0300

    readme : incoming BREAKING CHANGE

README.md

commit 097e121e2f17ed3541cf02c55ff7e9febc091b19
Author: slaren <slarengh@gmail.com>
Date:   Fri Aug 18 12:44:58 2023 +0200

    llama : add benchmark example (#2626)
    
    * llama : add benchmark example
    
    * add to examples CMakeLists.txt
    
    * fix msvc build
    
    * add missing include
    
    * add Bessel's correction to stdev calculation
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    * improve markdown formatting
    
    * add missing include
    
    * print warning is NDEBUG is not defined
    
    * remove n_prompt and n_gen from the matrix, use each value separately instead
    
    * better checks for non-optimized builds
    
    * llama.cpp : fix MEM_REQ_SCRATCH0 reusing the value of n_ctx of the first call
    
    * fix json formatting
    
    * add sql output
    
    * add basic cpu and gpu info (linx/cuda only)
    
    * markdown: also show values that differ from the default
    
    * markdown: add build id
    
    * cleanup
    
    * improve formatting
    
    * formatting
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

.gitignore
Makefile
examples/CMakeLists.txt
examples/llama-bench/CMakeLists.txt
examples/llama-bench/llama-bench.cpp
ggml-cuda.cu
ggml-cuda.h
llama.cpp
llama.h

commit eaf98c2649d7da705de255712f0038ac7e47c610
Author: mdrokz <mohammadmunshi@gmail.com>
Date:   Fri Aug 18 15:47:58 2023 +0530

    readme : add link to Rust bindings (#2656)

README.md

commit e9b12c332ec6e215fbac4b2ef165353acbcd8319
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Aug 18 12:48:55 2023 +0300

    perplexity : more meaningful ETA number - 2 decimal points

examples/perplexity/perplexity.cpp

commit 604b8bdfa6320bbcb018eebcc1252dfede603c6b
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Thu Aug 17 19:54:44 2023 -0400

    Fix unicode in grammars (fixes #2501) (#2553)
    
    * Fix unicode in grammars (fixes #2501)
    
    * add more comments
    
    * fix test-llama-grammar

llama.cpp
tests/test-llama-grammar.cpp

commit 10151bee2e38b5711335c4a38f6ca93b50223acf
Author: staviq <staviq@gmail.com>
Date:   Thu Aug 17 23:34:01 2023 +0000

    server : support for saving templates in browser LocalStorage (#2486)
    
    * support for templates in browser LocalStorage
    
    * sync accepted #2409 fix from upstream
    
    * convert autosave invocation to useEffect
    
    * Apply suggestions from code review
    
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>
    
    * Regen index.html.cpp, suggested from code review
    
    ---------
    
    Co-authored-by: Jhen-Jie Hong <iainst0409@gmail.com>

examples/server/index.html.hpp
examples/server/public/index.html

commit 0992a7b8b18a89e29a205efb48ceb559c9a08203
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Aug 17 23:57:59 2023 +0200

    README: fix LLAMA_CUDA_MMV_Y documentation (#2647)

README.md

commit 6ddeefad9b634c5c79e6bcf046523493ff1fdf7d
Author: Henri Vasserman <henv@hot.ee>
Date:   Thu Aug 17 23:11:18 2023 +0300

    [Zig] Fixing Zig build and improvements (#2554)
    
    * Fix zig after console.o was split
    
    * Better include and flag management
    
    * Change LTO to option

README.md
build.zig

commit 8dae7ce68437faf1fa96ec0e7687b8700956ef20
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Aug 17 07:29:44 2023 -0600

    Add --cfg-negative-prompt-file option for examples (#2591)
    
    Add --cfg-negative-prompt-file option for examples

examples/common.cpp

commit a73ccf1aa34de49f61bfeb7f8a679c3bfdb3abe3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Aug 17 10:47:09 2023 +0300

    llama : replace (permute + reshape + view_1d) with (view_3d) (#2538)
    
    ggml-ci

llama.cpp

commit 7cf54e1f746941279d81d485796777c01f88049c
Author: drbh <david.richard.holtz@gmail.com>
Date:   Thu Aug 17 03:41:01 2023 -0400

    tests : adds simple llama grammar tests (#2618)
    
    * adds simple llama grammar tests
    
    * fix lint and add Makefile
    
    * 0 terminate code_points
    
    * avoid dangling pointers in candidate cleanup
    
    * cleanup grammar at end of test

Makefile
tests/CMakeLists.txt
tests/test-llama-grammar.cpp

commit a872a2b28eaefc8d464eaa535c94deeb501666f9
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Thu Aug 17 03:35:53 2023 -0400

    ggml-alloc : fix discrepency between measure&eval (#2639)
    
    The GGML memory allocator consistently places a tensor within the
    optimal-fit memory block, which is the smallest block capable of
    accommodating the tensor's size. During the measurement phase, the final
    block is generously sized, ensuring it never qualifies as the
    optimal-fit block as long as there exists another block capable of
    accommodating the tensor. Nevertheless, in the evaluation phase, the
    last block is constrained in size and could potentially qualify as the
    optimal-fit block. Consequently, there exists the possibility of a
    tensor being allocated to a different region during evaluation, leading
    to more memory fragmentation in our scratch buffer.
    
    This recent commit guarantees uniform behavior of the allocator across
    both the measurement and evaluation phases, eliminating discrepancies
    between the two.

ggml-alloc.c

commit 0919a0f73d95cfb93a1646a1d1741a0615fe2c5e
Author: Kolen Cheung <ickc@users.noreply.github.com>
Date:   Wed Aug 16 21:09:49 2023 +0100

    cmake : install ggml-meta.metal if LLAMA_METAL (#2449)

CMakeLists.txt

commit ed53db86c3b0e0815331a96d7a379edb5e62472c
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Thu Aug 17 04:09:03 2023 +0800

    metal : print error of load pipeline state (#2564)
    
    * metal : print error of load pipeline state
    
    * metal : return null if load pipeline failed

ggml-metal.m

commit fc8ef549e50087762a0b4f901cd74b2defcc6ae3
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Wed Aug 16 16:08:28 2023 -0400

    metal : enable ggml-alloc (#2627)
    
    * metal: enable ggml-alloc
    
    Make ggml-alloc work with concurrently dispatch.
    
    * style-fix
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-alloc.c
ggml-alloc.h
ggml-metal.h
ggml-metal.m
llama.cpp

commit bf83bff6742c0f1795b4c18695a13a34ac7adf62
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Wed Aug 16 16:07:04 2023 -0400

    metal : matrix-matrix multiplication kernel (#2615)
    
    * metal: matrix-matrix multiplication kernel
    
    This commit removes MPS and uses custom matrix-matrix multiplication
    kernels for all quantization types. This commit also adds grouped-query
    attention to support llama2 70B.
    
    * metal: fix performance degradation from gqa
    
    Integers are slow on the GPU, and 64-bit divides are extremely slow.
    In the context of GQA, we introduce a 64-bit divide that cannot be
    optimized out by the compiler, which results in a decrease of ~8% in
    inference performance. This commit fixes that issue by calculating a
    part of the offset with a 32-bit divide. Naturally, this limits the
    size of a single matrix to ~4GB. However, this limitation should
    suffice for the near future.
    
    * metal: fix bugs for GQA and perplexity test.
    
    I mixed up ne02 and nb02 in previous commit.

CMakeLists.txt
Makefile
flake.nix
ggml-metal.m
ggml-metal.metal
llama.cpp

commit b5ffb2849d23afe73647f68eec7b68187af09be6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 15 10:04:58 2023 +0300

    scripts : add helper script to get wikitext

scripts/get-wikitext-2.sh

commit 3ebb00935f3f0522b75df49c2769ab1774b91380
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Tue Aug 15 06:14:14 2023 +0800

    server : add missing /json-schema-to-grammar.mjs (#2616)
    
    fixes #2611

examples/server/server.cpp

commit d783f7982e0e823a2626a9956359c0d36c1a7e21
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Mon Aug 14 21:37:39 2023 +0800

    metal : return null instead of exit(1) (#2573)

ggml-metal.m
llama.cpp

commit d75561df207d22790609ee0ad924302f66ac2599
Author: Cheng Shao <terrorjack@type.dance>
Date:   Mon Aug 14 15:36:42 2023 +0200

    server : add --numa support (#2524)

examples/server/README.md
examples/server/server.cpp

commit 348acf188c9fbe66396990f2dc83229df367969b
Author: Kamil Tomšík <info@tomsik.cz>
Date:   Mon Aug 14 15:35:16 2023 +0200

    llama : add missing enum keyword in function signatures (#2610)

llama.h

commit 1cd06fa25eb859b14b3427a1d815a48f25fc3c34
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Aug 14 10:41:22 2023 +0200

    CUDA: launch_bounds, small q4_K, q5_K mmq refactor (#2596)

ggml-cuda.cu

commit 2feb8934eb75ca63f3c42724229cce1df9579c8e
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Mon Aug 14 16:20:17 2023 +0800

    server : fix default grammar by use empty string in the UI (#2604)

examples/server/index.html.hpp
examples/server/public/index.html

commit 5517d6e69214cdead000a76983b9fe175c3f8329
Author: Jhen-Jie Hong <iainst0409@gmail.com>
Date:   Mon Aug 14 15:16:54 2023 +0800

    server : implement json-schema-to-grammar.mjs & add grammar param in the UI (#2588)
    
    * server : implement json-schema-to-grammar.mjs by follow python impl
    
    * server : add grammar support in chat.mjs
    
    * server : implement grammer param in the UI
    
    * server : generate .hpp
    
    * server : remove trailing whitespaces
    
    * server : generate .hpp
    
    * server : fix sort of prop pairs
    
    * server : optimize regex & iteration

examples/server/chat.mjs
examples/server/index.html.hpp
examples/server/index.js.hpp
examples/server/json-schema-to-grammar.mjs.hpp
examples/server/public/index.html
examples/server/public/index.js
examples/server/public/json-schema-to-grammar.mjs

commit f31b5397143009d682db90fd2a6cde83f1ef00eb
Author: vxiiduu <73044267+vxiiduu@users.noreply.github.com>
Date:   Mon Aug 14 13:59:16 2023 +1000

    Enhance Windows 7 and below compatibility. (#2592)
    
    * Enhance Windows 7 compatibility.
    * Clean away unnecessary preprocessor conditional

llama-util.h

commit ee77efea2a1e3f7d153976b0934522b6bbaa62e6
Author: drbh <david.richard.holtz@gmail.com>
Date:   Sun Aug 13 10:00:48 2023 -0400

    test : add simple grammar parsing tests (#2594)
    
    * adds simple grammar parsing tests
    
    * adds cassert header

.gitignore
Makefile
tests/CMakeLists.txt
tests/test-grammar-parser.cpp

commit f64d44a9b9581cd58f7ec40f4fa1c3ca5ca18e1e
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Aug 13 00:24:45 2023 +0200

    CUDA: Fixed OpenLLaMA 3b mmq, reduced compile time (#2590)

CMakeLists.txt
ggml-cuda.cu

commit b19edd54d51cef5e3616c18b1d0d8626895b2cba
Author: byte-6174 <88070277+byte-6174@users.noreply.github.com>
Date:   Fri Aug 11 19:17:25 2023 -0400

    Adding support for llama2.c models (#2559)

.gitignore
Makefile
examples/CMakeLists.txt
examples/convert-llama2c-to-ggml/CMakeLists.txt
examples/convert-llama2c-to-ggml/README.md
examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp

commit 53dc399472d5bd35ee739b865e843b1996bd3814
Author: Equim <sayaka@ekyu.moe>
Date:   Sat Aug 12 06:35:14 2023 +0800

    server: fixed wrong variable name in timing json (#2579)
    
    * server: fixed wrong variable name in timing json
    
    * remove redunct entry

examples/server/server.cpp

commit 9ca4abed893685692f90413e4d43153af12342d9
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu Aug 10 13:11:36 2023 -0700

    Handle `ENABLE_VIRTUAL_TERMINAL_PROCESSING` more gracefully on earlier versions of Windows.

examples/console.cpp

commit e59fcb2bc129881f4a269fee748fb38bce0a64de
Author: Christian Demsar <crasm@git.vczf.us>
Date:   Thu Aug 10 10:28:27 2023 -0400

    Add --n-predict -2 for stopping generation on full context (#2565)

examples/common.cpp
examples/main/README.md
examples/main/main.cpp

commit 1638757767072a4957f52b9e3594f0b67610631b
Author: Martin Krasser <krasserm@googlemail.com>
Date:   Thu Aug 10 12:16:38 2023 +0200

    Fix grammar-based sampling issue in server (#2566)

examples/server/server.cpp

commit 916a9acdd0a411426690400ebe2bb7ce840a6bba
Author: Sam Spilsbury <smspillaz@gmail.com>
Date:   Wed Aug 9 23:47:42 2023 +0300

    ggml-alloc: Don't try to re-use buffers of external tensors (#2562)
    
    * ggml-alloc: Don't try to re-use buffers of external tensors
    
    They might be weights that came from another context, so we
    have no control over them (and they might be re-used elsewhere
    so writing to them would be a bad idea).
    
    * ggml-alloc: >= when checking for out-of-bounds
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

ggml-alloc.c

commit ea04a4ca1940d92becc0ee26523aa2c4a18cf938
Author: grahameth <96447521+grahameth@users.noreply.github.com>
Date:   Wed Aug 9 22:46:40 2023 +0200

    add log_callback to llama_context_params for custom logging. (#2234)
    
    * add log_callback to llama_context_params for custom logging.
    
    * Fix macro expansion on gcc
    
    * Add struct llama_state for global variables and move log_callback there
    
    * Turn log level into enum and some minor changes.
    
    * Remove model_for_logging parameter (not needed anymore)
    
    * Convert remaining fprintf(stderr, ...) calls to use new macros.
    
    * Fix enum and initialize g_state
    
    * Fix log calls after merge
    
    * Fix missing static
    
    * Add back all the new lines in the logging strings
    
    * Add comment for llama_log_callback and replace remaining printf calls
    
    ---------
    
    Co-authored-by: grahameth <->
    Co-authored-by: Helmut <helmut.buhler@inf.h-brs.de>

llama.cpp
llama.h

commit 25d43e0eb578b6e73046d9d6644a3a14d460600d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Aug 9 09:42:34 2023 +0200

    CUDA: tuned mul_mat_q kernels (#2546)

Makefile
README.md
ggml-cuda.cu

commit f5bfea0580e417f99850d5456ca541d871a3e48c
Author: Martin Krasser <krasserm@googlemail.com>
Date:   Tue Aug 8 15:29:19 2023 +0200

    Allow passing grammar to completion endpoint (#2532)
    
    * Allow passing grammar to completion endpoint

Makefile
examples/server/README.md
examples/server/server.cpp

commit acfc5478ff3446ca3b54553967a3dea09b7c771a
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Aug 8 14:38:16 2023 +0200

    CUDA: tighter VRAM scratch size for 65b/70b (#2551)

llama.cpp

commit 7ed8d1fe7f8cbe6a6763e6b46759795ac8d21e12
Author: chaihahaha <chai836275709@gmail.com>
Date:   Tue Aug 8 20:07:02 2023 +0800

    llm.vim : multiline autocompletion, get rid of "^@" (#2543)

examples/llm.vim

commit e7f94d6fdc83b41ba449b4b8c80821673dd12ffc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Aug 8 15:05:30 2023 +0300

    vim : bring back simple llm.vim example

examples/llm.vim

commit 2d7baaf50f3277e65cf71071f61ea34823d14c30
Author: AustinMroz <austinmroz@utexas.edu>
Date:   Tue Aug 8 06:44:48 2023 -0500

    vim : streaming and more (#2495)
    
    * Update Vim plugin
    
    * Remove getbufoneline usage, Add input bind example.
    
    getbufoneline() appears to be a recently added function and has been
    replaced with getbufline for compatibility.
    
    An additional example that explains how to add a keybind that works in
    insert mode was added.

examples/llama.vim
examples/llm.vim

commit f3c3b4b1672d860800639c87d3b5d17564692469
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Mon Aug 7 19:07:19 2023 +0200

    Add --rope-scale parameter (#2544)
    
    * common.cpp : Add --rope-scale parameter
    * README.md : Add info about using linear rope scaling

examples/common.cpp
examples/main/README.md

commit 93356bdb7a324a8f6570f99d02af392cd4c45796
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 14:25:58 2023 +0300

    ggml : mul mat tweaks (#2372)
    
    * ggml : mul mat wip
    
    ggml-ci
    
    * ggml : alternative thread distribution for mul_mat
    
    ggml-ci
    
    * ggml : mul_mat block tiling attempt
    
    * ggml : mul_mat threads yield
    
    ggml-ci

ggml.c

commit 60baff7c8584ec369e53469cad5f92e102b1efe4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 14:24:42 2023 +0300

    ggml : pad result of ggml_nbytes()

ggml.c

commit 9082b5dfbfae01243a0b822dcd2812877e63bf1b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 13:55:18 2023 +0300

    ggml : change params pointer (style change) (#2539)
    
    ggml-ci

ggml.c

commit 99d29c0094476c4962023036ecd61a3309d0e16b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 13:20:09 2023 +0300

    ggml : sync (custom ops) (#2537)
    
    ggml-ci

ggml.c
ggml.h

commit 3d9a55181603e85a26378a850a14068034e5002d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Aug 7 10:09:40 2023 +0200

    Fixed mmap prefetch for GPU offloading (#2529)

llama-util.h
llama.cpp

commit f6f9896ac3d2ff207e18f87dab85d126ceef5236
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Aug 7 10:52:57 2023 +0300

    metal : fix out-of-bounds access + inc concurrency nodes (#2416)
    
    * metal : fix out-of-bounds access + style changes
    
    * metal : increase concurrency nodes to 2*GGML_MAX_NODES

ggml-metal.m

commit 34a14b28ff7f3c98730339bacee035091b2a812a
Author: GiviMAD <GiviMAD@users.noreply.github.com>
Date:   Sun Aug 6 23:21:46 2023 -0700

    [Makefile] Move ARM CFLAGS before compilation (#2536)

Makefile

commit 7297128db8159c7b12db4c28a4532b993025c2e5
Author: Henri Vasserman <henv@hot.ee>
Date:   Mon Aug 7 08:35:53 2023 +0300

    [Zig] Rewrite build for Zig 0.11 (#2514)
    
    * zig build fixes
    
    * Disable LTO on Windows.

build.zig

commit 86c32198954a2bc482025703d6875e11f1c2a574
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Sat Aug 5 23:49:34 2023 -0700

    console : fix issue related to Windows 11 PowerShell console mode persistence (#2521)

examples/console.cpp

commit 2e8265ae1764d6288aab0e2df641909072e2d58e
Author: Keiichi Tabata <keiichi.tabata@outlook.com>
Date:   Sun Aug 6 15:34:05 2023 +0900

    convert.py : add missing abstract methods for quantized data (#2491)

convert.py

commit f514d1b306e1114c2884fcb25dd9bd48ae64ba32
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Aug 5 18:20:44 2023 +0200

    CUDA: faster k-quant mul_mat_q kernels (#2525)

ggml-cuda.cu

commit 332311234a0aa2974b2450710e22e09d90dd6b0b
Author: Jonas Wunderlich <32615971+jonas-w@users.noreply.github.com>
Date:   Fri Aug 4 20:16:11 2023 +0000

    fix firefox autoscroll (#2519)

examples/server/index.html.hpp
examples/server/public/index.html

commit 182af739c4ce237f7579facfe8f94dc53a1f573f
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Aug 4 15:00:57 2023 -0400

    server: regenerate completion.js.hpp (#2515)

examples/server/completion.js.hpp

commit 4329d1acb01c353803a54733b8eef9d93d0b84b2
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Aug 4 11:35:22 2023 -0400

    CUDA: use min compute capability of GPUs actually used (#2506)

ggml-cuda.cu

commit 02f9d96a866268700b8d8e7acbbcb4392c5ff345
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Fri Aug 4 11:34:32 2023 -0400

    CUDA: check if event is NULL before cudaStreamWaitEvent (#2505)
    
    Fixes #2503

ggml-cuda.cu

commit 3498588e0fb4daf040c4e3c698595cb0bfd345c0
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Fri Aug 4 08:20:12 2023 -0700

    Add --simple-io option for subprocesses and break out console.h and cpp (#1558)

Makefile
examples/CMakeLists.txt
examples/common.cpp
examples/common.h
examples/console.cpp
examples/console.h
examples/main/main.cpp

commit 5f631c26794b6371fcf2660e8d0c53494a5575f7
Author: Stephen Nichols <snichols@users.noreply.github.com>
Date:   Fri Aug 4 06:37:24 2023 -0500

    Fixing race condition in server and partial stream handling in frontend. (#2391)
    
    * Fixing race condition in server.cpp and partial stream handling in completion.js
    
    * Reverting assert edits.
    
    * Adding newline to eof

examples/server/public/completion.js
examples/server/server.cpp

commit 415e99fec27be5a2e4283f1937afd17eb33fbd66
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Fri Aug 4 19:29:52 2023 +0800

    Stream save llama context data to file instead of allocating entire buffer upfront (#2488)
    
    * added stream saving context data to file to avoid allocating unnecessary amounts of memory
    
    * generalised copying state data to file or buffer
    
    * added comments explaining how copy_state_data works
    
    * fixed trailing whitespaces
    
    * fixed save load state example
    
    * updated save load state to use public function in llama.cpp
    
    * - restored breakage of the llama_copy_state_data API
    - moved new logic for copying llama state data to internal function
    
    * fixed function declaration order
    
    * restored save load state example
    
    * fixed whitepace
    
    * removed unused llama-util.h include
    
    * Apply suggestions from code review
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    * Apply code review suggestions
    
    Co-authored-by: slaren <slarengh@gmail.com>
    
    ---------
    
    Co-authored-by: slaren <slarengh@gmail.com>

llama-util.h
llama.cpp

commit ff966e7ca6af127c9405523cdb07ef8fa01bf6d6
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Fri Aug 4 13:07:21 2023 +0300

    build : fix several cast and printf warnings (#2499)

examples/embd-input/embd-input-lib.cpp
examples/grammar-parser.cpp
examples/perplexity/perplexity.cpp
examples/simple/simple.cpp

commit 8183159cf3def112f6d1fe94815fce70e1bffa12
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Wed Aug 2 22:05:44 2023 -0400

    examples : generate JSON according to schema (#1887)
    
    * examples : add JSON schema grammars
    
    * complete JSON grammar
    
    * ensure primitive types can be used as root of schema
    
    * support integer type and adjust usage text

examples/json-schema-to-grammar.py
grammars/json.gbnf

commit 468ea24fb4633a0d681f7ac84089566c1c6190cb
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Aug 2 18:04:04 2023 +0200

    CUDA: faster non k-quant mul_mat_q kernels (#2483)

ggml-cuda.cu

commit 4f6b60c77652cdfc9d5545fe247ae5d764815598
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Aug 2 16:48:10 2023 +0200

    CUDA: Fix models with output size != 32000 (#2480)

CMakeLists.txt
ggml-cuda.cu

commit 220d9318647a8ce127dbf7c9de5400455f41e7d8
Author: ldwang <ftgreat@163.com>
Date:   Wed Aug 2 16:21:11 2023 +0800

    readme : add Aquila-7B model series to supported models (#2487)
    
    * support bpe tokenizer in convert
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * support bpe tokenizer in convert
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * support bpe tokenizer in convert, fix
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * Add Aquila-7B models in README.md
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * Up Aquila-7B models in README.md
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    ---------
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    Co-authored-by: ldwang <ftgreat@gmail.com>

README.md

commit 81844fbcfd93a162b7aeaea9e4f2ab1358f7f97e
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Wed Aug 2 04:06:19 2023 -0400

    tests : Fix compilation warnings (Linux/GCC) (#2451)
    
    * fix hellaswag print format, cast away warning in test-double-float
    
    * c++11 cannot use designated initializers
    
    * add static to test-grad0.c internal functions
    
    * use memcpy in test-double-float.c
    
    * port c tests to c++
    
    * use initializer list for ggml_init_params

Makefile
examples/common.cpp
scripts/sync-ggml.sh
tests/CMakeLists.txt
tests/test-double-float.cpp
tests/test-grad0.cpp
tests/test-opt.cpp

commit a312193e184b919047f33a5e844d846c5538dbe6
Author: Yiming Cui <conandiy@vip.qq.com>
Date:   Wed Aug 2 14:18:31 2023 +0800

    readme : Add Chinese LLaMA-2 / Alpaca-2 to supported models (#2475)
    
    * add support for chinese llama-2 / alpaca-2
    
    * remove white spaces

README.md

commit c574bddb368424b5996cbee2ec45ec050967d404
Author: Bono Lv <lvscar@users.noreply.github.com>
Date:   Tue Aug 1 20:54:28 2023 +0800

    fix a typo in examples/server/README.md (#2478)

examples/server/README.md

commit 86aeb27734481751592abd85590020b40d9224c8
Author: ebraminio <ebraminio@gmail.com>
Date:   Tue Aug 1 01:56:23 2023 -0700

    server : Support dark mode (#2414)
    
    * server : Support dark mode
    
    So it respects user system light / dark settings.
    
    * Update index.html.hpp by running ./deps.sh

examples/server/index.html.hpp
examples/server/public/index.html

commit 1873ff586bd8499a18f763632711bf15d253585e
Author: Matteo Boschini <12133566+mbosc@users.noreply.github.com>
Date:   Tue Aug 1 09:43:12 2023 +0200

    metal : add gqa8 kernel to allow llama-2-70B on metal (#2459)
    
    * Added gqa8 kernel to allow llama-2-70B on metal
    
    * Update ggml-metal.m
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>
    
    * Extend kernel_mul_mat_f16_f32 to handle gqa broadcast
    
    * Added ne03==ne13 assertion
    
    ---------
    
    Co-authored-by: Cebtenzzre <cebtenzzre@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 49e7cb5bb1f75c91dd5db7d2d88cbc11bd9ee0c5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 21:02:19 2023 +0200

    CUDA: fixed LLAMA_FAST compilation option (#2473)

Makefile

commit b772bba42e3bbca3cdab224456f8ff2ce427fd0b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 19:52:22 2023 +0200

    CUDA: fixed cmake F16 option (#2471)

CMakeLists.txt

commit 0728c5a8b9569183ffca0399caac099afef87595
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 15:44:35 2023 +0200

    CUDA: mmq CLI option, fixed mmq build issues (#2453)

CMakeLists.txt
Makefile
README.md
examples/common.cpp
examples/common.h
examples/server/server.cpp
ggml-cuda.cu
ggml-cuda.h
llama.cpp
llama.h

commit 1215ed7d5ccf854a55eccb52661427bb985e7f2c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 14:32:30 2023 +0200

    CUDA: Implemented row flattening for non-glm RoPE (#2468)

ggml-cuda.cu

commit 2dbf518911926ef5a30f43aa83a0b1b1cdeaaa11
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jul 31 13:18:51 2023 +0200

    CUDA: fewer memory bank conflicts for mul_mat_q (#2458)

ggml-cuda.cu

commit 9d2382b3e45b5815fc6a054045a2f2c2b18c22a2
Author: slaren <slarengh@gmail.com>
Date:   Mon Jul 31 11:02:53 2023 +0200

    Fix Metal backend broken from the allocator changes (#2455)
    
    * fix Metal backend broken from the allocator changes

llama.cpp

commit a113689571420fb4d6540f1a324d12965781356a
Author: slaren <slarengh@gmail.com>
Date:   Sun Jul 30 15:58:01 2023 +0200

    ggml : add graph tensor allocator (#2411)
    
    * ggml : add graph tensor allocator
    
    * ggml : don't calculate data pointer of unallocated tensors when creating a view with an offset
    
    * ggml : refactor ggml_view_Nd into ggml_view_tensor_offset

CMakeLists.txt
Makefile
ggml-alloc.c
ggml-alloc.h
ggml.c
ggml.h
llama.cpp

commit 11f3ca06b8c66b0427aab0a472479da22553b472
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 29 23:04:44 2023 +0200

    CUDA: Quantized matrix matrix multiplication (#2160)
    
    * mmq implementation for non k-quants
    
    * q6_K
    
    * q2_K
    
    * q3_k
    
    * q4_K
    
    * vdr
    
    * q5_K
    
    * faster q8_1 loading
    
    * loop unrolling
    
    * add __restrict__
    
    * q2_K sc_high
    
    * GGML_CUDA_MMQ_Y
    
    * Updated Makefile
    
    * Update Makefile
    
    * DMMV_F16 -> F16
    
    * Updated README, CMakeLists
    
    * Fix CMakeLists.txt
    
    * Fix CMakeLists.txt
    
    * Fix multi GPU out-of-bounds

CMakeLists.txt
Makefile
README.md
ggml-cuda.cu

commit 9baf9ef304f330009d5a93b7390280a0fd27c9a1
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 29 23:04:10 2023 +0200

    CUDA: faster multi GPU synchronization (#2448)

ggml-cuda.cu

commit 8a88e5855c3b93024be0f93290b01a4206b65b38
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Fri Jul 28 20:25:36 2023 +0200

    perplexity : add Hellaswag calculation (#2389)
    
    * common.h : add hellaswag / remove perplexity-lines
    
    * common.cpp : add hellaswag / remove perplexity-lines
    
    * perplexity.cpp : add hellswag scores / remove perplexity-lines
    
    * perplexity.cpp : clean up
    
    * common.h : change default param value
    
    * common.cpp : Change default param
    
    * perplexity.cpp : alter wording
    
    * common.h : alter wording
    
    * common.cpp : alter wording

examples/common.cpp
examples/common.h
examples/perplexity/perplexity.cpp

commit a9559bf77b903d94eb21614ceae5ae8950f0f1fc
Author: Lee <44310445+lx200916@users.noreply.github.com>
Date:   Sat Jul 29 02:17:45 2023 +0800

    ggml : workaround for missing _mm256_setr_m128i in GCC < 8 in k_quants.c (#2405)

k_quants.c

commit ee1b497c985f61d6ec519c39fcfed78a3c6f1d06
Author: eric8607242 <e0928021388@gmail.com>
Date:   Sat Jul 29 02:10:05 2023 +0800

    llama : support more diverse tokenizers? (#2420)
    
    * supporting more diverse tokenizers
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp

commit d73b8d48b45d6e2c0ae9bb8c39623c4024adc275
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 28 21:05:08 2023 +0300

    examples : fix whitespace

examples/server-llama2-13B.sh

commit 34ae1caf7fdea4c4c09087002e15e6230102e6a9
Author: nhamanasu <45545786+nhamanasu@users.noreply.github.com>
Date:   Sat Jul 29 03:02:10 2023 +0900

    examples : server chat mode with llama2 (#2400)
    
    * add: server chat mode with llama2
    
    * fix: remove the unnecessary last \n

examples/server-llama2-13B.sh
examples/server/chat-llama2.sh

commit d91f3f0c55663719ea03b76311e8c36ed55eb0e2
Author: Weird Constructor <weirdconstructor@gmail.com>
Date:   Fri Jul 28 10:44:43 2023 +0200

    readme : fix the description of the Tail free sampling (TFS) method (#2431)

examples/main/README.md

commit 65cdf34bdc469fa86248e667a5880992684ef114
Author: Rand Xie <randxiexyy29@gmail.com>
Date:   Fri Jul 28 01:42:53 2023 -0700

    llama : use n_embd_gqa instead of n_embd to handle llama-2 70B (#2433)

examples/save-load-state/save-load-state.cpp
llama.cpp

commit edcc7ae7d26007bbf83136e9d33f863fcad9b871
Author: niansa/tuxifan <tuxifan@posteo.de>
Date:   Fri Jul 28 03:14:11 2023 +0200

    Obtaining LLaMA 2 instructions (#2308)
    
    * Obtaining LLaMA 2 instructions
    
    * Removed sharing warning for LLaMA 2
    
    * Linked TheBloke's GGML repos
    
    * Add LLaMA 2 to list of supported models
    
    * Added LLaMA 2 usage instructions
    
    * Added links to LLaMA 2 70B models

README.md

commit 7c529cede6e84054e77a3eceab31c53de7b2f55b
Author: mj-shifu <77107165+mj-shifu@users.noreply.github.com>
Date:   Thu Jul 27 22:39:17 2023 +0200

    convert.py : Update to support 70B HF format model files (#2427)
    
    * convert.py : fix llama 2 70b conversion from Huggingface

convert.py

commit 1a941869cbef8e9cc351a6c6987e4ae3b0f021f7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 27 11:00:54 2023 +0300

    metal : disable graph concurrency optimization due to bug (#2413)

llama.cpp

commit b5472ea0ada081a6e1c06998ebbc9a24aa2cd4a4
Author: slaren <slarengh@gmail.com>
Date:   Wed Jul 26 23:57:23 2023 +0200

    ggml : fix assert in ggml_set_unary_op (#2410)

ggml.c

commit 6df1f5940f889adde32fe47dc8881f010dcf9aba
Author: Cebtenzzre <cebtenzzre@gmail.com>
Date:   Wed Jul 26 14:00:04 2023 -0400

    make : build with -Wmissing-prototypes (#2394)

CMakeLists.txt
Makefile

commit 5488fb789ea5692268309baa76f67598155060be
Author: slaren <slarengh@gmail.com>
Date:   Wed Jul 26 15:56:53 2023 +0200

    ggml : allocate graphs in a context (#2392)
    
    * ggml : graph allocation in contexts
    
    * allocate work buffer as a ggml_object in ggml_graph_compute_with_ctx
    
    * llama.cpp : allocate graph in the context
    
    * add GGML_PAD
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c
ggml.h
llama.cpp

commit eb542d39324574a6778fad9ba9e34ba7a14a82a3
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jul 25 18:35:53 2023 +0300

    Add LLAMA_DEFAULT_RMS_EPS so we can change the default (#2384)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/baby-llama/baby-llama.cpp
examples/common.h
examples/train-text-from-scratch/train-text-from-scratch.cpp
llama.cpp
llama.h

commit 07aaa0f63fccaeab099b3a732abda20b921bc5a5
Author: slaren <slarengh@gmail.com>
Date:   Tue Jul 25 16:20:12 2023 +0200

    ggml : fix ggml_flash_attn to use op_params (#2387)
    
    * ggml : fix ggml_flash_attn to use op_params

ggml.c

commit fce48caf9a6b9930eee9e2a5971428cdff403ba8
Author: ldwang <ftgreat@163.com>
Date:   Tue Jul 25 21:22:09 2023 +0800

    convert.py : support bpe tokenizer (#2228)
    
    * support bpe tokenizer in convert
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * support bpe tokenizer in convert
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    * support bpe tokenizer in convert, fix
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    
    ---------
    
    Signed-off-by: ldwang <ftgreat@gmail.com>
    Co-authored-by: ldwang <ftgreat@gmail.com>

convert.py

commit 875086bdb95ce1f3294439811536533199e3b579
Author: Jiahao Li <liplus17@163.com>
Date:   Tue Jul 25 20:58:32 2023 +0800

    ggml : relax contiguous constraints in activation function (#2371)

ggml.c

commit da1889834a036a63ead2b0ca5c9ed8967712568c
Author: slaren <slarengh@gmail.com>
Date:   Tue Jul 25 14:32:20 2023 +0200

    ggml : improve graph build time via hash table lookup (#2329)
    
    * improve graph build time
    
    * ggml_tensor : use 1 bit per flag
    
    * use a hash table instead

ggml.c
ggml.h
llama.cpp

commit 82552b7f5403ca13957ac9a2cdc1732470057b62
Author: Hesen Peng <hesen.peng@gmail.com>
Date:   Tue Jul 25 05:24:09 2023 -0700

    build : fix line breaking error in build-info.sh (#2349)
    
    * fix line breaking
    
    * build number line break removal

scripts/build-info.sh

commit 0c06204fb39aa5560e883e0ae74be9518c57d88e
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Tue Jul 25 07:19:11 2023 -0500

    main : add `--in-prefix-bos` to prefix BOS to user inputs; keep EOS (#2304)
    
    * add `--in-prefix-bos` to prefix BOS to user inputs; keep EOS
    
    The BOS precedes the string specified by `--in-prefix`.
    Model generated EOS is now kept in the context.
    
    It provides a way to strictly following the prompt format used in
    Llama-2-chat.
    
    The EOS handling also benefits some existing finetunes that uses
    EOS to mark the end of turn.
    
    * examples/common: move input_prefix_bos to other bools

examples/common.cpp
examples/common.h
examples/main/main.cpp

commit 1fed755b1fb9babb6dbc1b4023e492950cd5a5be
Author: Eve <139727413+netrunnereve@users.noreply.github.com>
Date:   Tue Jul 25 08:16:13 2023 -0400

    ci : add non-AVX scalar build/test (#2356)
    
    * noavx build and test
    
    * we don't need to remove f16c in windows

.github/workflows/build.yml

commit be2301bcdaf6c9f0921141bd071de7788e2a351a
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Tue Jul 25 21:13:41 2023 +0900

    k_quants : add AVX support to dot functions with QK_K as 64 (#2339)
    
    * add AVX to ggml_vec_dot_q2_K_q8_K()
    
    * add AVX to ggml_vec_dot_q3_K_q8_K()
    
    * add AVX to ggml_vec_dot_q4_K_q8_K()
    
    * add AVX to ggml_vec_dot_q5_K_q8_K()
    
    * add AVX to ggml_vec_dot_q6_K_q8_K()
    
    * refactor AVX code in ggml_vec_dot_q6_K_q8_K()

k_quants.c

commit 1aa18ef994a6a2b531434eb13251ef48e56d345b
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Tue Jul 25 08:00:19 2023 -0400

    metal : concurrently dispatch commands (#2358)
    
    * metal: concurrently dispatch commands
    
    Function `ggml_metal_graph_find_concurrency` will run and write
    commands that can be issued concurrently to metal context `concur_list`
    array, when `ggml_metal_graph_compute` is called for the first time.
    
    * metal: don't call find_concurrency automatically.
    
    * metal : code style changes
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.h
ggml-metal.m
llama.cpp

commit 9a08eaf3c4010962d0126e9e5bfbe9af64b2ac90
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jul 25 13:48:29 2023 +0300

    Another speed gain for Q4_0 and Q4_1 on Metal (#2375)
    
    * Another speed gain for Q4_0 and Q4_1 on Metal
    
    * Have N_DST, etc., be template parameters
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.metal

commit 129d844c87d90e74aafc23dcc84c980fd408def4
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Jul 25 13:48:04 2023 +0300

    Fix Q4_K and Q5_K for QK_K = 64 on CUDA (#2359)
    
    * Fix Q4_K and Q5_K for QK_K = 64
    
    * Very slightly better Q5_K bit fiddling
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu

commit d5512b782b27ff698007dcd175da18959d5f163f
Author: slaren <slarengh@gmail.com>
Date:   Tue Jul 25 11:36:17 2023 +0200

    server: add rms_norm_eps parameter (#2380)

examples/server/server.cpp

commit c798308e3a425eae050a1f249a576fa8c6433327
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue Jul 25 10:27:34 2023 +0300

    [Server] Escape HTML in webchat (#2368)
    
    * escape HTML in webchat
    * add amp

examples/server/index.html.hpp
examples/server/public/index.html

commit 41c674161fb2459bdf7806d1eebead15bc5d046e
Author: slaren <slarengh@gmail.com>
Date:   Mon Jul 24 17:57:12 2023 +0200

    make rms_norm_eps a parameter (#2374)
    
    * make rms_norm_eps a parameter
    
    * add rms_norm_eps to command line
    
    * fix baby llama, test-grad0
    
    * use scientific notation for eps param in the help
    
    ggml-ci

examples/baby-llama/baby-llama.cpp
examples/common.cpp
examples/common.h
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-cuda.cu
ggml-metal.m
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-grad0.c

commit b3f138d05849ccbce67303ac17b50ebbc268128a
Author: Aarni Koskela <akx@iki.fi>
Date:   Mon Jul 24 17:54:22 2023 +0300

    Chat UI extras (#2366)
    
    * makefile: correct deps for server
    
    * server: tighten settings layout a little
    
    * server: expose all currently configured generation params in UI
    
    * server: expose remaining generation params, for the adventurous
    
    * server: embetter mirostat fields

Makefile
examples/server/index.html.hpp
examples/server/public/index.html

commit 5b2b2dc6ae8086bff7c9b3c17fb435cf319b7185
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jul 24 14:46:21 2023 +0300

    ggml : sync (unary ops refactor, static-correctness) (#2370)
    
    * ggml : sync (unary ops, tests)
    
    ggml-ci
    
    * tests : remove unnecessary funcs

ggml-cuda.cu
ggml-metal.m
ggml.c
ggml.h
tests/test-grad0.c
tests/test-opt.c

commit 42f70cb2f6a8089e0a0560a459e4ba317bac4d49
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jul 24 12:55:02 2023 +0300

    Fix scalar version of Q5_K when QK_K = 64 (#2362)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

k_quants.c

commit 84e09a7d8bc4ab6d658b5cd81295ac0add60be78
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Sun Jul 23 23:58:10 2023 -0400

    llama : add grammar-based sampling (#1773)
    
    * llama, main : constrain sampling to grammar
    
    * allow loading grammar from file
    
    * fix whitespace errors
    
    * handle & print parser errors
    
    * add comments to grammar syntax and allow newlines where unambiguous
    
    * add missing include
    
    * support alternates in root rule
    
    * fix bugs with empty token and EOS
    
    * adjust JSON grammar
    
    * remove swp file
    
    * rewrite ternary expressions
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    
    * use struct for grammar elements and add Unicode support
    
    * add unicode escapes
    
    * add inverse char ranges
    
    * only sample full tokens (no peeking or truncation)
    
    * llama : minor style changes
    
    blindly applied in online editor - hopefully I didn't break something
    
    * update help text
    
    * add warning message if EOS is disabled
    
    ---------
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

Makefile
examples/CMakeLists.txt
examples/common.cpp
examples/common.h
examples/grammar-parser.cpp
examples/grammar-parser.h
examples/main/main.cpp
grammars/arithmetic.gbnf
grammars/chess.gbnf
grammars/japanese.gbnf
grammars/json.gbnf
grammars/list.gbnf
llama.cpp
llama.h

commit 2f9cf974a066ac0e03fbb235d834b01b0164d743
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jul 24 00:19:47 2023 +0300

    Some more Q4_K and Q5_K speedup on CUDA (#2346)
    
    * Faster Q5_K on CUDA
    
    * Small Q5_K improvement on older GPUs
    
    * Spped up Q4_K on CUDA
    
    GTX1660: 29.5 ms/t -> 25.6 ms/t
    RTX4080: 8.40 ms/t -> 8.25 ms/t
    
    * Spped up Q4_K on CUDA
    
    GTX1660: 36.7 ms/t -> 35.6 ms/t
    RTX4080:  9.8 ms/t ->  9.5 ms/t
    
    * Address PR comments
    
    * Add some comments to satisfy PR reviewer
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu

commit 4f06592cc6b83979e4b442e8cb97b3948c857188
Author: IgnacioFDM <ignaciofdm@gmail.com>
Date:   Sun Jul 23 17:31:17 2023 -0300

    Add gqa parameter support to the server (#2351)
    
    * Add gqa parameter support to the server
    * Change help from stderr to stdout

examples/server/server.cpp

commit 70d26ac3883009946e737525506fa88f52727132
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jul 23 17:49:06 2023 +0200

    Fix __dp4a documentation (#2348)

README.md

commit 57921ca6db53e08eb90010fba99add85be28b5a1
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Sun Jul 23 21:33:02 2023 +0800

    common : n_threads == -1 uses std::thread::hardware_concurrency() (#2347)
    
    * Fix #2345, fix incorrect n_threads
    
    * Update examples/common.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/common.cpp

commit 3602ac4255fd4cd589821346290b464a64b955d1
Author: slaren <slarengh@gmail.com>
Date:   Sun Jul 23 15:19:39 2023 +0200

    fix n_tasks (#2342)
    
    ggml-ci

ggml.c

commit 95a6c595e7ca8dbe47ccf8824e04213e10357f9a
Author: slaren <slarengh@gmail.com>
Date:   Sun Jul 23 14:36:02 2023 +0200

    ggml: move op parameters from tensors to ggml_tensor::op_params (#2333)
    
    * ggml: move op parameters from tensors to ggml_tensor::op_params
    
    * alibi: use memcpy for float params
    
    * remove `src[1] = NULL` in ops

ggml-cuda.cu
ggml-metal.m
ggml.c
ggml.h

commit e76d630df17e235e6b9ef416c45996765d2e36fb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jul 23 15:09:47 2023 +0300

    llama : grouped-query attention + LLaMAv2 70B support (#2276)
    
    * CUDA: GQA implementation
    
    * llama : support for GQA and LLaMAv2 70B
    
    ggml-ci
    
    * py : fix hparams parsing (if-else blocks)
    
    ggml-ci
    
    * py : oh boy ..
    
    ggml-ci
    
    * help : fix gqa value for 70B
    
    ggml-ci
    
    ---------
    
    Co-authored-by: JohannesGaessler <johannesg@5d6.de>

convert.py
examples/common.cpp
examples/common.h
examples/main/main.cpp
ggml-cuda.cu
llama.cpp
llama.h

commit 1d0824b2476e7fda09751a0235c9e571b76d6f2c
Author: maddes8cht <55592906+maddes8cht@users.noreply.github.com>
Date:   Sun Jul 23 13:59:48 2023 +0200

    llama : print help to stdout (#2338)

examples/common.cpp

commit bc3ec2cdc9ea20b0faba2e1b4576fab3a911e4d1
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Sun Jul 23 19:57:02 2023 +0800

    flake : support `nix build '.#opencl'` (#2337)

flake.nix

commit a940458e4814e87bd0d3fbdb3f3d2733b4a3ccb1
Author: Christian Demsar <christian@github.email.demsar.us>
Date:   Sun Jul 23 07:56:34 2023 -0400

    llama : print max tensor size to stderr (#2336)

llama.cpp

commit 91171b8072f6f0c8ae3a61e23451acb538bb9ece
Author: Jose Maldonado <63384398+yukiteruamano@users.noreply.github.com>
Date:   Sun Jul 23 07:52:08 2023 -0400

    make : fix CLBLAST compile support in FreeBSD (#2331)
    
    * Fix Makefile for CLBLAST compile support and instructions for compile llama.cpp FreeBSD
    
    * More general use-case for CLBLAST support (Linux and FreeBSD)

Makefile
README.md

commit 355c80f49e32b0b15c0a457f3bad380e57f5b9ac
Author: AustinMroz <austinmroz@utexas.edu>
Date:   Sun Jul 23 06:16:48 2023 -0500

    examples : simplify vim plugin (#2327)
    
    Uses builtin json_encode and json_decode functions to simplify escaping
    Removes the need for temp files

examples/llm.vim

commit 83a00ce69bef9124c0702424a012ea799128b77d
Author: Jiahao Li <liplus17@163.com>
Date:   Sun Jul 23 19:00:37 2023 +0800

    metal : support bcast add & dup & cont op (#2323)

ggml-metal.m
ggml-metal.metal

commit d2a43664f93ba30a84e42713bb69f936cbdacf2a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jul 23 08:49:20 2023 +0300

    Speed up Q4_K (#2322)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu

commit b9b7d94fc10a8039befd1bc3af4f4b09c620c351
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 22 21:27:34 2023 +0200

    CUDA: Fixed 7b q3_K_S with mul_mat_vec_q (#2313)

ggml-cuda.cu

commit b47b8a9cfeb439d271bf997fb985fd6d82b3af5e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 22 21:17:57 2023 +0300

    llama : optimize memory buffers (#2325)

examples/common.cpp
examples/main/main.cpp
llama.cpp

commit b5fe67f8c69113bd9354bc1adcfe2df6be323740
Author: klosax <131523366+klosax@users.noreply.github.com>
Date:   Sat Jul 22 14:21:24 2023 +0200

    Perplexity: Compute scores correlated to HellaSwag (#2312)
    
    * Add parameter --perplexity-lines to perplexity.cpp

examples/common.cpp
examples/common.h
examples/perplexity/perplexity.cpp

commit 24baa54ac1ff3d4156a2360deb1473af04a9b1a2
Author: whoreson <139810751+whoreson@users.noreply.github.com>
Date:   Sat Jul 22 12:34:51 2023 +0200

    examples : basic VIM plugin
    
    VIM plugin for server exe

examples/llm.vim

commit dd6c67d3cbb7b360747f776412bf01976aa32f4b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 22 12:00:56 2023 +0300

    ci : fix args

ci/run.sh

commit 5d500e8ccf5eee3de3ae66685cc3be75e43e08b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 22 11:48:22 2023 +0300

    ci : add 7B CUDA tests (#2319)
    
    * ci : add 7B CUDA tests
    
    ggml-ci
    
    * ci : add Q2_K to the tests
    
    * ci : bump CUDA ppl chunks
    
    ggml-ci
    
    * ci : increase CUDA TG len + add --ignore-eos
    
    * ci : reduce CUDA ppl cunks down to 4 to save time

ci/README.md
ci/run.sh

commit 7d5f18468ceabd7a38f414f9f21b26b0c137f994
Author: Richard Roberson <richardr1126@gmail.com>
Date:   Fri Jul 21 13:01:10 2023 -0600

    examples : add easy python script to create quantized (k-bit support) GGML models from local HF Transformer models (#2311)
    
    * Resync my fork with new llama.cpp commits
    
    * examples : rename to use dash instead of underscore
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/make-ggml.py

commit d924522a46c5ef097af4a88087d91673e8e87e4d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jul 21 17:27:51 2023 +0300

    Custom RoPE + bettter memory management for CUDA (#2295)
    
    * Custom RoPE + bettter memory management for CUDA
    
    * Adjusted look ahead in ggml_cuda_pool_malloc to 5%
    
    This is sufficient it seems.
    We end up using about 200 MB less VRAM that way when running
    the 13B model with context 8192.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu

commit 4d76a5f49b9b5382dba5d13d92edb9159536c225
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jul 21 17:05:30 2023 +0300

    Faster Q3_K implementation on Metal (#2307)
    
    * Faster Q3_K on Metal
    
    * Additional Q3_K speedup on Metal
    
    * Q3_K for QK_K = 64
    
    * Better Q3_K for QK_K = 64
    
    21.6 ms/t -> 21.1 ms/t
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 0db14fef06836caaa13cc123c0a24dc598bdb9f0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 15:16:55 2023 +0300

    ggml : fix the rope fix (513f8619535a64fa9ace808cdcbcf66211535f5c)

ggml.c

commit 03e566977b277937c5f706180171c5d12b597b0b
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Fri Jul 21 20:53:07 2023 +0900

    examples :  fix typo in minigpt4.py (#2298)
    
    promt -> prompt

examples/embd-input/minigpt4.py

commit 513f8619535a64fa9ace808cdcbcf66211535f5c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 14:51:34 2023 +0300

    ggml : fix rope args order + assert (#2054)

examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml.c
ggml.h
llama.cpp

commit 3973b25a64a37a47eac156a3fd28f83c16f14bf2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 14:42:41 2023 +0300

    gitignore : fix final newline

.gitignore

commit ab0e26bdfb7b3adb1e3145c61a0fa92d1abd21d0
Author: Guillaume "Vermeille" Sanchez <Guillaume.V.Sanchez@gmail.com>
Date:   Fri Jul 21 12:58:36 2023 +0200

    llama : remove cfg smooth factor as it is only a reparameterization of the guidance scale (#2280)

examples/common.cpp
examples/common.h
examples/main/main.cpp
llama.cpp
llama.h

commit 73643f5fb1136dc2b65ae910bdc5a431520d70a2
Author: Jose Maldonado <63384398+yukiteruamano@users.noreply.github.com>
Date:   Fri Jul 21 06:53:27 2023 -0400

    gitignore : changes for Poetry users + chat examples (#2284)
    
    A fix in Makefile for FreeBSD users. In the platfrom x86_64 is amd64. This fix resolve compilation using CFLAGS and CXXFLAGS with -march=native and -mtune=native
    Add two examples for interactive mode using Llama2 models (thx TheBloke for models)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile
examples/llama2-13b.sh
examples/llama2.sh

commit a814d04f81121e0429b39a61fe4afd946cd42046
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 13:50:55 2023 +0300

    make : fix indentation

Makefile

commit 4c013bb7385a0e52ce721480c40c45bec5ef103f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 13:48:18 2023 +0300

    ci : fix MNT realpath usage (#2250)

ci/run.sh

commit 42c7c2e2e9cae79330f57456fbc0eae1eaff17fa
Author: Sky Yan <skyan83@gmail.com>
Date:   Fri Jul 21 18:38:57 2023 +0800

    make : support customized LLAMA_CUDA_NVCC and LLAMA_CUDA_CCBIN (#2275)
    
    Under certain environment, nvcc and gcc is installed under customized path but not standard path
    
    Co-authored-by: Yan Lin <yanlin@baidu.com>

Makefile

commit 78a3d13424b01c3f8ea94ea7e59650ab0501e902
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Fri Jul 21 18:26:34 2023 +0800

    flake : remove intel mkl from flake.nix due to missing files (#2277)
    
    NixOS's mkl misses some libraries like mkl-sdl.pc. See #2261
    Currently NixOS doesn't have intel C compiler (icx, icpx). See https://discourse.nixos.org/t/packaging-intel-math-kernel-libraries-mkl/975
    So remove it from flake.nix
    
    Some minor changes:
    
    - Change pkgs.python310 to pkgs.python3 to keep latest
    - Add pkgconfig to devShells.default
    - Remove installPhase because we have `cmake --install` from #2256

CMakeLists.txt
README.md
flake.nix

commit ae178ab46bfd6ecb2422da5dad441a4e2fef8b7e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 21 13:10:51 2023 +0300

    llama : make tensor_split ptr instead of array (#2272)

examples/common.cpp
ggml-cuda.cu
llama.cpp
llama.h

commit 54e3bc76fed914f8d4a30a7a50c19867cccb1338
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Fri Jul 21 12:09:16 2023 +0200

    make : add new target for test binaries (#2244)
    
    Programs in the tests directory are now build with target tests
    and placed in the same location.
    
    * clean target was expanded to remove new binaries
    
    * test target binaries are listed in a variable
    
    * Locations of binaries were added to the .gitignore
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile

commit 019fe257bbf699f400231683a8b816ad90281275
Author: Hatsune Miku <129688334+at8u@users.noreply.github.com>
Date:   Fri Jul 21 08:13:18 2023 +0000

    MIKU MAYHEM: Upgrading the Default Model for Maximum Fun 🎉 (#2287)
    
    * Miku.sh: Set default model to llama-2-7b-chat
    
    * Miku.sh: Set ctx_size to 4096
    
    * Miku.sh: Add in-prefix/in-suffix opts
    
    * Miku.sh: Switch sampler to mirostat_v2 and tiny prompt improvements

examples/Miku.sh

commit e68c96f7fee8fc22814a4a1209ffc97bbf35f7bd
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jul 21 10:44:40 2023 +0300

    Faster Q2_K on Metal (#2297)
    
    * Faster Q2_K on Metal
    
    * Deleting unnoticed and dangereous trailing white space
    
    * Fixed bug in new metal Q2_K implementation
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 9cf022a1889e50113fd348dc96b4557fc75a6296
Author: Przemysław Pawełczyk <przemoc@gmail.com>
Date:   Fri Jul 21 09:42:21 2023 +0200

    make : fix embdinput library and server examples building on MSYS2 (#2235)
    
    * make : fix embdinput library and server examples building on MSYS2
    
    * cmake : fix server example building on MSYS2

Makefile
examples/server/CMakeLists.txt

commit e782c9e735f93ab4767ffc37462c523b73a17ddc
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jul 20 18:19:45 2023 +0300

    Faster Q5_K and Q6_K on Metal (#2294)
    
    * Faster Q6_K on Metal
    
    * Faster Q5_K on Metal
    
    * Another Q5_K speedup
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 785829dfe8baf0213f2ff66963d28c62f92d7930
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jul 20 15:18:43 2023 +0300

    Faster Q4_K on Metal (#2290)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit fff0e0eafe817eef429ecb64f892ab7bdae31846
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 20 13:47:26 2023 +0300

    llama : fix regression from #2000 - could not load no-mmap models

llama.cpp

commit 417a85a0010519224cf154eb85d383ffeafeeead
Author: Shouzheng Liu <lshzh.hi@gmail.com>
Date:   Thu Jul 20 06:32:22 2023 -0400

    metal: minor q4 optimization and reduce code size (#2248)
    
    * metal: use uint16_t instead of uint8_t.
    
    Apple GPU doesn't like uint8_t. For every operation on uint8_t
    the gpu need to copy the uint8_t to an empty 16 bit register, then
    it can issue other instructions.
    
    For the matrix-vector multiplication kernel only, we observed a
    340~350 GB/s memory read speed on M1 Max after this commit, which is
    very close to the reported hardware limit.
    
    * metal: update rms_norm kernel
    
    This commit double the speed of rms_norm operations by using 512 threads
    per threadgroup, combining with SIMD primitives to minimize the need for
    thread group barriers.
    
    * metal: use template to reduce size
    
    Revert modifications on block_q4_0 and block_q4_1.

ggml-metal.m
ggml-metal.metal

commit 294f424554c1599784ac9962462fc39ace92d8a5
Author: Rinne <AsakusaRinne@gmail.com>
Date:   Wed Jul 19 15:06:40 2023 +0800

    llama : extend API to get max devices at runtime (#2253)

llama.cpp
llama.h

commit 45a1b07e9b20c33d71d8c849ff27d693a75a0269
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Wed Jul 19 15:01:55 2023 +0800

    flake : update flake.nix (#2270)
    
    When `isx86_32 || isx86_64`, it will use mkl, else openblas
    
    According to
    https://discourse.nixos.org/t/rpath-of-binary-contains-a-forbidden-reference-to-build/12200/3,
    add -DCMAKE_SKIP_BUILD_RPATH=ON
    
    Fix #2261, Nix doesn't provide mkl-sdl.pc.
    When we build with -DBUILD_SHARED_LIBS=ON, -DLLAMA_BLAS_VENDOR=Intel10_lp64
    replace mkl-sdl.pc by mkl-dynamic-lp64-iomp.pc

CMakeLists.txt
README.md
flake.nix

commit b1f429095328a34556c0e9a7a2fefced3db3368c
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Wed Jul 19 15:01:11 2023 +0800

    cmake : install targets (#2256)
    
    fix #2252

CMakeLists.txt
convert-lora-to-ggml.py
convert.py
examples/baby-llama/CMakeLists.txt
examples/benchmark/CMakeLists.txt
examples/embd-input/CMakeLists.txt
examples/embedding/CMakeLists.txt
examples/main/CMakeLists.txt
examples/metal/CMakeLists.txt
examples/perplexity/CMakeLists.txt
examples/quantize-stats/CMakeLists.txt
examples/quantize/CMakeLists.txt
examples/save-load-state/CMakeLists.txt
examples/server/CMakeLists.txt
examples/simple/CMakeLists.txt
examples/train-text-from-scratch/CMakeLists.txt
tests/CMakeLists.txt

commit d01bccde9f759b24449fdaa16306b406a50eb367
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 18 14:24:43 2023 +0300

    ci : integrate with ggml-org/ci (#2250)
    
    * ci : run ctest
    
    ggml-ci
    
    * ci : add open llama 3B-v2 tests
    
    ggml-ci
    
    * ci : disable wget progress output
    
    ggml-ci
    
    * ci : add open llama 3B-v2 tg tests for q4 and q5 quantizations
    
    ggml-ci
    
    * tests : try to fix tail free sampling test
    
    ggml-ci
    
    * ci : add K-quants
    
    ggml-ci
    
    * ci : add short perplexity tests
    
    ggml-ci
    
    * ci : add README.md
    
    * ppl : add --chunks argument to limit max number of chunks
    
    ggml-ci
    
    * ci : update README

.gitignore
ci/README.md
ci/run.sh
examples/common.cpp
examples/common.h
examples/perplexity/perplexity.cpp
llama.cpp
tests/test-sampling.cpp

commit 6cbf9dfb32f0e23ed3afd02d30ab066ed53e2c4d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 18 11:50:49 2023 +0300

    llama : shorten quantization descriptions

examples/quantize/quantize.cpp

commit 7568d1a2b206331412106ea66da3f871025e0c3c
Author: Jiahao Li <liplus17@163.com>
Date:   Tue Jul 18 01:39:29 2023 +0800

    Support dup & cont ops on CUDA (#2242)

ggml-cuda.cu

commit b7647436ccc80970b44a270f70f4f2ea139054d1
Author: Alex Klinkhamer <git@grencez.dev>
Date:   Sun Jul 16 14:01:45 2023 -0700

    llama : fix t_start_sample_us initialization warning (#2238)

llama.cpp

commit 672dda10e4d8ac79df5d5970da7fb69d242ca9a7
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Mon Jul 17 03:57:28 2023 +0800

    ggml : fixed runtime bugs and compile errors related to GGML_PERF and GGML_DEBUG (#2219)
    
    * fixed runtime bugs and compile errors related to GGML_PERF and GGML_DEBUG
    
    * remove ifdef GGML_PERF; update fmt

ggml.c

commit 27ab66e437797aedbb23b3599385756b6c26ac39
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Sun Jul 16 21:54:47 2023 +0200

    py : turn verify-checksum-models.py into executable (#2245)
    
    README.md was adjusted to reflect the change.
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>

README.md
scripts/verify-checksum-models.py

commit 6e7cca404748dd4b1a3affd0d1296e37f4ac0a6f
Author: Xiao-Yong Jin <jinxiaoyong@gmail.com>
Date:   Sat Jul 15 06:34:16 2023 -0400

    llama : add custom RoPE (#2054)
    
    * Implement customizable RoPE
    
    The original RoPE has pre-defined parameters
    
    theta_i = 10000^(−2(i−1)/d), for i in [1, 2, ..., d/2]
    
    Our customizable RoPE, ggml_rope_custom_inplace, uses
    
    theta_i = scale * base^(−2(i−1)/d), for i in [1, 2, ..., d/2]
    
    with the default matches the original
    
    scale = 1.0
    base = 10000
    
    The new command line arguments
    --rope-freq-base
    --rope-freq-scale
    set the two new RoPE parameter.
    
    Recent researches show changing these two parameters extends the context limit with minimal loss.
    
    1. Extending Context to 8K
       kaiokendev
       https://kaiokendev.github.io/til#extending-context-to-8k
    
    2. Extending Context Window of Large Language Models via Positional Interpolation
       Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian
       https://arxiv.org/abs/2306.15595
    
    3. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.
       https://www.reddit.com/user/bloc97
       https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/
    
    For the bold, try adding the following command line parameters to your favorite model:
    -c 16384 --rope-freq-base 80000 --rope-freq-scale 0.5
    
    * ggml-metal: fix custom rope
    
    * common: fix argument names in help
    
    * llama: increase MEM_REQ_EVAL for MODEL_3B
    
    It avoids crashing for quantized weights on CPU.
    Better ways to calculate the required buffer size would be better.
    
    * llama: make MEM_REQ_EVAL depend on n_ctx
    
    * server: use proper Content-Type in curl examples
    
    Without the header Content-Type: application/json, curl will POST with
    Content-Type: application/x-www-form-urlencoded
    
    Though our simple server doesn't care, the httplib.h used has a limit
    with CPPHTTPLIB_FORM_URL_ENCODED_PAYLOAD_MAX_LENGTH 8192
    
    With Content-Type: application/json, we can send large json data.
    
    * style : minor fixes, mostly indentations
    
    * ggml : fix asserts
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/common.cpp
examples/common.h
examples/main/main.cpp
examples/server/README.md
examples/server/chat.sh
examples/server/server.cpp
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
llama.cpp
llama.h

commit a6803cab946c817fb7aaf2a40b317f5d3e373bd1
Author: Dave Della Costa <ddellacosta+github@gmail.com>
Date:   Fri Jul 14 15:13:38 2023 -0400

    flake : add runHook preInstall/postInstall to installPhase so hooks function (#2224)

flake.nix

commit 7dabc66f3c63f8ea0f61bac346fa138e01df675f
Author: wzy <32936898+Freed-Wu@users.noreply.github.com>
Date:   Sat Jul 15 03:05:08 2023 +0800

    make : use pkg-config for OpenBLAS (#2222)

Makefile

commit 7cdd30bf1f84339c55a5e3de29384f6bbdebb61c
Author: Bach Le <bach@bullno1.com>
Date:   Sat Jul 15 03:00:58 2023 +0800

    cuda : allocate all temporary ggml_tensor_extra_gpu from a fixed-size buffer (#2220)

ggml-cuda.cu

commit e8035f141e1f71d739fa5cfc9c01531cdee6fc16
Author: Evan Miller <emmiller@gmail.com>
Date:   Fri Jul 14 14:55:56 2023 -0400

    ggml : fix static_assert with older compilers #2024 (#2218)

ggml.c
k_quants.h

commit 7513b7b0a1c11faa00ad5a34d22681e5f07d32e4
Author: Bach Le <bach@bullno1.com>
Date:   Sat Jul 15 02:55:24 2023 +0800

    llama : add functions that work directly on model (#2197)
    
    * Remove vocab reference from context
    
    * Add functions that works directly with model

llama.cpp
llama.h

commit de8342423d9600cf6e15455c1a27bae441262b45
Author: Ali Chraghi <63465728+alichraghi@users.noreply.github.com>
Date:   Fri Jul 14 11:50:58 2023 -0700

    build.zig : install config header (#2216)

build.zig

commit c48c525f8711780f3f7c59bf92f1760f38317218
Author: Shangning Xu <32517059+xushangning@users.noreply.github.com>
Date:   Sat Jul 15 02:40:05 2023 +0800

    examples : fixed path typos in embd-input (#2214)

examples/embd-input/README.md
examples/embd-input/llava.py

commit 206e01de117cc65ed8713ac9fdebc57ba4532ec3
Author: Jiahao Li <liplus17@163.com>
Date:   Sat Jul 15 02:38:24 2023 +0800

    cuda : support broadcast add & mul (#2192)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-cuda.cu

commit 4304bd3cded73c867a882ea5ca4517e3995cc996
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jul 14 19:44:08 2023 +0200

    CUDA: mul_mat_vec_q kernels for k-quants (#2203)

ggml-cuda.cu

commit 229aab351c375899debad45fcb213bf0565bba4e
Author: James Reynolds <magnusviri@users.noreply.github.com>
Date:   Fri Jul 14 11:34:40 2023 -0600

    make : fix combination of LLAMA_METAL and LLAMA_MPI (#2208)
    
    Fixes https://github.com/ggerganov/llama.cpp/issues/2166 by moving commands after the CFLAGS are changed.

Makefile

commit 697966680b27d9b4f05668605b863cb9aea3e15f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 14 16:36:41 2023 +0300

    ggml : sync (ggml_conv_2d, fix mul_mat bug, CUDA GLM rope)

ggml-cuda.cu
ggml.c

commit 27ad57a69b85bf12420a27e9945e580cc280be57
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jul 14 12:46:21 2023 +0300

    Metal: faster Q4_0 and Q4_1 matrix x vector kernels (#2212)
    
    * 3-5% faster Q4_0 on Metal
    
    * 7-25% faster Q4_1 on Metal
    
    * Oops, forgot to delete the original Q4_1 kernel
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 32c54116318929c90fd7ae814cf9b5232cd44c36
Author: Howard Su <howard0su@gmail.com>
Date:   Thu Jul 13 21:58:25 2023 +0800

    Revert "Support using mmap when applying LoRA (#2095)" (#2206)
    
    Has perf regression when mlock is used.
    
    This reverts commit 2347463201a9f4159ae95b737e1544dd300569c8.

examples/common.cpp
examples/main/README.md
examples/server/README.md
examples/server/server.cpp
llama-util.h

commit ff5d58faecf1f02b05bd015bdfc6a394cf2bc9ba
Author: Howard Su <howard0su@gmail.com>
Date:   Thu Jul 13 21:58:09 2023 +0800

    Fix compile error on Windows CUDA (#2207)

ggml-cuda.cu

commit b782422a3e090d0aeab84bfa03ba008dcd1c2a3d
Author: Bodo Graumann <mail@bodograumann.de>
Date:   Thu Jul 13 15:49:14 2023 +0200

    devops : add missing quotes to bash script (#2193)
    
    This prevents accidentally expanding arguments that contain spaces.

.devops/tools.sh

commit 1cbf561466e957b25f0e8163c2386683f8674369
Author: Shouzheng Liu <61452103+lshzh-ww@users.noreply.github.com>
Date:   Wed Jul 12 16:10:55 2023 -0400

    metal : new q4_0 matrix-vector kernel (#2188)
    
    Prefetch data to improve GPU utilization. ~48% faster for 33B model.

ggml-metal.m
ggml-metal.metal

commit 975221e9548ef6d9f4af8d39cdffc4811c050beb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 12 20:51:29 2023 +0300

    ggml : broadcast mul_mat + conv batch support (#2199)
    
    * ggml : broadcast mul_mat + conv batch support
    
    * ggml : apply mul_mat broadcast fix by @jploski

ggml.c

commit 4523d10d0cf8c088f1b26c76d38d73290eb3b444
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 12 20:27:03 2023 +0300

    ggml : add ggml_pool_1d and ggml_pool_2d

ggml.c
ggml.h

commit 680e6f91775f972f0df34f56807f30826370db59
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 12 20:26:18 2023 +0300

    cuda : add gelu support

ggml-cuda.cu

commit 4e7464ef88885cb3532738b03cac890f4077fa20
Author: Howard Su <howard0su@gmail.com>
Date:   Wed Jul 12 20:18:40 2023 +0800

    FP16 is supported in CM=6.0 (#2177)
    
    * FP16 is supported in CM=6.0
    
    * Building PTX code for both of 60 and 61
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

CMakeLists.txt

commit 2b5eb72e109577ed84e44bb8fa47e4956f337300
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jul 12 10:38:52 2023 +0200

    Fixed __dp4a compute capability: 6.0 -> 6.1 (#2189)

ggml-cuda.cu

commit f7d278faf308cb989c221895968f2a26f14b2155
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 12 10:54:19 2023 +0300

    ggml : revert CUDA broadcast changes from #2183 (#2191)

ggml-cuda.cu

commit 20d7740a9b45f6e5b247fa3738fdda35e18c2e8a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 11 22:53:34 2023 +0300

    ggml : sync (abort callback, mul / add broadcast, fix alibi) (#2183)

ggml-cuda.cu
ggml.c
ggml.h
tests/test-grad0.c
tests/test-opt.c

commit 5bf2a2771886ee86137e01dbc7492f78fb392066
Author: Spencer Sutton <spencersutton@users.noreply.github.com>
Date:   Tue Jul 11 12:31:10 2023 -0400

    ggml : remove src0 and src1 from ggml_tensor and rename opt to src (#2178)
    
    * Add ggml changes
    
    * Update train-text-from-scratch for change
    
    * mpi : adapt to new ggml_tensor->src
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-cuda.cu
ggml-metal.m
ggml-mpi.c
ggml.c
ggml.h

commit c9c74b4e3f9dcfab8b0032749ff8a579ab4e4d8d
Author: Bach Le <bach@bullno1.com>
Date:   Wed Jul 12 00:18:43 2023 +0800

    llama : add classifier-free guidance (#2135)
    
    * Initial implementation
    
    * Remove debug print
    
    * Restore signature of llama_init_from_gpt_params
    
    * Free guidance context
    
    * Make freeing of guidance_ctx conditional
    
    * Make Classifier-Free Guidance a sampling function
    
    * Correct typo. CFG already means context-free grammar.
    
    * Record sampling time in llama_sample_classifier_free_guidance
    
    * Shift all values by the max value before applying logsoftmax
    
    * Fix styling based on review

examples/common.cpp
examples/common.h
examples/main/main.cpp
llama.cpp
llama.h

commit 3ec7e596b2ba3f43c22f441254ca2bcfa91102ba
Author: Jinwoo Jeong <33892306+williamjeong2@users.noreply.github.com>
Date:   Wed Jul 12 01:12:35 2023 +0900

    docker : add '--server' option (#2174)

.devops/tools.sh

commit 917831c63a4138814d23da1917bf2b5d5b9faa6c
Author: Chad Brewbaker <crb002@gmail.com>
Date:   Tue Jul 11 11:03:06 2023 -0500

    readme : fix zig build instructions (#2171)

README.md

commit 2347463201a9f4159ae95b737e1544dd300569c8
Author: Howard Su <howard0su@gmail.com>
Date:   Tue Jul 11 22:37:01 2023 +0800

    Support using mmap when applying LoRA (#2095)
    
    * Support using mmap when applying LoRA
    
    * Fix Linux
    
    * Update comment to reflect the support lora with mmap

examples/common.cpp
examples/main/README.md
examples/server/README.md
examples/server/server.cpp
llama-util.h

commit bbef28218fe827265716b66977719b9ee2b21165
Author: LostRuins <39025047+LostRuins@users.noreply.github.com>
Date:   Tue Jul 11 22:01:08 2023 +0800

    Possible solution to allow K-quants on models with n_vocab!=32000 (#2148)
    
    * This allows LLAMA models that were previously incompatible with K quants to function mostly as normal. This happens when a model has a vocab != 32000, e.g 32001 which means it's not divisible by 256 or 64. Since the problematic dimensions only apply for `tok_embeddings.weight` and `output.weight` (dimentions 4096 x n_vocab), we can simply quantize these layers to Q8_0 whereas the majority of the hidden layers are still K-quanted since they have compatible dimensions.
    
    * Fix indentation
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * As an alternative, to avoid failing on Metal due to lack of Q8_0 support, instead quantize tok_embeddings.weight to Q4_0 and retain output.weight as F16. This results in a net gain of about 55mb for a 7B model compared to previous approach, but should minimize adverse impact to model quality.
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp

commit 5656d10599bd756dc0f17284e418e704200b43f3
Author: Evan Miller <emmiller@gmail.com>
Date:   Mon Jul 10 11:49:56 2023 -0400

    mpi : add support for distributed inference via MPI (#2099)
    
    * MPI support, first cut
    
    * fix warnings, update README
    
    * fixes
    
    * wrap includes
    
    * PR comments
    
    * Update CMakeLists.txt
    
    * Add GH workflow, fix test
    
    * Add info to README
    
    * mpi : trying to move more MPI stuff into ggml-mpi (WIP) (#2099)
    
    * mpi : add names for layer inputs + prep ggml_mpi_graph_compute()
    
    * mpi : move all MPI logic into ggml-mpi
    
    Not tested yet
    
    * mpi : various fixes - communication now works but results are wrong
    
    * mpi : fix output tensor after MPI compute (still not working)
    
    * mpi : fix inference
    
    * mpi : minor
    
    * Add OpenMPI to GH action
    
    * [mpi] continue-on-error: true
    
    * mpi : fix after master merge
    
    * [mpi] Link MPI C++ libraries to fix OpenMPI
    
    * tests : fix new llama_backend API
    
    * [mpi] use MPI_INT32_T
    
    * mpi : factor out recv / send in functions and reuse
    
    * mpi : extend API to allow usage with outer backends (e.g. Metal)
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
.gitignore
CMakeLists.txt
Makefile
README.md
examples/embd-input/embd-input-lib.cpp
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize/quantize.cpp
examples/server/server.cpp
examples/simple/simple.cpp
ggml-metal.m
ggml-mpi.c
ggml-mpi.h
llama.cpp
llama.h
tests/test-tokenizer-0.cpp

commit 1d1630996920f889cdc08de26cebf2415958540e
Author: oobabooga <112222186+oobabooga@users.noreply.github.com>
Date:   Sun Jul 9 05:59:53 2023 -0300

    llama : remove "first token must be BOS" restriction (#2153)

llama.cpp

commit db4047ad5cd8eae04db3b2efe0245e69a376601a
Author: Nigel Bosch <pnigelb@gmail.com>
Date:   Sun Jul 9 03:56:18 2023 -0500

    main : escape prompt prefix/suffix (#2151)

examples/common.cpp

commit 18780e0a5e17348236230bbe891901b9b5718709
Author: JackJollimore <130917767+JackJollimore@users.noreply.github.com>
Date:   Sun Jul 9 05:20:43 2023 -0300

    readme : update Termux instructions (#2147)
    
    The file pathing is significant when running models inside of Termux on Android devices. llama.cpp performance is improved with loading a .bin from the $HOME directory.

README.md

commit 3bbc1a11f04a9adc0d0e08c2940ba4d2978755ab
Author: clyang <clyang@clyang.net>
Date:   Sun Jul 9 16:12:20 2023 +0800

    ggml : fix buidling with Intel MKL but ask for "cblas.h" issue (#2104) (#2115)
    
    * Fix buidling with Intel MKL but ask for "cblas.h" issue
    
    * Use angle brackets to indicate the system library

CMakeLists.txt
ggml.c

commit 2492a53fd0d8372ecc67f49f07b581905175eea8
Author: rankaiyx <rankaiyx@rankaiyx.com>
Date:   Sun Jul 9 15:38:42 2023 +0800

    readme : add more docs indexes (#2127)
    
    * Update README.md to add more docs indexes
    
    * Update README.md to add more docs indexes

README.md

commit 64639555ff93c8ead2b80becb49cc6b60aeac240
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 8 20:01:44 2023 +0200

    Fixed OpenLLaMA 3b CUDA mul_mat_vec_q (#2144)

ggml-cuda.cu

commit 061f5f8d2109bb7adcbd40f1b456d887c5a1df25
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 8 00:25:15 2023 +0200

    CUDA: add __restrict__ to mul mat vec kernels (#2140)

ggml-cuda.cu

commit 84525e7962bee0abef91108948bbf7f7bfdcf421
Author: dylan <canardleteer@users.noreply.github.com>
Date:   Fri Jul 7 11:25:25 2023 -0700

    docker : add support for CUDA in docker (#1461)
    
    Co-authored-by: canardleteer <eris.has.a.dad+github@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.devops/full-cuda.Dockerfile
.devops/main-cuda.Dockerfile
Makefile
README.md

commit a7e20edf2266169ccd97a4eb949a593d628fbd64
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 7 21:23:57 2023 +0300

    ci : switch threads to 1 (#2138)

.github/workflows/build.yml

commit 1d656d6360359cfdaaf5d64ed9690047b600dbcb
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Sat Jul 8 00:24:01 2023 +0800

    ggml : change ggml_graph_compute() API to not require context (#1999)
    
    * ggml_graph_compute: deprecate using ggml_context, try resolve issue #287
    
    * rewrite: no longer consider backward compitability; plan and make_plan
    
    * minor: rename ctx as plan; const
    
    * remove ggml_graph_compute from tests/test-grad0.c, but current change breaks backward
    
    * add static ggml_graph_compute_sugar()
    
    * minor: update comments
    
    * reusable buffers
    
    * ggml : more consistent naming + metal fixes
    
    * ggml : fix docs
    
    * tests : disable grad / opt + minor naming changes
    
    * ggml : add ggml_graph_compute_with_ctx()
    
    - backwards compatible API
    - deduplicates a lot of copy-paste
    
    * ci : enable test-grad0
    
    * examples : factor out plan allocation into a helper function
    
    * llama : factor out plan stuff into a helper function
    
    * ci : fix env
    
    * llama : fix duplicate symbols + refactor example benchmark
    
    * ggml : remove obsolete assert + refactor n_tasks section
    
    * ggml : fix indentation in switch
    
    * llama : avoid unnecessary bool
    
    * ggml : remove comments from source file and match order in header
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
examples/baby-llama/baby-llama.cpp
examples/benchmark/benchmark-matmult.cpp
examples/metal/metal.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml-metal.h
ggml-metal.m
ggml.c
ggml.h
llama.cpp
tests/CMakeLists.txt
tests/test-grad0.c
tests/test-opt.c

commit 72421402834141df6cbdcf595fe46dbd11874dce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jul 7 18:36:37 2023 +0300

    ggml : remove sched_yield() call in ggml_graph_compute_thread() (#2134)

ggml.c

commit 3e08ae99ceb143d67f9273fda47541e9d98ff23f
Author: Aarni Koskela <akx@iki.fi>
Date:   Fri Jul 7 16:12:49 2023 +0300

    convert.py: add mapping for safetensors bf16 (#1598)
    
    Fixes #1473

convert.py

commit 481f793acc3882a09d45d8d2c3076ad3d1c60cfc
Author: Howard Su <howard0su@gmail.com>
Date:   Fri Jul 7 11:34:18 2023 +0800

    Fix opencl by wrap #if-else-endif with \n (#2086)

ggml-opencl.cpp

commit dfd9fce6d65599bf33df43e616e85aa639bdae4c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jul 6 19:41:31 2023 +0300

    ggml : fix restrict usage

ggml.h

commit 36680f6e40e4440c3ec3385d0b7e5ca8bb6c37f7
Author: Judd <foldl@users.noreply.github.com>
Date:   Fri Jul 7 00:23:49 2023 +0800

    convert : update for baichuan (#2081)
    
    1. guess n_layers;
    2. relax warnings on context size;
    3. add a note that its derivations are also supported.
    
    Co-authored-by: Judd <foldl@boxvest.com>

README.md
convert.py
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/server/README.md

commit a17a2683d8fdb899ba497d0c28ccafb28c62efb6
Author: tslmy <tslmy@users.noreply.github.com>
Date:   Thu Jul 6 09:17:50 2023 -0700

    alpaca.sh : update model file name (#2074)
    
    The original file name, `ggml-alpaca-7b-q4.bin`, implied the first-generation GGML. After the breaking changes (mentioned in https://github.com/ggerganov/llama.cpp/issues/382), `llama.cpp` requires GGML V3 now. Those model files are named `*ggmlv3*.bin`. We should change the example to an actually working model file, so that this thing is more likely to run out-of-the-box for more people, and less people would waste time downloading the old Alpaca model.

examples/alpaca.sh

commit 31cfbb1013a482e89c72146e2063ac4362becae7
Author: Tobias Lütke <tobi@shopify.com>
Date:   Wed Jul 5 16:51:13 2023 -0400

    Expose generation timings from server & update completions.js (#2116)
    
    * use javascript generators as much cleaner API
    
    Also add ways to access completion as promise and EventSource
    
    * export llama_timings as struct and expose them in server
    
    * update readme, update baked includes
    
    * llama : uniform variable names + struct init
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/server/README.md
examples/server/completion.js.hpp
examples/server/deps.sh
examples/server/index.html.hpp
examples/server/public/completion.js
examples/server/public/index.html
examples/server/server.cpp
llama.cpp
llama.h

commit 983b555e9ddb36703cee4d22642afe958de093b7
Author: Jesse Jojo Johnson <williamsaintgeorge@gmail.com>
Date:   Wed Jul 5 18:03:19 2023 +0000

    Update Server Instructions (#2113)
    
    * Update server instructions for web front end
    * Update server README
    * Remove duplicate OAI instructions
    * Fix duplicate text
    
    ---------
    
    Co-authored-by: Jesse Johnson <thatguy@jessejojojohnson.com>

examples/server/README.md

commit ec326d350c72afd23709a409944728a607188cc0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 5 20:44:11 2023 +0300

    ggml : fix bug introduced in #1237

ggml.c

commit 1b6efeab829f3eeda5b39bd47624bb60b3531b88
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jul 5 20:20:05 2023 +0300

    tests : fix test-grad0

scripts/sync-ggml.sh
tests/test-grad0.c

commit 1b107b8550dced48dc5f41184640061354226b96
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Jul 5 16:13:06 2023 +0000

    ggml : generalize `quantize_fns` for simpler FP16 handling (#1237)
    
    * Generalize quantize_fns for simpler FP16 handling
    
    * Remove call to ggml_cuda_mul_mat_get_wsize
    
    * ci : disable FMA for mac os actions
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
examples/quantize-stats/quantize-stats.cpp
ggml.c
ggml.h
llama.cpp
pocs/vdot/q8dot.cpp
pocs/vdot/vdot.cpp
tests/test-quantize-fns.cpp
tests/test-quantize-perf.cpp

commit 8567c76b5326e862be0755a8dc1dd988223fcae3
Author: Jesse Jojo Johnson <williamsaintgeorge@gmail.com>
Date:   Wed Jul 5 15:13:35 2023 +0000

    Update server instructions for web front end (#2103)
    
    Co-authored-by: Jesse Johnson <thatguy@jessejojojohnson.com>

examples/server/README.md

commit 924dd22fd3ba93e097f8d19ba5cda919ca2fe2fb
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jul 5 14:19:42 2023 +0200

    Quantized dot products for CUDA mul mat vec (#2067)

CMakeLists.txt
Makefile
README.md
ggml-cuda.cu

commit 051c70dcd55709c9cbbfa849af035951fe720433
Author: Howard Su <howard0su@gmail.com>
Date:   Wed Jul 5 18:31:23 2023 +0800

    llama: Don't double count the sampling time (#2107)

llama.cpp

commit 9e4475f5cf639315f61ed7b8da6258bb0c7c5ca9
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jul 5 08:58:05 2023 +0200

    Fixed OpenCL offloading prints (#2082)

llama.cpp

commit 7f0e9a775ecc4c6ade271c217f63d6dc93e79eaa
Author: Nigel Bosch <pnigelb@gmail.com>
Date:   Tue Jul 4 18:33:33 2023 -0500

    embd-input: Fix input embedding example unsigned int seed (#2105)

examples/embd-input/embd-input-lib.cpp

commit b472f3fca558b6335adbd87210ed58cfb5da37cb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 4 22:25:22 2023 +0300

    readme : add link web chat PR

README.md

commit ed9a54e5129a11c2a5b555e1dc65e875e3c37b4f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jul 4 21:54:11 2023 +0300

    ggml : sync latest (new ops, macros, refactoring) (#2106)
    
    - add ggml_argmax()
    - add ggml_tanh()
    - add ggml_elu()
    - refactor ggml_conv_1d() and variants
    - refactor ggml_conv_2d() and variants
    - add helper macros to reduce code duplication in ggml.c

ggml.c
ggml.h
scripts/sync-ggml.sh

commit f257fd255044decffad93dee2502875ce66ad80c
Author: jwj7140 <32943891+jwj7140@users.noreply.github.com>
Date:   Wed Jul 5 03:06:12 2023 +0900

    Add an API example using server.cpp similar to OAI. (#2009)
    
    * add api_like_OAI.py
    * add evaluated token count to server
    * add /v1/ endpoints binding

examples/server/README.md
examples/server/api_like_OAI.py
examples/server/server.cpp

commit 7ee76e45afae7f9a7a53e93393accfb5b36684e1
Author: Tobias Lütke <tobi@shopify.com>
Date:   Tue Jul 4 10:05:27 2023 -0400

    Simple webchat for server (#1998)
    
    * expose simple web interface on root domain
    
    * embed index and add --path for choosing static dir
    
    * allow server to multithread
    
    because web browsers send a lot of garbage requests we want the server
    to multithread when serving 404s for favicon's etc. To avoid blowing up
    llama we just take a mutex when it's invoked.
    
    
    * let's try this with the xxd tool instead and see if msvc is happier with that
    
    * enable server in Makefiles
    
    * add /completion.js file to make it easy to use the server from js
    
    * slightly nicer css
    
    * rework state management into session, expose historyTemplate to settings
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
examples/server/completion.js.hpp
examples/server/deps.sh
examples/server/index.html.hpp
examples/server/index.js.hpp
examples/server/public/completion.js
examples/server/public/index.html
examples/server/public/index.js
examples/server/server.cpp

commit acc111caf93fc6681450924df9f99679c384c59e
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue Jul 4 15:38:04 2023 +0300

    Allow old Make to build server. (#2098)
    
    Also make server build by default.
    
    Tested with Make 3.82

Makefile

commit 23c7c6fc9182b041f006b86ea1e7f99911ecf344
Author: ZhouYuChen <zhouyuchen@naver.com>
Date:   Tue Jul 4 20:15:16 2023 +0800

    Update Makefile: clean simple (#2097)

Makefile

commit 698efad5fbbf326f01288649b123eff5f79b417e
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Tue Jul 4 01:50:12 2023 +0200

    CI: make the brew update temporarily optional. (#2092)
    
    until they decide to fix the brew installation in the macos runners.
    see the open issues. eg https://github.com/actions/runner-images/pull/7710

.github/workflows/build.yml

commit 14a2cc71f62e45803ae70890ffbdeb0a172e6210
Author: Govlzkoy <gotope@users.noreply.github.com>
Date:   Tue Jul 4 07:50:00 2023 +0800

    [ggml] fix index for ne03 value in ggml_cl_mul_f32 (#2088)

ggml-opencl.cpp

commit 1cf14ccef12e19c5a5b0b17ab456242d1f8c7fdd
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue Jul 4 00:05:23 2023 +0300

    fix server crashes (#2076)

examples/server/server.cpp

commit cc45a7feb8412e84ff292207621412fffc0d3d51
Author: Howard Su <howard0su@gmail.com>
Date:   Tue Jul 4 02:43:55 2023 +0800

    Fix crash of test-tokenizer-0 under Debug build (#2064)
    
    * Fix crash of test-tokenizer-0 under Debug build
    
    * Change per comment

ggml-cuda.cu
llama.cpp

commit 55dbb915cc2a95048f56e667b09dfad38d840421
Author: Howard Su <howard0su@gmail.com>
Date:   Mon Jul 3 19:58:58 2023 +0800

    [llama] No need to check file version when loading vocab score (#2079)

llama.cpp

commit d7d2e6a0f0c74f7a570dae384dfff371ac744d2a
Author: WangHaoranRobin <56047610+WangHaoranRobin@users.noreply.github.com>
Date:   Mon Jul 3 05:38:44 2023 +0800

    server: add option to output probabilities for completion (#1962)
    
    * server: add option to output probabilities for completion
    * server: fix issue when handling probability output for incomplete tokens for multibyte character generation
    * server: fix llama_sample_top_k order
    * examples/common.h: put all bool variables in gpt_params together

examples/common.h
examples/server/server.cpp

commit 46088f72318981341a2d646f12f6eee6aec06d65
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jul 2 09:46:46 2023 +0300

    ggml : fix build with OpenBLAS (close #2066)

ggml.c

commit 0bc2cdfc875fa7877d8e01c8bb17066f99c08f21
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 1 21:49:44 2023 +0200

    Better CUDA synchronization logic (#2057)

ggml-cuda.cu
ggml-cuda.h

commit befb3a35627432473f143c90994557d78ff5bc67
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jul 1 21:47:26 2023 +0200

    Test-based VRAM scratch size + context adjustment (#2056)

llama.cpp

commit b2132270678c473f7cd9ba871b03d694126bc33a
Author: Daniel Drake <drake@endlessos.org>
Date:   Sat Jul 1 20:31:44 2023 +0200

    cmake : don't force -mcpu=native on aarch64 (#2063)
    
    It's currently not possible to cross-compile llama.cpp for aarch64
    because CMakeLists.txt forces -mcpu=native for that target.
    
    -mcpu=native doesn't make sense if your build host is not the
    target architecture, and clang rejects it for that reason, aborting the
    build. This can be easily reproduced using the current Android NDK to build
    for aarch64 on an x86_64 host.
    
    If there is not a specific CPU-tuning target for aarch64 then -mcpu
    should be omitted completely. I think that makes sense, there is not
    enough variance in the aarch64 instruction set to warrant a fixed -mcpu
    optimization at this point. And if someone is building natively and wishes
    to enable any possible optimizations for the host device, then there is
    already the LLAMA_NATIVE option available.
    
    Fixes #495.

CMakeLists.txt

commit 2f8cd979ecd1fa582852e7136e92ff8990b98fd8
Author: Aaron Miller <apage43@ninjawhale.com>
Date:   Sat Jul 1 11:14:59 2023 -0700

    metal : release buffers when freeing metal context (#2062)

ggml-metal.m
llama.cpp

commit 471aab6e4cb89d8ef6d043f1bc93acb6eb78ab67
Author: Judd <foldl@users.noreply.github.com>
Date:   Sun Jul 2 01:00:25 2023 +0800

    convert : add support of baichuan-7b (#2055)
    
    Co-authored-by: Judd <foldl@boxvest.com>

README.md
convert.py

commit 463f2f4c4f8dd5ca9594b7d65849f346f0effe05
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 1 19:05:09 2023 +0300

    llama : fix return value of llama_load_session_file_internal (#2022)

llama.cpp

commit cb44dbc7de287b3d17772cfb1aa49d55e082ce5b
Author: Rand Xie <randxiexyy29@gmail.com>
Date:   Sun Jul 2 00:02:58 2023 +0800

    llama : catch llama_load_session_file_internal exceptions (#2022)
    
    * convert checks in llama_load_session_file to throw and handle them
    
    * make llama_load_session_file_internal static
    
    * address feedbacks to avoid using exceptions

llama.cpp

commit 79f634a19d1c32a6cfb1befc21551ee684fced6b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 1 18:46:00 2023 +0300

    embd-input : fix returning ptr to temporary

examples/embd-input/embd-input-lib.cpp
examples/embd-input/embd-input.h

commit 04606a159947566b27810508433e6ca5dbc684ba
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jul 1 18:45:44 2023 +0300

    train : fix compile warning

examples/train-text-from-scratch/train-text-from-scratch.cpp

commit b1ca8f36a9cdbcee5f5c425df717611a1040a897
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Sat Jul 1 23:42:43 2023 +0800

    ggml : disable GGML_TASK_INIT and GGML_TASK_FINALIZE by default (#1995)
    
    Will not be scheduled unless explicitly enabled.

ggml.c
ggml.h

commit b8c8dda75fdf5fdea49c80af36818e7c30fe0ddf
Author: Howard Su <howard0su@gmail.com>
Date:   Thu Jun 29 21:15:15 2023 +0800

    Use unsigned for random seed (#2006)
    
    * Use unsigned for random seed. Keep -1 as the value to use a time based seed.
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/common.cpp
examples/common.h
examples/embedding/embedding.cpp
examples/main/README.md
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/server/README.md
examples/train-text-from-scratch/train-text-from-scratch.cpp
llama.cpp
llama.h

commit 96a712ca1b7f427e3bd7ffc0c70b2105cfc7fbf1
Author: LostRuins <39025047+LostRuins@users.noreply.github.com>
Date:   Thu Jun 29 11:56:43 2023 +0800

    Porting the improved K-Quant CUDA kernels to OpenCL (#1966)
    
    * Added broken new q4k quant
    
    * xx + ib0
    
    * Fix q2_k fast kernel
    
    * Use preprocessor for QK_K
    
    * Add q6_k fast matmul kernel
    
    * ported q3k speedup successfully
    
    * ported q2k and q5k speedups
    
    * remove old dot kernels and template
    
    * fixed global const struct types
    
    * fixing address spaces
    
    * fixed string too long CI issue
    
    ---------
    
    Co-authored-by: 0cc4m <picard12@live.de>

ggml-opencl.cpp

commit d3494bb86bf7ad5b0b60aae0220ea576f273b5c0
Author: m3ndax <adrian.goessl@outlook.com>
Date:   Wed Jun 28 20:39:08 2023 +0200

    llama : replacing auto &kv with const auto &kv (#2041)
    
    * Replacing auto &kv with const auto &kv
    
    * Create codacy.yml
    
    * Delete codacy.yml

llama.cpp

commit 5b351e94d041742cd50ffcf2d44718d63bab398a
Author: Salvador E. Tropea <stropea@inti.gob.ar>
Date:   Wed Jun 28 14:27:31 2023 -0300

    cuda : remove nchannels_x argument from mul_mat_vec_nc_f16_f32 (#2028)
    
    - Not used

ggml-cuda.cu

commit 6432aabb6dc887436e4d57414b63116189c3b13b
Author: Salvador E. Tropea <stropea@inti.gob.ar>
Date:   Wed Jun 28 14:26:26 2023 -0300

    cuda : fix missing const qualifier in casts (#2027)

ggml-cuda.cu

commit b922bc351b69770cec2d35d2aa50fa052b95ca93
Author: Howard Su <howard0su@gmail.com>
Date:   Wed Jun 28 10:13:02 2023 -0700

    llama : remove shards weight file support (#2000)
    
    * Remove multiple shards
    
    * Remove multiple file loaders
    
    * Remove llama_load_tensor_shard class
    
    * Simplify load logic
    
    * Remove dead code guess_n_parts function
    
    * Remove vocab_only from constructor of llama_model_loader
    
    * Remove alignment_prevents_mmap which is not more needed.
    
    * Remove useless check

llama.cpp

commit 7f9753fa1263c4eded9a3de19778562f0e1093d7
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 28 18:35:54 2023 +0200

    CUDA GPU acceleration for LoRAs + f16 models (#1970)

examples/common.cpp
ggml-cuda.cu
ggml-cuda.h
llama.cpp

commit cfa0750bc9dbc2d957a91b8ed09ab0035d8f3d4e
Author: ningshanwutuobang <ningshanwutuobang@gmail.com>
Date:   Wed Jun 28 23:53:37 2023 +0800

    llama : support input embeddings directly  (#1910)
    
    * add interface for float input
    
    * fixed inpL shape and type
    
    * add examples of input floats
    
    * add test example for embd input
    
    * fixed sampling
    
    * add free for context
    
    * fixed add end condition for generating
    
    * add examples for llava.py
    
    * add READMD for llava.py
    
    * add READMD for llava.py
    
    * add example of PandaGPT
    
    * refactor the interface and fixed the styles
    
    * add cmake build for embd-input
    
    * add cmake build for embd-input
    
    * Add MiniGPT-4 example
    
    * change the order of the args of llama_eval_internal
    
    * fix ci error

.gitignore
Makefile
convert-lora-to-ggml.py
examples/CMakeLists.txt
examples/embd-input/.gitignore
examples/embd-input/CMakeLists.txt
examples/embd-input/README.md
examples/embd-input/embd-input-lib.cpp
examples/embd-input/embd-input-test.cpp
examples/embd-input/embd-input.h
examples/embd-input/embd_input.py
examples/embd-input/llava.py
examples/embd-input/minigpt4.py
examples/embd-input/panda_gpt.py
llama.cpp
llama.h

commit 9d23589d638dc74577d5ff880e6d4248b795f12e
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Tue Jun 27 19:06:33 2023 +0200

    fix pthreads setaffinity usage on android (#2020)

ggml.c

commit 0be54f75a6c3e9a09ea71bdfcdabf9a996a0549b
Author: Howard Su <howard0su@gmail.com>
Date:   Tue Jun 27 13:07:13 2023 +0800

    baby-llama : fix build after ggml_rope change (#2016)

examples/baby-llama/baby-llama.cpp

commit 181e8d975528a4e27eabb8ae6e9865f9ceae4b37
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 27 00:37:13 2023 +0300

    llama : fix rope usage after ChatGLM change

examples/train-text-from-scratch/train-text-from-scratch.cpp
llama.cpp

commit d9779021bd59ed96daae75e820a5ac5da47ca8ff
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 27 00:06:51 2023 +0300

    ggml : add support for ChatGLM RoPE

ggml.c
ggml.h

commit d38e45157862b58a1824387e64860d68ca3533a7
Author: Roman Parykin <donderom@gmail.com>
Date:   Mon Jun 26 22:47:59 2023 +0300

    readme : add Scala 3 bindings repo (#2010)

README.md

commit eaa6ca5a61b8c9501df9ebe3d264f45b75a5f8aa
Author: David Yang <davidyang6us@gmail.com>
Date:   Tue Jun 27 03:45:32 2023 +0800

    ggml : increase max tensor name + clean up compiler warnings in train-text (#1988)
    
    * Clean up compiler warnings in train-text
    
    Some brackets to disambiguate order of operations
    
    * Increase GGML_MAX_NAME
    
    Avoiding strncpy danger in train-text-from-scratch and reducing potential future name length issues

examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml.h

commit aa777abbb73655c4e1e9237b7c0ad66745e8e48c
Author: Gustavo Rocha Dias <91472747+gustrd@users.noreply.github.com>
Date:   Mon Jun 26 16:34:45 2023 -0300

    readme : LD_LIBRARY_PATH complement for some Android devices when building with CLBlast inside Termux (#2007)
    
    * docs - Alternative way to build at Android, with CLBlast.
    
    * doc - LD_LIBRARY_PATH complement for some Android devices when building with CLBlast inside Termux.
    
    * doc- fix typo

README.md

commit c824d2e368d193d9f564ff29880a51cda9f90527
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 26 21:03:59 2023 +0300

    ggml : avoid conv 2d kernel round up

ggml.c

commit b853d456018b10820686362af41b2f2f75f1eec6
Author: zrm <trustiosity.zrm@gmail.com>
Date:   Mon Jun 26 13:57:59 2023 -0400

    ggml : add NUMA support (#1556)
    
    * detect NUMA systems and pin work threads to nodes (linux)
    
    * disable mmap prefetch/readahead for NUMA systems
    
    * avoid sending finalize op to thread pool if it does nothing
    
    * silence robot
    
    * fix args
    
    * make --numa a param
    
    * recommendation that n_nodes evenly divide n_threads did not warrant such aggressive enforcement
    
    * lower synchronization overhead
    
    * statically allocate
    
    * move numa state to g_state
    
    * add description for --numa
    
    * ggml : minor style changes
    
    * ggml : minor style + try fix sanitizer build
    
    * llama : allow to initialize backend with NUMA support
    
    * llama : avoid ggml include in llama-util.h
    
    * ggml : style / formatting
    
    * ggml : fix handling of ops with n_threads > n_tasks > 1
    
    * server : utilize numa parameter
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/common.cpp
examples/common.h
examples/embedding/embedding.cpp
examples/main/README.md
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize/quantize.cpp
examples/server/server.cpp
examples/simple/simple.cpp
ggml.c
ggml.h
llama-util.h
llama.cpp
llama.h

commit 9225baef71407d799a6f7f563b77fd7f82791416
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 26 20:10:52 2023 +0300

    k-quants : fix indentation

k_quants.c

commit a84ab1da8dc6a59a5b67420ae1322f09503ffc72
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Tue Jun 27 01:47:02 2023 +0900

    tests : fix quantize perf (#1990)
    
    * fix test quantize perf
    
    * avoid the global state

tests/test-quantize-perf.cpp

commit 5743ca80928d8410754ec64a5673d5c2dd6cfbb7
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Tue Jun 27 01:46:07 2023 +0900

    k-quants : add AVX support to dot functions (#1916)
    
    * k_quants : add AVX support
    
    * k_quants : apply review comments

k_quants.c

commit 412c60e4739367144e51e59add5dc7749d084115
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 26 19:45:09 2023 +0300

    readme : add link to new k-quants for visibility

README.md

commit 6769e944c727c63612dcafbef52009d21ae00fff
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 26 19:43:07 2023 +0300

    k-quants : support for super-block size of 64 (#2001)
    
    * k_quants: WIP super-blocks with 64 weights
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q6_K scalar and AVX2 works
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q4_K scalar and AVX2 works
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q2_K scalar and AVX2 works. Q2_K is way too slow (it is actually slower
    than the scalar implementation)
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q3_K scalar and AVX2 works.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q5_K scalar and AVX2 works, and with that all
    k_quants are done on AVX2 and scalar
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q6_K working on CUDA. Cannot make it run quite as gast as
    with super-blocks with 256 weigths: 8% slower on 4080,
    20% slower on the 1660 (but there we fit 1 less layer on the
    GPU because pf the larger model size), so some fraction of
    these 20% is due to that,
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q4_K working on CUDA. ~10% slower on GTX-1660,
    16% slower on 4080.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q2_K working on CUDA. ~3% slower on GTX-1660,
    10% slower on 4080.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q3_K working on CUDA.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q5_K working on CUDA, and with this CUDA is done.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q6_K working on ARM_NEON
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q4_K working on ARM_NEON, but quite a bit slower than 256 weights
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q2_K working on ARM_NEON, but quite a bit slower than 256 weights
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q3_K working on ARM_NEON, but quite a bit slower than 256 weights.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q5_K working on ARM_NEON, but quite a bit slower than 256 weights.
    
    With that, we have full support for ARM_NEON, although
    performance is not quite there.
    
    * k_quants: WIP super-blocks with 64 weights
    
    Slightly more efficient Q3_K and Q5_K
    
    * k_quants: WIP super-blocks with 64 weights
    
    Another small improvement for Q3_K and Q5_K on ARM_NEON
    
    * k_quants: WIP super-blocks with 64 weights
    
    Yet another speedup for Q5_K on ARM_NEON.
    We are now within 10% of the QK_K = 256 version.
    
    * k_quants: WIP super-blocks with 64 weights
    
    * We are able to pass preprocessor macros to the Metal
      compiler
    * Q6_K works and is actually slightly more efficient than
      the QK_K = 256 version (25.2 ms vs 25.8 ms)
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q4_K works on Metal and is actually slightly faster
    than QK_K = 256 (21.95 ms vs 24.0 ms).
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q2_K works on Metal and is very slightly faster
    than QK_K = 256 (23.8 ms vs 24.2 ms).
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q3_K works on Metal and is slightly faster
    than QK_K = 256 (26.6 ms vs 28.3 ms).
    
    * k_quants: WIP super-blocks with 64 weights
    
    Q5_K works on Metal and is slightly faster
    than QK_K = 256 (23.7 ms vs 26.3 ms).
    
    * k_quants: call them _K, not _k, also on Metal
    
    * k_quants: correctly define QK_K in llama.cpp
    
    * Fixed bug in q4_K quantization added with the 64-block addition
    
    * Simplify via lambda
    
    * k_quants: swicth Q3_K to 4-bit scales when QK_K = 64
    
    Otherwise there isn't much benefit from this
    quantization type. There is some very slight loss
    in accuracy, but we reduce size by ~7%.
    E.g., for OpenLLaMA-3B, Q3_K_S perplexity is
    8.6131 with 8-bit scales and 8.6352 with 4-bit,
    while file size decreases from 1.53G to 1.44G.
    
    * k_quants: switch Q4_K to 4-bit scales when QK_K = 64
    
     Here the loss in accuracy is greater than for Q3_K,
     but the Q4_K points still move further to the left on
     the perplexity vs size curve.
    
    * k_quants: forgot to add the Metal changes in last commit
    
    * k_quants: change Q5_K to be type 0 when QK_K = 64
    
    Still needs AVX2 implementation
    
    * k_quants: AVX2 implementation for new 64-weight Q5_K
    
    * k_quants: 10% faster ARM_NEON Q5_K dot product
    
    * k_quants: fixed issue caused by merging with master
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

CMakeLists.txt
Makefile
ggml-cuda.cu
ggml-metal.m
ggml-metal.metal
k_quants.c
k_quants.h
llama.cpp

commit cbebf61ca7584e9709265395f0127ae7fc0f1882
Author: Howard Su <howard0su@gmail.com>
Date:   Mon Jun 26 23:15:47 2023 +0800

    Fix assert when free invalid cuda pointer (#2005)
    
    Fix assert via initializing extra structure always.
    CUDA error 1 at C:\GPT\llama.cpp\ggml-cuda.cu:2536: invalid argument

ggml-cuda.cu

commit 447ccbe8c39332fcdd0d98a041b6e2ff6f06219d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 25 16:08:12 2023 +0300

    readme : add new roadmap + manifesto

README.md

commit bd34cdde38f8fd661890ddd5f57ca30bf279877b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 25 14:25:08 2023 +0300

    ggml : sync latest ggml (custom operators)

ggml.c
ggml.h

commit c2a08f87b8d180115d04b8688f383d1b2761b16d
Author: anon998 <131767832+anon998@users.noreply.github.com>
Date:   Sun Jun 25 08:48:36 2023 +0000

    fix server sampling: top k sampler first (#1977)
    
    Co-authored-by: anon <anon@example.org>

examples/server/server.cpp

commit 66a2555ba6cab954c56d653b29c27bfbbacfbfb1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 25 09:07:03 2023 +0300

    readme : add Azure CI discussion link

README.md

commit e65ca7e14ac76c4046091da39d41a9017abaa9b3
Author: sjinzh <sjinzh@gmail.com>
Date:   Sun Jun 25 13:45:44 2023 +0800

    zig : upgrade build system support (#1981)
    
    * upgrade zig build system support
    
    * zig : add new line at the end of the file
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

build.zig

commit 5ec8dd5a3c6a9a109351d2257bb9d53869bd0a94
Author: Robyn <robyngraf@users.noreply.github.com>
Date:   Sun Jun 25 04:10:29 2023 +1000

    #1869 Fix null reference errors when training from scratch with CUDA (#1907)
    
    * #1869 Fix null reference errors when training from scratch with CUDA build
    
    Calling ggml_compute_forward when node->src0 was null was causing train-text-from-scratch.exe to terminate unexpectedly.
    
    * ggml : do not dereference src0 if NULL
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-cuda.cu
ggml.c

commit 65bdd52a867539691007f85c5508146d507f72c1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 24 19:40:18 2023 +0300

    tests : sync test-grad0 from ggml

tests/test-grad0.c

commit fdd18609113862dc6eb34dfc44a093d54c59ff1f
Author: Rowan Hart <rowanbhart@gmail.com>
Date:   Sat Jun 24 04:07:08 2023 -0700

    flake : fix ggml-metal.metal path and run nixfmt (#1974)

flake.nix

commit c943d823c14cef33092205ca3944de6fdf7abf99
Author: AN Long <aisk@users.noreply.github.com>
Date:   Sat Jun 24 19:02:06 2023 +0800

    convert : fix invalid params in write_vocab_only (#1975)

convert.py

commit f2c754e1c38936fdde74e4848ac468a696eb73c6
Author: slaren <slarengh@gmail.com>
Date:   Sat Jun 24 12:57:18 2023 +0200

    ggml : improve ggml_graph_dump_dot, add ggml_format_name (#1978)
    
    * Improve ggml_graph_dump_dot, add ggml_format_name
    
    * add more automatic names to view ops
    
    * fix name of copies

ggml.c
ggml.h

commit 11da1a85cd69af84b5861134738c7e9e20907470
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 24 13:38:18 2023 +0300

    readme : fix whitespaces

README.md

commit 235b610d650cbfed6dbd5d671f750d35fc18cd7d
Author: Alberto <57916483+albbus-stack@users.noreply.github.com>
Date:   Sat Jun 24 12:32:13 2023 +0200

    readme : fixed termux instructions (#1973)

README.md

commit b061ba9e2a7a2c335a200df8c11aed5e31e4ccbb
Author: Alex Renda <alexrenda@users.noreply.github.com>
Date:   Sat Jun 24 03:15:01 2023 -0700

    llama : fix top-p sampling to match the canonical definition (#1953)
    
    * Fix top-p sampling to match the standard definition (smallest set that has probability mass at least p, not largest set with probability mass less than p)
    
    * top-p: correct gt to gte
    
    * add test for correct top-p behavior

llama.cpp
tests/test-sampling.cpp

commit 527b6fba1d237befb324fd846bda7418c0fa394d
Author: Didzis Gosko <didzis@users.noreply.github.com>
Date:   Sat Jun 24 11:47:58 2023 +0300

    llama : make model stateless and context stateful (llama_state) (#1797)
    
    * llama : make model stateless and context stateful
    
    * llama : minor cleanup
    
    * llama : update internal API declaration
    
    * Apply suggestions from code review
    
    fix style
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Missing model memory release
    
    * Fix style
    
    * Add deprecated warning for public API function llama_init_from_file
    
    * Update public API use cases: move away from deprecated llama_init_from_file
    
    * Deprecate public API function llama_apply_lora_from_file
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/common.cpp
examples/common.h
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
examples/save-load-state/save-load-state.cpp
examples/server/server.cpp
examples/simple/simple.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
llama.cpp
llama.h
tests/test-tokenizer-0.cpp

commit d7b7484f74d486f77feb4c0b7af7e1718ed91651
Author: eiery <19350831+eiery@users.noreply.github.com>
Date:   Fri Jun 23 04:38:01 2023 -0400

    Add OpenLLaMA instructions to the README (#1954)
    
    * add openllama to readme

README.md

commit 7487137227eb32ed9b12156338b865cb29b2dfd1
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Thu Jun 22 14:20:47 2023 +0200

    rework convert.py to read hyper-parameters from config.json (#1958)
    
    * Read hyper-parameters from HuggingFace-transformer config.json, if they exist, and fall back to guessing, like before otherwise.
      This allows converting open_llama 3B and other non-standard model designs.

convert.py

commit bbca06e26949686d61a5126332680ba3cccf235c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 21 23:49:25 2023 +0200

    cmake: revert CUDA arch default to 52, 61 if f16 (#1959)

CMakeLists.txt

commit fb98254f99d769fcbbf20966ef386abdb48ef601
Author: Rahul Vivek Nair <68507071+RahulVivekNair@users.noreply.github.com>
Date:   Thu Jun 22 03:18:43 2023 +0530

    Fix typo in README.md (#1961)

README.md

commit 049aa16b8c5c6d086246e4e6b9feb18de4fbd663
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 20 19:05:54 2023 +0300

    readme : add link to p1

README.md

commit 2322ec223a21625dfe9bd73ee677444a98a24ac9
Author: Xiake Sun <xiake.sun@intel.com>
Date:   Tue Jun 20 05:42:40 2023 -0700

    Fix typo (#1949)

README.md

commit aacdbd40562684665b6f7b8ba6695b7a2088bbb0
Author: Ettore Di Giacinto <mudler@users.noreply.github.com>
Date:   Tue Jun 20 03:24:39 2023 +0200

    llama : fix params struct slignment (#1936)
    
    * Workaround struct misalignment during value-copy
    
    Signed-off-by: mudler <mudler@localai.io>
    
    * Move booleans at the bottom of the structure
    
    Signed-off-by: mudler <mudler@localai.io>
    
    * Add comment
    
    Signed-off-by: mudler <mudler@localai.io>
    
    ---------
    
    Signed-off-by: mudler <mudler@localai.io>

llama.cpp
llama.h

commit 20568fe60f00155fa25e92eb3a7f6b911d557967
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue Jun 20 01:12:39 2023 +0300

    [Fix] Reenable server embedding endpoint (#1937)
    
    * Add back embedding feature
    
    * Update README

examples/server/README.md
examples/server/server.cpp

commit 18b35625c3c19c64b7818a12460ba5ddb006dfdc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 19 20:43:30 2023 +0300

    ggml : fix bug in LBFGS optimizer (found by ggml tests)

ggml.c

commit ba4e85a8339b9dd7cdffad31838235f2fe45a8ea
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Mon Jun 19 23:20:06 2023 +0800

    llama : use aligned memory during ggml_init call from loading saved sessions (#1934)
    
    * fixed issue: memory is not guaranteed to be aligned properly during ggml_init call from loading saved sessions
    
    * - removed commented out old code from fix
    - updated another instance of same issue below original

llama.cpp

commit 23fc5c219a9aebd57c8af3fac454062cc4622980
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 19 18:18:34 2023 +0300

    cmake : fix trailing whitespaces

CMakeLists.txt

commit cb40dfca694b5cb849837548fd69932117c78362
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 19 18:17:03 2023 +0300

    llama : only use Q6_K for output weights if tensor size is multiple of 256 (#1932)
    
    * Only use Q6_K for output weights if tensor size is multiple of 256
    
    * Fixed copy/paste mistake
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit ca7c3f4da5d144d4cd1dd44903552e6ba49b8ec8
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 19 18:14:09 2023 +0300

    cuda : faster k-quants on older GPUs (#1930)
    
    * k_quants: hopefully much faster Q4_K on older GPUs
    
    On the GTX-1660 that I have available to represent
    "old GPUs", token prediction drops from 65.5 ms/tok
    to 41.5 ms/tok!
    
    * k_quants: hopefully much faster Q3_K on older GPUs
    
    On the GTX-1660 that I have available to represent
    "old GPUs", token prediction drops from 60.3 ms/tok
    to 41.0 ms/tok!
    
    * k_quants: faster Q2_K on older GPUs
    
    It looks like I didn't need to change anything
    compared to what we already had, so this is just
    adding clarifying comments. But I now measure
    36.3 ms/tok on the GTX-1660, instead fo the
    47.2 ms/tok that I have written in the faster
    k-quants PR.
    
    * k_quants: faster Q5_K on older GPUs
    
    68.5 ms/tok -> 62.0 ms/tok on GTX-1660.
    For some reason the same access pattern that leads
    to such resounding success for Q2_K to Q4_K did not
    work at all for Q5_K.
    
    It is also more difficult to measure because for Q5_K_S
    we only have 32 layers on the GTX-1660, so output, tok embeddings
    and kv cache are done on the CPU.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-cuda.cu

commit b97ca431db35ec96a339a721acb1219c1dd78bed
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 19 18:12:33 2023 +0300

    ggml : sync latest ggml repo (#1924)
    
    * ggml : sync latest ggml repo
    
    * ggml : remove unused comments
    
    * ggml : asserts

ggml.c
ggml.h

commit 1e3abfcef073e73c2b31e8570cb06c5cb2fd1f55
Author: Howard Su <howard0su@gmail.com>
Date:   Mon Jun 19 23:10:37 2023 +0800

    cmake : fix build shared ggml when CUDA is enabled (#1929)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt

commit 16b9cd193965769089881bb8ec012fccca7b37b6
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon Jun 19 10:23:56 2023 +0200

    Convert vector to f16 for dequantize mul mat vec (#1913)
    
    * Convert vector to f16 for dmmv
    
    * compile option
    
    * Added compilation option description to README
    
    * Changed cmake CUDA_ARCHITECTURES from "OFF" to "native"

CMakeLists.txt
Makefile
README.md
ggml-cuda.cu
llama.cpp

commit b24c3049d96557c24782e4d32feaae65f47277af
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jun 18 17:41:26 2023 +0200

    Added tokens per second to info prints (#1928)

llama.cpp

commit 0ede372a51fd8160688e01b587582666c14e94e5
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun Jun 18 16:07:09 2023 +0200

    Fixed incorrectly applying RMS norm twice (#1925)

llama.cpp

commit 8596af427722775f0df4a7c90b9af067ba90d4ef
Author: l3utterfly <gc.pthzfoldr@gmail.com>
Date:   Sun Jun 18 19:19:16 2023 +0800

    ggml : fix bug in ggml_compute_forward_add_q_f32 (#1918)

ggml.c

commit e1886cf4fe0d0f31661dda52a4a9f34bd9b9009a
Author: Mike <ytianhui2004@gmail.com>
Date:   Sun Jun 18 16:28:26 2023 +0800

    readme : update Android build instructions (#1922)
    
    Add steps for using termux on android devices to prevent common errors.

README.md

commit 8ab8ba62eb27cc340be2edf3418e051b1d967416
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jun 18 11:13:43 2023 +0300

    llama : prevent usage of k-quants when tensor size is not a multiple of 256 (#1921)
    
    * Fix examples/metal
    
    * k-quants: prevent usage when tensor size is not divisible by 256
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

llama.cpp

commit 90cc59d6ab1363a5c69c60c4b94db647d3a54a18
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sun Jun 18 10:52:10 2023 +0300

    examples : fix examples/metal (#1920)
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

examples/metal/metal.cpp

commit ce2c7d72e2d06988b5ddec6811ab923254542077
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 18 09:09:47 2023 +0300

    metal : handle buffers larger than device's maxBufferLength (#1826)
    
    * metal : handle buffers larger than device's maxBufferLength
    
    * metal : print more verbose device info + handle errors
    
    * metal : fix prints for overlapping views
    
    * metal : minimize view overlap to try to utilize device memory better

Makefile
ggml-metal.h
ggml-metal.m
ggml.c
ggml.h
llama.cpp

commit 57cd69460f736031a3fc54af1e97c03f80128478
Author: Howard Su <howard0su@gmail.com>
Date:   Sun Jun 18 12:29:47 2023 +0800

    cmake : add CUDA_ARCHITECTURES to new target ggml_static (#1917)

CMakeLists.txt

commit b2416493ab3ab21686d47c96669da6d6c6af08a4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 17 20:55:03 2023 +0300

    make : do not print help for simple example

Makefile

commit 4f9c43e3bd488b7561119785485e1155dba338d7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 17 20:24:11 2023 +0300

    minor : warning fixes

examples/main/main.cpp
ggml-metal.m

commit 2c9380dd2f77e41149340f3ecb09764d793b16db
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat Jun 17 19:15:02 2023 +0200

    Only one CUDA stream per device for async compute (#1898)

README.md
examples/common.cpp
ggml-cuda.cu

commit 051e1b0e6a6e3aee7d989b47760980e6fda5861c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 17 19:30:22 2023 +0300

    llama : fix kv_cache `n` init (close #1903)

.gitignore
examples/CMakeLists.txt
llama.cpp

commit 86c7571864ff331f8cdb9e092f3abeb123729a56
Author: DaniAndTheWeb <57776841+DaniAndTheWeb@users.noreply.github.com>
Date:   Sat Jun 17 18:17:22 2023 +0200

    make : update for latest Arch (#1701)
    
    With the upcoming change to the openblas package in arch the Makefile workaround is no longer needed.

Makefile

commit 3d59ec5935ea1d33e9d51060a8dd737169b9b89b
Author: Howard Su <howard0su@gmail.com>
Date:   Sat Jun 17 23:46:15 2023 +0800

    ggml : fix warnings under MSVC (#1908)

ggml-cuda.cu
ggml-opencl.cpp
llama.cpp

commit 0711a5f6dce7f04c2a791b14bc47f7d4cb545408
Author: Aaron Miller <apage43@ninjawhale.com>
Date:   Sat Jun 17 07:37:49 2023 -0700

    metal : add norm, cpy f16->f16, alibi kernels (#1823)

ggml-metal.m
ggml-metal.metal

commit fc45a81bc642b9ef33d9004f2b363d558438a6c9
Author: Faez Shakil <faez.shakil@gmail.com>
Date:   Sat Jun 17 17:13:05 2023 +0500

    exposed modules so that they can be invoked by nix run github:ggerganov/llama.cpp#server etc (#1863)

flake.nix

commit 794db3e7b982fee37e3995db9c3a216a57ff65e3
Author: Randall Fitzgerald <randall@dasaku.net>
Date:   Sat Jun 17 07:53:04 2023 -0400

    Server Example Refactor and Improvements (#1570)
    
    A major rewrite for the server example.
    
    Note that if you have built something on the previous server API, it will probably be incompatible.
    Check out the examples for how a typical chat app could work.
    
    This took a lot of effort, there are 24 PR's closed in the submitter's repo alone, over 160 commits and a lot of comments and testing.
    
    Summary of the changes:
    
    - adds missing generation parameters: tfs_z, typical_p, repeat_last_n, repeat_penalty, presence_penalty, frequency_penalty, mirostat, penalize_nl, seed, ignore_eos
    - applies missing top k sampler
    - removes interactive mode/terminal-like behavior, removes exclude parameter
    - moves threads and batch size to server command-line parameters
    - adds LoRA loading and matches command line parameters with main example
    - fixes stopping on EOS token and with the specified token amount with n_predict
    - adds server timeouts, host, and port settings
    - adds expanded generation complete response; adds generation settings, stop reason, prompt truncated, model used, and final text
    - sets defaults for unspecified parameters between requests
    - removes /next-token endpoint and as_loop parameter, adds stream parameter and server-sent events for streaming
    - adds CORS headers to responses
    - adds request logging, exception printing and optional verbose logging
    - adds better stopping words handling when matching multiple tokens and while streaming, or when it finishes on a partial stop string
    - adds printing an error when it can't bind to the host/port specified
    - fixes multi-byte character handling and replaces invalid UTF-8 characters on responses
    - prints timing and build info on startup
    - adds logit bias to request parameters
    - removes embedding mode
    - updates documentation; adds streaming Node.js and Bash examples
    - fixes code formatting
    - sets server threads to 1 since the current global state doesn't work well with simultaneous requests
    - adds truncation of the input prompt and better context reset
    - removes token limit from the input prompt
    - significantly simplified the logic and removed a lot of variables
    
    ---------
    
    Co-authored-by: anon998 <131767832+anon998@users.noreply.github.com>
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    Co-authored-by: Felix Hellmann <privat@cirk2.de>
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    Co-authored-by: Lesaun Harvey <Lesaun@gmail.com>

.gitignore
Makefile
examples/server/CMakeLists.txt
examples/server/README.md
examples/server/chat.mjs
examples/server/chat.sh
examples/server/server.cpp

commit 5ddf7ea1fb42bac21026de2f77e0f9c069b92234
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Sat Jun 17 12:32:48 2023 +0200

    hooks : setting up flake8 and pre-commit hooks (#1681)
    
    Small, non-functional changes were made to non-compliant files.
    These include breaking up long lines, whitespace sanitation and
    unused import removal.
    
    Maximum line length in python files was set to a generous 125 chars,
    in order to minimize number of changes needed in scripts and general
    annoyance. The "txt" prompts directory is excluded from the checks
    as it may contain oddly formatted files and strings for a good reason.
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>

.flake8
.pre-commit-config.yaml
convert.py
examples/jeopardy/graph.py
scripts/verify-checksum-models.py

commit bac19927c302737465a1deb14ac0943a221863e8
Author: Gustavo Rocha Dias <91472747+gustrd@users.noreply.github.com>
Date:   Sat Jun 17 06:01:06 2023 -0300

    readme :  alternative way to build for Android with CLBlast. (#1828)

README.md

commit b4c6f46f17b6e02f1cd55a81339e7e64f3aaa688
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Jun 17 01:49:42 2023 -0600

    Allow cmake to build ggml as a library (#1896)
    
    * Allow cmake to build ggml as a library
    
    * A ggml_static library will be created
    
    * When BUILD_SHARED_LIBS is enabled, ggml_shared will also be built

CMakeLists.txt

commit 92f20d9942c86daeb78637bdad7296a572f4da28
Author: David Yang <davidyang6us@gmail.com>
Date:   Sat Jun 17 14:51:54 2023 +0800

    train : get raw text instead of page with html (#1905)
    
    We probably want to train using just the text of Shakespeare instead of the html of the page displaying his work.

examples/train-text-from-scratch/README.md

commit d411968e990c37f51328849c96a743dd78f3c3dd
Author: 0cc4m <picard12@live.de>
Date:   Fri Jun 16 20:59:49 2023 +0200

    opencl : support k-quants (#1836)
    
    * Porting q2_k kernel to OpenCL
    
    * Set global and local sizes for kernel calls for dequantizing k-quants
    
    * Added q6_k kernel
    
    * Fix q4_k opencl struct order
    
    * Replace uchar with uint8_t
    
    * Finish dequant kernels
    
    * Added OpenCL DMMV kernels
    
    * Fix q2_k, improve code
    
    * Fix q3_k
    
    * Shorten switch statements
    
    * Improve code formatting
    
    ---------
    
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>

ggml-opencl.cpp

commit b41b4cad6f956b5f501db0711dd7007c32b5eee5
Author: SuperUserNameMan <yoann@terminajones.com>
Date:   Fri Jun 16 20:58:09 2023 +0200

    examples : add "simple" (#1840)
    
    * Create `simple.cpp`
    
    * minimalist example `CMakeLists.txt`
    
    * Update Makefile for minimalist example
    
    * remove 273: Trailing whitespace
    
    * removed trailing white spaces simple.cpp
    
    * typo and comments simple.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

Makefile
examples/simple/CMakeLists.txt
examples/simple/simple.cpp

commit 13fe9d2d84f30cab613c960bf66ac83916006694
Author: Zenix <zenixls2@gmail.com>
Date:   Sat Jun 17 03:53:04 2023 +0900

    cmake : add auto detection of BLAS_INCLUDE_DIRS (#1886)

CMakeLists.txt

commit ac3b8869538c7fbdb48ff141d78c4dea091789f0
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jun 16 20:25:51 2023 +0200

    llama : fix embd when offloading non-repeating layers (#1891)

llama.cpp

commit 5b9ccaf104cc1054d4f8f17bc8a4b8dc949e5527
Author: FrankHB <frankhb1989@gmail.com>
Date:   Sat Jun 17 02:25:01 2023 +0800

    Fixed possible macro redefinition (#1892)
    
    MinGW libstdc++ may define `NOMINMAX` unconditionally. This fixes the case when it is already defined.

examples/main/main.cpp

commit 9cbf50c041a525d781c7764f493a5443924e4e38
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Fri Jun 16 21:23:53 2023 +0300

    build : fix and ignore MSVC warnings (#1889)

examples/baby-llama/baby-llama.cpp
examples/benchmark/benchmark-matmult.cpp
examples/common.cpp
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
examples/save-load-state/save-load-state.cpp
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml.c
llama.cpp
pocs/vdot/vdot.cpp
tests/test-quantize-fns.cpp
tests/test-quantize-perf.cpp
tests/test-sampling.cpp
tests/test-tokenizer-0.cpp

commit 3d0112261042b356621e93db3fa4c6798a5d098f
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jun 16 20:08:44 2023 +0300

    CUDA : faster k-quant dot kernels (#1862)
    
    * cuda : faster k-quant dot kernels
    
    * Imrove Q2_K dot kernel on older GPUs
    
    We now have a K_QUANTS_PER_ITERATION macro, which should be
    set to 1 on older and to 2 on newer GPUs.
    With this, we preserve the performance of the original
    PR on RTX-4080, and are faster compared to master on
    GTX-1660.
    
    * Imrove Q6_K dot kernel on older GPUs
    
    Using the same K_QUANTS_PER_ITERATION macro as last commit,
    we preserve performance on RTX-4080 and speed up
    Q6_K on a GTX-1660.
    
    * Add LLAMA_CUDA_KQUANTS_ITER to CMakeLists.txt and Makefile
    
    Allowed values are 1 or 2. 2 gives the best performance on
    modern GPUs and is set as default. On older GPUs 1 may work
    better.
    
    * PR comments
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

CMakeLists.txt
Makefile
ggml-cuda.cu

commit 602c748863e15270d80d74aa2c3bf86ab8139e07
Author: Borislav Stanimirov <b.stanimirov@abv.bg>
Date:   Fri Jun 16 09:58:11 2023 +0300

    gitignore : add several entries specific to Visual Studio (#1888)

.gitignore

commit a09f9195be39afb4b023b646c0a6ec8a86915174
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 15 21:49:08 2023 +0200

    Fixed CUDA runtime version check (#1879)

ggml-cuda.cu

commit bed92756172d4514b23aaf9744cf8e2dc892fc7b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 15 21:56:50 2023 +0300

    cmake : remove whitespaces

CMakeLists.txt

commit c36e81da62ebfe09a768201cc44fa8d712dd00ed
Author: yangli2 <yangli2@gmail.com>
Date:   Thu Jun 15 11:05:53 2023 -0700

    examples : add chat-vicuna.sh (#1854)
    
    Co-authored-by: Yang Li <yangliyl@google.com>

examples/chat-vicuna.sh
llama.h

commit 3559433fecedf365e7aba2fe3d5f89d9abb817c1
Author: Igor Okulist <okigan@gmail.com>
Date:   Thu Jun 15 12:51:26 2023 -0500

    cmake : set include path for OpenBlas (#1830)

CMakeLists.txt

commit 69b34a0e80300bfb3e996983ac3ea075f5526675
Author: Frederik Vogel <Schaltfehler@users.noreply.github.com>
Date:   Fri Jun 16 02:47:04 2023 +0900

    swift : Package compile breaks due to ggml-metal.metal (#1831)
    
    * Ignore metal file in spm
    
    * Add ggml.h to spm public Headers
    
    ---------
    
    Co-authored-by: Vogel Frederik <vogel.frederik@linecorp.com>

Package.swift
spm-headers/ggml.h

commit cf267d1c71a781700698f8518e903239c3bcc929
Author: daboe01 <daboe01@googlemail.com>
Date:   Thu Jun 15 19:42:48 2023 +0200

    make : add train-text-from-scratch (#1850)
    
    * make finetuning example accessible
    
    * fixed: targed was in wrong line
    
    * fixed: name of executable was wrong
    
    * fixed: naming of binary
    
    * fixed: model path was wrong
    
    * fixed clean target
    
    * Update examples/train-text-from-scratch/README.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.gitignore
Makefile

commit 9dda13e5e1f70bdfc25fbc0f0378f27c8b67e983
Author: Srinivas Billa <nivibilla@gmail.com>
Date:   Thu Jun 15 18:36:38 2023 +0100

    readme : server compile flag (#1874)
    
    Explicitly include the server make instructions for C++ noobsl like me ;)

examples/server/README.md

commit 37e257c48e350cf03c353c10d31e777f8d00123d
Author: sandyiscool <sandyiscool@gmail.com>
Date:   Thu Jun 15 23:06:06 2023 +0530

    make : clean *.so files (#1857)

Makefile

commit 64cc19b4fe3df03bc20e520aa111c30cff3a655e
Author: Howard Su <howard0su@gmail.com>
Date:   Fri Jun 16 01:29:59 2023 +0800

    Fix the validation of main device (#1872)

ggml-cuda.cu

commit 4bfcc855abdb2c9fcc3c5a84747974521909fa41
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 15 20:29:48 2023 +0300

    metal : parallel command buffer encoding (#1860)
    
    * metal : parallel command buffer encoding
    
    * metal : determine number of command buffers based on gf->n_threads

ggml-metal.h
ggml-metal.m

commit 6b8312e7979b852f6b6ac9d29cd51fda16c17948
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu Jun 15 19:06:46 2023 +0200

    Better error when using both LoRA + GPU layers (#1861)

examples/common.cpp

commit 254a7a7a5ff4c874ff8488f1f5cbdd7e9c89d682
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Wed Jun 14 19:47:19 2023 +0200

    CUDA full GPU acceleration, KV cache in VRAM (#1827)
    
    * Fixed CUDA RoPE
    
    * ggml_cuda_mul_mat_vec_p021
    
    * ggml_cuda_scale
    
    * ggml_cuda_diag_mask_inf
    
    * ggml_is_permuted
    
    * ggml_cuda_cpy
    
    * flatten rows for ggml_cuda_op
    
    * Added a --low-vram option
    
    * Fixed Windows performance
    
    * Fixed LLAMA_CUDA_DMMV_Y > 1 for WizardLM

examples/common.cpp
examples/common.h
examples/main/README.md
examples/server/README.md
examples/server/server.cpp
ggml-cuda.cu
ggml-cuda.h
ggml.c
ggml.h
llama.cpp
llama.h

commit 92549202659fc23ba9fec5e688227d0da9b06b40
Author: 0xspringtime <110655352+0xspringtime@users.noreply.github.com>
Date:   Tue Jun 13 15:37:54 2023 -0400

    baby-llama : fix operator!= (#1821)
    
    * Update baby-llama.cpp
    
    Seems to be an error in the implementation of the operator!= function. It attempts to compare the this pointer (a llama_hparams_lora object) with the other pointer (a llama_hparams object) using memcmp. This can lead to incorrect results because the sizes of the objects being compared (sizeof(llama_hparams) and sizeof(llama_hparams_lora)) are different, should now be able to compare two llama_hparams_lora objects for inequality.
    
    * Update baby-llama.cpp
    
    * Update baby-llama.cpp

examples/baby-llama/baby-llama.cpp

commit e32089b2c20b1b87b22912f4a8b93fe01647d5b9
Author: xaedes <xaedes@gmail.com>
Date:   Tue Jun 13 21:04:40 2023 +0200

    train : improved training-from-scratch example (#1652)
    
    * add python wrapper
    
    https://gist.github.com/abetlen/2b90e5f153f6efd00931d098de5c73ce
    
    * fix decoding error. adds errors=ignore parameter
    
    * add python bindings for functions to get and set the whole llama state
    (rng, logits, embedding and kv_cache)
    
    * update python bindings
    
    * add text generating baby-llama from scratch example
    
    * fix race condition bug in ggml_compute_forward_diag_mask_f32
    
    * implement ggml_soft_max_back for more performant backward pass of soft_max
    
    avoids creating big intermediate matrices of size n_embd x n_embd for llama layers and n_vocab x n_vocab for cross entropy loss
    
    * improve softmax backward pass
    
    go from quadratic runtime to linear runtime by simplifying the formulas
    
    * fix race condition bug in non-inplace ggml_compute_forward_diag_mask_f32
    
    memcpy needs to be synchronized across threads to avoid race conditions.
    => do it in INIT phase
    
    * fix bug in ggml_compute_forward_soft_max_back_f32 on DEBUG build
    
    * improve performance of mul_mat backward pass
    
    avoid transpose by using mul_mat with swapped arguments
    
    * avoid printing too much newlines in baby-llama-text
    
    * activate threading in baby-llama-text
    
    * add ggml_out_prod and use it for mul_mat backward pass for improved performance
    
    performance stats report improvement from 37 seconds to 16 seconds runtime during my training tests
    
    * better weight initialization improves training convergence at start
    
    * better weight initialization improves training convergence at start
    
    * improve ggml_out_prod performance
    
    - change iteration order (>15s -> 10s runtime)
    - parallelize over one more dimension: over dst matrix rows (10s -> <5s runtime)
    
    * add llama sampler, shuffle samples and constrain sampling to tokens occurring in train data
    
    * fix get_samples call, add model tensor names, increase model size, start training samples after newline
    
    * save train trained model to checkpoint and load model to be trained from checkpoint
    
    * use inplace functions where possible
    
    * initialize rng with srand
    
    * use different arguments for input and output checkpoint
    
    * ggml fixes to support backward pass on inplace operations
    
    * remove duplicate include
    
    * fix cross entropy loss
    
    - add target probabilities for each sample which is then used in cross entropy loss
    
    * print used memory before and after optimization
    
    * sample with non-greedy sampling parameters at the end of training
    
    * add cmake target for baby-llama-text
    
    * add ggml_add1_inplace to header
    
    * enable gradient propagation for inplace add1 and scale operations
    
    those functions backward passes don't need the original src0, so they also work when forward is inplace
    
    * implement AdamW in ggml_opt_adam by adding weight decay parameter (default 0.001f)
    
    also add a schedule parameter (default 1.0f) that can be used to scale alpha and decay according to learning schedule.
    setting the decay parameter to zero disables AdamW resulting in normal Adam optimizer.
    
    since the difference between Adam and AdamW is minimal it is not implemented as another optimizer, but integrated into the existing Adam optimizer.
    
    * use inplace operations in cross_entropy_loss
    
    * fix random weight initialization scale
    
    * add missing default parameters for adam optimizer
    
    * add ggml_opt_context, so that we can properly resume training
    
    otherwise the optimizer states, tracking statistics about the error function and its derivates,
    will reset to zero each time ggml_opt is called, hindering convergence on resumed training.
    
    now the optimizer context and all its memory is stored in a separate struct.
    
    * fix bug in llama_sample_token_mirostat_v2
    
    when all candidates are filtered out through mu threshold, the following soft_max operation will fail.
    so keep at least one.
    
    * add forward function without using cache, for more performant training
    
    during training on whole samples no cache is required.
    removing the cache and simplifying the remaining code results in performance and memory usage improvement.
    
    * print suppressed newline tokens as string "\n"
    
    printing too much actual newlines is suppressed to avoid flooding the console.
    
    * store optimizer state in training checkpoint and add learning schedule
    
    persistent optimizer state allows to resume training without resetting the optimizer
    learning schedule consists of linear warmup ramp followed by cosine decay with restarts
    
    * remove unused functions
    
    * fix bug in get_samples which corrupted training targets
    
    * save checkpoint only when it was trained
    
    * simplify code
    
    * remove trailing whitespace
    
    * simplify backward pass for SQRT
    
    * replace inefficient repeat backward pass with dedicated repeat_back operation
    
    * add ggml_cross_entropy_loss with backward pass for faster training
    
    cross entropy loss can also be implemented using softmax and log, but as dedicated operation it is faster and especially avoids unnecessary memory overhead.
    
    * add tests for cross_entropy_loss backward pass
    
    finite differences regularly results in estimated gradient of zero, despite the backward pass giving non zero gradient.
    _probably_ the finite differences fails due to numerical issues
    
    * use ggml_cross_entropy_loss in text training example
    
    * remove trailing whitespace
    
    * slightly improve how cross entropy loss is compute
    
    btw: directly implemented cross entropy loss seems to have way lower magnitudes than when implemented with softmax and log.
    probably the input to log gets closer to zero due to float numerics.
    maybe the multiplication by (1.0-eps)/sum is more accurate..
    
    * add llama_get_vocab to get the vocabulary as output parameters
    
    * set default model.type for unknown models with few layers
    
    * add export of training checkpoint to llama compatible model file
    
    * get vocabulary for exporting training checkpoint to llama compatible model file
    
    * implement backward pass of flash attention
    
    * bugfixes for backward pass of flash attention
    
    * test flash attention backward pass
    
    need to set loose error bounds to pass.
    the finitie differences are close to numeric limits and often return quite different values than the backward pass.
    reducing eps further lets the gradients vanish completely.
    likewise setting eps to big results in wronger values.
    the softmax in the middle of the function is probably the most responsible for the numeric issues using finite differences.
    
    * add option to train with flash attention and move options to the top of the main function
    
    training from scratch also works with flash attention
    training convergence and generation results after fix number of iterations are worse than when not using flash attention.
    maybe there still lingers a bug in the flash attention backward pass?
    but training works, just with slower convergence.
    
    flash attention is still worth to use, because it requires way less memory and is faster with high n_ctx
    
    * add train_params and command line option parser
    
    * remove unnecessary comments
    
    * add train params to specify memory size
    
    * remove python bindings
    
    * rename baby-llama-text to train-text-from-scratch
    
    * replace auto parameters in lambda function
    
    * add #include <climits>
    
    * add explicit cast to fix compile error
    
    "error: non-constant-expression cannot be narrowed from type 'int64_t' (aka 'long long') to 'uint32_t' (aka 'unsigned int') in initializer list [-Wc++11-narrowing]"
    
    * remove trailing whitespace
    
    * add ggml_opt_resume_g which accepts forward and backward cgraphs
    
    * fix formulas in comments
    
    * bug fix for ggml_compute_forward_get_rows_back_f32
    
    the result should be set to zero, not to whatever data is in opt0
    
    * improve training memory usage with scratch buffers
    
    instead of relying on the automatic backward pass, we manually create the graph for the backward pass.
    it turns out that all backward pass operations need only temporary memory which can be reused after each layer.
    
    will compute backward pass for ALL model parameters
    
    * add option to use scratch buffers in training or not
    
    make it configurable because currently training with scratch buffers implies flash attention and optimization over all parameters.
    
    * ci : disable temporary
    
    * store view offset and permute axes in opt[0] instead of storing it in padding
    
    use memcpy to store offset, because offset is of type size_t.
    when storing it as int32_t offset would have to be smaller than 2^31 which is not necessarily true.
    
    * minor : fix compile warnings + minor style changes
    
    * fix bug in threaded indices calculation of ggml_compute_forward_flash_attn_back_f32
    
    * store view offset like in master branch
    
    * bug fix in forward_batch_wo_cache_flash_attn_train
    
    * scratch buffer bug fixes in forward_batch_wo_cache_flash_attn_train
    
    data of permute and reshape is the same as their input.
    if we want to preserve the output of permute/reshape, we also need to preserve their inputs.
    
    replace reshape(src0, src1) with reshape_nd calls so that we don't need src1.
    
    replace (temporary) t03 with ggml_repeat(ctx0, layer.attention_norm, t02).
    in the future we could also use the new broadcasting ggml_mul to avoid these repeat calls.
    for this we need backward pass of broadcasting ggml_mul.
    
    * remove unnecessary scratch buffer 0
    
    buf 0 is persistent memory, so we can just disable scratch for this by using buf -1
    
    * avoid creating unnecessary grad tensors
    
    previously we need to create grads for model parameters, so that expand(..) correctly populates cgraph->leafs & cgraph->grads
    this wasted memory, because unnecessary grad for each op were automatically created:
    the automatically generated grad was unnecessary because we later manually set the grad (e.g. t35->grad = expand(gb, ...) ).
    this discarded the automatically generated grad resulting in wasted memory.
    
    improved this by changing expand(..) to not use ggml_build_forward_expand.
    expand set cgraph->nodes but not the leafs.
    cgraph->leafs & cgraph->grads are set in another pass after the last expand call.
    
    * print used training seed
    
    * zero initialize gfbuf and gbbuf
    
    * ci : re-enable workflows + add README for training
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/CMakeLists.txt
examples/baby-llama/baby-llama.cpp
examples/train-text-from-scratch/CMakeLists.txt
examples/train-text-from-scratch/README.md
examples/train-text-from-scratch/train-text-from-scratch.cpp
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-grad0.c

commit 2347e45e7bdb09c9a7d74b2c0bc86c2b65f0c343
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 13 20:20:07 2023 +0300

    llama : do a warm-up eval at start for better timings (#1824)

examples/main/main.cpp

commit 74d4cfa3438cb58bd177eed30014e6588694aaa8
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Tue Jun 13 04:23:23 2023 -0600

    Allow "quantizing" to f16 and f32 (#1787)
    
    * Allow "quantizing" to f16 and f32
    
    Fix an issue where quantizing didn't respect LLAMA_NO_K_QUANTS
    
    Add brief help to the list of quantization types in the quantize tool
    
    Ignore case for quantization type arguments in the quantize tool

Makefile
examples/quantize/quantize.cpp
ggml.c
llama.cpp

commit 74a6d922f12ccfe16b0c265f43be8978c6f25e98
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 12 22:39:21 2023 +0300

    Metal implementation for all k_quants (#1807)
    
    * metal : improve q4_K
    
    28.3 -> 26.0 ms/token by avoiding a branch in the
    calculation of the scales.
    
    * metal : small improvement for Q4_K
    
    * metal : still optimizing Q4_K
    
    This commit pushes it down to 25.3 ms / token.
    
    The crazy idea of using 6 bits for the scales is really costly on
    Metal: if I remove the bit fiddling necessary to make the block
    scales, time goes almost to the Q4_0 23 ms/token.
    
    Before pushing the k-quants upstream I had a Q4_K variant that
    had used 8-bit scales. It wasn't more accurate, used 0.125 bits more per weight,
    was running slightly slower on the CPU (due to the larger model size
    and being memory bound there), and the difference was entirely
    negligible under CUDA. So, I decided to publish the version with 6-bit
    scales. Perhaps I should re-consider and change to 8-bit scales?
    
    * metal : some more optimizations
    
    Q2_K: 25.4 ms/token
    Q6_K: 27.3 ms/token
    Q4_0: 22.8 ms/token
    Q4_1: 23.1 ms/token
    
    * metal : Q3_K support
    
    Something is not quite right yet.
    
    * metal : Q5_K support
    
    Initial version achieves 31.2 ms/token, 210 GB/s
    
    * metal : still not able to figure out why q3_K does not work
    
    * Minor
    
    * metal : yet another failed attempt to make q3_K work
    
    * metal : optimize Q5_K
    
    31.2 ms -> 27.8 ms.
    250 GB/s.
    
    * metal : q3_K still not working
    
    Adding a heavily commented q3_K metal kernel to explain
    my obviously faulty logic. Perhaps someone could spot the issue?
    
    * metal : q3_K finally working
    
    Not optimized at all.
    
    What was the issue? The scales are not 4-bytes aligned,
    and I was accessing them with a uint32_t pointer.
    When I tried that on CUDA, I got an error (illegal memory access)
    and added a memcpy to a local array of 3 uint32_t's.
    But on Metal it told me there is no memcpy, so I tried
    accessing directly. There is no error, just garbage results.
    At some point I did try accessing the scales with an uint16_t
    pointer (the scales are for sure 2-byte aligned), but was
    still getting garbage. I guess, there must have been another bug.
    
    No access to scales is via a uint16_t pointer and, after starting
    from scratch from the C dequantize function, it finally works.
    
    * metal : Q3_K 1st optimization pass
    
    * metal : Q3_K second optimization pass - 29.6 ms/token
    
    * metal : Q3_K cleanup
    
    * metal : fixed accidentally broken Q2_K
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal
llama.cpp

commit e4caa8da59c1c97dc23fa336f4d726984a20560f
Author: slaren <slarengh@gmail.com>
Date:   Mon Jun 12 19:12:47 2023 +0200

    ci : run when changing only the CUDA sources (#1800)

.github/workflows/build.yml

commit 58970a4c39124a647ac2a640d9e178ea6c961e65
Author: Howard Su <howard0su@gmail.com>
Date:   Mon Jun 12 20:44:16 2023 +0800

    Leverage mmap for offloading tensors to GPU (#1597)
    
    * Rebase to latest
    
    * Show progress
    
    * Add assert to make sure we only allocate temp buffer for non-CPU backend tensor
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>

ggml-cuda.cu
ggml-cuda.h
ggml-opencl.cpp
ggml-opencl.h
llama.cpp

commit 8c0a10e64dbf60fd9946c0cd5e6f59690800b123
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 12 14:31:36 2023 +0300

    metal : fix failure to load model (#1817)
    
    The number of buffers in the ggml context was left unitialized.
    This leads to sporadic failures to load the model on
    startup. It is actually strange that the failure occurred so
    infrequantly.
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m

commit fa84c4b3e80199a5683438f062009c031a06c4fa
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun Jun 11 08:19:17 2023 -0600

    Fix issue where interactive mode crashes when input exceeds ctx size (#1789)
    
    * Fix issue where interactive mode in the main example crashes when input exceeds ctx size
    
    * Ensure the context size is at least 8 tokens in the main example.
    
    Closes #1768

examples/common.cpp
examples/common.h
examples/main/main.cpp

commit 12b063f0ecf280e98028e444fc492ee6222cdcdc
Author: Kyle Liang <liangmanlai@gmail.com>
Date:   Sun Jun 11 21:20:52 2023 +0800

    Fixed WSL cuda's OOM error (#1594)
    
    * In the function , add the cuda error bypass.
    
    * remove excessive codes and prints
    
    ---------
    
    Co-authored-by: liang <liangmanlai@126.com>

ggml-cuda.cu

commit 31d2b5f4a4bae081e59b36ab37c6ff6f5b5940ad
Author: Ryan Landay <rlanday@gmail.com>
Date:   Sun Jun 11 17:38:53 2023 +0800

    Update SHA256SUMS with current hashes for models quantized using q4_0 (#1798)

SHA256SUMS

commit 4de0334f5cabf4696eced2e5d6e279fdfaa6c0f2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 10 22:56:53 2023 +0300

    cmake : fix Metal build (close #1791)

CMakeLists.txt

commit 3f1223155a462477ac933474ebc4eab0ce3ca264
Author: Artyom Lebedev <vagran.ast@gmail.com>
Date:   Sat Jun 10 22:51:36 2023 +0300

    k-quants : GCC12 compilation fix (#1792)

k_quants.c

commit 303f5809f1b4ec49823dbe70cacd2124ec1d0df0
Author: Andrei <abetlen@gmail.com>
Date:   Sat Jun 10 10:47:34 2023 -0400

    metal : fix issue with ggml-metal.metal path. Closes #1769 (#1782)
    
    * Fix issue with ggml-metal.metal path
    
    * Add ggml-metal.metal as a resource for llama target
    
    * Update flake.nix metal kernel substitution

CMakeLists.txt
flake.nix
ggml-metal.m

commit 059e99066d95d73d1ca26c3375d47c0e35596229
Author: Aisuko <urakiny@gmail.com>
Date:   Sun Jun 11 00:08:11 2023 +1000

    doc : fix wrong address of BLIS.md (#1772)
    
    Signed-off-by: Aisuko <urakiny@gmail.com>

README.md

commit 17c10acfb44ecb7af25e37fb67b9501cbc0034d2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Jun 10 12:06:45 2023 +0300

    ggml : force no_alloc == false when creating opt tensors (close #1699)
    
    This is needed to make operators like ggml_view() be able to store their
    parameters in the ggml context's memory and not get discarded when
    no_alloc is true

ggml.c

commit e9b66ee9829039d4ab54550d6222e42a0b31e52a
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Sat Jun 10 11:28:11 2023 +0300

    metal : add Q4_1 implementation (#1785)
    
    23.3 ms / token, so just ~1% slower than q4_0.
    Achieves 290 GB/s memory throughput.
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 4f0154b0bad775ac4651bf73b5c216eb43c45cdc
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat Jun 10 01:59:17 2023 -0600

    llama : support requantizing models instead of only allowing quantization from 16/32bit (#1691)
    
    * Add support for quantizing already quantized models
    
    * Threaded dequantizing and f16 to f32 conversion
    
    * Clean up thread blocks with spares calculation a bit
    
    * Use std::runtime_error exceptions.

examples/quantize/quantize.cpp
llama.cpp
llama.h

commit ef3171d16241c18581d4d08374f0b9e396ade6b7
Author: Xingchen Song(宋星辰) <xingchensong1996@163.com>
Date:   Sat Jun 10 15:49:40 2023 +0800

    ggml : workaround for missing _mm256_setr_m128i in GCC < 8 (#1638)

ggml.c

commit 555275a693843273759230547001f9ae07fb537e
Author: rankaiyx <rankaiyx@rankaiyx.com>
Date:   Sat Jun 10 14:41:59 2023 +0800

    make : add SSSE3 compilation use case (#1659)

Makefile

commit 98ed16557432d7a5179c57eddcc3a08a7ae6d54d
Author: Robert Sung-wook Shin <edp1096@users.noreply.github.com>
Date:   Sat Jun 10 01:24:40 2023 +0900

    OpenCL: Add release memory (#1741)
    
    * Add opencl release memory
    
    * Rename function name

ggml-opencl.cpp
ggml-opencl.h
llama.cpp

commit ae9663f1887513e152839e91f61c513075a19422
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Jun 9 13:58:15 2023 +0200

    Windows nvcc workaround (#1753)
    
    Fix gibberish output on Windows when using CUDA

ggml-cuda.cu

commit b33dee282f5d8032b5f780152732dc45cbf2d349
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Jun 9 11:11:04 2023 +0300

    metal : fix build "tanhf" -> "tanh"

ggml-metal.metal

commit 92f44ff7f778ef1b94028b2ba6d39943b5ca0ada
Author: AT <manyoso@users.noreply.github.com>
Date:   Fri Jun 9 04:00:51 2023 -0400

    metal : add GELU implementation (#1770)
    
    Co-authored-by: Adam Treat <adam@nomic.ai>

ggml-metal.m
ggml-metal.metal

commit 245fc3c37da5ac5963f9f11a9f4f2ac08d96afc6
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Jun 9 10:39:59 2023 +0300

    metal : faster q4_0 (#1775)
    
    * metal : 8% faster q4_0
    
    Avoid copying into local uchar4 anf float4.
    
    * metal : 17% faster Q4_0
    
    Use 64 threads in a thread group.
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 72ff5282bf0388c60821f504c4c8cc2b1f491aa6
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jun 8 22:28:21 2023 +0300

    metal : add Q2_K implementation (#1762)
    
    * metal : add Q2_K implementation
    
    27.1 ms / token on M2 Max 30-core GPU, so about the
    same speed as Q4_0. Memory throughput is ~156 GB/s.
    
    The access pattern used in the Q2_K
    CUDA implementation resulted in significantly lower
    performance (~31 ms/token).
    
    * Fixing merge conflicts
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 0bf7cf1b296fc9fca05411b37afdf08a531487d2
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 8 20:48:14 2023 +0300

    Revert "ggml : load data into int8x16x4_t using vld4q_s8 on arm64 (#1738)"
    
    This reverts commit 8432d4d9f716b25133e3ed671d91e21f6f3be867.

k_quants.c

commit 8432d4d9f716b25133e3ed671d91e21f6f3be867
Author: le.chang <cljs118@126.com>
Date:   Fri Jun 9 00:47:56 2023 +0800

    ggml : load data into int8x16x4_t using vld4q_s8 on arm64 (#1738)

k_quants.c

commit 0f291e1f65c1d68201e71ce99c89562a36686b6d
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jun 8 19:46:22 2023 +0300

    metal : Q6_K implementation (#1752)
    
    * Metal implementation for Q4_K
    
    Very slow for now:
    42 ms / token, Q4_0 runs in 28 ms/token on my
    30-core M2 Max GPU.
    
    * Optimizing Q4_K on metal
    
    The first token always takes longer, I guess because
    the metal kernel is being jit-compiled.
    So, using n = 128 to measure time.
    
    At this point Q4_K takes 29.5 ms / token
    compared to 27.2 ms / token for Q4_0.
    Quite a bit better than the initial attempt,
    but still not good enough.
    
    * Optimizing q4_K metal dot some more
    
    For n = 256 it is now 28.1 ms/token compared to
    27 ms/token for q4_0.
    
    * Fix after merge with master
    
    * Metal implementation for Q6_K
    
    Similar to the CUDA implementation.
    No idea if this is the optimum for Metal, but the few
    alternative variants I tried all had a lower performance.
    
    We get 36.5 ms / token on M2 Max with 30 GPU cores.
    This corresponds to ~200 GB/second throughput.
    
    * clang-tidy : add config back
    
    * Much better Q6_K implementation for metal
    
    28.3 ms / token for 7B. Subtracting ~9 ms that is spent in
    other compute graph operations, we are left with ~19 ms
    for the matrix multiplications. The model is ~5.5 GB,
    so we are getting 1000 / 19 * 5.5 = 290 GB/s!
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml-metal.m
ggml-metal.metal

commit 8fc8179919a11738910db07a800f2b176f8adf09
Author: qingfengfenga <41416092+qingfengfenga@users.noreply.github.com>
Date:   Thu Jun 8 15:58:53 2023 +0800

    Add llama.cpp docker support for non-latin languages (#1673)
    
    * Modify Dockerfile default character set to improve compatibility (#1673)

.devops/full.Dockerfile
.devops/main.Dockerfile

commit b50b570ed9d699d3d126d72fc02de92926bcd937
Author: Steven Roussey <sroussey@gmail.com>
Date:   Thu Jun 8 00:12:28 2023 -0700

    ggml : fix fprintf warnings (#1720)

ggml.c

commit 53aba3f393f2e02a78ddaba2e934893a8bbf3246
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Jun 8 10:09:08 2023 +0300

    clang-tidy : restore dot file from accidental deletion

.clang-tidy

commit 4161bdc04debb70bf5f275492b4d89fd9330087c
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Jun 8 10:08:23 2023 +0300

    metal : add Q4_K implementation (#1733)
    
    * Metal implementation for Q4_K
    
    Very slow for now:
    42 ms / token, Q4_0 runs in 28 ms/token on my
    30-core M2 Max GPU.
    
    * Optimizing Q4_K on metal
    
    The first token always takes longer, I guess because
    the metal kernel is being jit-compiled.
    So, using n = 128 to measure time.
    
    At this point Q4_K takes 29.5 ms / token
    compared to 27.2 ms / token for Q4_0.
    Quite a bit better than the initial attempt,
    but still not good enough.
    
    * Optimizing q4_K metal dot some more
    
    For n = 256 it is now 28.1 ms/token compared to
    27 ms/token for q4_0.
    
    * Fix after merge with master
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

.clang-tidy
ggml-metal.m
ggml-metal.metal

commit 0035858273ebe0694926bf4414d279f3e1cd109d
Author: johnson442 <56517414+johnson442@users.noreply.github.com>
Date:   Thu Jun 8 08:02:48 2023 +0100

    k-quants : add missing compile definition to CMakeLists (#1748)

CMakeLists.txt

commit 5c64a0952ee58b2d742ee84e8e3d43cce5d366db
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 7 10:59:52 2023 +0300

    k-quants : allow to optionally disable at compile time (#1734)
    
    * k-quants : put behind optional compile flag LLAMA_K_QUANTS
    
    * build : enable k-quants by default

CMakeLists.txt
Makefile
ggml-cuda.cu
ggml.c
k_quants.c
k_quants.h

commit 5b57a5b72676540b6a45a3f527126299969ad241
Author: jacobi petrucciani <8117202+jpetrucciani@users.noreply.github.com>
Date:   Wed Jun 7 00:15:31 2023 -0400

    flake : update to support metal on m1/m2 (#1724)

flake.lock
flake.nix

commit 4dc62c545df0af60635d579e9e4dd91bc5afff51
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Jun 7 07:15:08 2023 +0300

    readme : add June roadmap

README.md

commit 35a84916fb029905c44746127026079268216e7a
Author: Willy Tarreau <w@1wt.eu>
Date:   Wed Jun 7 04:10:17 2023 +0200

    main: add the possibility to open the prompt cache read-only (#1640)
    
    The prompt cache constitutes a nice speed up when using the same prompt
    prefix across multiple evaluations, but when using it, it will also be
    updated, which is not always desirable. One use case is to have a large
    prompt containing some context and usage rules, and a second part
    containing variable data of the problem being studied. In this case it's
    desirable to be able to save the first part once, and to always reuse it
    as-is without updating it with the second part.
    
    The new argument --prompt-cache-ro enables this read-only mode on the
    prompt cache. The prompt's contents that match the cache are loaded
    from the cache but the rest is not modified. This allowed to reduce a
    total analysis time from 112s to 49.7s here, without having to backup
    and restore a copy of the prompt, which takes significant time at 500
    MB.
    
    Signed-off-by: Willy Tarreau <w@1wt.eu>

examples/common.cpp
examples/common.h
examples/main/main.cpp

commit 2d7bf110edd8c49209401a16132052cba706ffd0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 22:54:39 2023 +0300

    llama : fix vram_scratch var

llama.cpp

commit 2a4e41a086ce80da68c402457c75c77e52dcc698
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 22:41:53 2023 +0300

    llama : fix compile warnings

ggml.c
llama.cpp

commit 17366df842e358768c0df7024484fffecfc7865b
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Tue Jun 6 21:33:23 2023 +0200

    Multi GPU support, CUDA refactor, CUDA scratch buffer (#1703)
    
    * CUDA multi GPU + scratch
    
    ggml_cuda_compute_forward
    
    Tensor parallelism
    
    ggml_cuda_add
    
    ggml_cuda_rms_norm
    
    ggml_cuda_silu
    
    CUDA scratch buffer
    
    --main-gpu CLI option

examples/common.cpp
examples/common.h
examples/main/README.md
examples/server/README.md
examples/server/server.cpp
ggml-cuda.cu
ggml-cuda.h
ggml-opencl.cpp
ggml.c
ggml.h
llama.cpp
llama.h

commit 44f906e8537fcec965e312d621c80556d6aa9bec
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 20:16:57 2023 +0300

    metal : add f16 support

ggml-metal.m
ggml-metal.metal
llama.cpp

commit d5b111f53d14972669eb52055f9df2567663ad8b
Author: LostRuins <39025047+LostRuins@users.noreply.github.com>
Date:   Wed Jun 7 01:00:01 2023 +0800

    Clblast fixes + enhancements to save VRAM and offload more layers (#1675)
    
    * Use events instead of clFinish, where possible
    
    * OpenCL: Don't load gpu layers into RAM, add mul_f32 kernel
    
    * Reduce queueing overhead for contiguous tensors by using single mul kernel call
    
    * Adapt to #1612 cl_mem malloc changes
    
    * Reduce code duplication between cuda and opencl branches
    
    * Improve implementation
    
    * Clblast fixes + enhancements to save VRAM:
    
    1. Change all Clblast buffers to CL_MEM_READ_WRITE, as the pool malloc currently doesn't properly handle them.
    2. When recycling buffers in pool malloc, always assign the SMALLEST available buffer that fits, instead of the FIRST available buffer
    3. When failing to recycle a buffer in pool malloc (all too small), instead recycle the largest available free buffer by resizing it.
    
    * change max value size_t to use limits
    
    * removed flags from the CL pool malloc, apply code tidying suggestions.

ggml-opencl.cpp

commit 2d43387dafe9c60f15f57aa23ee0b37864b98b32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 10:18:03 2023 +0300

    ggml : fix builds, add ggml-quants-k.o (close #1712, close #1710)

.gitignore
Makefile
ggml.c

commit 7ad7750c5c9f6bcea73a1895ffc57e7d21f2ab95
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 09:55:10 2023 +0300

    gitignore : add .clang-tidy

.gitignore

commit 7a74dee6b4e0e80862191141c0037abe28967d5c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Jun 6 09:39:38 2023 +0300

    llama : temporary disable Q6_K output quantization (#1711)

llama.cpp

commit 590250f7a9847bc9c83aa063dbaac8fa0fea27c8
Author: Spencer Sutton <spencersutton@users.noreply.github.com>
Date:   Mon Jun 5 23:28:17 2023 -0400

    metal : add checks for buffer size (#1706)
    
    Co-authored-by: Spencer Sutton <Spencer.Sutton@precisely.com>

ggml-metal.m
llama.cpp

commit f4c55d3bd7e124b101bc974cbbf0e0dbbc32d5a3
Author: Yuval Peled <31162840+Yuval-Peled@users.noreply.github.com>
Date:   Mon Jun 5 23:32:36 2023 +0300

    docs : add performance troubleshoot + example benchmark documentation (#1674)
    
    * test anchor link
    
    * test table
    
    * add benchmarks
    
    * Add performance troubleshoot & benchmark
    
    * add benchmarks
    
    * remove unneeded line
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
docs/BLIS.md
docs/token_generation_performance_tips.md

commit f1465624c2cbc8ee65b566e3d87f2af27796d4c4
Author: Foul-Tarnished <107711110+Foul-Tarnished@users.noreply.github.com>
Date:   Mon Jun 5 22:28:37 2023 +0200

    readme : fix typo (#1700)
    
    Fix a typo in a command in README.md

README.md

commit c2df36d60dc0ff1576541b965d751eadbacbeada
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Mon Jun 5 22:24:29 2023 +0200

    llama : consistently catch and throw only exceptions deriving from std::exception (#1599)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp

commit 9d0693bce38013364b1042568d9083353bfff48f
Author: kiltyj <kiltyj@gmail.com>
Date:   Mon Jun 5 13:24:04 2023 -0700

    metal : use shared buffers between CPU and GPU (#1696)
    
    * Use MTLDevice.newBufferWithBytesNoCopy to share buffers between CPU and GPU
    
    * Page-align buffers used by Metal
    
    * Remove trailing whitespace
    
    * Only import unistd.h for Metal builds
    
    * metal : remove unnecessary copies
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml-metal.m
ggml.c
llama-util.h
llama.cpp

commit efe05076323f5c6bafece109e21cce046f5e4b07
Author: grahameth <96447521+grahameth@users.noreply.github.com>
Date:   Mon Jun 5 22:11:49 2023 +0200

    ggml : fix internal overflow in ggml_time_us on Windows (#1702)
    
    Co-authored-by: grahameth <->

ggml.c

commit e7fe66e670537990ccc075cce9286df88bba052a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 5 23:05:05 2023 +0300

    ci : disable auto tidy (#1705)

.github/workflows/tidy-post.yml

commit 99009e72f8072fa552eb02efee436be596c71cdd
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Mon Jun 5 22:56:18 2023 +0300

    ggml : add SOTA 2,3,4,5,6 bit k-quantizations (#1684)
    
    * Starting to add k-quantization to ggml
    
    I think it is better to have quantization separate from
    ggml. For now just adding the k-quants there, but it would be
    better to also factor out the existing ggml quantizations.
    
    * Adding Q3_K and Q8_K (de)-quantization
    
    * Q3_K now working on CUDA and AVX2/scalar
    
    CUDA is not ideal - ~50% slower than Q4_0 for
    single token prediction, about the same in batch
    mode (perplexity). CPU single token is ~55 ms
    (on Ryzen 7950X).
    
    * Some improvement for Q3_K on CUDA
    
    It is now ~22.5 ms/token on my GPU, so ~30% slower than Q4_0.
    
    * Some more CUDA optimizations for Q3_K
    
    Single token is now 20.5 ms/token (~20% slower than Q4_0).
    Perplexity is on par with Q4_0.
    
    * Adding Q4_K - scalar, AVX2, CUDA
    
    Performance is the same or perhaps very slightly better than Q4_0 on the CPU.
    On the GPU, single token prediction is ~10% better than Q4_0,
    batch mode (perplexity is about the same).
    
    * Adding Q6_K - scalar, AVX2, CUDA
    
    Performance is ~40% lower compared to Q4_K on the CPU.
    This is to be expected, considering that we are memory bound
    on the CPU and the 6-bit model is ~44% larger than the 4-bit.
    On the GPU, single token prediction is ~6% lower than Q4_0,
    batch mode (perplexity) is even closer (but still slower).
    
    * Adding Q5_K - scalar, AVX2, CUDA
    
    Performance is ~20% lower compared to Q4_K on the CPU.
    This is to be expected, considering that we are memory bound
    on the CPU and the 5-bit model is ~22% larger than the 4-bit.
    On the GPU, single token prediction is about the same as Q4_0
    for both, single token and batch prediction.
    
    * Per convention, all QX_K quantizations use Q5_K for output.weight
    
    * Adding quantization mixes
    
    * Quantization mixes: didn't quite get what I wanted in the last commit
    
    * Q4_K dot product for ARM_NEON
    
    * Q6_K dot product for ARM_NEON
    
    * Q5_K dot product for ARM_NEON
    
    * Adding Q3_K dot for ARM_NEON
    
    It is 22% slower than Q4_K, despite the smaller model size.
    On x86_64, where we are memory bound, the Q3_K model is
    quite a bit faster than Q4_K.
    
    * A very slightly faster ARM_NEON Q3_K dot
    
    * Adding Q2_K - just CUDA for now
    
    Token prediction is pretty good - about 15.5 ms on a RTX 4080.
    Perplexity is about the same as Q4_K.
    
    * Adding scalar and AVX2 Q2_K dot
    
    * Adding ARM_NEON Q2_K dot
    
    About the same performance as Q4_K.
    
    * A slightly faster ARM_NEON Q2_K dot
    
    Single token prediction is now ~36 ms on M2 Max.
    The code is much simpler too.
    
    * Fixed bug in Q2_K CUDA dot product kernel
    
    Stranegly enough, for the few prompts I tried with the 7B model
    the responses looked perfectly reasonable. Only realized something
    is not quite right when I tried the larger models and started getting
    nonse back.
    
    In any case, Q2_K single token evaluation time on an RTX 4080 in a Ryzen7950X
    box iusing CUDA and model fully loaded on the GPU are
      ~15.5 ms for 7B, ~25.4 ms for 13B, and ~55.8 ms for 30B.
    The max number of layers that fit in VRAM for The 65B is 32.
    With that, we get ~330 ms per token, which is not that much faster
    than just running on the CPU (~470 ms per token).
    
    * Don't print zeros/NaNs when no count histogram has been collected
    
    * A 10% faster CUDA vector dot kernel for Q3_K
    
    Q3_K is now running at ~18.5 ms / token on CUDA,
    so the gap to Q4_0 is only 10%.
    It seems memory acccess pattern is more important for
    performance than the amount of computation the kernel
    does.
    
    * A slightly daster Q4_K AVX2 dot product
    
    For perplexity, where we are less memory bound, time per
    pass drops by ~5%. Barely measurable difference for single
    token prediction.
    
    * A slightly faster ARM_NEON A4_K dot product
    
    * Minor
    
    * Fix quantization error test
    
    We cannot possibly be expecting rmse < 0.002 for 2- and 3-bit
    quantization variants.
    
    * Fix docker build
    
    I have been sloppy with vector reinterpret casts on ARM_NEON.
    It seems clang is very forgiving in that regard.
    
    * Added forgotten ggml.o dependence on k_quants.h to the Makefile
    
    * Had unintentionally committed the Makefile with -Ofast enabled
    
    * ggml : rename k_quants -> ggml-quants-k, use lowercase in code
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
Makefile
examples/quantize-stats/quantize-stats.cpp
examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-quants-k.c
ggml-quants-k.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-quantize-fns.cpp

commit 5220a991a5e92bddad9542267ab445a2c033681c
Author: Henri Vasserman <henv@hot.ee>
Date:   Mon Jun 5 13:43:08 2023 +0300

    Increase 3B scratch buffers. (#1698)
    
    The 128 MB was too optimistic.
    Too bad it is not dynamically computed.

llama.cpp

commit d1f563a743a83dabc11e125d4a7d64189c16498c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Jun 5 10:19:03 2023 +0300

    llama : fix Metal KV cache sync (close #1695)

llama.cpp

commit 827f5eda91e5b7299848ee2c7179d873bdee0f7b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 4 23:38:19 2023 +0300

    readme : update hot topics

README.md

commit ecb217db4fcfa3880300ad08531a5fb6bb142d45
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Jun 4 23:34:30 2023 +0300

    llama : Metal inference (#1642)
    
    * mtl : export the LLaMA computation graph
    
    * ci : disable temporary
    
    * mtl : adapt the MNIST example as starter
    
    * mtl : no need for mtl-export tool, add cli arg for main instead
    
    * mtl : export just a small part of the graph for now to make it easier
    
    * mtl : move MSL code into separate file for easy editing
    
    * mtl : initial get_rows_q4_0 kernel
    
    * mtl : confirmed get_rows_q4_0 is working correctly
    
    * mtl : add rms_norm kernel + confirm working
    
    * mtl : add mul kernel + confirm working
    
    * mtl : initial mul_mat Q4 kernel (wrong results)
    
    * mtl : mul_mat fixes (still wrong)
    
    * mtl : another mul_mat Q4 (still does not work)
    
    * mtl : working mul_mat q4
    
    * ggml : fix handling of "view" ops in ggml_graph_import()
    
    * mtl : add rope kernel
    
    * mtl : add reshape and transpose handling
    
    * ggml : store offset as opt arg for ggml_view_xd() operators
    
    * mtl : add cpy kernel + handle view ops
    
    * mtl : confirm f16 x f32 attention mul mat
    
    * mtl : add scale kernel
    
    * mtl : add diag_mask_inf kernel
    
    * mtl : fix soft_max kernel
    
    * ggml : update ggml_nbytes() to handle non-contiguous tensors
    
    * mtl : verify V tensor contents
    
    * mtl : add f32 -> f32 cpy kernel
    
    * mtl : add silu kernel
    
    * mtl : add non-broadcast mul kernel
    
    * mtl : full GPU inference of the computation graph
    
    * mtl : optimize rms_norm and soft_max kernels
    
    * mtl : add f16 mat x f32 vec multiplication kernel
    
    * mtl : fix bug in f16 x f32 mul mat + speed-up computation
    
    * mtl : faster mul_mat_q4_0_f32 kernel
    
    * mtl : fix kernel signature + roll inner loop
    
    * mtl : more threads for rms_norm + better timing
    
    * mtl : remove printfs from inner loop
    
    * mtl : simplify implementation
    
    * mtl : add save/load vocab to ggml file
    
    * mtl : plug Metal inference into llama.cpp (very quick-n-dirty)
    
    * mtl : make it work with main example
    
    Lots of hacks but at least now it generates text
    
    * mtl : preparing for merge
    
    * mtl : clean-up ggml mtl interface + suport scratch / inplace
    
    * mtl : remove temp / debug code
    
    * metal : final refactoring and simplification
    
    * Revert "ci : disable temporary"
    
    This reverts commit 98c267fc77fe811082f672538fc91bcfc9072d63.
    
    * metal : add comments
    
    * metal : clean-up stuff, fix typos
    
    * readme : add Metal instructions
    
    * readme : add example for main

.gitignore
CMakeLists.txt
Makefile
README.md
examples/CMakeLists.txt
examples/common.cpp
examples/common.h
examples/main/main.cpp
examples/metal/CMakeLists.txt
examples/metal/metal.cpp
ggml-metal.h
ggml-metal.m
ggml-metal.metal
ggml.c
ggml.h
llama.cpp
llama.h

commit dcb2ed48268e421baf25adc00d602dad0f415564
Author: 0cc4m <picard12@live.de>
Date:   Sun Jun 4 08:12:05 2023 +0200

    OpenCL: Fix duplication of layers in VRAM and RAM, add GPU mul kernel (#1653)
    
    * Use events instead of clFinish, where possible
    
    * OpenCL: Don't load gpu layers into RAM, add mul_f32 kernel
    
    * Reduce queueing overhead for contiguous tensors by using single mul kernel call
    
    * Adapt to #1612 cl_mem malloc changes
    
    * Reduce code duplication between cuda and opencl branches
    
    * Improve implementation

ggml-opencl.cpp
ggml-opencl.h
ggml.c
llama.cpp

commit d8bd0013e8768aaa3dc9cfc1ff01499419d5348e
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat Jun 3 16:35:20 2023 +0300

    Add info about CUDA_VISIBLE_DEVICES (#1682)

README.md

commit b5c85468a3eadf424420af5bf11c2353ff828cda
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Sat Jun 3 14:11:53 2023 +0200

    Docker: change to calling convert.py (#1641)
    
    Deprecation disclaimer was added to convert-pth-to-ggml.py

.devops/tools.sh
convert-pth-to-ggml.py

commit 136476e898fb96c302b0829ee3e79267ae12660f
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Sat Jun 3 07:28:45 2023 -0400

    Fix prompt cache saving and chat-persistent rollover (#1678)
    
    * Fix prompt cache saving and chat-persistent rollover (fixes #1670)
    
    * clang-tidy
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    ---------
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>

examples/main/main.cpp

commit ffb06a345e3a9e30d39aaa5b46a23201a74be6de
Author: Henri Vasserman <henv@hot.ee>
Date:   Tue May 30 21:24:22 2023 +0300

    OpenLLaMA 3B support (#1588)
    
    This adds support to llama.cpp to load the model.
    
    Currently missing are changes that are required from convert.py to convert the model correctly. It needs some changes to start reading the JSON configuration for HF models instead of deriving the values by guessing.
    
    Co-authored-by: FNsi <125447286+FNsi@users.noreply.github.com>

llama.cpp

commit 7552ac586380f202b75b18aa216ecfefbd438d94
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 29 19:31:44 2023 +0300

    ggml : sync cgraph import / export API

ggml.c
ggml.h

commit 5d1830b99dfd85bb6279adb4dd94aa444afd5b5e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 29 19:30:49 2023 +0300

    ggml : fix bug in ggml_alibi

ggml.c

commit 248367605ead6fb7c36d2bfb1ebd8f00a23f7c71
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Mon May 29 05:13:40 2023 -0700

    Work around for recalculating logits in cached prompts (Fixes #1585) (#1609)
    
    * Work around for recalculating logits in cached prompts

examples/main/main.cpp

commit 0e730dd23b0fb5f93dba574e0a48d9a69dc5dbae
Author: Jiří Podivín <66251151+jpodivin@users.noreply.github.com>
Date:   Mon May 29 06:45:50 2023 +0200

    Adding git in container package dependencies (#1621)
    
    Git added to build packages for version information in docker image
    
    Signed-off-by: Jiri Podivin <jpodivin@gmail.com>

.devops/full.Dockerfile
.devops/main.Dockerfile

commit 3b126f654fceb4f5f195a1c2e825bb18e101188c
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 28 21:01:02 2023 +0200

    LLAMA_DEBUG adds debug symbols (#1617)

Makefile

commit 1b78ed20818b72306edc7208b9bfb69a1a0d3297
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sun May 28 11:48:57 2023 -0600

    Only show -ngl option when relevant + other doc/arg handling updates (#1625)
    
    1. Add a `LLAMA_SUPPORTS_GPU_OFFLOAD` define to `llama.h` (defined when compiled with CLBlast or cuBLAS)
    2. Update the argument handling in the common example code to only show the `-ngl`, `--n-gpu-layers` option when GPU offload is possible.
    3. Add an entry for the `-ngl`, `--n-gpu-layers` option to the `main` and `server` examples documentation
    4. Update `main` and `server` examples documentation to use the new style dash separator argument format
    5. Update the `server` example to use dash separators for its arguments and adds `-ngl` to `--help` (only shown when compiled with appropriate support). It will still support `--memory_f32` and `--ctx_size` for compatibility.
    6. Add a warning discouraging use of `--memory-f32` for the `main` and `server` examples `--help` text as well as documentation. Rationale: https://github.com/ggerganov/llama.cpp/discussions/1593#discussioncomment-6004356

examples/common.cpp
examples/main/README.md
examples/server/README.md
examples/server/server.cpp
llama.h

commit 337aea11390221bc925e4acb1f603f1649af2735
Author: Vladimir Zorin <vladimir@deviant.guru>
Date:   Sun May 28 20:14:24 2023 +0300

    examples : add --alias option to gpt_params to set use friendly model name (#1614)

examples/common.cpp
examples/common.h
examples/server/server.cpp

commit bb051d9723d628414b9e929e5264e23262a2f1b2
Author: Howard Su <howard0su@gmail.com>
Date:   Mon May 29 01:13:36 2023 +0800

    opencl : no need to allocate cl_mem on heap (#1612)

ggml-opencl.cpp

commit ca74884f6625b15ef69832f07fc60fe00db5f90c
Author: Howard Su <howard0su@gmail.com>
Date:   Mon May 29 01:09:56 2023 +0800

    opencl : use strstr to check if fp16 supported (#1611)
    
    * Use strstr to check if fp16 supported
    
    * Ensure ext_buffer is null terminated

ggml-opencl.cpp

commit a6704643b62243bc4b6bbcd727d63d44e01a1002
Author: apcameron <37645737+apcameron@users.noreply.github.com>
Date:   Sat May 27 21:03:25 2023 +0100

    ggml : add support for the RISCV architecture (#1616)

ggml.c

commit 0df7d63e5ba0ab8856476e121a03b985d6f15c9d
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Sat May 27 11:04:14 2023 -0600

    Include server in releases + other build system cleanups (#1610)
    
    Set `LLAMA_BUILD_SERVER` in workflow so the `server` example gets build. This currently only applies to Windows builds because it seems like only Windows binary artifacts are included in releases.
    
    Add `server` example target to `Makefile` (still uses `LLAMA_BUILD_SERVER` define and does not build by default)
    
    Fix issue where `vdot` binary wasn't removed when running `make clean`.
    
    Fix compile warnings in `server` example.
    
    Add `.hpp` files to trigger workflow (the server example has one).

.github/workflows/build.yml
Makefile
examples/server/server.cpp

commit 97c9b77c4fc5e2283755c4418759cfc5fc73ad05
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 27 18:47:55 2023 +0300

    Add documentation about CLBlast (#1604)
    
    Installing, compiling and using.

README.md

commit 0ecb1bbbeb16e36a2ea7a5ce525c6c59ef74312b
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 27 17:24:06 2023 +0300

    [CI] Fix openblas (#1613)
    
    * Fix OpenBLAS build
    
    * Fix `LLAMA_BLAS_VENDOR` CMake variable that should be a string and not a boolean.

.github/workflows/build.yml
CMakeLists.txt

commit 93618031c7ccdd949d976370f24953d261048575
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 27 16:19:56 2023 +0300

    ggml : add ggml_tensor_overhead()

ggml.c
ggml.h

commit 83c54e6da58f1970556741b143bd26e30b1f46af
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 27 15:18:25 2023 +0300

    [CI] CLBlast: Fix directory name (#1606)

.github/workflows/build.yml

commit bdbda1b17afb78e8613d03c8210a57fac632397b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 27 12:22:05 2023 +0300

    ggml : sync ggml core (minor additions, e.g. ggml_get_tensor_by_name())

ggml.c
ggml.h

commit 66874d4fbcc7866377246efbcee938e8cc9c7d76
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu May 25 20:18:01 2023 -0600

    Some improvements to loading the session with --prompt-cache (#1550)
    
    Improvements to loading the session with `--prompt-cache` in the `main` example.
    
    1. Fix an issue where the `--seed` parameter was ignored when loading a cached prompt.
    2. When loading a cached prompt, you previously had to specify the saved prompt (or a prefix of it) again. This pull changes that behavior to default to the prompt that was cached if a prompt wasn't specified by the user.

examples/main/README.md
examples/main/main.cpp

commit 1fcdcc28b119a6608774d52de905931bd5f8a43d
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Thu May 25 23:07:29 2023 +0200

    cuda : performance optimizations (#1530)
    
    * xor hack
    
    * block y dim
    
    * loop unrolling
    
    * Fixed cmake LLAMA_CUDA_BY option
    
    * Removed hipblas compatibility code
    
    * Define GGML_CUDA_DMMV_BLOCK_Y if not defined
    
    * Fewer iters, more ops per iter
    
    * Renamed DMMV X/Y compilation options

CMakeLists.txt
Makefile
ggml-cuda.cu

commit ac7876ac20124a15a44fd6317721ff1aa2538806
Author: Henri Vasserman <henv@hot.ee>
Date:   Wed May 24 10:30:09 2023 +0300

    Update CLBlast to 1.6.0 (#1580)
    
    * Update CLBlast to 1.6.0

.github/workflows/build.yml

commit c31bbe934b9666af42f32ce12d32cae9160e5dc4
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Wed May 24 02:24:01 2023 -0400

    readme : add docs for chat-persistent.sh (#1568)
    
    * readme : add docs for chat-persistent.sh
    
    * Update README.md

README.md

commit 1359b6aba55d5b0410f6adaa0aa2e49bbfd01d84
Author: Senemu <10880819+Senemu@users.noreply.github.com>
Date:   Wed May 24 06:16:22 2023 +0000

    chat-persistent.sh : use bracket expressions in grep (#1564)

examples/chat-persistent.sh

commit 7d873811f31d4d8c909015c946a862c0089cda7d
Author: Maarten ter Huurne <maarten@treewalker.org>
Date:   Tue May 23 18:01:15 2023 +0200

    Fix handling of "invalid property" when creating OpenCL command queue (#1565)
    
    The `clCreateCommandQueue()` function will return the code
    `CL_INVALID_QUEUE_PROPERTIES` when passed unsupported properties,
    not `CL_INVALID_PROPERTY` as the original code was checking for.

ggml-opencl.cpp

commit 2e6cd4b02549e343bef3768e6b946f999c82e823
Author: 0cc4m <picard12@live.de>
Date:   Mon May 22 23:33:24 2023 +0200

    OpenCL Token Generation Acceleration (#1459)
    
    * Move back to C++ for OpenCL
    
    * Refactor OpenCL code to work more like the CUDA code, add missing functions
    
    * Deduplicate dequant kernels
    
    * Add OpenCL compile options
    
    * Use compile args for preprocessing constants
    
    * Restore default platform + device selection by id behavior
    
    ---------
    
    Co-authored-by: Johannes Gäßler <johannesg@5d6.de>
    Co-authored-by: Henri Vasserman <henv@hot.ee>

CMakeLists.txt
Makefile
ggml-opencl.c
ggml-opencl.cpp
ggml-opencl.h
ggml.c
ggml.h
llama.cpp

commit 7e4ea5beff567f53be92f75f9089e6f11fa5dabd
Author: Steward Garcia <57494570+FSSRepo@users.noreply.github.com>
Date:   Sun May 21 11:51:18 2023 -0600

    examples : add server example with REST API (#1443)
    
    * Added httplib support
    
    * Added readme for server example
    
    * fixed some bugs
    
    * Fix the build error on Macbook
    
    * changed json11 to nlohmann-json
    
    * removed some whitespaces
    
    * remove trailing whitespace
    
    * added support custom prompts and more functions
    
    * some corrections and added as cmake option

CMakeLists.txt
examples/CMakeLists.txt
examples/server/CMakeLists.txt
examples/server/README.md
examples/server/httplib.h
examples/server/json.hpp
examples/server/server.cpp

commit 7780e4f479dc5af106287c164b8e186cd9b6215c
Author: Stefan Sydow <stefan@sydow.email>
Date:   Sun May 21 16:03:44 2023 +0200

    make : .PHONY clean (#1553)

Makefile

commit 265db9834e761b7c8210ea1888117efcd3262f52
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 21 11:56:23 2023 +0300

    ggml : output 3d sizes in ggml_graph_dump_dot()

ggml.c

commit fab49c685e09d95942de34e3eadd72f880de21d5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 20:00:41 2023 +0300

    ggml : update WASM SIMD

ggml.c

commit b8ee340abe4a46143eb8cc4a135b976856c9136c
Author: Zenix <zenixls2@gmail.com>
Date:   Sat May 20 23:58:31 2023 +0900

    feature : support blis and other blas implementation  (#1536)
    
    * feature: add blis support
    
    * feature: allow all BLA_VENDOR to be assigned in cmake arguments. align with whisper.cpp pr 927
    
    * fix: version detection for BLA_SIZEOF_INTEGER, recover min version of cmake
    
    * Fix typo in INTEGER
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Fix: blas changes on ci
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
BLIS.md
CMakeLists.txt
Makefile
README.md

commit 9ecb30f9594f222d8318fb1e803a4c363b0c39e5
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 20 17:57:39 2023 +0300

    OpenCL: Fixes for older devices. (#1435)
    
    * Remove `constant`
    
    * Rewrite platform and device selection
    
    * Fix Q8_0

ggml-opencl.c

commit 29cf5596fe0c37213f9b74e80d8f631193a93f0f
Author: Juuso Alasuutari <juuso.alasuutari@gmail.com>
Date:   Sat May 20 15:58:15 2023 +0300

    llama : define magic numbers as integer constants (#1518) (#1520)
    
    The underlying representation of multibyte character literals is
    implementation-defined. This could, at least in principle, cause
    cross-build data export/import issues independent of endianness.
    
    Define magic numbers as integer literals to be on the safe side.
    
    Signed-off-by: Juuso Alasuutari <juuso.alasuutari@gmail.com>

llama.cpp
llama.h

commit 3de84b26066d95068409c1dc79bcc41c1eea2a03
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 15:34:45 2023 +0300

    ggml : add ggml_clamp() (#1539)
    
    * ggml : add ggml_clamp()
    
    * ggml : indentation

ggml.c
ggml.h

commit affc76edfdefa7b326f526e463cc65ff13fcfb92
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 20 14:19:28 2023 +0200

    cuda : loading models directly into VRAM, norm calculation on GPU, broadcasting for ggml_mul (#1483)
    
    * Broadcasting for ggml_mul
    
    * CUDA kernel for ggml_mul, norms in VRAM
    
    * GPU weights not in RAM, direct loading with cuFile
    
    * fixup! GPU weights not in RAM, direct loading with cuFile
    
    * fixup! GPU weights not in RAM, direct loading with cuFile
    
    * define default model path once, sync path with readme (#1366)
    
    * ~7% faster Q5_1 AVX2 code (#1477)
    
    * convert.py: Support models which are stored in a single pytorch_model.bin (#1469)
    
    * Support models in a single pytorch_model.bin
    
    * Remove spurious line with typo
    
    * benchmark-matmul: Print the average of the test results (#1490)
    
    * Remove unused n_parts parameter (#1509)
    
    * Fixes #1511 lambda issue for w64devkit (mingw) (#1513)
    
    * Fix for w64devkit and mingw
    
    * make kv_f16 the default for api users (#1517)
    
    * minor : fix compile warnings
    
    * readme : adds WizardLM to the list of supported models (#1485)
    
    * main : make reverse prompt option act as a stop token in non-interactive mode (#1032)
    
    * Make reverse prompt option act as a stop token in non-interactive scenarios
    
    * Making requested review changes
    
    * Update gpt_params_parse and fix a merge error
    
    * Revert "Update gpt_params_parse and fix a merge error"
    
    This reverts commit 2bb2ff1748513591ad45b175a75ed1d8089d84c8.
    
    * Update gpt_params_parse and fix a merge error take 2
    
    * examples : add persistent chat (#1495)
    
    * examples : add persistent chat
    
    * examples : fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * tests : add missing header
    
    * ggml : use F16 instead of F32 in Q4_0, Q4_1, Q8_0 (#1508)
    
    * ggml : use F16 instead of F32 in Q4_0, Q4_1 and Q8_0
    
    * llama : bump LLAMA_FILE_VERSION to 3
    
    * cuda : update Q4 and Q8 dequantize kernels
    
    * ggml : fix AVX dot products
    
    * readme : update performance table + hot topics
    
    * ggml : fix scalar implementation of Q4_1 dot
    
    * llama : fix compile warnings in llama_set_state_data()
    
    * llama : fix name shadowing and C4146 (#1526)
    
    * Fix name shadowing and C4146
    
    * Fix if macros not using defined when required
    
    * Update llama-util.h
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    * Update llama-util.h
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    * Code style
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Fix for mingw (#1462)
    
    * llama : add llama_init_backend() API (close #1527)
    
    * feature : add blis and other BLAS implementation support (#1502)
    
    * feature: add blis support
    
    * feature: allow all BLA_VENDOR to be assigned in cmake arguments. align with whisper.cpp pr 927
    
    * fix: version detection for BLA_SIZEOF_INTEGER, recover min version of cmake
    
    * Fix typo in INTEGER
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Revert "feature : add blis and other BLAS implementation support (#1502)"
    
    This reverts commit 07e9ace0f9da424d82e75df969642522880feb92.
    
    * GPU weights not in RAM, direct loading with cuFile
    
    * llama : code style fixes + progress print fix
    
    * ggml : ggml_mul better broadcast support
    
    * cmake : workarounds for cufile when CMake version < 3.25
    
    * gg rebase fixup
    
    * Loop in llama.cpp, fixed progress callback
    
    * Attempt clang-tidy fix
    
    * llama : fix vram size computation
    
    * Add forgotten fclose()
    
    ---------
    
    Co-authored-by: András Salamon <ott2@users.noreply.github.com>
    Co-authored-by: Ilya Kurdyukov <59548320+ilyakurdyukov@users.noreply.github.com>
    Co-authored-by: Tom Jobbins <784313+TheBloke@users.noreply.github.com>
    Co-authored-by: rankaiyx <rankaiyx@rankaiyx.com>
    Co-authored-by: Stephan Walter <stephan@walter.name>
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>
    Co-authored-by: Erik Scholz <Green-Sky@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    Co-authored-by: David Kennedy <dakennedyd@gmail.com>
    Co-authored-by: Jason McCartney <jmac@theroot.org>
    Co-authored-by: Evan Jones <evan.q.jones@gmail.com>
    Co-authored-by: Maxime <672982+maximegmd@users.noreply.github.com>
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Zenix <zenixls2@gmail.com>

ggml-cuda.cu
ggml-cuda.h
ggml.c
llama-util.h
llama.cpp

commit ea600071cb005267e9e8f2629c1e406dd5fde083
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 12:03:48 2023 +0300

    Revert "feature : add blis and other BLAS implementation support (#1502)"
    
    This reverts commit 07e9ace0f9da424d82e75df969642522880feb92.

BLIS.md
CMakeLists.txt
Makefile
README.md

commit 07e9ace0f9da424d82e75df969642522880feb92
Author: Zenix <zenixls2@gmail.com>
Date:   Sat May 20 18:02:48 2023 +0900

    feature : add blis and other BLAS implementation support (#1502)
    
    * feature: add blis support
    
    * feature: allow all BLA_VENDOR to be assigned in cmake arguments. align with whisper.cpp pr 927
    
    * fix: version detection for BLA_SIZEOF_INTEGER, recover min version of cmake
    
    * Fix typo in INTEGER
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

BLIS.md
CMakeLists.txt
Makefile
README.md

commit ec2e10c4443209da56b431b24dd0845b60e757fb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 11:06:11 2023 +0300

    llama : add llama_init_backend() API (close #1527)

examples/benchmark/benchmark-matmult.cpp
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize/quantize.cpp
llama.cpp
llama.h

commit d2c59b8ba498ab01e65203dde6fe95236d20f6e7
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Sat May 20 00:40:02 2023 -0700

    Fix for mingw (#1462)

examples/common.cpp

commit 503db28849d8641d66244385e7e9649608a2e4d0
Author: Maxime <672982+maximegmd@users.noreply.github.com>
Date:   Sat May 20 09:22:37 2023 +0200

    llama : fix name shadowing and C4146 (#1526)
    
    * Fix name shadowing and C4146
    
    * Fix if macros not using defined when required
    
    * Update llama-util.h
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    * Update llama-util.h
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    
    * Code style
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c
llama-util.h
llama.cpp

commit 8a203f9fa1b24e010be8f35ebbbd6786293684cb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 10:14:31 2023 +0300

    llama : fix compile warnings in llama_set_state_data()

llama.cpp
llama.h

commit 4fd3e29297e3246a7be291932c115636fadb0f52
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 20 10:13:19 2023 +0300

    ggml : fix scalar implementation of Q4_1 dot

ggml.c

commit 2d5db48371052087a83974abda3767d1aedec598
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 19 22:17:18 2023 +0300

    ggml : use F16 instead of F32 in Q4_0, Q4_1, Q8_0 (#1508)
    
    * ggml : use F16 instead of F32 in Q4_0, Q4_1 and Q8_0
    
    * llama : bump LLAMA_FILE_VERSION to 3
    
    * cuda : update Q4 and Q8 dequantize kernels
    
    * ggml : fix AVX dot products
    
    * readme : update performance table + hot topics

README.md
ggml-cuda.cu
ggml.c
ggml.h
llama.cpp
llama.h

commit 6986c7835adc13ba3f9d933b95671bb1f3984dc6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 19 21:17:28 2023 +0300

    tests : add missing header

tests/test-sampling.cpp

commit 943e6081cc939df7584f8f0ab7057a39c2ef3271
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Fri May 19 13:39:51 2023 -0400

    examples : add persistent chat (#1495)
    
    * examples : add persistent chat
    
    * examples : fix whitespace
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/chat-persistent.sh

commit 7694b52b9a206b93d59139c3c7c9b55da0f5aa59
Author: Jason McCartney <jmac@theroot.org>
Date:   Fri May 19 10:24:59 2023 -0700

    main : make reverse prompt option act as a stop token in non-interactive mode (#1032)
    
    * Make reverse prompt option act as a stop token in non-interactive scenarios
    
    * Making requested review changes
    
    * Update gpt_params_parse and fix a merge error
    
    * Revert "Update gpt_params_parse and fix a merge error"
    
    This reverts commit 2bb2ff1748513591ad45b175a75ed1d8089d84c8.
    
    * Update gpt_params_parse and fix a merge error take 2

examples/common.cpp
examples/main/main.cpp

commit 79e3efb0e97b65b6cc72cd9ee970fa8189ad79a4
Author: David Kennedy <dakennedyd@gmail.com>
Date:   Fri May 19 13:16:30 2023 -0400

    readme : adds WizardLM to the list of supported models (#1485)

README.md

commit 4b7e245adf63db675c3daab4a9bfddd451ef4097
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 19 20:14:51 2023 +0300

    minor : fix compile warnings

examples/common.cpp
examples/common.h
llama.cpp
tests/test-sampling.cpp

commit 5ea43392731040b454c293123839b90e159cbb99
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Thu May 18 19:31:01 2023 +0200

    make kv_f16 the default for api users (#1517)

llama.cpp

commit ee9654138ab0ae5f138f4abddf56ca234ea3c352
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu May 18 10:30:40 2023 -0700

    Fixes #1511 lambda issue for w64devkit (mingw) (#1513)
    
    * Fix for w64devkit and mingw

examples/main/main.cpp

commit dc271c52ed65e7c8dfcbaaf84dabb1f788e4f3d0
Author: Stephan Walter <stephan@walter.name>
Date:   Wed May 17 22:12:01 2023 +0000

    Remove unused n_parts parameter (#1509)

examples/common.cpp
examples/common.h
examples/quantize-stats/quantize-stats.cpp
examples/save-load-state/save-load-state.cpp
llama.cpp
llama.h

commit c238b5873a1ea496db03ffcfe124c9d0d83afbc6
Author: rankaiyx <rankaiyx@rankaiyx.com>
Date:   Wed May 17 22:47:58 2023 +0800

    benchmark-matmul: Print the average of the test results (#1490)

examples/benchmark/benchmark-matmult.cpp

commit 2b2646931bd2a2eb3e21c6f3733cc0e090b2e24b
Author: Tom Jobbins <784313+TheBloke@users.noreply.github.com>
Date:   Tue May 16 23:04:35 2023 +0100

    convert.py: Support models which are stored in a single pytorch_model.bin (#1469)
    
    * Support models in a single pytorch_model.bin
    
    * Remove spurious line with typo

convert.py

commit 42627421ece816e632e6a0d757fa75150c687f87
Author: Ilya Kurdyukov <59548320+ilyakurdyukov@users.noreply.github.com>
Date:   Wed May 17 01:36:47 2023 +0700

    ~7% faster Q5_1 AVX2 code (#1477)

ggml.c

commit 9560655409dc80771a9b19e838ff47c5c1df6483
Author: András Salamon <ott2@users.noreply.github.com>
Date:   Tue May 16 16:46:34 2023 +0100

    define default model path once, sync path with readme (#1366)

examples/common.h
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/save-load-state/save-load-state.cpp

commit 2a5ee023ad3022bc0b505343394b9754587fb731
Author: sandyiscool <sandyiscool@gmail.com>
Date:   Tue May 16 14:00:15 2023 +0530

    Add alternate include path for openblas (#1476)
    
    In some linux distributions (fedora, for example), the include path for openblas is located at '/usr/local/include'

Makefile

commit 63d20469b85467c5729cc9a97bd44cc3da63423f
Author: zrm <trustiosity.zrm@gmail.com>
Date:   Sun May 14 22:25:42 2023 -0400

    fix get_num_physical_cores() (#1436)
    
    * fix get_num_physical_cores()
    had been broken on complex topologies because "cpu cores" in /proc/cpuinfo is per-"physical id"
    
    * Add spaces to maintain consistent formatting
    
    ---------
    
    Co-authored-by: slaren <ddevesa@gmail.com>

examples/common.cpp

commit b5c9295eef2b56e307393b35b3a923e3518d226e
Author: slaren <slarengh@gmail.com>
Date:   Sun May 14 22:46:00 2023 +0200

    benchmark-matmul: fix clang-tidy issues, report results in GFLOPS (#1458)
    
    * benchmark-matmul: fix command line parsing, replace macros with functions, report results in GFLOPS

examples/benchmark/benchmark-matmult.cpp

commit eb363627fda5f47de8ab5e9be8abd426049d00df
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sun May 14 20:53:23 2023 +0200

    cuda : deduplicated dequantization code (#1453)

ggml-cuda.cu

commit 79b2d5b69d80be0bf29312fb9a95854876b0a8a5
Author: xaedes <xaedes@gmail.com>
Date:   Sun May 14 17:55:02 2023 +0200

    ggml : alternative fix for race condition bug in non-inplace ggml_compute_forward_diag_mask_f32 (#1454)
    
    * fix race condition bug in non-inplace ggml_compute_forward_diag_mask_f32
    
    memcpy needs to be synchronized across threads to avoid race conditions.
    => do it in INIT phase
    
    * remove trailing whitespace
    
    * Update ggml.c
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c

commit 13c351ad7292c5b5ab35db25c7a4f993e75d9cfd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 14 18:22:50 2023 +0300

    ggml : various fixes (#1450)
    
    - `ggml_rope()`
    - `ggml_diag_mask_inf()` multi-threaded
    - compatibility with scratch buffers

ggml.c
ggml.h

commit 60f8c361ca26328ef8523dfb08077fe2f1034490
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Sun May 14 19:03:51 2023 +0900

    ggml : add AVX support based on AVX2 code (#1430)

ggml.c

commit 601a033475645370483973817d987928ea95f36c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun May 14 10:20:19 2023 +0300

    ggml : add GGML_QNT_VERSION to track quantization format changes
    
    https://github.com/ggerganov/ggml/issues/150#issuecomment-1546625668

ggml.h

commit 08737ef720f0510c7ec2aa84d7f70c691073c35d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 17:40:58 2023 +0300

    cuda : fix convert function (#1412)

ggml-cuda.cu

commit bda4d7c215aa16b2a78e522521dfc0e1c2e8b194
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 17:25:09 2023 +0300

    make : fix PERF build with cuBLAS

Makefile

commit 5a5aeb1e91009c72bf816400b758bb8a305616d7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 16:55:14 2023 +0300

    llama : fix unused warning

llama.cpp

commit 66841fdb0eaf0cc210757cc7f683d0f4eebadc21
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 16:48:03 2023 +0300

    ggml : multi-thread mul and diag_mask ops (#1428)

ggml.c

commit 905d87b70aa189623d500a28602d7a3a755a4769
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Sat May 13 15:38:36 2023 +0200

    ggml : GPU-accelerated token generation (#1412)
    
    * CUDA kernel for q4_0 dequant. + mat. vec. mult.
    
    * Added q4_1 via template
    
    * Added missing __syncthreads();
    
    * --gpu_layers -> --gpu-layers
    
    * Shorter dequantize_mul_mat_vec line
    
    * q5_0 dequantize_mul_mat kernel
    
    * More readable dequantize_mul_mat_vec logic
    
    * dequantize_mul_mat_vec kernels for q5_1, q8_0, f16
    
    * llama : offload "output" tensor to GPU too + coding style fixes
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/common.cpp
examples/common.h
ggml-cuda.cu
ggml-cuda.h
ggml.c
ggml.h
llama.cpp
llama.h

commit f954edda935a70a14cf0cc45ecc7fe7d60cf3e4b
Author: xaedes <xaedes@gmail.com>
Date:   Sat May 13 14:56:40 2023 +0200

    ggml : implement backward pass for llama + small training-llama-from-scratch example (#1360)
    
    * implement 8 of 14 missing backward pass operations used by llama
    
    - GGML_OP_ADD_AT
    - GGML_OP_CPY
    - GGML_OP_MUL_MAT (src0.grad)
    - GGML_OP_PERMUTE
    - GGML_OP_RESHAPE
    - GGML_OP_SCALE
    - GGML_OP_TRANSPOSE
    - GGML_OP_VIEW
    
    implement additional ggml operation GGML_OP_ADD_AT, which is necessary for backward pass of GGML_OP_VIEW.
    
    this operation adds src1 to src0 with data offset, i.e. to view(src0, ..., offset).
    the values are return in a tensor size of src0. values outside of [data+offset:data+offset+nbytes(src1)] are just the original values from src0.
    
    still missing backward passes for llama:
    
    - GGML_OP_DIAG_MASK_INF
    - GGML_OP_GET_ROWS
    - GGML_OP_RMS_NORM
    - GGML_OP_ROPE
    - GGML_OP_SILU
    - GGML_OP_SOFT_MAX
    
    * implement 5 of 6 missing backward pass operations used by llama
    
    - GGML_OP_DIAG_MASK_INF
    - GGML_OP_GET_ROWS
    - GGML_OP_RMS_NORM
    - GGML_OP_SILU
    - GGML_OP_SOFT_MAX
    
    add necessary ggml operations GGML_OP_ADD1, GGML_OP_SILU_BACK, GGML_OP_RMS_NORM_BACK, GGML_OP_DIAG_MASK_ZERO, and GGML_OP_ROPE_BACK
    
    GGML_OP_ADD1 is necessary to add a scalar value in the backward pass of GGML_OP_SOFT_MAX
    GGML_OP_ADD1 could also be replaced by using GGML_OP_ADD and GGML_OP_REPEAT, but the performance would be worse. additionally GGML_OP_REPEAT will return unexpected value when the the input to GGML_OP_SOFT_MAX contains only a single scalar. in this case GGML_OP_REPEAT will not return the value that should be repeated (src1) but the value which shape the result should take (src0). So in this case it can not replace GGML_OP_ADD1.
    
    GGML_OP_SILU_BACK, GGML_OP_RMS_NORM_BACK and GGML_OP_ROPE_BACK are necessary for backward pass of GGML_OP_SILU, GGML_OP_RMS_NORM and GGML_OP_ROPE. The backward pass for these functions cannot be easily composed of existing operations. Since the backward pass builds a computation graph we need operations forward pass implementations of the the required backward passes. Sounds a bit confusing at first, I know...
    
    GGML_OP_DIAG_MASK_ZERO is necessary for backward pass of GGML_OP_DIAG_MASK_INF.
    
    Some operations where previously inplace-only. for backward pass there needs to be non-inplace variants.
    staying consistent with other operations that have non-inplace and inplace variants, the operations are changed to non-inplace and
    functions with "_inplace" are added which are inplace.
    in llama we need to call the inplace variants so that it is implemented as before.
    for llama backward pass we need to use the non-inplace variants.
    
    still not completely implemented backward passes for llama:
    
    - GGML_OP_ROPE: needs forward pass for GGML_OP_ROPE_BACK
    - GGML_OP_GET_ROWS: only necessary for tokenizer
    
    * norm & rms_norm can not be threaded:
    
    after investigation rms norm for quite some time I come to the conclusion that neither norm, nor rms_norm can be threaded, because we need mean over all items, not just of the slices each thread sees.
    
    * remove already resolved TODO
    
    * implement backward pass of ggml_rope and ggml_rope_back
    
    * implement backward pass for ggml_get_rows and for new operation ggml_get_rows_back
    
    * add test-grad0.c
    
    * use GGML_PRINT_DEBUG for debug messages which will otherwise flood the console
    
    * test both gradients of mul_mat
    
    * disable graph dot export as it floods console
    
    * bug fixes for silu_back
    
    * successfully test silu backward
    
    * bug fix for scale backward pass
    
    use sum instead of mean for gradient of scalar scale parameter
    
    * successfully test scale backward
    
    * improve performance of sum backward pass
    
    use add1(x,y) instead of add(x,repeat(y,x))
    
    * improve performance of sqr backward pass
    
    use scale(x,y) instead of mul(x,repeat(y,x))
    
    * successfully test rope backward
    
    * bug fix for cpy backward pass
    
    * successfully test cpy backward
    
    * bug fix for reshape backward pass
    
    * successfully test reshape backward
    
    * add test-opt.c
    
    this uses ggml_opt to train a,b for minimal e=sum(sqr(c - a*b)) for random initial a,b,c
    
    * correctly implement softmax backward pass using new operation ggml_diag
    
    ggml_diag constructs diagonal matrices with entries.
    ggml_diag(shape[a,1,c,d]) -> shape[a,a,c,d]
    
    * successfully test soft_max backward
    
    * align shape annotations
    
    * add shape annotations for llama
    
    * de-duplicate ggml_forward_dup code taking care of contiguous tensors of same type.
    
    with this we can duplicate tensor of any typ as long as they are contiguous.
    
    * fix ggml_compute_forward_dup_same_cont for when nelements < nthreads
    
    when more threads are used than elements exist ie1 was less than ie0, resulting in invalid negative byte count argument in memcpy
    
    * bug fix for add_at forward
    
    required for view backward pass
    
    src0 values must be copied to dst, because during addition we don't touch all dst elements in contrast to the normal add function.
    
    * successfully test view backward
    
    * minor code format improvement
    
    * fix ggml_forward_add functions to work correctly with transposed tensors
    
    uses the same logic as in ggml_compute_forward_add_q_f32, but make it consistent across all ggml_compute_forward_add_... functions.
    this also slightly changes the mem access pattern of the different threads to works as in ggml_compute_forward_add_q_f32.
    
    * fix ggml_forward_add1 functions to work correctly with transposed tensors
    
    uses the same logic as in ggml_compute_forward_add1_q_f32, but make it consistent across all ggml_compute_forward_add1_... functions.
    this also slightly changes the mem access pattern of the different threads to works as in ggml_compute_forward_add1_q_f32.
    
    * test-grad0.c : add print_elements to help with debugging
    
    * successfully test permute backward
    
    * some minor test-grad0 fixes
    
    * fix sub, mul and div functions to work correctly with transposed tensors
    
    uses the same logic as in add
    
    * implement ggml_cont backward pass
    
    * successfully test transpose backward and permute for all permutations
    
    also test sub, mul and div up to max n_dims
    
    * test-grad0.c add TODO for view_2d and view_3d
    
    add_at (required for view backward pass) is a bit tricky for n_dims > 1.
    
    * fix comments
    
    * successfully test diag_mask_inf and diag_mask_zero backward
    
    * test-grad0 : fix test for div
    
    nargs and ndims was swapped, corrupting the stack
    
    * fix diag_mask to work with non-inplace input
    
    * move dup call into the actual add_at functions
    
    * fix get rows backward pass
    
    * successfully test get_rows backward
    
    * fix view backward pass
    
    add nb parameters to add_at like in view.
    together with offset they define how to view dst and src0 during the add_at operation.
    
    * successfully test backward pass of view_1d, view_2d and view_3d
    
    * fix backward pass for rms_norm
    
    I would have used formulas from other frameworks, but they differed so I could not decide which is correct.
    Instead it was derived here in comment using manual forward-backward automatic differention of rms_norm and simplification.
    
    * successfully test backward pass of rms_norm
    
    some tests may fail when gradients are large.
    could not find a satisfying configuration to check for abs error and relative error that passes all tests while still actually testing the results with tight enough error bounds.
    when looking at the values the "failed" tests look actually ok. for example:
    
    rms_norm: ndims=2, i=0, k=2, x0=0.000153, xm=0.000053, xp=0.000253, f0=0.278594, f1=0.086213, g0=961.905457, g1=966.064941, eps=0.000100, error_abs=4.159485, error_rel=0.004324
    
    it is due to the test logic in check_gradients that they fail.
    
    * add todos for llama backward pass
    
    - implementation for ADD1 backward pass should probably use sum instead of mean (but this backward pass is not required)
    - repeat is not yet tested and looks like it only works for single element src0 inputs.
    
    * add operation ggml_sum_rows
    
    ggml_sum_rows(shape[a,b,c,d]) -> shape[1,b,c,d]
    
    * add missing GGML_OP_SUM_ROWS
    
    * fix backward pass for repeat
    
    requires ggml_sum_rows
    
    * successfully test backward pass of repeat
    
    * update quantization types in switch-case of add_at and add1
    
    * add baby-llama example training a very small llama model from scratch to output a sinusoidal wave.
    
    had to increase maximum number of optimization parameters to train from scratch.
    
    * fix softmax in baby-llama example
    
    * switching from training with adam to lbfgs produces much better results in the baby-llama example
    
    * train with two examples, creating new tensors each time..
    
    * fix bug when using ggml_opt to optimize params in one context and use a renewable context for eval and opt
    
    when not keeping gradients of model parameters they are overwritten by tensors created by opt, which may be invalid after opt context is renewed.
    so we need to keep the original gradients and make dups for opt
    
    * train on multiple examples, generate & print tokens with trained model afterwards
    
    ctx0 for evaluation and optimization is renewed for each sample
    
    * add ggml_reshape_1d, ggml_reshape_4d and ggml_view_4d
    
    * fix soft_max backward pass for input->ne[1] != 1
    
    * add ggml_log operation necessary for cross entropy loss
    
    * add test for ggml_log gradients
    
    * implement backward pass for ggml_sum_rows, necessary for cross entropy loss
    
    * implement ggml_repeat support for rank > 2 tensors
    
    * add test for ggml_sum_rows gradients
    
    * fix training get_example_targets
    
    predict the next token, not the current token!
    
    * add square_error_loss and cross_entropy_loss functions
    
    * optimize loss over multiple samples
    
    this increases computation graph, need parallel batched forward for more efficiency.
    
    * fix backward pass for add_at and change arguments to have same order as in view
    
    * add ggml_set(ctx, a, b) to set b in view of a and return modified a
    
    necessary to set values into kv_self cache and properly propagate the gradients
    
    * fix kv_self gradients for training
    
    use ggml_set instead of ggml_cpy to set kv_self cache with properly propagating gradients
    
    * replace inplace operations for training with copying operations to allow gradient propagation
    
    * add GGML_ASSERT to catch ggml_rope and back value errors
    
    * add trainable lora-only model with all big matrices C split into A,B with A*B=C
    
    this is not a lora-finetune, but the whole model changed to have only low-rank "lora" matrices.
    
    training this instead of the normal model resulted in much worse results though...
    
    * vastly improve training results
    
    instead of logit targets 0 and 1 use -1 and +1.
    
    * shorten code using a variable
    
    * change name of GGML_OP_ADD_AT to GGML_OP_ACC
    
    * smaller default values for baby llama model parameters
    
    * update static assert of GGML_OP_COUNT
    
    * remove shape annotations in llama_eval_internal
    
    * revert disabling of threading for rms_norm and norm
    
    * rename print functions in baby-llama example
    
    * fix call to ggml_set_name
    
    * add missing include for strcmp, etc
    
    * remove trailing whitespace
    
    * reduce number of test-grad0 iterations
    
    avoid exceeding timeout of automated tests
    
    * remove busy loop that was used as sleep for slower sinus wave generation
    
    * disable slow tests grad0 and opt to avoid exceeding timeouts
    
    * c++ in baby-llama example
    
    use c++ includes instead of c includes
    use std::min, std::max instead of MIN, MAX macros
    
    * c++ in baby-llama example
    
    use c++ includes instead of c includes
    use std::min, std::max instead of MIN, MAX macros
    
    * ggml : fix compiler warnings + cosmetic changes
    
    * ggml : fix nullptr derefs in GGML_OP_CONT and GGML_OP_RESHAPE back
    
    * swap arguments to vDSP_vdiv call
    
    documentation for vDSP_vdiv states: "Note that B comes before A!"
    
    * swap arguments to vDSP_vdiv call
    
    documentation for vDSP_vdiv states: "Note that B comes before A!"
    
    * ggml : swap vDSP_vsub args as per documentation
    
    * add parallel batched forward function for baby-llama training
    
    * cleanup code for batched training
    
    * remove trailing whitespace
    
    * minor : fix compiler warnings + indentation style
    
    * ggml : fix null ptr deref in backward pass
    
    * ggml : remove Q4_2 remnants
    
    * ggml : fix clang-tidy warnings
    
    * baby-llama : couple of clang-tidy warnings
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/CMakeLists.txt
examples/baby-llama/CMakeLists.txt
examples/baby-llama/baby-llama.cpp
ggml.c
ggml.h
llama.cpp
tests/CMakeLists.txt
tests/test-grad0.c
tests/test-opt.c

commit f048af0230ad5381bb20b9e3c1467ec6fc7debdf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 11:54:33 2023 +0300

    ggml : sync alibi fix from ggml repo

ggml.c

commit ac0cd259d54be7e45ffa2c7b2508812cf4f83ccf
Author: 3ooabkhxtn <31479382+3ooabkhxtn@users.noreply.github.com>
Date:   Sat May 13 10:43:33 2023 +0200

    Adding SSE instructions to ggml_vec_dot_q4_0_q8_0 (#1413)

ggml.c

commit 0cd22e190aeaef867fa5db025b4d274f2fcfdcf6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 11:23:15 2023 +0300

    llama : fix various warnings

.gitignore
llama.cpp

commit 6456a4eb9f9c1f4de8f6908ef100daa78802ad00
Author: Rinne <AsakusaRinne@gmail.com>
Date:   Sat May 13 15:24:20 2023 +0800

    embedding : remove unused code (#1426)

examples/embedding/embedding.cpp

commit cdd5350892b1d4e521e930c77341f858fcfcd433
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 09:12:44 2023 +0300

    readme : update Q4_0 perplexities
    
    I think these were affected by the removal of the `round` during quantization

README.md

commit 738ace394a6f8cf0174e90a97185d9e512c0e200
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat May 13 09:08:52 2023 +0300

    llama : free ggml context in set / copy state data (close #1425)

llama.cpp
llama.h

commit 699b1ad7fe6f7b9e41d3cb41e61a8cc3ea5fc6b5
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat May 13 09:01:15 2023 +0300

    opencl : fix kernels for the new formats (#1422)
    
    * Fix OpenCL kernels for the new formats
    
    * Fix Q5_0 alignment issues.

ggml-opencl.c

commit fb62f924336c9746da9976c6ab3c2e6460258d54
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 12 21:44:20 2023 +0300

    llama : fix --mtest option (close #1414)

examples/main/main.cpp

commit 773ee249fb6c14f791ac39f6ec05536f40dedc54
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri May 12 16:34:55 2023 +0200

    CLI args use - instead of _, backwards compatible (#1416)

examples/common.cpp

commit 553fd4d4b5f3314f338be8006f62a4fdf7670186
Author: slaren <slarengh@gmail.com>
Date:   Fri May 12 15:40:53 2023 +0200

    Add clang-tidy reviews to CI (#1407)

.clang-tidy
.github/workflows/tidy-post.yml
.github/workflows/tidy-review.yml

commit 089b1c93ba2b93bc9a605af293730a028fad2c4e
Author: Rinne <liu_yaohui1998@126.com>
Date:   Fri May 12 13:39:40 2023 +0800

    readme : add C#/.NET bindings repo (#1409)

README.md

commit b9fd7eee57df101d4a3e3eabc9fd6c2cb13c9ca1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri May 12 00:23:08 2023 +0300

    ggml : remove bit shuffling (#1405)
    
    * ggml : remove Q4_0 bit shufling (ARM NEON)
    
    * ggml : remove Q4_1 bit shuffling (ARM NEON + reference)
    
    * ggml : nibbles_from_floats() + bytes_from_nibbles() (ARM NEON)
    
    * ggml : remove Q4_2 bit shuffling (WIP, BROKEN)
    
    * ggml : remove Q5_0 bit shuffling (ARM NEON)
    
    * ggml : 2x faster scalar implementations
    
    * ggml : remove Q5_1 bit shuffling (ARM NEON + scalar)
    
    * ggml : simplify scalar dot
    
    * ggml : remove WASM SIMD bit shuffling + remove vzip for ARM 32-bit
    
    * ggml : fix Q4_1 quantization
    
    * ggml : update cuBLAS + normalize variable names
    
    * ggml : remove Q4_2 mode
    
    * ggml : minor formatting
    
    * ggml : fix Q5_0 quantization
    
    * scripts : add script for measuring the time per token
    
    * AVX implementations (#1370)
    
    * ggml : uniform 5th bit extraction
    
    * llama : produce error upon loading old model files
    
    * llama : fix model magic/version write
    
    * ggml : speed-up Q5_0 + Q5_1 at 4 threads
    
    * ggml : preserve old Q4 and Q5 formats
    
    * ggml : simplify Q8_1 - no need for low / high sums anymore
    
    * ggml : fix Q8_0 and Q8_1 rounding
    
    * Revert "AVX implementations (#1370)"
    
    This reverts commit 948d124837f9d287d8490f41338e0e4cceb0814f.
    
    * ggml : fix AVX2 implementation
    
    * sha : update hashes for 7B and 13B
    
    * readme : update timings + remove warning banner
    
    * llama : update v2 PR number to 1405
    
    * ggml : fix WASM comments
    
    * ggml : back to original bit order
    
    * readme : add note that Q4 and Q5 have been changed
    
    * llama : fix return for unknown version
    
    ---------
    
    Co-authored-by: Stephan Walter <stephan@walter.name>

.gitignore
README.md
SHA256SUMS
examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-opencl.c
ggml.c
ggml.h
llama.cpp
llama.h
scripts/perf-run-all.sh
scripts/ppl-run-all.sh

commit b608b55a3ea8e4760c617418538465449175bdb8
Author: CRD716 <crd716@gmail.com>
Date:   Thu May 11 10:10:19 2023 -0500

    prompts : model agnostic DAN (#1304)
    
    * add model-agnostic dan prompt
    
    * quick readme update
    
    * save a token
    
    * Revert "quick readme update"
    
    This reverts commit 8dc342c069cbdca8ce63ad974becec6fc844e1e4.

prompts/dan-modified.txt
prompts/dan.txt

commit cf348a60e0af3905acd1d297cb064b918265b7ac
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Wed May 10 11:37:14 2023 -0400

    main : add option to save full output to session (#1338)
    
    * main : add option to save full output to session
    
    * split behavior into --session and --prompt-cache
    
    * restore original implementation with new names
    
    * PR comments
    
    * move the check for incompatible parameters to gpt_params_parse
    
    * Fix whitespace
    
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>
    
    ---------
    
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>

examples/common.cpp
examples/common.h
examples/main/README.md
examples/main/main.cpp

commit e6a46b0ed1884c77267dc70693183e3b7164e0e0
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Tue May 9 10:53:28 2023 -0700

    Locale fix for Windows (#1379)

examples/common.cpp

commit 9f8dbc4787f32cd9c13b5c647d497d13c1a06db2
Author: Sami Farin <3876865+Safari77@users.noreply.github.com>
Date:   Tue May 9 15:29:20 2023 +0300

    use pause asm insn in busyloop to run the CPU (13600K) 10 °C cooler (#1314)
    
    * use pause asm insn in busyloop to run the CPU (13600K) 10 °C cooler
    
    Tested with a 13B model.
    
    * use _mm_pause() in busyloop
    
    * use _mm_pause() in busyloop on x86_64 to reduce power consumption

ggml.c

commit 41654efea879bbdf4fd794e13335929d4cf0eb90
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Mon May 8 19:45:48 2023 -0700

    Interface improvements and `--multiline-input` (previously `--author-mode`) (#1040)
    
    * Interface improvements
    * Multiline input
    * Track character width
    * Works with all characters and control codes + Windows console fixes

examples/common.cpp
examples/common.h
examples/main/main.cpp

commit 56551bc11f46b2716fdf61bb48ac28414889dc0a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 8 22:52:18 2023 +0300

    readme : add notice about upcoming breaking change

README.md

commit fe60904eef4b504685fa0406cb19864ae619fb4f
Author: AlpinDale <52078762+AlpinDale@users.noreply.github.com>
Date:   Mon May 8 21:03:30 2023 +0430

    readme : add TOC and Pygmalion instructions (#1359)

README.md

commit 003ba2fb4309e2339487564bd249e4fcc8d7ea01
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Mon May 8 16:48:21 2023 +0200

    llama : fix hparams shadow (#1367)
    
    fixes #1363

llama.cpp

commit f9a6364912fd0463fddfdbc9ef9f79fdc281570d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 8 17:41:54 2023 +0300

    llama : require first token to be BOS (#1303)
    
    * llama : require first token to be BOS
    
    * scripts : add ppl-run-all.sh
    
    * perplexity : add BOS for each chunk
    
    * readme : update perplexity values after BOS fix
    
    * perplexity : add clarifying comments

.gitignore
README.md
examples/common.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
llama.cpp
scripts/ppl-run-all.sh

commit 95078cc554fe03d4512363c7e4dec963f0047c72
Author: ubik2 <ubik2@users.noreply.github.com>
Date:   Mon May 8 04:54:26 2023 -0700

    convert: add ability to convert safetensors files (#1276)
    
    * when loading a safetensors file, ignore the metadata header
    * check for safetensors files first, and only use PyTorch versions when safetensors aren't available

convert.py

commit 1f48b0abcfbd6cc99571e42348e0ec97e4be8b93
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Mon May 8 02:42:01 2023 +0200

    Documented CUDA reproducibility, added warning (#1346)

README.md
examples/common.cpp
ggml-cuda.cu

commit e1295513a48ae8254d8af5ec0250b56d6eaffefd
Author: Henri Vasserman <henv@hot.ee>
Date:   Sun May 7 14:20:09 2023 +0300

    CI: add Windows CLBlast and OpenBLAS builds (#1277)
    
    * Add OpenCL and CLBlast support
    
    * Add OpenBLAS support
    
    * Remove testing from matrix
    
    * change build name to 'clblast'

.github/workflows/build.yml

commit 1b0fd454650ef4d68a980e3225488b79e6e9af25
Author: swittk <switt1995@gmail.com>
Date:   Sun May 7 10:03:23 2023 +0700

    ggml : Allow usage of CLBlast alongside Accelerate.framework (#1336)
    
    Minor edit in ggml.c which originally would prevent OpenCL from loading completely if GGML_USE_ACCELERATE was defined.
    Minor speedup in prompt eval time.

ggml.c

commit 3924088512d9e12e90ed6dbf28a6c5712481d33e
Author: Jed Fox <git@jedfox.com>
Date:   Sat May 6 17:01:47 2023 -0400

    Remove default arguments from sampling functions (#1343)

.gitignore
examples/main/main.cpp
llama.cpp
llama.h
tests/test-sampling.cpp

commit 173d0e6419e8f8f3c1f4f13201b777f4c60629f3
Author: DaniAndTheWeb <57776841+DaniAndTheWeb@users.noreply.github.com>
Date:   Fri May 5 23:57:14 2023 +0200

    makefile: automatic Arch Linux detection (#1332)
    
    This commit is a port of a detection method used in koboldcpp's Makefile in order to automatically set the -lcblas option on Arch Linux

Makefile
README.md

commit a3b85b28da84c67c3406807aef5e0457bcc4b00f
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Fri May 5 22:56:09 2023 +0200

    ci : add cublas to windows release (#1271)

.github/workflows/build.yml

commit 921dcee00a55d9aba3b3026d0509d31ac8386e2a
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri May 5 16:43:36 2023 +0200

    readme: add missing info (#1324)

README.md

commit 2d13786e91ec9fd28ddf737053822042a824da78
Author: Ionoclast Laboratories <brigham@ionoclast.com>
Date:   Fri May 5 08:18:21 2023 -0400

    Fix for OpenCL / clbast builds on macOS. (#1329)

Makefile

commit a90e96b266873ebb5e947c9864b12193bdada0fb
Author: Benjamin Lecaillon <84293038+blecaillon@users.noreply.github.com>
Date:   Fri May 5 02:17:07 2023 +0200

    Convert.py @staticmethod (#1327)
    
    * Line 698 has one #staticmethod and should not
    
    otherwise throw error at unpickle.load() as not callable
    
    * Update convert.py
    
    ---------
    
    Co-authored-by: Ivan Stepanov <ivanstepanovftw@gmail.com>

convert.py

commit 94c5652fc0f4d04ac54412c4d81e2ebcdafb6ede
Author: slaren <slarengh@gmail.com>
Date:   Fri May 5 00:58:56 2023 +0200

    quantize: make output filename optional, default to ggml-model-<ftype>.bin (#1301)

examples/quantize/quantize.cpp

commit 34d9f22f44c42d345cc72c8f3aa4cb71c5df0acb
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Thu May 4 19:56:27 2023 +0300

    Wrap exceptions in std::exception to verbose output on exception. (#1316)

llama-util.h

commit d3e8093e9b5845514b049ede3b12728c8f013eba
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Thu May 4 19:54:37 2023 +0300

    convert: support DT_BF16 tensors (#1309)
    
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

convert.py

commit 360cfe5bec852805b84eec799102fc6f45df9fef
Author: 44670 <44670@users.noreply.github.com>
Date:   Fri May 5 00:33:31 2023 +0800

    readme : add OpenBuddy link (#1321)

README.md

commit 2edbdb0f99336cb41f0995061c7602ed54beb863
Author: 44670 <44670@users.noreply.github.com>
Date:   Thu May 4 23:41:12 2023 +0800

    main : add --in-suffix option (#1318)
    
    * adding --in-suffix option
    
    * print input suffix before generation

examples/common.cpp
examples/common.h
examples/main/README.md
examples/main/main.cpp

commit 20fbf2a2a08d8edefe9b3435fa86f8b2f63f8588
Author: Ron Jailall <rojailal@gmail.com>
Date:   Thu May 4 11:05:59 2023 -0400

    ggml : change immintrin.h to intrin.h for compatibility (#1307)
    
    * change immintrin.h to intrin.h for compatibility
    
    Building on windows11 arm throws an error on this line. Seems like using intrin.h covers x86 and and arm
    
    * conditional def of intrin.h
    
    * fix typo in ggml.c

ggml.c

commit db1080876a62ec3bb4119d90b16e7dce7594b733
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu May 4 05:08:25 2023 -0700

    Only escape prompts when used with `-e` (#1311)

examples/common.cpp
examples/main/README.md

commit c65a7fbfa9c736416a25369cc05d356789df4c15
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Thu May 4 03:02:59 2023 -0700

    Update main's README.md with new features (#1296)

examples/main/README.md

commit f647ce040ff06348d2ceaa5443a6a7a8b80c70c9
Author: Tomas <tom.tomas.36478119@gmail.com>
Date:   Thu May 4 17:02:30 2023 +0700

    fix #1224 reverse prompt and multi line (#1297)
    
    * fix reverse prompt and multi line
    
    * Code Formatting
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/main/main.cpp

commit 799fdc1b5d888b8a8682baf112e1c2a2df0df1c4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 3 23:24:20 2023 +0300

    ggml : vectorize Q8_0 quantization
    
    https://github.com/ggerganov/ggml/pull/127#issuecomment-1533648531

ggml.c

commit 6daa09d87926fe654385c2887e39ec3eeaa58120
Author: khimaros <me@khimaros.com>
Date:   Wed May 3 10:58:11 2023 -0700

    examples : read chat prompts from a template file (#1196)

examples/chat-13B.sh
prompts/chat-with-vicuna-v0.txt
prompts/chat-with-vicuna-v1.txt
prompts/chat.txt

commit bca9ad938a2a43621cf406d993b755cc91728dd5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 3 20:09:42 2023 +0300

    minor : fix whitespaces (#1302)

README.md
scripts/verify-checksum-models.py

commit e2a937ca6abadc7e01e139db31e6db9dce16e3e9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed May 3 18:43:23 2023 +0300

    minor : fix trailing whitespaces

scripts/verify-checksum-models.py

commit b0c71c7b6dc0c0adb507d78f401e95e7ab0f5a38
Author: KASR <karim.asrih@gmail.com>
Date:   Wed May 3 17:31:28 2023 +0200

    scripts : platform independent script to verify sha256 checksums (#1203)
    
    * python script to verify the checksum of the llama models
    
    Added Python script for verifying SHA256 checksums of files in a directory, which can run on multiple platforms. Improved the formatting of the output results for better readability.
    
    * Update README.md
    
    update to the readme for improved readability and to explain the usage of the python checksum verification script
    
    * update the verification script
    
    I've extended the script based on suggestions by @prusnak
    
    The script now checks the available RAM, is there is enough to check the file at once it will do so. If not the file is read in chunks.
    
    * minor improvment
    
    small change so that the available ram is checked and not the total ram
    
    * remove the part of the code that reads the file at once if enough ram is available
    
    based on suggestions from @prusnak i removed the part of the code that checks whether the user had enough ram to read the entire model at once. the file is now always read in chunks.
    
    * Update verify-checksum-models.py
    
    quick fix to pass the git check

README.md
scripts/verify-checksum-models.py

commit a8a2efdc8161d4f69a0dd863e741c11fbd5df85c
Author: CRD716 <crd716@gmail.com>
Date:   Wed May 3 10:26:47 2023 -0500

    examples : various prompt and example fixes (#1298)
    
    * fix dan.txt
    
    * miku prompt improvements
    
    * use common characters

examples/Miku.sh
examples/chat-13B.sh
prompts/dan.txt

commit e216aa04633892b972d013719e38b59fd4917341
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Tue May 2 22:26:13 2023 -0400

    llama : only copy used KV cache in get / set state (#1272)
    
    * llama : only copy used KV cache in get / set state
    
    * switch to ggml for copying k, v
    
    * avoid designated initializers

llama.cpp
llama.h

commit 2485d7a4d39406cd0f468e35551b472cceb5bd61
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Tue May 2 18:46:20 2023 -0700

    Process escape sequences given in prompts (#1173)

examples/common.cpp

commit 13b0c68ed7a9948db0720f7393df094ab1005b14
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Tue May 2 18:01:57 2023 -0700

    Handle signals properly on Windows (#1123)

examples/main/main.cpp

commit 55bc5f0900d925c539488901c5538b637d68665c
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Tue May 2 17:52:35 2023 -0700

    Call sh on build-info.sh (#1294)

Makefile

commit 9daff419f6be818595ddbea293a0ea7283af726e
Author: kuvaus <22169537+kuvaus@users.noreply.github.com>
Date:   Wed May 3 03:43:43 2023 +0300

    fix build-info.h for git submodules (#1289)
    
    
    * make git build info work with submodules
    
    ---------
    
    Co-authored-by: Green Sky <green@g-s.xyz>

CMakeLists.txt
scripts/build-info.cmake
scripts/build-info.h.in

commit bf4b22ffe433dc5e2fba7588c4485a7e51b1a30d
Author: slaren <slarengh@gmail.com>
Date:   Wed May 3 01:36:45 2023 +0200

    fix missing parameters in `llama_init_from_gpt_params` (#1293)

examples/common.cpp

commit 67c77799e025a8425c23a6a0599c007f46ded590
Author: Ron Evans <ron@hybridgroup.com>
Date:   Tue May 2 22:39:51 2023 +0200

    examples : add llama_init_from_gpt_params() common function (#1290)
    
    Signed-off-by: deadprogram <ron@hybridgroup.com>

examples/common.cpp
examples/common.h
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp

commit 0e6cbff1b7509628c588e661166f6e187137734d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 2 23:09:08 2023 +0300

    llama : fix compile warnings

examples/benchmark/benchmark-matmult.cpp
llama.cpp
llama.h
tests/test-sampling.cpp

commit 5d5817ca603d4cb451bed26594aa3dcd93f4ec56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue May 2 22:14:50 2023 +0300

    ggml : fix 32-bit ARM

ggml.c

commit 8c9be35ff998cbb4178b0fedcb9afd85cb6852e2
Author: Ron Evans <ron@hybridgroup.com>
Date:   Tue May 2 19:53:52 2023 +0200

    examples : improve vertical alignment of a few variables (#1286)
    
    Signed-off-by: deadprogram <ron@hybridgroup.com>

examples/main/main.cpp

commit cc0bb7235c72e50a621800e366d0e4fe315f0e11
Author: Marvin Gießing <marvin.giessing@gmail.com>
Date:   Tue May 2 18:42:16 2023 +0200

    ggml : fix ppc64le build error and make cmake detect Power processors (#1284)
    
    * Fix ppc64le build issue
    
    * Added support to detect ppc64* processors

CMakeLists.txt
ggml.c

commit 2bb992f034689e2ddd7b9ac05163b0359a5957b3
Author: Robert Brisita <986796+rbrisita@users.noreply.github.com>
Date:   Tue May 2 12:23:44 2023 -0400

    llama : allow 0 as a seed number. (#1275)

examples/common.cpp
examples/embedding/embedding.cpp
examples/main/README.md
examples/main/main.cpp
examples/perplexity/perplexity.cpp
llama.cpp
llama.h

commit e2cd5069999181a9e4a22cf420e0491878b3062f
Author: Ron Evans <ron@hybridgroup.com>
Date:   Tue May 2 18:13:26 2023 +0200

    main : switch input_noecho to input_echo to remove negation (#979)
    
    Signed-off-by: deadprogram <ron@hybridgroup.com>

examples/main/main.cpp

commit 2d099e5193d73f800b646c39e2fad08c1c1f1096
Author: slaren <slarengh@gmail.com>
Date:   Tue May 2 16:03:00 2023 +0200

    ggml: add names to tensors (#1268)
    
    * ggml: add names to tensors
    
    * minor improvements to dot file formatting

ggml.c
ggml.h
llama.cpp

commit f4cef87edfd1b2f8d5befd4fde54ca2e03987bea
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Mon May 1 09:23:47 2023 -0700

    Add git-based build information for better issue tracking (#1232)
    
    * Add git-based build information for better issue tracking
    
    * macOS fix
    
    * "build (hash)" and "CMAKE_SOURCE_DIR" changes
    
    * Redo "CMAKE_CURRENT_SOURCE_DIR" and clearer build messages
    
    * Fix conditional dependency on missing target
    
    * Broke out build-info.cmake, added find_package fallback, and added build into to all examples, added dependencies to Makefile
    
    * 4 space indenting for cmake, attempt to clean up my mess in Makefile
    
    * Short hash, less fancy Makefile, and don't modify build-info.h if it wouldn't change it

.gitignore
CMakeLists.txt
Makefile
examples/benchmark/CMakeLists.txt
examples/benchmark/benchmark-matmult.cpp
examples/embedding/CMakeLists.txt
examples/embedding/embedding.cpp
examples/main/CMakeLists.txt
examples/main/main.cpp
examples/perplexity/CMakeLists.txt
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
examples/quantize/CMakeLists.txt
examples/quantize/quantize.cpp
examples/save-load-state/CMakeLists.txt
examples/save-load-state/save-load-state.cpp
scripts/build-info.cmake
scripts/build-info.sh

commit 58b367c2d757c0ea12aec672382462b42204c724
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Mon May 1 18:11:07 2023 +0200

    cuBLAS: refactor and optimize f16 mat mul performance (#1259)
    
    * cuBLAS: refactor, convert fp16 to fp32 on device
    
    * cuBLAS: use multiple streams, choose smartly between mul_mat_q and mul_mat_f16
    
    * fix build
    
    * cuBLAS: update block_q5_1

ggml-cuda.cu
ggml-cuda.h
ggml.c
ggml.h

commit ea3a0ad6b6b5ca4693b94acd4cb32e2803f66fae
Author: xloem <0xloem@gmail.com>
Date:   Mon May 1 08:58:51 2023 -0400

    llama : update stubs for systems without mmap and mlock (#1266)
    
    Co-authored-by: John Doe <john.doe@example.com>

llama-util.h

commit 2bdc09646d8c6cb74a6f573e9081586b4b83b9d1
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Mon May 1 05:56:07 2023 -0600

    ggml : fix ggml_used_mem() (#1264)

ggml.c

commit 70269cae37538461ff816e714afbb3ebcdcdc26b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon May 1 14:54:59 2023 +0300

    llama : fix session load / save (#1263)

examples/main/main.cpp
llama.cpp
llama.h

commit b925f1f1b082319ee69943f8d1a83ac9b6ff09ca
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Mon May 1 13:32:22 2023 +0200

    cuBLAS: fall back to pageable memory if pinned alloc fails (#1233)
    
    * cuBLAS: fall back to pageable memory if pinned alloc fails
    
    * cuBLAS: do not use pinned memory if env variable GGML_CUDA_NO_PINNED is set

ggml-cuda.cu
llama-util.h
llama.cpp

commit 90b19bd6eee943832584f9cac0b6f9ea29cc42a4
Author: Alex Klinkhamer <git@grencez.dev>
Date:   Mon May 1 00:24:20 2023 -0700

    llama : let context be const when accessing const data (#1261)

llama.cpp
llama.h

commit 7ff0dcd32091c703a12adb0c57c32c565ce17664
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 30 22:28:51 2023 +0300

    ggml : fix UB (int << 31)

ggml.c

commit 6f796992869f306c48484d62a39f2a181ae2fd6f
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sun Apr 30 20:48:38 2023 +0200

    build: add armv{6,7,8} support to cmake (#1251)
    
    - flags copied from Makefile
    - updated comments in both CMakeLists.txt and Makefile to match reality

CMakeLists.txt
Makefile

commit a5d30b1f53677cb50791fec41c43e93274347303
Author: jon-chuang <9093549+jon-chuang@users.noreply.github.com>
Date:   Sun Apr 30 14:41:35 2023 -0400

    common : better default number of threads (#934)
    
    * commit
    
    * fix
    
    * try-catch
    
    * apply code review
    
    * improve
    
    * improve
    
    * add macos headers
    
    * done
    
    * remove color
    
    * fix windows
    
    * minor
    
    * fix
    
    * Apply suggestions from code review
    
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>
    
    * remove
    
    * minor
    
    * minor
    
    ---------
    
    Co-authored-by: jon-chuang <jon-chuang@users.noreply.github.com>
    Co-authored-by: DannyDaemonic <DannyDaemonic@gmail.com>

examples/common.cpp
examples/common.h

commit 76a884920aa1d2fc0dc7a7ac12dfc5ec5816377c
Author: 0cc4m <picard12@live.de>
Date:   Sun Apr 30 20:34:52 2023 +0200

    ggml : add CLBlast q5_0, q5_1, q8_0 dequant kernels (#1225)
    
    * Implement q5_0, q5_1 and q8_0
    
    * Work around q5_0 OpenCL issue
    
    * Fix q8_0 dequant kernel
    
    * Move cl kernels into ggml-opencl.c
    
    * Use two memcpy calls for q5_0 buffer transfer

ggml-opencl-dequant.cl
ggml-opencl.c

commit 6bc4400e67e6bc4faad3ad3d5e9d8a6576a9752d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 30 19:07:00 2023 +0300

    ggml : add Q5 WASM SIMD + GGML_FTYPE

ggml.c
ggml.h

commit f0d70f147d969e41fa410b8af2965a27aa901eb9
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Apr 30 12:32:37 2023 +0000

    Various fixes to mat_mul benchmark (#1253)

.gitignore
Makefile
examples/CMakeLists.txt
examples/benchmark/CMakeLists.txt
examples/benchmark/benchmark-matmult.cpp

commit 3e5aa8a1c44051153d6d7b3eeca2f4b4e5fb310c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 30 10:25:46 2023 +0300

    ggml : fix labels for GGML_OP_ALIBI

ggml.c

commit c3ca7a5f0546c561eb278be3f2fe335795679e01
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 21:34:23 2023 +0300

    ggml : fix 32-bit ARM NEON

ggml.c

commit e8c051611abfc9a7f37fd4bba48217180893bd68
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 21:12:56 2023 +0300

    ggml : use vzip instead of vuzp for consistency

ggml.c

commit 0b5a9350993e6fc8be45dc2a3eafc1fd0812d392
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 19:28:36 2023 +0300

    ggml : fix visibility and unused warnings

ggml.c
ggml.h

commit ec728e44d7488c2da3560970317708b2b12b9c04
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 18:43:42 2023 +0300

    ggml : fix #if for f32_f32 mul_mat (CLBlast) (#1229)

ggml.c

commit 214b6a35702a489e3738acd81fad6d46182d3036
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 18:43:28 2023 +0300

    ggml : adjust mul_mat_f16 work memory (#1226)
    
    * llama : minor - remove explicity int64_t cast
    
    * ggml : reduce memory buffer for F16 mul_mat when not using cuBLAS
    
    * ggml : add asserts to guard for incorrect wsize

Makefile
ggml.c
llama.cpp

commit 305eb5afd51325e3142c01c17431febb7c67de87
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 13:53:12 2023 +0300

    build : fix reference to old llama_util.h

CMakeLists.txt
Makefile
examples/save-load-state/save-load-state.cpp

commit 84ca9c2ecf3391d911589d0fe2b483cbfb4b82a6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 13:48:11 2023 +0300

    examples : fix save-load-state + rename llama-util.h

examples/save-load-state/save-load-state.cpp
llama-util.h
llama.cpp

commit 334637e43e3a0529b4b50e2c22968b1ed1633353
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 29 09:51:06 2023 +0300

    common : change default parameters to pre-#1126 (#1223)

examples/common.h
examples/main/main.cpp

commit dd7eff57d8491792010b1002b8de6a4b54912e5c
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Sat Apr 29 08:34:41 2023 +0300

    llama : new sampling algorithms (#1126)
    
    * Sample interface, new samplers.
    
    New samplers:
    - locally typical sampling
    - tail free sampling
    - frequency and presence penalty
    - mirostat
    
    Ignore EOS fix: -inf should be used.
    
    * mirostat
    
    * Added --logit-bias and --no-penalize-nl, removed std::span
    
    * Use C++11, clarify llama API documentation, rename Mirostat parameters to --mirostat_lr and --mirostat_ent, add temperature sampling for Mirostat, simplify Mirostat sampling API parameters (removed N and *k)
    
    Use C++11, clarify llama API documentation, rename Mirostat parameters to --mirostat_lr and --mirostat_ent, add temperature sampling for Mirostat, simplify Mirostat sampling API parameters (removed N and *k)
    
    * Save and load example adjust
    
    * Tests
    
    * Windows build fix
    
    * Windows test fix

examples/common.cpp
examples/common.h
examples/main/main.cpp
examples/save-load-state/save-load-state.cpp
llama.cpp
llama.h
tests/CMakeLists.txt
tests/test-sampling.cpp

commit 7fc50c051ae8a78e9643fdf172d12e20f2dd9b6c
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sat Apr 29 02:04:18 2023 +0200

    cuBLAS: use host pinned memory and dequantize while copying (#1207)
    
    * cuBLAS: dequantize simultaneously while copying memory
    
    * cuBLAS: use host pinned memory
    
    * cuBLAS: improve ggml_compute_forward_mul_mat_f16_f32 with pinned memory
    
    * cuBLAS: also pin kv cache
    
    * fix rebase

Makefile
ggml-cuda.cu
ggml-cuda.h
ggml.c
llama.cpp
llama_util.h

commit b1ee8f59b4101b46999a0995d9a34506f7285466
Author: Henri Vasserman <henv@hot.ee>
Date:   Sat Apr 29 02:31:56 2023 +0300

    cuBLAS: non-contiguous tensor support (#1215)
    
    * Cuda: non-contiguous tensor support
    
    * remove extra stuff
    
    * rename
    
    * fix error
    
    * more fixes, now OpenBLAS and CLBlast build too
    
    * now then?

ggml-cuda.cu
ggml-cuda.h
ggml.c

commit 36d19a603b221d1bd7897fcb10e823e2103b052d
Author: Stephan Walter <stephan@walter.name>
Date:   Fri Apr 28 23:10:43 2023 +0000

    Remove Q4_3 which is no better than Q5 (#1218)

README.md
SHA256SUMS
examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-cuda.h
ggml-opencl-dequant.cl
ggml-opencl.c
ggml.c
ggml.h
llama.cpp
llama.h

commit 7f15c5c477d9933689a9d1c40794483e350c2f19
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 28 21:32:52 2023 +0300

    readme : update hot topics

README.md

commit 55390bcaf2579a5435564d7267ae3ed367837fd6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 28 20:37:43 2023 +0300

    ggml : sync ggml (ggml_alibi)

ggml.c
ggml.h

commit 5fba3c016bfd1d73a070e7c93dac14162ce118d0
Author: CRD716 <crd716@gmail.com>
Date:   Fri Apr 28 11:13:33 2023 -0500

    examples : add Jeopardy example (#1168)
    
    * Basic Setup
    
    * Prevent Results.txt from coming up
    
    * Prefixes, Line separators, etc
    
    * editorcheck
    
    * introduction to give more consistent results
    
    * Basic graph thing
    
    * Grading, ready for testing!
    
    * Y'all ready to get funky?
    
    * fix column removal stuff
    
    * missed a few

.gitignore
examples/jeopardy/README.md
examples/jeopardy/graph.py
examples/jeopardy/jeopardy.sh
examples/jeopardy/qasheet.csv
examples/jeopardy/questions.txt

commit 1481a9cf25ea2e4abef6b13a57660a35f3e66af1
Author: Evan Jones <evan.q.jones@gmail.com>
Date:   Fri Apr 28 11:59:37 2023 -0400

    llama : add session file format and saved sessions in main (#1169)

examples/chat-13B.sh
examples/common.cpp
examples/common.h
examples/main/main.cpp
llama.cpp
llama.h

commit 11d902364b0e3b503a02a4e757ee2dc38aacb68f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 28 17:58:44 2023 +0300

    ggml : add helper debug printf in soft_max

ggml.c

commit 7296c961d9303010a2b98379f738da2a8a55aa1b
Author: 0cc4m <picard12@live.de>
Date:   Fri Apr 28 16:57:16 2023 +0200

    ggml : add CLBlast support (#1164)
    
    * Allow use of OpenCL GPU-based BLAS using ClBlast instead of OpenBLAS for context processing
    
    * Improve ClBlast implementation, avoid recreating buffers, remove redundant transfers
    
    * Finish merge of ClBlast support
    
    * Move CLBlast implementation to separate file
    
    Add buffer reuse code (adapted from slaren's cuda implementation)
    
    * Add q4_2 and q4_3 CLBlast support, improve code
    
    * Double CLBlast speed by disabling OpenBLAS thread workaround
    
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>
    Co-authored-by: slaren <2141330+slaren@users.noreply.github.com>
    
    * Fix device selection env variable names
    
    * Fix cast in opencl kernels
    
    * Add CLBlast to CMakeLists.txt
    
    * Replace buffer pool with static buffers a, b, qb, c
    
    Fix compile warnings
    
    * Fix typos, use GGML_TYPE defines, improve code
    
    * Improve btype dequant kernel selection code, add error if type is unsupported
    
    * Improve code quality
    
    * Move internal stuff out of header
    * Use internal enums instead of CLBlast enums
    * Remove leftover C++ includes and defines
    * Make event use easier to read
    
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    
    * Use c compiler for opencl files
    
    * Simplify code, fix include
    
    * First check error, then release event
    
    * Make globals static, fix indentation
    
    * Rename dequant kernels file to conform with other file names
    
    * Fix import cl file name
    
    ---------
    
    Co-authored-by: Concedo <39025047+LostRuins@users.noreply.github.com>
    Co-authored-by: slaren <2141330+slaren@users.noreply.github.com>
    Co-authored-by: Henri Vasserman <henv@hot.ee>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
Makefile
ggml-opencl-dequant.cl
ggml-opencl.c
ggml-opencl.h
ggml.c
ggml.h
llama.cpp

commit 78ec543733d10a1629f984fd0302fdaa4e87fe66
Author: Folko-Ven <71110216+Folko-Ven@users.noreply.github.com>
Date:   Fri Apr 28 19:22:48 2023 +0500

    Correcting link to w64devkit (#1214)
    
    Correcting link to w64devkit (change seeto to skeeto).

README.md

commit 92a6e13a31ba052abd9062af6cb8df2a293ce661
Author: Johannes Gäßler <johannesg@5d6.de>
Date:   Fri Apr 28 15:40:32 2023 +0200

    Add Manjaro CUDA include and lib dirs to Makefile (#1212)

Makefile

commit 04aaae1d79482cad2564412f3b32e70298ac7789
Author: Yann Follet <131855179+YannFollet@users.noreply.github.com>
Date:   Fri Apr 28 19:59:48 2023 +0800

    add avx2 for dot_q8_0_q8_0, 2x faster than scalar (#1211)

ggml.c

commit 0b2da20538d01926b77ea237dd1c930c4d20b686
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Apr 26 20:26:42 2023 +0000

    ggml : slightly faster AVX2 implementation for Q5 (#1197)

ggml.c

commit f9be42add0bf3ce61814b7ede0e6d0dda9ff22c6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 26 23:24:42 2023 +0300

    readme : add quantization info

README.md

commit 574406dc7e350ddbffaeca33bf0392b7bfeb1436
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 26 23:14:13 2023 +0300

    ggml : add Q5_0 and Q5_1 quantization (#1187)
    
    * ggml : add Q5_0 quantization (cuBLAS only)
    
    * ggml : fix Q5_0 qh -> uint32_t
    
    * ggml : fix q5_0 histogram stats
    
    * ggml : q5_0 scalar dot product
    
    * ggml : q5_0 ARM NEON dot
    
    * ggml : q5_0 more efficient ARM NEON using uint64_t masks
    
    * ggml : rename Q5_0 -> Q5_1
    
    * ggml : adding Q5_0 mode
    
    * quantize : add Q5_0 and Q5_1 to map
    
    * ggml : AVX2 optimizations for Q5_0, Q5_1 (#1195)
    
    ---------
    
    Co-authored-by: Stephan Walter <stephan@walter.name>

.gitignore
examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-cuda.h
ggml.c
ggml.h
llama.cpp
llama.h

commit 87a6f846d3e929632c45916dd08f1e2a9c72d2a3
Author: Ásgeir Bjarni Ingvarsson <asgeir@fundinn.org>
Date:   Wed Apr 26 20:08:43 2023 +0000

    Allow setting the rng seed after initialization. (#1184)
    
    The llama_set_state_data function restores the rng state to what it
    was at the time llama_copy_state_data was called. But users may want
    to restore the state and proceed with a different seed.

llama.cpp
llama.h

commit ea3ad7eb60cfb44526a58122e8019850f437cd1b
Author: DaniAndTheWeb <57776841+DaniAndTheWeb@users.noreply.github.com>
Date:   Wed Apr 26 22:03:03 2023 +0200

    Updating build instructions to include BLAS support (#1183)
    
    * Updated build information
    
    First update to the build instructions to include BLAS.
    
    * Update README.md
    
    * Update information about BLAS
    
    * Better BLAS explanation
    
    Adding a clearer BLAS explanation and adding a link to download the CUDA toolkit.
    
    * Better BLAS explanation
    
    * BLAS for Mac
    
    Specifying that BLAS is already supported on Macs using the Accelerate Framework.
    
    * Clarify the effect of BLAS
    
    * Windows Make instructions
    
    Added the instructions to build with Make on Windows
    
    * Fixing typo
    
    * Fix trailing whitespace

README.md

commit 859fee6dfb00fab7ce6bc215b4adae78d82f4759
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Wed Apr 26 18:43:27 2023 +0200

    quantize : use `map` to assign quantization type from `string` (#1191)
    
    instead of `int` (while `int` option still being supported)
    
    This allows the following usage:
    
    `./quantize ggml-model-f16.bin ggml-model-q4_0.bin q4_0`
    
    instead of:
    
    `./quantize ggml-model-f16.bin ggml-model-q4_0.bin 2`

.devops/tools.sh
README.md
examples/quantize/quantize.cpp

commit 4afcc378698e057fcde64e23eb664e5af8dd6956
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Apr 25 21:41:56 2023 +0000

    Update SHA256SUMS after quantization change (#1181)
    
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

SHA256SUMS

commit 667c501334ace706e3abc3f7a37cf1d6b4228745
Author: ostix360 <55257054+ostix360@users.noreply.github.com>
Date:   Tue Apr 25 23:33:08 2023 +0200

    py : cast lora_alpha to int in convert-lora-to-ggml (#1170)
    
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

convert-lora-to-ggml.py

commit bb98e77be704584fb40b0400394b4c16ae75f8e2
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Tue Apr 25 23:19:57 2023 +0200

    nix: use convert.py instead of legacy wrapper convert-pth-to-ggml.py (#981)

flake.nix

commit 7a32fcb3b29f4db8aed8a85dc58eb958fb118153
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 25 23:40:51 2023 +0300

    ggml : add Q8_0 quantization format (rename the old one to Q8_1) (ARM NEON) (#1179)
    
    * ggml : add Q8_0 quantization format (rename the old one to Q8_1)
    
    * tests : fix test-quantize-fns
    
    * ggml : finalize Q8_0 implementation
    
    * ggml : use q4_0_q8_0 and q4_2_q8_0
    
    * ggml : fix Q8_0 dot product bug (ARM)
    
    * ggml : Q8_0 unroll x2
    
    * ggml : fix bug - using wrong block type
    
    * ggml : extend quantize_fns_t with "vec_dot_type"
    
    * ggml : fix Q8_0 to use 255 values out of 256
    
    * ggml : fix assert using wrong QK4_2 instead of QK4_3

examples/quantize/quantize.cpp
ggml-cuda.cu
ggml-cuda.h
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-quantize-fns.cpp

commit dd0eabc049fb1efc631cab8eb0a646808d704e18
Author: unbounded <haakon@likedan.net>
Date:   Tue Apr 25 19:20:46 2023 +0200

    ggml : use full range for Q4_0 and Q4_2 quantization (#729)
    
    * Use full range for q4_0 quantization
    
    By keeping the sign of the highest magnitude, we can make sure the
    highest value maps to -8, which is currently unused.
    This is a bit of a freebie since it is fully backwards compatible with
    the current format.
    
    * Update quantize_row_q4_0 for AVX/AVX2
    
    * Update quantize_row_q4_0 for WASM
    
    Untested
    
    * Update quantize_row_q4_0 for Arm NEON
    
    * Update quantize_row_q4_0 for PowerPC
    
    Untested
    
    * Use full range for q4_2 quantization

ggml.c

commit 54bb60e26858be251a0eb3cb70f80322aff804a0
Author: xaedes <xaedes@gmail.com>
Date:   Mon Apr 24 23:02:02 2023 +0200

    ggml : fix bug in ggml_compute_forward_sum_f32 (#1162)
    
    The sum over all rows is now computed instead of just the last row

ggml.c

commit 8a0f8673ba1cdc6aa6df27a9fbc698431ca70e8d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 24 22:18:25 2023 +0300

    ggml : export symbols (#1155)

ggml.h

commit 0c5692345d5c046dbc6a7d311a00ae5842ac39c3
Author: xaedes <xaedes@gmail.com>
Date:   Mon Apr 24 18:23:31 2023 +0200

    examples : add save_load_state example (#1150)
    
    * add save_load_state example
    
    * use <cstdio> instead of <iostream> and fprintf / printf instead of cout
    
    * renamed save-load-state example files replacing underscores by dashes

examples/CMakeLists.txt
examples/save-load-state/CMakeLists.txt
examples/save-load-state/save-load-state.cpp

commit 957c8ae21d1e7052ea45a40ee8c0407b909e90cc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 24 18:47:03 2023 +0300

    llama : increase scratch buffer size for 65B (ref #1152)
    
    Temporary solution

llama.cpp

commit 9b0a4d421459f4e5e1af735c9784c3247b379025
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Mon Apr 24 17:45:32 2023 +0200

    examples/main README improvements and some light refactoring (#1131)

README.md
examples/common.cpp
examples/common.h
examples/main/README.md
examples/main/main.cpp

commit 2ec83428de7a876ecbbe484e1de42b73b5a40e25
Author: Stephan Walter <stephan@walter.name>
Date:   Mon Apr 24 15:38:26 2023 +0000

    Fix build for gcc 8 and test in CI (#1154)

.github/workflows/build.yml
ggml.c

commit e4cf982e0d4fcfbb4b977a52dbeacd115da10c3b
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Mon Apr 24 17:29:58 2023 +0200

    Fix cuda compilation (#1128)
    
    * Fix: Issue with CUBLAS compilation error due to missing -fPIC flag
    
    ---------
    
    Co-authored-by: B1gM8c <89020353+B1gM8c@users.noreply.github.com>

Makefile

commit c4fe84fb0d28851a5c10e5a633f82ae2ba3b7fae
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 24 07:40:02 2023 +0300

    llama : refactor get / set state + remove redundant kv cache API (#1143)

llama.cpp
llama.h

commit 1d78fecdab4087028a38517e86ed129f077174d8
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sun Apr 23 23:03:44 2023 +0200

    Fix LoRA acronym (#1145)

examples/main/README.md

commit 284685f1692258c2bcf08b86b723b80ba2e66c7a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 23 19:57:09 2023 +0300

    scripts : add helper scripts to synch ggml repo

scripts/sync-ggml.sh

commit edce63baa9dbd3963c3441bce07ee0acbb635697
Author: DannyDaemonic <DannyDaemonic@gmail.com>
Date:   Sun Apr 23 08:37:02 2023 -0700

    Added README.md for main with examples and explanations (#1139)

examples/main/README.md

commit ec9cdb6752dd96b3cc74d90ad1adeba5b4fa2b0e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 23 18:32:52 2023 +0300

    ggml : do not print perf ops that have not been used at all

ggml.c

commit e4422e299c10c7e84c8e987770ef40d31905a76b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 23 18:15:39 2023 +0300

    ggml : better PERF prints + support "LLAMA_PERF=1 make"

Makefile
ggml.c
llama.cpp

commit 53c8434398b3cba7ac6298cdd44abd40f0e640b1
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Apr 23 11:01:03 2023 +0000

    Improve AVX2 for vec_dot_q4_3_q8_0 (#1138)

ggml.c

commit c6524f46eb93fdb949330293a8469fd70080bd5a
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sun Apr 23 10:21:26 2023 +0200

    readme : update gpt4all instructions (#980)

README.md

commit c9e2c26f413377b352845f442cdab976ce85a05d
Author: Yishuo Wang <MeouSker77@outlook.com>
Date:   Sun Apr 23 15:57:05 2023 +0800

    A better `packNibbles` and `mul_sum_i8_pairs_float` implementation using AVX512 (#1119)

ggml.c

commit 0e018fe008eacebdbcfa2d61b6c988c245c961cd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 22 16:31:56 2023 +0300

    ggml : fix Q4_3 cuBLAS

CMakeLists.txt
ggml.c

commit 857308d1e8fb6afe33edb481d48560eee8fe7d7c
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 22 13:12:29 2023 +0000

    ci : trigger CI for drafts, but not most PR actions (#1125)

.github/workflows/build.yml

commit c50b628810f36a3e6e0324371f6db579eacefa0e
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 22 10:54:13 2023 +0000

    Fix CI: ARM NEON, quantization unit tests, editorconfig (#1122)

examples/main/main.cpp
ggml.c
llama.cpp
tests/test-quantize-fns.cpp
tests/test-quantize-perf.cpp

commit 5f939498d517b4dddbe904f202e895a3ecfb9dc4
Author: unbounded <haakon@likedan.net>
Date:   Sat Apr 22 11:10:39 2023 +0200

    ggml : unit test for quantization functions (#953)
    
    * Unit test for quantization functions
    
    Use the ggml_internal_get_quantize_fn function to loop through all
    quantization formats and run a sanity check on the result.
    
    Also add a microbenchmark that times these functions directly without
    running the rest of the GGML graph.
    
    * test-quantize-fns: CI fixes
    
    Fix issues uncovered in CI
     - need to use sizes divisible by 32*8 for loop unrolling
     - use intrinsic header that should work on Mac
    
    * test-quantize: remove
    
    Per PR comment, subsumed by test-quantize-fns
    
    * test-quantize: fix for q8_0 intermediates

tests/CMakeLists.txt
tests/test-quantize-fns.cpp
tests/test-quantize-perf.cpp
tests/test-quantize.c

commit 36b4f7e06406eed8a605cc9f2921d9244ef6a8e5
Author: wbpxre150 <100937007+wbpxre150@users.noreply.github.com>
Date:   Sat Apr 22 16:56:35 2023 +0800

    llama : print timings on ctrl+c exit (#1021)
    
    * print timings on ctrl+c exit
    
    * remove redundant free memory call.
    
    * add global pointer to ctx.

examples/main/main.cpp

commit 10f19c1121068ce3dab9bece03a8b9caaea2db36
Author: eiery <19350831+eiery@users.noreply.github.com>
Date:   Sat Apr 22 04:27:05 2023 -0400

    llama : have n_batch default to 512 (#1091)
    
    * set default n_batch to 512 when using BLAS
    
    * spacing
    
    * alternate implementation of setting different n_batch for BLAS
    
    * set n_batch to 512 for all cases

examples/common.h

commit 7e312f165c5047d6e16680d1eebc83055e95c313
Author: Howard Su <howard0su@gmail.com>
Date:   Sat Apr 22 16:18:20 2023 +0800

    cmake : fix build under Windows when enable BUILD_SHARED_LIBS (#1100)
    
    * Fix build under Windows when enable BUILD_SHARED_LIBS
    
    * Make AVX512 test on Windows to build the shared libs

.github/workflows/build.yml
CMakeLists.txt

commit 872c365a9176a011b13d31269bb3121fa89c37e1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 22 11:08:12 2023 +0300

    ggml : fix AVX build + update to new Q8_0 format

Makefile
ggml.c
llama.cpp

commit 955ef9a5d53d8f911fe00580ac9bd0caa56430af
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 22 10:55:35 2023 +0300

    ggml : alternative Q4_3 implementation using modified Q8_0 (#1109)
    
    * ggml : prefer vzip to vuzp
    
    This way we always use the same type of instruction across all quantizations
    
    * ggml : alternative Q4_3 implementation using modified Q8_0
    
    * ggml : fix Q4_3 scalar imlpementation
    
    * ggml : slight improvement of Q4_3 - no need for loop unrolling
    
    * ggml : fix AVX paths for Q8_0 quantization

ggml.c

commit c5aa5e577741d0359ad26ec50b9e21a74c65d911
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 22 07:37:05 2023 +0000

    ggml : AVX2 optimization for vec_dot_q4_3_q8_0 and refactoring (#1099)
    
    * AVX2 optimization for vec_dot_q4_3_q8_0 and refactoring
    
    * finish AVX vectorization of quantize_row_q8_0
    
    * Rename hsum_int_8 to hsum_i32_8

ggml.c

commit e9a9cb0c54461ffbda75b7b2f99f3ea5562291c2
Author: Clint Herron <hanclinto@gmail.com>
Date:   Sat Apr 22 02:54:33 2023 -0400

    examples : Improve Alpaca Default Repeat Penalty: Better Match Alpaca.cpp Experience (#1107)
    
    * Moving parameters to separate lines for readability.
    
    * Increasing repeate_penalty to 1.1 to make alpaca more usable by default.
    
    * Adding trailing newline.

examples/alpaca.sh

commit b6e7f9b09e9c340ec97a2fae61c1eb8db861f2f9
Author: xaedes <xaedes@gmail.com>
Date:   Sat Apr 22 08:21:32 2023 +0200

    llama : add api for getting/setting the complete state: rng, logits, embedding and kv_cache (#1105)
    
    * reserve correct size for logits
    
    * add functions to get and set the whole llama state:
    
    including rng, logits, embedding and kv_cache
    
    * remove unused variables
    
    * remove trailing whitespace
    
    * fix comment

llama.cpp
llama.h

commit 50cb666b8a2e35a49b08c0f6bc81138c8f6f2ac1
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Fri Apr 21 21:59:17 2023 +0200

    Improve cuBLAS performance by using a memory pool (#1094)
    
    * Improve cuBLAS performance by using a memory pool
    
    * Move cuda specific definitions to ggml-cuda.h/cu
    
    * Add CXX flags to nvcc
    
    * Change memory pool synchronization mechanism to a spin lock
    General code cleanup

Makefile
ggml-cuda.cu
ggml-cuda.h
ggml.c

commit 25d7abbd1f73582b7e0fdc422a936e8541c0780b
Author: apaz <aarpazdera@gmail.com>
Date:   Fri Apr 21 13:48:06 2023 -0500

    llama : fixed rlimit error message (#888)

llama_util.h

commit 018f2279f5fe3ef743bd8254b23ea8f0efae7e73
Author: 源文雨 <41315874+fumiama@users.noreply.github.com>
Date:   Sat Apr 22 02:27:06 2023 +0800

    cmake : link threads publicly to ggml (#1042)
    
    * fix: ld link test-tokenizer-0 error
    
    ```
    cmake3 --build . --config Release
    [  5%] Built target ggml
    [ 16%] Built target llama
    [ 22%] Linking CXX executable ../bin/test-tokenizer-0
    ../libllama.a(ggml.c.o)：在函数‘ggml_graph_compute’中：
    ggml.c:(.text+0xf2db)：对‘pthread_create’未定义的引用
    ggml.c:(.text+0xf9d4)：对‘pthread_join’未定义的引用
    collect2: error: ld returned 1 exit status
    gmake[2]: *** [bin/test-tokenizer-0] 错误 1
    gmake[1]: *** [tests/CMakeFiles/test-tokenizer-0.dir/all] 错误 2
    gmake: *** [all] 错误 2
    ```
    
    * Update CMakeLists.txt
    
    * Update CMakeLists.txt
    
    * Update CMakeLists.txt

CMakeLists.txt

commit 9411288271ab548216902a029f42a0a38ebcedb7
Author: Alex Klinkhamer <from.github.com.917@grencez.dev>
Date:   Fri Apr 21 11:18:09 2023 -0700

    main : evaluate tokens in batches after swapping context (#1014)
    
    * examples : evaluate tokens in batches after swapping context
    
    * Update examples/main/main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/main/main.cpp

commit 8687c1f2581d059cd5b6a9502f89bd343566062a
Author: xaedes <xaedes@googlemail.com>
Date:   Fri Apr 21 17:25:21 2023 +0200

    llama : remember and restore kv cache data pointers (#1104)
    
    because their value is stored in buf and overwritten by memcpy

llama.cpp

commit 1bfc153e2f35ddd9d64b084e8d1a5e6fa57ad1c9
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Fri Apr 21 17:18:26 2023 +0200

    ggml : a faster version for Q4_1 x Q8_0 dot products (#1083)
    
    * A faster version for Q4_1 x Q8_0 dot products
    
    The idea nehind being that Q8_0 quantized
    values get used many times in the matrix multiplications
    where they are involved. In the current implementations,
    when we are evaluating the dot products, we need to compute
    the sum of the quants in the Q8_0 vector, so the same
    operation is repeated many times. Here we pre-compute
    the sum during Q8_0 quantization, store it in the
    now modified block_q8_0 struct, and then reuse this
    result in the subsequent dot products.
    
    In a synthetic benchmark (just compute a bunch of dot
    products), this change speeds up the Q4_1 * Q8_0 dot
    product by 80%, making the performance identical to
    Q4_0 * Q8_0.
    
    In practical application, I see a ~15% gain in speed for
    token prediction on M2, and ~5% gain on Ryzen 7950X.
    The speed gain in the prompt evaluation is much bigger
    (around 50%).
    
    I have only done the change for the scalar version,
    ARM_NEON, and AVX2, so we still need an AVX implementation.
    
    * Cleaning up
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

ggml.c
pocs/vdot/CMakeLists.txt
pocs/vdot/q8dot.cpp

commit 3d59769c3bb7e72c915646ddb1e239b1face19f5
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Fri Apr 21 14:57:57 2023 +0200

    Show perplexity ETA in hours and minutes (#1096)

examples/perplexity/perplexity.cpp

commit d40fded93e1a533e969768e1e335c15c61c296ce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 21 10:23:36 2023 +0300

    llama : fix comment for "output.weight" tensor

llama.cpp

commit 2510c1831fac874f32e272f6079f01b5461f3986
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Apr 20 21:56:44 2023 +0000

    Add ggml-model-*.bin checksums for 7B, 13B, 30B, 65B (#1088)
    
    * Add ggml-model-*.bin checksums for 7B, 13B, 30B
    * Add ggml-model-*.bin checksums for 65B
    
    ---------
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

SHA256SUMS

commit 12b5900dbc9743dee3ce83513cf5c3a44523a1b6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 23:32:59 2023 +0300

    ggml : sync ggml (add GPT-NeoX RoPE implementation)

ggml.c
ggml.h
llama.cpp

commit 9ff334f3c9b960a44c5e149b08c748a2914fb882
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 21:58:05 2023 +0300

    ggml : fix bug in ggml_compute_forward_dup_f32()

ggml.c

commit 2005469ea130cf920c50175d4f47a87bfd8aaf4d
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Thu Apr 20 20:49:53 2023 +0200

    Add Q4_3 support to cuBLAS (#1086)

Makefile
ggml-cuda.cu
ggml-cuda.h

commit 8a1756abdf1f48cb4dcb898bc8fbe9102ef49dc6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 21:43:50 2023 +0300

    ggml : do not break cuBLAS build (Q4_3 is not yet implemented)

ggml.c

commit 66aab46079609972ee1f7bd6f319d826205a2fbd
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 20:44:05 2023 +0300

    ggml : fix Q4_3 quantization
    
    Broke it during conflict resolution in last PR

ggml.c

commit 38de86a7114c97ecf3644e3a60159f1ed893e1b0
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Thu Apr 20 19:42:27 2023 +0200

    llama : multi-threaded quantization (#1075)
    
    * Multi-threading quantization.
    
    Not much gain for simple quantizations, bit it will be important
    for quantizations that require more CPU cycles.
    
    * Multi-threading for quantize-stats
    
    It now does the job in ~14 seconds on my Mac for
    Q4_0, Q4_1 and Q4_2. Single-threaded it was taking
    more than 2 minutes after adding the more elaborate
    version of Q4_2.
    
    * Reviewer comments
    
    * Avoiding compiler confusion
    
    After changing chunk_size to const int as suggested by
    @ggerganov, clang and GCC starting to warn me that I don't
    need to capture it in the lambda. So, I removed it from the
    capture list. But that makes the MSVC build fail. So,
    making it a constexpr to make every compiler happy.
    
    * Still fighting with lambda captures in MSVC
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/quantize-stats/quantize-stats.cpp
examples/quantize/quantize.cpp
ggml.c
ggml.h
llama.cpp
llama.h

commit e0305ead3a072db9c08b35c9600c49273b38a4b5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 20 20:35:53 2023 +0300

    ggml : add Q4_3 quantization (#1082)

examples/quantize/quantize.cpp
ggml.c
ggml.h
llama.cpp
llama.h

commit 6a9661ea5ad72166b700ae5e87976e4452499dda
Author: Ivan Komarov <Ivan.Komarov@dfyz.info>
Date:   Thu Apr 20 17:15:18 2023 +0200

    ci : remove the LLAMA_ACCELERATE matrix dimension from Ubuntu builds in the CI (#1074)
    
    [Accelerate](https://developer.apple.com/documentation/accelerate) is an Apple framework which can only be used on macOS, and the CMake build [ignores](https://github.com/ggerganov/llama.cpp/blob/master/CMakeLists.txt#L102) the `LLAMA_ACCELERATE` variable when run on non-Apple platforms. This implies setting `LLAMA_ACCELERATE` is a no-op on Ubuntu and can be removed.
    
    This will reduce visual noise in CI check results (in addition to reducing the number of checks we have to run for every PR). Right now every sanitized build is duplicated twice for no good reason (e.g., we have `CI / ubuntu-latest-cmake-sanitizer (ADDRESS, Debug, ON)` and `CI / ubuntu-latest-cmake-sanitizer (ADDRESS, Debug, OFF)`).

.github/workflows/build.yml

commit 5addcb120cf2682c7ede0b1c520592700d74c87c
Author: 源文雨 <41315874+fumiama@users.noreply.github.com>
Date:   Thu Apr 20 21:28:43 2023 +0800

    fix: LLAMA_CUBLAS=1 undefined reference 'shm_open' (#1080)

Makefile

commit c8c2c524827be8fd681a63f0e5a697b0bf4c587b
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Apr 20 06:45:41 2023 +0000

    AVX2 optimization for vec_dot_q4_2_q8_0 (#1068)

ggml.c

commit 02d6988121510c067e06d498a273a351a888f5b9
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Thu Apr 20 03:14:14 2023 +0200

    Improve cuBLAS performance by dequantizing on the GPU (#1065)

CMakeLists.txt
Makefile
ggml-cuda.cu
ggml-cuda.h
ggml.c

commit 834695fe3a3ed2a962e774c9615e3f7b41d360a8
Author: CRD716 <crd716@gmail.com>
Date:   Wed Apr 19 14:52:14 2023 -0500

    Minor: Readme fixed grammar, spelling, and misc updates (#1071)

README.md

commit f7d05095b404b5500b4a702ea16f67fc22446e49
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Wed Apr 19 20:20:14 2023 +0200

    Q4_2 quantization with rmse-optimized scale and quants (#1062)
    
    * Q4_2 quantization with rmse-optimized scale and quants
    
    For quantize-stats we get
    q4_2: rmse 0.00159301, maxerr 0.17480469, 95pct<0.0030, median<0.0012
    
    For 7B perplexity with BLAS enabled we get 6.2038 after 655 chunks.
    
    Quantization is slow (~90 seconds on my Mac for 7B) as not
    multi-threaded as in PR #896.
    
    * ggml : satisfy the sanitizer builds
    
    Not sure why this makes them fail
    
    * Better follow ggml conventions for function names
    
    * Fixed type as per reviewer comment
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c

commit 884e7d7a2bfd7325b107442d6758983f5886ed3d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 19 20:10:08 2023 +0300

    ggml : use 8-bit precision for Q4_1 intermediate results (#1047)
    
    * ggml : use 8-bit precision for Q4_1 intermediate results (ARM)
    
    * ggml : optimize ggml_vec_dot_q4_1_q8_0() via vmalq_n_f32
    
    56 ms/token with Q4_1 !
    
    * ggml : AVX2 implementation of ggml_vec_dot_q4_1_q8_0 (#1051)
    
    * gitignore : ignore ppl-*.txt files
    
    ---------
    
    Co-authored-by: slaren <2141330+slaren@users.noreply.github.com>

.gitignore
ggml.c

commit 7cd5c4a3e9106151d48f328bb3c94c298a211f18
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 19 19:07:54 2023 +0300

    readme : add warning about Q4_2 and Q4_3

README.md

commit f3d4edf504c19ee9d1381c5727fe38667205d979
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Apr 19 16:06:37 2023 +0000

    ggml : Q4 cleanup - remove 4-bit dot product code (#1061)
    
    * Q4 cleanup
    
    * Remove unused AVX512 Q4_0 code

CMakeLists.txt
Makefile
ggml.c

commit 8944a1329648c57bb7d66851170938230587a52c
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Apr 19 11:22:45 2023 +0200

    Add NVIDIA cuBLAS support (#1044)

CMakeLists.txt
Makefile
ggml.c
ggml.h
llama.cpp

commit 66674012389f9537140044290f8517bc4a5e0b74
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Apr 19 00:53:24 2023 +0200

    Multi-threaded ggml_cpy (#1035)
    
    * Multi-threaded ggml_cpy
    
    * Update ggml.c
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * Also fix wdata offset in ggml_compute_forward_add_q_f32
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c

commit 77a73403ca8eaced2590559d0f9cebd2b3649d32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 23:54:57 2023 +0300

    ggml : add new Q4_2 quantization (ARM only) (#1046)
    
    * ggml : Q4_2 ARM
    
    * ggml : add ggml_is_quantized()
    
    * llama : update llama_type_name() with Q4_2 entry
    
    * ggml : speed-up q4_2
    
    - 4 threads: ~100ms -> ~90ms
    - 8 threads:  ~55ms -> ~50ms
    
    * ggml : optimize q4_2 using vmlaq_n_f32 + vmulq_n_f32

examples/quantize/quantize.cpp
ggml.c
ggml.h
llama.cpp
llama.h

commit 50a8a2af97cb92e53e7a3195aa201c3d87da5415
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 23:11:23 2023 +0300

    ggml : scratch that - vmlaq_n_f32 is always better
    
    Had a background process that was messing with the timings

ggml.c

commit 4caebf6d408b91c2d29d0abc7b1e867b5de64db5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 23:00:08 2023 +0300

    gitignore : vdot

.gitignore

commit dcdd65e2969bc03c91a1ebd1160162d5054e6923
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 22:59:17 2023 +0300

    ggml : optimize ggml_vec_dot_q4_0_q8_0() using vectorized accumulators

ggml.c

commit 5ecff35151156118c2df74899637ad34ee384b9b
Author: Kawrakow <48489457+ikawrakow@users.noreply.github.com>
Date:   Tue Apr 18 21:00:14 2023 +0200

    Adding a simple program to measure speed of dot products (#1041)
    
    On my Mac, the direct Q4_1 product is marginally slower
    (~69 vs ~55 us for Q4_0). The SIMD-ified ggml version
    is now almost 2X slower (~121 us).
    
    On a Ryzen 7950X CPU, the direct product for Q4_1 quantization
    is faster than the AVX2 implementation (~60 vs ~62 us).
    
    ---------
    
    Co-authored-by: Iwan Kawrakow <iwan.kawrakow@gmail.com>

CMakeLists.txt
Makefile
pocs/CMakeLists.txt
pocs/vdot/CMakeLists.txt
pocs/vdot/vdot.cpp

commit 7faa7460f03bdd88becf1e659cf359f274055404
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Apr 18 20:10:26 2023 +0300

    readme : update hot topics about new LoRA functionality

README.md

commit 5af8e32238c7d9c4cdb7fc640472c9a26538b9da
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 17 18:00:10 2023 +0300

    ci : do not run on drafts

.github/workflows/build.yml
.github/workflows/docker.yml

commit 42747220b4cac548b6e3059b66b3e960b517cfa4
Author: Ivan Komarov <Ivan.Komarov@dfyz.info>
Date:   Tue Apr 18 03:15:50 2023 +0200

    Do not close file after mmap (Windows version) (#1034)

llama_util.h

commit e9298af3896b536d0c6d740447dc764e56b22102
Author: Atsushi Tatsuma <yoshoku@outlook.com>
Date:   Tue Apr 18 04:34:35 2023 +0900

    readme : add Ruby bindings (#1029)

README.md

commit 4ad73137a1aadc40a62f7101aab883f69e7172c6
Author: Cameron <csteele@steelecameron.com>
Date:   Mon Apr 17 11:26:23 2023 -0700

    add 4_0 to default outfile namestr dict (#1031)
    
    this came up when trying to convert the gpt4all-lora-unfiltered-quantized.bin file

convert.py

commit 315a95a4d30db726fb7d244dd3b9e90a83fb1616
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Mon Apr 17 17:28:55 2023 +0200

    Add LoRA support (#820)

convert-lora-to-ggml.py
examples/common.cpp
examples/common.h
examples/main/main.cpp
examples/perplexity/perplexity.cpp
ggml.c
ggml.h
llama.cpp
llama.h
llama_util.h

commit efd05648c88a0923a55f56e7ce1b0f9c33410afb
Author: Arik Poznanski <arikpoz@users.noreply.github.com>
Date:   Mon Apr 17 17:41:53 2023 +0300

    llama : well-defined static initialization of complex objects (#927)
    
    * Replaced static initialization of complex objects with a initialization on first use. This prevents an undefined behavior on program run, for example, crash in Release build, works in Debug build
    
    * replaced use of auto with exact type to avoid using -std=c++14
    
    * Made the assessors functions for static maps be static const

llama.cpp
tests/test-tokenizer-0.cpp

commit eb17a026fd23d1c1b612fa4600f7f5c58e501a28
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 17 17:31:06 2023 +0300

    quantize-stats : fix bug in --type argument

examples/quantize-stats/quantize-stats.cpp

commit 69b740289f9b3756ea9dd2a23f241c6f688d88b9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 17 16:16:23 2023 +0300

    ggml : avoid using ggml_fp16_to_fp32() and ggml_fp32_to_fp16() in ggml.c

ggml.c

commit f266259ad9a2bce5a34d919592310147af23f3dc
Author: Ivan Komarov <Ivan.Komarov@dfyz.info>
Date:   Mon Apr 17 15:10:57 2023 +0200

    Speedup the AVX-512 implementation of ggml_vec_dot_q4_0() (#933)

CMakeLists.txt
ggml.c
ggml.h
llama.cpp

commit 47f61aaa5f76d04286792e2fbd0c95b659ab2af0
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sun Apr 16 21:27:38 2023 +0200

    Fix: do not close file on mmap (#1017)

llama_util.h

commit 3173a62eb9f90b94fb3184131032c1c8b7aa8d86
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Apr 16 13:58:48 2023 +0300

    stdout : vertical align outputs for better readibility

convert.py
llama.cpp

commit 489537e6cf6c93b74a029a11533dbcaa89791dcc
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sun Apr 16 12:13:00 2023 +0200

    examples: add missing <ctime> include for time() (#1011)

examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp

commit 2d3481c72125cd388258864c7ad8d7d36777bad7
Author: nanahi <130121847+na-na-hi@users.noreply.github.com>
Date:   Sun Apr 16 17:13:42 2023 +0800

    Fix msys2 build error and warnings (#1009)

llama.cpp
llama_util.h

commit 74f5899df4a6083fc467b620baa1cf821e37799d
Author: comex <comexk@gmail.com>
Date:   Sat Apr 15 14:53:21 2023 -0700

    convert.py: Fix loading safetensors and ggml format on Windows (#991)
    
    Calling `mmap.mmap` on Windows apparently resets the file offset of the
    raw file object (and makes the BufferedReader return a *negative* file
    offset).  For safetensors, avoid using the file offset after calling
    mmap.  For GGML format, explicitly save and restore the offset.
    
    Fixes #966.

convert.py

commit 2f7c8e014e3c0ceaf39688845c2ff6f919fb03b7
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 15 18:28:56 2023 +0000

    Fix potential int8 overflow in non-SIMD vec_dot (#986)

ggml.c

commit 0ad964631f9b3970f1936008fcfb1eadef59c7ed
Author: Stephan Walter <stephan@walter.name>
Date:   Sat Apr 15 16:25:38 2023 +0000

    Refactor ggml.c for future tensor types (#1001)

ggml.c

commit e95b6554b493e71a0275764342e09bd5784a7026
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 15 17:53:22 2023 +0300

    ggml : add Q8_0 quantization for intermediate results (#951)
    
    * ggml : add Q8_0 quantization for intermediate results
    
    * quantize-stats : fix test + add it to Makefile default
    
    * Q8: use int8_t, AVX/AVX2 optimizations
    
    * ggml : fix quantize_row_q8_0() ARM_NEON rounding
    
    * minor : updates after rebase to latest master
    
    * quantize-stats : delete obsolete strings
    
    * ggml : fix q4_1 dot func
    
    ---------
    
    Co-authored-by: Stephan Walter <stephan@walter.name>

Makefile
ggml.c
ggml.h

commit aa485cee334e84437e21681c14b6f80b65876d8b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Apr 15 14:25:45 2023 +0300

    ggml : use posix_memalign on non-Windows env

ggml.c

commit c12b14b77fced0ce9a0e2d81f670c3a746dec251
Author: Ivan Komarov <Ivan.Komarov@dfyz.info>
Date:   Sat Apr 15 07:51:54 2023 +0200

    benchmark : fix result validation in benchmark-q4_0-matmult (#987)

examples/benchmark/benchmark-q4_0-matmult.c

commit 106faaf2971d6c89d6010279a9a95737772470ef
Author: katsu560 <118887472+katsu560@users.noreply.github.com>
Date:   Sat Apr 15 14:51:11 2023 +0900

    cmake : add finding the OpenBLAS header file (#992)

CMakeLists.txt

commit c85e03d12e4b8af22cb13aa9c618dcd5935862fd
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 21:58:43 2023 +0200

    Revert "main : alternative instruct mode (Vicuna support, etc.) (#863)" (#982)
    
    This reverts commit f4d277ae17247ee51129ef1a9ff74d377cc90b1b.

configs/alpaca-native-enhanced.txt
configs/alpaca.txt
configs/chat-with-bob.txt
configs/llama.txt
configs/vicuna-simple.txt
configs/vicuna-stop.txt
configs/vicuna.txt
examples/common.cpp
examples/common.h
examples/main/main.cpp
prompts/alpaca.txt
prompts/chat-with-bob.txt

commit 489093548c89c67520109ab25c4df4a4614a32a0
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 21:46:49 2023 +0200

    py : bump sentencepiece to 0.1.98 to support Python 3.11 (#976)

requirements.txt

commit 93265e988af32b8be314bfed334f795a3037555d
Author: Stephan Walter <stephan@walter.name>
Date:   Fri Apr 14 19:39:48 2023 +0000

    make : fix dependencies, use auto variables (#983)

Makefile

commit c56b7152690ca25cfd66b20210b3629e6c1e739b
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 20:05:37 2023 +0200

    Expose type name from ggml (#970)
    
    Avoid duplication of type names in utils
    
    Co-authored-by: Håkon H. Hitland <haakon@likedan.net>

examples/quantize-stats/quantize-stats.cpp
ggml.c
ggml.h
llama.cpp

commit f4d277ae17247ee51129ef1a9ff74d377cc90b1b
Author: Tomáš Pazdiora <tomas.pazdiora@gmail.com>
Date:   Fri Apr 14 17:19:17 2023 +0200

    main : alternative instruct mode (Vicuna support, etc.) (#863)
    
    * Add support for configs, add configurable prefixes / suffixes, deprecate instruct mode, add stop prompt
    
    * Add multiline mode, update text input.
    
    * bugfix
    
    * update implementation
    
    * typos
    
    * Change --multiline implementation to be toggled by EOF.
    
    * bugfix
    
    * default multiline mode
    
    * add more configs
    
    * update formating
    
    * update formatting
    
    * apply suggestions

configs/alpaca-native-enhanced.txt
configs/alpaca.txt
configs/chat-with-bob.txt
configs/llama.txt
configs/vicuna-simple.txt
configs/vicuna-stop.txt
configs/vicuna.txt
examples/common.cpp
examples/common.h
examples/main/main.cpp
prompts/alpaca.txt
prompts/chat-with-bob.txt

commit c9a59b70a54e0bc05777df287feaea3dbe0310c4
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Fri Apr 14 08:43:55 2023 -0600

    ggml : add unary and binary map operations (#874)
    
    * GGML map ops proof of concept.
    
    * Various cleanups.
    
    Add handling for task setting.
    
    Add handling for ggml_compute_backward.
    
    Rename functions to ggml_map_unary_f32 and ggml_map_binary_f32
    
    Fix compiler warnings related to casting function pointers and `void *`
    
    Reorder functions and definitions based on the GGML op number.
    
    Use typedefs for map op function pointer types.
    
    * Fix position of map ops cases in ggml_compute_forward

ggml.c
ggml.h

commit a32f7acc9f54dba1c728cb1e596bd00bf3b4eb5f
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 15:37:11 2023 +0200

    py : cleanup dependencies (#962)
    
    after #545 we do not need torch, tqdm and requests in the dependencies

.devops/full.Dockerfile
flake.nix

commit 43ffdefb7424f79a3d510c199e2ea86684b4f824
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Apr 14 14:23:21 2023 +0200

    py : fix flake8 and isort nitpicks (#960)

convert.py

commit 1623a6e9b46453bff30afd7d0f6c3fd188499c2f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 14 13:31:29 2023 +0300

    ggml : minor

ggml.c

commit c14e0d2f23e6d1e785255f4da8c253c1b4723659
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 14 13:31:15 2023 +0300

    ggml : always allocate buffers with size multiple of GGML_MEM_ALIGN

ggml.c

commit 723dac55fa2ba7adc6e3fc8609781d1ad0378906
Author: comex <comexk@gmail.com>
Date:   Fri Apr 14 00:03:03 2023 -0700

    py : new conversion script (#545)
    
    Current status: Working, except for the latest GPTQ-for-LLaMa format
      that includes `g_idx`.  This turns out to require changes to GGML, so
      for now it only works if you use the `--outtype` option to dequantize it
      back to f16 (which is pointless except for debugging).
    
      I also included some cleanup for the C++ code.
    
      This script is meant to replace all the existing conversion scripts
      (including the ones that convert from older GGML formats), while also
      adding support for some new formats.  Specifically, I've tested with:
    
      - [x] `LLaMA` (original)
      - [x] `llama-65b-4bit`
      - [x] `alpaca-native`
      - [x] `alpaca-native-4bit`
      - [x] LLaMA converted to 'transformers' format using
            `convert_llama_weights_to_hf.py`
      - [x] `alpaca-native` quantized with `--true-sequential --act-order
            --groupsize 128` (dequantized only)
      - [x] same as above plus `--save_safetensors`
      - [x] GPT4All
      - [x] stock unversioned ggml
      - [x] ggmh
    
      There's enough overlap in the logic needed to handle these different
      cases that it seemed best to move to a single script.
    
      I haven't tried this with Alpaca-LoRA because I don't know where to find
      it.
    
      Useful features:
    
      - Uses multiple threads for a speedup in some cases (though the Python
        GIL limits the gain, and sometimes it's disk-bound anyway).
    
      - Combines split models into a single file (both the intra-tensor split
        of the original and the inter-tensor split of 'transformers' format
        files).  Single files are more convenient to work with and more
        friendly to future changes to use memory mapping on the C++ side.  To
        accomplish this without increasing memory requirements, it has some
        custom loading code which avoids loading whole input files into memory
        at once.
    
      - Because of the custom loading code, it no longer depends in PyTorch,
        which might make installing dependencies slightly easier or faster...
        although it still depends on NumPy and sentencepiece, so I don't know
        if there's any meaningful difference.  In any case, I also added a
        requirements.txt file to lock the dependency versions in case of any
        future breaking changes.
    
      - Type annotations checked with mypy.
    
      - Some attempts to be extra user-friendly:
    
          - The script tries to be forgiving with arguments, e.g. you can
            specify either the model file itself or the directory containing
            it.
    
          - The script doesn't depend on config.json / params.json, just in
            case the user downloaded files individually and doesn't have those
            handy.  But you still need tokenizer.model and, for Alpaca,
            added_tokens.json.
    
          - The script tries to give a helpful error message if
            added_tokens.json is missing.

README.md
convert-ggml-to-pth.py
convert-gpt4all-to-ggml.py
convert-gptq-to-ggml.py
convert-pth-to-ggml.py
convert-unversioned-ggml-to-ggml.py
convert.py
migrate-ggml-2023-03-30-pr613.py
requirements.txt

commit 0f07cacb05f49704d35a39aa27cfd4b419eb6f8d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Apr 14 09:45:42 2023 +0300

    ggml : fix q4_1 dot product types

ggml.c

commit c5d70f5c9ea5a8f0f6b0d6aa741455978a1dabfd
Author: Howard Su <howard0su@gmail.com>
Date:   Fri Apr 14 14:24:52 2023 +0800

    ggml : optimize rope function to avoid call powf in the tight loop (#807)

ggml.c

commit be87b6ed20a5f7528bf491a83e759a9fc6a24fea
Author: Gary Linscott <glinscott@gmail.com>
Date:   Thu Apr 13 14:50:42 2023 -0700

    perplexity : add support for batch size to `--perplexity` (#407)
    
    * Add support to batch size for perplexity
    
    * Revert "Fix memory allocation issues and seg faults"
    
    This reverts commit 4870e455b3653f7d7769fa5772b2c90ffad088df.
    
    * update from merge
    
    * Remove perplexity from main
    
    * updates
    
    * Update batch size for efficiency

examples/perplexity/perplexity.cpp

commit 0e07e6a8399fd993739a3ba3c6f95f92bfab6f58
Author: CRD716 <crd716@gmail.com>
Date:   Thu Apr 13 10:39:25 2023 -0500

    common : remove unnecessary includes (#947)

examples/common.cpp

commit a3a2a0eda8828b60436e9f69d9ac2c1060d03e7a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 13 18:36:40 2023 +0300

    ggml : add GGML_DEFAULT_N_THREADS

ggml.c
ggml.h

commit d990e3fffc5b0f5448e90a16c79a4f2675100af0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 13 18:32:36 2023 +0300

    ggml : speed-up ggml_vec_dot_q4_1() ARM_NEON + 32-bit ARM support (#900)
    
    * ggml : speed-up q4_1 ARM_NEON by ~5%
    
    * ggml : implement vaddvq when missing
    
    * ggml : implement vminvq and vmaxvq when missing
    
    * ggml : implement vzip when missing
    
    * ggml : fix comment
    
    * ggml : try to use correct ifdef

ggml.c

commit 9190e8eac8bdc108c40d2d7505e9b45fa773251f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 13 18:04:45 2023 +0300

    llama : merge llama_internal.h into llama.h
    
    Hide it behind an #ifdef

CMakeLists.txt
Makefile
examples/quantize-stats/quantize-stats.cpp
llama.cpp
llama.h
llama_internal.h

commit c85980acd04631a7c43d13676276f76ec72f5dfe
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Apr 13 18:01:22 2023 +0300

    gitignore : benchmark

.gitignore

commit 6232f2d7fd7a22d5eeb62182b2f21fcf01359754
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Apr 13 14:59:50 2023 +0000

    ggml : optimize non-SIMD Q4_0 vector dot product (#703)

ggml.c

commit 6c248707f51c8a50f7792e7f7787ec481881db88
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Thu Apr 13 16:08:32 2023 +0200

    ggml : introduce GGML_ALIGNED_MALLOC/GGML_ALIGNED_FREE macros (#884)
    
    which allows us to use aligned_alloc or _aligned_malloc functions

ggml.c

commit 8cda5c981d0bf4dcb7664194b2cb9a06e2dbdd54
Author: CRD716 <crd716@gmail.com>
Date:   Thu Apr 13 09:03:57 2023 -0500

    fix whitespace (#944)

Makefile
examples/benchmark/benchmark-q4_0-matmult.c

commit ec29272175d7a79681d9919f3e755b1bcefa0478
Author: CRD716 <crd716@gmail.com>
Date:   Thu Apr 13 08:59:53 2023 -0500

    readme : remove python 3.10 warning (#929)

README.md

commit 7e941b95eba067cb5b92785e642fd803657376ee
Author: Genkagaku.GPT <hlhr202@163.com>
Date:   Thu Apr 13 21:54:27 2023 +0800

    readme : llama node binding (#911)
    
    * chore: add nodejs binding
    
    * chore: add nodejs binding

README.md

commit c729ff730a46a135817a3d9988a097e3678a9722
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Thu Apr 13 15:49:05 2023 +0200

    flake.nix: add all binaries from bin (#848)

flake.nix

commit 4579af95e8e16910f6dbab0994917a5b3901f0cf
Author: Judd <foldl@users.noreply.github.com>
Date:   Thu Apr 13 21:43:22 2023 +0800

    zig : update build.zig (#872)
    
    * update
    
    * update readme
    
    * minimize the changes.
    
    ---------
    
    Co-authored-by: zjli2019 <zhengji.li@ingchips.com>

README.md
build.zig

commit 8c3ffc2f048a372639906fb30ec3c2070288d3be
Author: Vladimir <bogdad@gmail.com>
Date:   Thu Apr 13 15:24:30 2023 +0200

    ggml : update cblas_sgemm columns var to be more reasonable (#838)

ggml.c

commit 107980d970808c2ccf9334ad033e2782a560b911
Author: niansa/tuxifan <anton-sa@web.de>
Date:   Thu Apr 13 15:03:39 2023 +0200

    examples : add -n to alpaca and gpt4all scripts (#706)

examples/alpaca.sh
examples/gpt4all.sh

commit 585d91a156794d30eec16ebe67c8d7a1d41406c1
Author: anzz1 <anzz1@live.com>
Date:   Thu Apr 13 15:48:21 2023 +0300

    cmake : add explicit F16C option (x86) (#576)
    
    Fixes building for x86 processors missing F16C featureset
    MSVC not included, as in MSVC F16C is implied with AVX2/AVX512

CMakeLists.txt

commit 95ea26f6e92d620a5437f576b80868aee7f808d6
Author: SebastianApel <13675545+SebastianApel@users.noreply.github.com>
Date:   Thu Apr 13 14:46:23 2023 +0200

    benchmark : add tool for timing q4_0 matrix multiplication (#653)
    
    * Initial version of q4_0 matrix multiplication benchmark
    
    * Bugfix: Added dependency to ggml.o to benchmark
    
    * Reviewer requests: added parameter for threads, switched to ggml_time_us()
    
    * Reviewer input: removed rtsc, use epsilon for check
    
    * Review comment: Removed set_locale
    
    * Feature: Param for numer of iterations, Bugfix for use of parameter threads
    
    * Reviewer suggestion: Moved to examples
    
    * Reviewer feedback: Updated clean: and benchmark: sections
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

Makefile
examples/benchmark/benchmark-q4_0-matmult.c

commit 82d146df9b43cf677e0dbce20b03cf864958a0cc
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Thu Apr 13 11:33:16 2023 +0200

    do not force the prompt file to end with a new line (#908)

.editorconfig
prompts/chat-with-bob.txt
prompts/reason-act.txt

commit e7f6997f897a18b6372a6460e25c5f89e1469f1d
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Apr 12 15:06:16 2023 +0000

    Don't crash on ftype (formerly f16) == 4 (#917)

llama.cpp
llama.h

commit f76cb3a34d6a6b03afb96650e39495f201eac042
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 12 14:48:57 2023 +0300

    readme : change "GPU support" link to discussion

README.md

commit 782438070f7568380755ffc7bf5e09b20c1e8272
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 12 14:31:12 2023 +0300

    readme : update hot topics with link to "GPU support" issue

README.md

commit 4dbbd407500cf500ca5f47e4e947635797997c05
Author: Nicolai Weitkemper <kontakt@nicolaiweitkemper.de>
Date:   Wed Apr 12 08:46:20 2023 +0200

    readme: link to sha256sums file (#902)
    
    This is to emphasize that these do not need to be obtained from elsewhere.

README.md

commit 8b679987cdce292ff36bd741f6715e4927e26f9b
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Tue Apr 11 21:45:44 2023 +0200

    Fix whitespace, add .editorconfig, add GitHub workflow (#883)

.devops/main.Dockerfile
.dockerignore
.ecrc
.editorconfig
.github/ISSUE_TEMPLATE/custom.md
.github/workflows/docker.yml
.github/workflows/editorconfig.yml
README.md
examples/Miku.sh
examples/common.cpp
examples/embedding/README.md
examples/main/README.md
examples/main/main.cpp
examples/perplexity/README.md
ggml.c

commit 3e6e70d8e8917b5bd14c7c9f9b89a585f1ff0b31
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Apr 11 15:03:51 2023 +0000

    Add enum llama_ftype, sync ggml_type to model files (#709)

examples/quantize/quantize.cpp
ggml.c
ggml.h
llama.cpp
llama.h

commit 2663d2c6784ad7b77998c6874df25648d597f74b
Author: comex <comexk@gmail.com>
Date:   Tue Apr 11 06:19:54 2023 -0700

    Windows fixes (#890)
    
    Mostly for msys2 and mingw64 builds, which are different from each other
    and different from standard Visual Studio builds.  Isn't Windows fun?
    
    - Define _GNU_SOURCE in more files (it's already used in ggml.c for
      Linux's sake).
    
    - Don't use PrefetchVirtualMemory if not building for Windows 8 or later
      (mingw64 doesn't by default).  But warn the user about this situation
      since it's probably not intended.
    
    - Check for NOMINMAX already being defined, which it is on mingw64.
    
    - Actually use the `increment` variable (bug in my `pizza` PR).
    
    - Suppress unused variable warnings in the fake pthread_create and
      pthread_join implementations for Windows.
    
    - (not Windows-related) Remove mention of `asprintf` from comment;
      `asprintf` is no longer used.
    
    Fixes #871.

examples/main/main.cpp
ggml.c
llama.cpp
llama_util.h

commit a0caa34b162449b5c13b8d604573053300ff54a1
Author: qouoq <qouoq@fastmail.com>
Date:   Tue Apr 11 04:41:53 2023 +0800

    Add BAIR's Koala to supported models (#877)

README.md

commit 461ba9e66ed3885f80680d71495e055580573c74
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 10 23:20:01 2023 +0300

    ggml : fix WASM build

ggml.c

commit c3ac702e5ee3533457e0489df4906ee112fe88e7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 10 22:40:28 2023 +0300

    ggml : add ggml_cont() + optimize ggml_cpy() for contiguous dst

ggml.c
ggml.h

commit 9d634ef452d0fc24fcd49592952d13d0ab0f41b7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 10 19:32:45 2023 +0300

    ggml : remove trailing whitespaces

ggml.c

commit d9a239c4104c888eafda672c1e42c9bbc5084cb8
Author: Marco Matthies <71844+marcom@users.noreply.github.com>
Date:   Mon Apr 10 19:57:59 2023 +0200

    Simplify to include lower-case windows.h always, fix compile on mingw32 (#747)

ggml.c

commit 684da25926e5c505f725b4f10b5485b218fa1fc7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Apr 10 19:29:48 2023 +0300

    ggml : fix quantize_row_q4_1() ARM_NEON (close #876)

ggml.c

commit 180b693a47b6b825288ef9f2c39d24b6eea4eea6
Author: comex <comexk@gmail.com>
Date:   Sat Apr 8 13:08:21 2023 -0700

    Print model version.
    
    Also improve model type printing, and fix indentation of an unrelated
    switch statement.

llama.cpp

commit f963b63afa0e057cfb9eba4d88407c6a0850a0d8
Author: comex <comexk@gmail.com>
Date:   Sat Apr 8 12:24:37 2023 -0700

    Rewrite loading code to try to satisfy everyone:
    
    - Support all three formats (ggml, ggmf, ggjt).  (However, I didn't
      include the hack needed to support GPT4All files without conversion.
      Those can still be used after converting them with convert.py from my
      other PR.)
    
    - Support both mmap and read (mmap is used by default, but can be
      disabled with `--no-mmap`, and is automatically disabled for pre-ggjt
      files or on platforms where mmap is not supported).
    
    - Support multi-file models like before, but automatically determine the
      number of parts rather than requiring `--n_parts`.
    
    - Improve validation and error checking.
    
    - Stop using the per-file type field (f16) entirely in favor of just
      relying on the per-tensor type/size fields.  This has no immediate
      benefit, but makes it easier to experiment with different formats, and
      should make it easier to support the new GPTQ-for-LLaMa models in the
      future (I have some work in progress on that front).
    
    - Support VirtualLock on Windows (using the same `--mlock` option as on
      Unix).
    
        - Indicate loading progress when using mmap + mlock.  (Which led me
          to the interesting observation that on my Linux machine, with a
          warm file cache, mlock actually takes some time, whereas mmap
          without mlock starts almost instantly...)
    
          - To help implement this, move mlock support from ggml to the
            loading code.
    
    - madvise/PrefetchVirtualMemory support (based on #740)
    
    - Switch from ifstream to the `fopen` family of functions to avoid
      unnecessary copying and, when mmap is enabled, allow reusing the same
      file descriptor for both metadata reads and mmap (whereas the existing
      implementation opens the file a second time to mmap).
    
    - Quantization now produces a single-file output even with multi-file
      inputs (not really a feature as much as 'it was easier this way').
    
    Implementation notes:
    
    I tried to factor the code into more discrete pieces than before.
    
    Regarding code style: I tried to follow the code style, but I'm naughty
    and used a few advanced C++ features repeatedly:
    
    - Destructors to make it easier to ensure everything gets cleaned up.
    
    - Exceptions.  I don't even usually use exceptions when writing C++, and
      I can remove them if desired... but here they make the loading code
      much more succinct while still properly handling a variety of errors,
      ranging from API calls failing to integer overflow and allocation
      failure.  The exceptions are converted to error codes at the
      API boundary.)
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io> (for the bit I copied from #740)

CMakeLists.txt
Makefile
examples/common.cpp
examples/common.h
examples/embedding/embedding.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize-stats/quantize-stats.cpp
ggml.c
ggml.h
llama.cpp
llama.h
llama_internal.h
llama_util.h

commit aaf3b23debc1fe1a06733c8c6468fb84233cc44f
Author: Tomáš Pazdiora <tomas.pazdiora@gmail.com>
Date:   Sat Apr 8 17:49:39 2023 +0200

    fix for windows utf-8 input (#840)
    
    Use UTF-16 as input on Windows, since UTF-8 does not work and reads multibyte characters as zeros

examples/common.cpp
examples/common.h
examples/main/main.cpp

commit f2d1c472946dee2aba9077e8df73346796752b10
Author: eiery <19350831+eiery@users.noreply.github.com>
Date:   Sat Apr 8 07:15:17 2023 -0400

    cmake should link openblas properly with -lopenblas like how it's done in the makefile (#839)

CMakeLists.txt

commit 317fb12fbd7cef5d86476574bffe0e904af884ca
Author: lon <114724657+longregen@users.noreply.github.com>
Date:   Sat Apr 8 07:04:23 2023 -0300

    Add new binaries to flake.nix (#847)

flake.nix

commit 62cfc54f77e519057110265b52b0d614fa363e2a
Author: unbounded <haakon@likedan.net>
Date:   Sat Apr 8 00:09:18 2023 +0200

    Add quantize-stats command for testing quantization (#728)
    
    Command that calculates some statistics over the errors introduced by
    quantization, like mean square error, max error and some percentile errors for layer
    weights. Should be useful for testing quantization improvements.
    
    Exposes some internal state from ggml and llama for testing

.gitignore
Makefile
examples/CMakeLists.txt
examples/quantize-stats/CMakeLists.txt
examples/quantize-stats/quantize-stats.cpp
ggml.c
ggml.h
llama.cpp
llama.h

commit 698f7b5d6316a1f8453b3b32fd0d637d24952ffd
Author: bhubbb <79117352+bhubbb@users.noreply.github.com>
Date:   Sat Apr 8 02:11:58 2023 +1000

    make : add libllama.so target for llama-cpp-python (#797)
    
    I was able to get llama-cpp-python working but only when I build libllama.so with make.

Makefile

commit c1950c343109ab1fd15fc2ae1c83650c85d4eeef
Author: iacore <74560659+iacore@users.noreply.github.com>
Date:   Fri Apr 7 16:05:29 2023 +0000

    zig : don't link examples/common.cpp for non-example (#814)

build.zig

commit 4953e9007f86327aabc8312a7211c18019a3a40e
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Fri Apr 7 19:02:12 2023 +0300

    llama : always sort logits before nucleus sampling (#812)
    
    * Always sort logits before nucleus sampling
    
    * remove second normalization
    
    - fix windows build
    - remove normalization since std::discrete_distribution does not require it

llama.cpp

commit cc9cee8e9e7598bd280295f6264f36d3a9224006
Author: Sergey Alirzaev <zl29ah@gmail.com>
Date:   Thu Apr 6 17:59:11 2023 +0200

    Do not crash when it has nothing to say. (#796)
    
    Otherwise observing this in the interactive mode:
    /usr/lib/gcc/x86_64-pc-linux-gnu/12/include/g++-v12/bits/stl_vector.h:1230: reference std::vector<int>::back() [_Tp = int, _Alloc = std::allocator<int>]: Assertion '!this->empty()' failed.

examples/main/main.cpp

commit d2beca95dcfcd6f1145886e914b879ffc3604b7a
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Thu Apr 6 08:56:58 2023 +0200

    Make docker instructions more explicit (#785)

README.md

commit eeaa7b0492fc79baab8bb1fe195d6c87159f2bd3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 22:11:03 2023 +0300

    ggml : multi-thread ggml_rope() (~3-4 times faster on M1) (#781)

ggml.c

commit 986b6ce9f99503c51ec5afd8a10baa32359434c6
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 22:07:33 2023 +0300

    ggml, llama : avoid heavy V transpose + improvements (#775)
    
    ggml :
    
    - added ggml_view_3d()
    - ggml_view_tensor() now inherits the stride too
    - reimplement ggml_cpy() to account for dst stride
    - no longer require tensor->data to be memory aligned
    
    llama :
    
    - compute RoPE on 32-bit tensors (should be more accurate)
    - store RoPE-ed K in the KV cache
    - store transposed V in the KV cache (significant speed-up)
    - avoid unnecessary Q copy

ggml.c
ggml.h
llama.cpp

commit 34162989297fdfe3ab7305451ce55bc87e3f4c9c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 19:54:30 2023 +0300

    Update README.md

README.md

commit 5a8c4f624077373a198cd562146ffa67b02ebc75
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Wed Apr 5 19:20:05 2023 +0300

    llama : define non-positive top_k; top_k range check (#779)
    
    * Define non-positive top_k; top_k range check
    
    * minor : brackets
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp

commit ff05d05c960076b42f027e4d318cbd6ea59b3030
Author: at8u <129688334+at8u@users.noreply.github.com>
Date:   Wed Apr 5 15:59:13 2023 +0000

    miku.sh : add executable bit (#780)

examples/Miku.sh

commit 62b3e81aaeafb282934de8b21de13b0104f12f8c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 18:58:06 2023 +0300

    media : add logos and banners

media/llama-leader.jpeg
media/llama0-banner.png
media/llama0-logo.png
media/llama1-banner.png
media/llama1-logo.png

commit 8d10406d6ee230e6c1a96590cc8273f86ae606f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Apr 5 18:56:20 2023 +0300

    readme : change logo + add bindings + add uis + add wiki

README.md

commit ed1c214e667d58ac442b3c6664831fe0eed2941d
Author: iacore <74560659+iacore@users.noreply.github.com>
Date:   Wed Apr 5 15:06:02 2023 +0000

    zig : add build.zig (#773)
    
    Co-authored-by: Locria Cyber <74560659+locriacyber@users.noreply.github.com>

.gitignore
build.zig

commit 0c44427df10ee024b4e7ef7bfec56e993daff1db
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Wed Apr 5 17:38:37 2023 +0300

    make : missing host optimizations in CXXFLAGS (#763)

Makefile

commit 594cc95fabab0b662dabba3ea619ca5dca18bf6b
Author: Adithya Balaji <adithya.b94@gmail.com>
Date:   Wed Apr 5 16:36:12 2023 +0200

    readme : update with CMake and windows example (#748)
    
    * README: Update with CMake and windows example
    
    * README: update with code-review for cmake build

README.md

commit 88ed5761b869a221bc26847ff3f6977e7ee6425e
Author: at8u <129688334+at8u@users.noreply.github.com>
Date:   Wed Apr 5 14:32:42 2023 +0000

    examples : add Miku.sh (#724)
    
    * Add Miku.sh to examples
    
    * Add missing line to prompt in Miku.sh
    
    * Add --keep param to Miku.sh
    
    * Remove '[end_of_conversation]' line from Miku.sh
    
    No longer is necessary.

examples/Miku.sh

commit 58c438cf7dfbbef710b1905a453a38a8a9ced07d
Author: Andrew Duffy <a10y@users.noreply.github.com>
Date:   Wed Apr 5 11:44:24 2023 +0100

    Add Accelerate/BLAS when using Swift (#765)

Package.swift

commit 53dbba769537e894ead5c6913ab2fd3a4658b738
Author: mgroeber9110 <45620825+mgroeber9110@users.noreply.github.com>
Date:   Mon Apr 3 18:00:55 2023 +0200

    Windows: reactive sigint handler after each Ctrl-C (#736)

examples/main/main.cpp

commit 437e77855a54e69c86fe03bc501f63d9a3fddb0e
Author: SebastianApel <13675545+SebastianApel@users.noreply.github.com>
Date:   Mon Apr 3 09:52:28 2023 +0200

    10+% performance improvement of ggml_vec_dot_q4_0 on AVX2 (#654)
    
    * Performance improvement of AVX2 code
    * Fixed problem with MSVC compiler
    * Reviewer comments: removed double semicolon, deleted empty line 1962

ggml.c

commit cd7fa956904cb8e321b72b3499f4a3a82e43c266
Author: Ivan Stepanov <ivanstepanovftw@gmail.com>
Date:   Mon Apr 3 03:19:04 2023 +0300

    Define non-positive temperature behavior (#720)

llama.cpp

commit a0c05164168297c04737936ad0cad849a512547a
Author: bsilvereagle <bsilvereagle@users.noreply.github.com>
Date:   Sun Apr 2 15:13:03 2023 -0700

    Remove torch GPU dependencies from the Docker.full image (#665)
    
    By using `pip install torch --index-url https://download.pytorch.org/whl/cpu`
    instead of `pip install torch` we can specify we want to install a CPU-only version
    of PyTorch without any GPU dependencies. This reduces the size of the Docker image
    from 7.32 GB to 1.62 GB

.devops/full.Dockerfile

commit d8d4e865cd481b18f10508ffee35db903767ef5c
Author: Thatcher Chamberlin <j.thatcher.c@gmail.com>
Date:   Sun Apr 2 06:48:57 2023 -0400

    Add a missing step to the gpt4all instructions (#690)
    
    `migrate-ggml-2023-03-30-pr613.py` is needed to get gpt4all running.

README.md

commit e986f94829bae0b9e66b326acbbba179931c84f1
Author: Christian Falch <875252+chrfalch@users.noreply.github.com>
Date:   Sun Apr 2 12:23:04 2023 +0200

    Added api for getting/setting the kv_cache (#685)
    
    The api provides access methods for retrieving the current memory buffer for the kv_cache and its token number.
    It also contains a method for setting the kv_cache from a memory buffer.
    
    This makes it possible to load/save history - maybe support --cache-prompt paramater as well?
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

llama.cpp
llama.h

commit c0bb1d3ce21005ab21d686626ba87261a6e3a660
Author: Marian Cepok <marian.cepok@gmail.com>
Date:   Sun Apr 2 12:21:31 2023 +0200

    ggml : change ne to int64_t (#626)

ggml.c
ggml.h
llama.cpp

commit 6e7801d08d81c931a5427bae46f00763e993f54a
Author: Leonardo Neumann <leonardo@neumann.dev.br>
Date:   Sun Apr 2 04:56:20 2023 -0300

    examples : add gpt4all script (#658)

examples/gpt4all.sh

commit 81040f10aae3160317c5787c9c59acb219927826
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Apr 2 07:18:53 2023 +0000

    llama : do not allocate KV cache for "vocab_only == true" (#682)
    
    Fixes sanitizer CI

llama.cpp

commit c4f89d8d73aab4318a6c61e3835135adfcf55407
Author: Fabian <cmdrf@users.noreply.github.com>
Date:   Sun Apr 2 09:17:05 2023 +0200

    make : use -march=native -mtune=native on x86 (#609)

Makefile

commit 5b70e7de4c0b8186669d0c5609ba61a2d46de562
Author: Murilo Santana <mvrilo@gmail.com>
Date:   Sat Apr 1 23:41:12 2023 -0300

    fix default params for examples/main (#697)

examples/common.cpp

commit a717cba8440b380f43cd3e2510862fc1ea3de9a2
Author: Ikko Eltociear Ashimine <eltociear@gmail.com>
Date:   Sun Apr 2 01:38:18 2023 +0900

    py: huggingface -> Hugging Face (#686)

convert-ggml-to-pth.py

commit d0a7f742e76bb48c0bd852f0b3bf09ec0b75b200
Author: rimoliga <53384203+rimoliga@users.noreply.github.com>
Date:   Sat Apr 1 11:57:30 2023 -0300

    readme: replace termux links with homepage, play store is deprecated (#680)

README.md

commit 0d054e292e5492981867be69c788edd04dc8adeb
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Fri Mar 31 20:03:48 2023 +0200

    Show error message when -f fails

examples/common.cpp

commit 3525899277d2e2bdc8ec3f0e6e40c47251608700
Author: Stephan Walter <stephan@walter.name>
Date:   Fri Mar 31 19:19:16 2023 +0000

    Enable -std= for cmake builds, fix warnings (#598)

CMakeLists.txt
ggml.c

commit 1d08882afa647c44195f4f6495a68ea455650cae
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Fri Mar 31 17:55:52 2023 +0200

    Optimize AVX2 ggml_vec_dot_q4_0 (#642)

ggml.c

commit 02c5b27e91a6d18cf1043d3a2d8dbc59610ac257
Author: perserk <perserk@gmail.com>
Date:   Fri Mar 31 16:55:44 2023 +0500

    Add AVX acceleration (#617)
    
    * ggml : add AVX quantize_row_q4_0()
    
    * ggml : add AVX ggml_vec_dot_q4_0()
    
    * ggml : refactor AVX part of ggml_vec_dot_q4_0()
    
    https://github.com/ggerganov/llama.cpp/pull/617#issuecomment-1489985645

ggml.c

commit cbef542879962fdc491656cd0c8cadd65a5f1356
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Wed Mar 29 21:31:24 2023 +0200

    py : cleanup the code
    
    - use f-strings where possible
    - drop first param of encode/decode functions since "utf-8" is the default

convert-ggml-to-pth.py
convert-gpt4all-to-ggml.py
convert-gptq-to-ggml.py
convert-pth-to-ggml.py
convert-unversioned-ggml-to-ggml.py
migrate-ggml-2023-03-30-pr613.py

commit 9733104be5389ebb1ff05095eca2a70280cd875a
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Fri Mar 31 00:52:06 2023 +0200

    drop quantize.py (now that models are using a single file)

README.md
quantize.py

commit 3df890aef432ce68143cfafcd7caf828bc4c3e55
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 30 22:31:54 2023 +0300

    readme : update supported models

README.md

commit ee0c40dd6de8c3c658ae43199939ef40bb1cf408
Author: Justine Tunney <jtunney@gmail.com>
Date:   Thu Mar 30 05:42:56 2023 -0700

    Introduce GGML migration tool for new file format
    
    If you deleted your old Meta LLaMA .pth files, then the
    migrate-ggml-2023-03-30-pr613.py script will allow you to convert your
    old ggml files into the new mmap()'able format.
    
    See #613

convert-pth-to-ggml.py
llama.cpp
migrate-ggml-2023-03-30-pr613.py

commit 6f23ba5ee235cbcb1eedd63b98422dd8d4392a78
Author: Justine Tunney <jtunney@gmail.com>
Date:   Thu Mar 30 01:53:36 2023 -0700

    Ensure --mlock works properly with mmap() support

ggml.c
ggml.h
llama.cpp

commit 78ca9838ee36660a776e97e3391b6fb5dcaacf7f
Author: Justine Tunney <jtunney@gmail.com>
Date:   Wed Mar 29 13:51:37 2023 -0700

    Make loading weights 10-100x faster
    
    This is a breaking change that's going to give you three benefits:
    
    1. Your inference commands should load 100x faster
    2. You may be able to safely load models 2x larger
    3. You can run many concurrent inference processes
    
    This was accomplished by changing the file format so we can mmap()
    weights directly into memory without having to read() or copy them
    thereby ensuring the kernel can make its file cache pages directly
    accessible to our inference processes; and secondly, that the file
    cache pages are much less likely to get evicted (which would force
    loads to hit disk) because they're no longer competing with memory
    pages that were needlessly created by gigabytes of standard i/o.
    
    The new file format supports single-file models like LLaMA 7b, and
    it also supports multi-file models like LLaMA 13B. Our Python tool
    now merges the foo.1, foo.2, etc. files back into a single file so
    that the C++ code which maps it doesn't need to reshape data every
    time. That's made llama.cpp so much simpler. Much of its load code
    has now been deleted.
    
    Furthermore, this change ensures that tensors are aligned properly
    on a 32-byte boundary. That opens the door to seeing if we can get
    additional performance gains on some microprocessors, by using ops
    that require memory alignment.
    
    Lastly note that both POSIX and the Windows platform are supported
    
    Fixes #91

.gitignore
convert-ggml-to-pth.py
convert-gptq-to-ggml.py
convert-pth-to-ggml.py
llama.cpp
llama.h
models/ggml-vocab.bin

commit a017390358cdb23fffb30988dc84bb190d0403ca
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 22:22:36 2023 +0200

    Initial windows support (untested)

llama.cpp

commit ac184d514723902f9b05b688703b1be6e8dc65de
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 08:53:14 2023 +0200

    Always initialize mm_addr and mm_length in llama_model

llama.cpp

commit 276e5b781155e3bbe6834472c58f03dfe62efabe
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 08:31:26 2023 +0200

    Unmap the file in llama_free

llama.cpp

commit d68c5dc4356c8f49e933df210f2ceca5002a8118
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 06:18:18 2023 +0200

    Make mmap_file static

llama.cpp

commit 64bde3ffd4aef799acb790a3eedddbd0a0612108
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 05:38:57 2023 +0200

    Fix ggml_init_params in quantize

examples/quantize/quantize.cpp
llama.cpp

commit c03ae8dca1d7c451054754979e60a6de1f64c3cd
Author: Slaren <2141330+slaren@users.noreply.github.com>
Date:   Wed Mar 29 02:03:43 2023 +0200

    Add mmap support for model files

ggml.c
ggml.h
llama.cpp

commit 3bcc129ba881c99795e850b0a23707a4dfdabe9d
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Mar 30 17:56:59 2023 +0000

    cmake : properly invoke CTest (#629)

CMakeLists.txt

commit a4755cf288deb83df646f91f8fc98613271322db
Author: Casey Primozic <me@ameo.link>
Date:   Thu Mar 30 10:53:35 2023 -0700

    Remove unused variable (#607)
    
    * It seems some new warning were added recently that exposed this.  I wrote the code that included this unused variable originally and it is indeed not needed.

ggml.c

commit 1f0414feecc336482163af6c1e5650f9373ed8c9
Author: david raistrick <keen99@users.noreply.github.com>
Date:   Thu Mar 30 13:34:45 2023 -0400

    make : fix darwin f16c flags check (#615)
    
    ...there was no check.  ported upstream from https://github.com/zanussbaum/gpt4all.cpp/pull/2 (I dont see any clean path for upstream patches)

Makefile

commit 77efdf5a501b1140801da5cd8751e9f9b259ec32
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 30 20:27:32 2023 +0300

    ggml : fix NEON signs (close #620, #622)

ggml.c

commit ed3c680bcd0e8ce6e574573ba95880b694449878
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Thu Mar 30 11:16:30 2023 +0200

    Fix GGML_F32Cx8_STORE in AVX without F16C path (#619)

ggml.c

commit 9cbc404ba6699a9ba4925ea25a60552b13491c7a
Author: anzz1 <anzz1@live.com>
Date:   Wed Mar 29 23:44:39 2023 +0300

    ci : re-enable AVX512 testing (Windows-MSVC) (#584)
    
    * CI: Re-enable AVX512 testing (Windows-MSVC)
    
    Now with 100% less base64 encoding
    
    * plain __cpuid is enough here

.github/workflows/build.yml

commit b51c717d5cf9181c33afcb84554e47f6d539c891
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 22:15:34 2023 +0300

    ggml : init time on first ggml_init() call

ggml.c

commit 0ba76c1e73ae21038b80bfb5a746157376c88173
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 22:13:12 2023 +0300

    llama : fix compile warnings when reading the vocab

llama.cpp

commit cea1c859483a5cfc7e2b31a06f8561d7a7604870
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 22:10:01 2023 +0300

    ggml : add ARM_NEON dequantize_row_q4_1()

ggml.c

commit f202ada131f60059112a948f660b2e0ac93d049a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 22:03:02 2023 +0300

    ggml : add ARM_NEON quantize_row_q4_1()

ggml.c

commit 3b44d30d9b618f0f2eb9abcfe912770a4e7d85d4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 21:47:33 2023 +0300

    ggml : add ARM_NEON ggml_vec_dot_q4_1()

ggml.c

commit 61cbfff5c95e45236883b1b60e025f8f6fa8c8a3
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Wed Mar 29 20:09:25 2023 +0200

    rename convert_ggml_to_pth.py -> convert-ggml-to-pth.py (#600)
    
    to match filenames of other converters

convert-ggml-to-pth.py

commit d9ad104440d84a0cc0734bff47ef0ba41ba740c4
Author: Thérence <13496987+Royalphax@users.noreply.github.com>
Date:   Wed Mar 29 19:21:09 2023 +0200

    Create chat-13B.bat (#592)
    
    * Create chat-13B.bat
    
    Same script than chat-13B.sh, but for windows users.
    Tested and working on windows 10/11 v 22H2
    
    * Apply suggestions from code review
    
    ---------
    
    Co-authored-by: anzz1 <anzz1@live.com>

examples/chat-13B.bat

commit b467702b87461543c75013207e9adc6d20dcc01d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 19:38:31 2023 +0300

    readme : fix typos

README.md

commit 516d88e75c9e768c0001a452dbad212494c586b3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 19:37:20 2023 +0300

    readme : add GPT4All instructions (close #588)

README.md

commit 53635c081c49321d523567112f9fddfbba6b787b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 29 19:29:26 2023 +0300

    py : add GPT4All conversion script
    
    For now: copy-paste
    Too much time for me to deduplicate the python code

convert-gpt4all-to-ggml.py
convert-unversioned-ggml-to-ggml.py

commit 41318d708ed196ff727dce14d263a64b23c7333d
Author: Maël Kerbiriou <m431.kerbiriou@gmail.com>
Date:   Wed Mar 29 18:10:07 2023 +0200

    llama : use the same threshold for OpenBLAS and ggml thread limiting (#577)

llama.cpp

commit a6956b25a1c783e5e96fe06c9c00438f846ef047
Author: Tobias Lütke <tobi@shopify.com>
Date:   Wed Mar 29 17:10:24 2023 +0200

    add example of re-act pattern (#583)
    
    * add example of re-act pattern
    
    * spelling...
    
    * fixed whitespace in reverse prompt issue

examples/reason-act.sh
prompts/reason-act.txt

commit 83df5639eb182ed7c122382907691d8baa3c32df
Author: anzz1 <anzz1@live.com>
Date:   Wed Mar 29 16:20:07 2023 +0300

    Fix GCC warning about binary literal (#595)
    
    0b10101010 -> 0xAA /* 0b10101010 */

ggml.c

commit a5c42c4b13b3be9e58fe8f9adbb6ee60417674a6
Author: anzz1 <anzz1@live.com>
Date:   Wed Mar 29 16:19:29 2023 +0300

    Fix typo in llama.h (#593)

llama.h

commit 5a5f8b1501fbb34367225544010ddfc306d6d2fe
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 28 22:44:29 2023 +0300

    Enable Fused-Multiply-Add (FMA) and F16C/CVT16 vector extensions on MSVC (#375)
    
    * Enable Fused-Multiply-Add (FMA) instructions on MSVC
    
    __FMA__ macro does not exist in MSVC
    
    * Enable F16C/CVT16 vector extensions on MSVC
    
    __F16C__ macro does not exist in MSVC, but is implied with AVX2/AVX512
    
    * MSVC cvt intrinsics
    
    * Add __SSE3__ macro for MSVC too because why not
    
    even though it's not currently used for anything when AVX is defined

ggml.c

commit f1217055eaedfc7214be93d98e529cae89830430
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 28 22:43:25 2023 +0300

    CI: fix subdirectory path globbing (#546)
    
    - Changes in subdirectories will now be detecter properly
    - (Windows-MSVC) AVX512 tests temporarily disabled

.github/workflows/build.yml

commit 7f4c5c66514227c3870c2bd189fb0609fdd0de10
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 28 21:23:09 2023 +0300

    llama : fix linkage with mingw (#551)
    
    * Revert 7e53955 (#542)
    
    Still needs to be fixed properly
    
    * Fix linking on mingw32

examples/embedding/CMakeLists.txt
examples/main/CMakeLists.txt
examples/perplexity/CMakeLists.txt
examples/quantize/CMakeLists.txt
llama.h

commit 2a98bc18ea34dbf15f261a0df37080e588a189d1
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Tue Mar 28 20:06:03 2023 +0200

    ggml : add AVX2 implementation of quantize_row_q4_1 (#515)
    
    * Add AVX2 implementation of quantize_row_q4_1
    
    * Actually use AVX2
    
    * Make quantize_row_q4_1 static
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c

commit d0aaff571cd5c316b68e3e11d57e274bfd2bd457
Author: thement <40525767+thement@users.noreply.github.com>
Date:   Tue Mar 28 19:55:42 2023 +0200

    py : add temporary script to convert old ggml files to newer version (#539)
    
    Co-authored-by: Jakub Horak <jakub.horak@ibawizard.net>

convert-unversioned-ggml-to-ggml.py
llama.cpp

commit d0330fd783d7c67349cdcce4a56604ef0aeccdb5
Author: Tai Duc Nguyen <taiducnguyen.drexel@gmail.com>
Date:   Tue Mar 28 13:51:29 2023 -0400

    py : add capabiliy to convert from ggml back to torch or hf format for further consumption/training/finetuning (#403)

convert_ggml_to_pth.py

commit 99c5b2765422232ebb4414f5a63693d734406a7f
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Mar 28 17:13:01 2023 +0000

    ggml : refactor quantized processing functions (#509)
    
    * Refactor quantized processing functions
    
    * ggml : minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c

commit 692ce3164ef1201ecb9cfad315cc0a08b965adb8
Author: DooWoong Lee (David) <manics99@naver.com>
Date:   Wed Mar 29 02:02:34 2023 +0900

    py : removed unused `model` variable and verified that the code functions correctly with `vocab_only` setting. Also confirmed that the code works as expected after running with reduced memory usage due to deletion of no-longer-needed variable. (#547)

convert-pth-to-ggml.py

commit 96f9c0506fa81cada6f96f45768c34f45406c4bb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 28 20:01:09 2023 +0300

    ci : make ctest verbose, hopefully we see what is wrong with the sanitizer

.github/workflows/build.yml

commit d502bc7c9d9d6dfb3a09aea404395b666d7b374d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 28 19:51:55 2023 +0300

    tests : free llama context at the end of the test

CMakeLists.txt
tests/test-tokenizer-0.cpp

commit 436e56193199a1625f8c561069f702e8840a9e08
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Mar 28 16:48:20 2023 +0000

    all : be more strict about converting float to double (#458)
    
    * Be more strict about converting float to double
    
    * Test equivalence of round, SILU implementations
    
    Test module is commented out in CMakeLists.txt because the tests may
    take a long time, depending on how much the compiler optimizes.
    
    * Fix softmax in perplexity.cpp
    
    * all : prefer float over double where appropriate
    
    * perplexity : add <cmath>
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt
Makefile
examples/common.cpp
examples/main/main.cpp
examples/perplexity/perplexity.cpp
examples/quantize/quantize.cpp
ggml.c
llama.cpp
llama.h
tests/CMakeLists.txt
tests/test-double-float.c

commit 20e1e84884376b3fb44ffbfd48d478b2934b0b5e
Author: Jed Fox <git@jedfox.com>
Date:   Tue Mar 28 11:39:01 2023 -0500

    deploy : add a Package.swift for SwiftPM support (#393)
    
    * Add a Package.swift for SwiftPM support
    
    * Swap from exclusions to allowlist

.gitignore
Package.swift
spm-headers/llama.h

commit c1f885067c61191a07a1aedf684168dda62f3f71
Author: Stephan Walter <stephan@walter.name>
Date:   Tue Mar 28 15:56:03 2023 +0000

    ggml : introduce structs for the q4 data blocks (#356)
    
    * Introduce structs for the q4 data blocks
    
    * ggml : rename quant struct variables + fix ARM_NEON
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

examples/quantize/quantize.cpp
ggml.c
ggml.h
llama.cpp
llama.h
tests/test-quantize.c

commit e0670260fb50a882b37074112b1881fb0820cf77
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 28 18:34:35 2023 +0300

    gitignore : add "embedding"

.gitignore

commit 28ba975aea1dcae2f31770516f5d542ff177771e
Author: dotpy314 <33351922+dotpy314@users.noreply.github.com>
Date:   Tue Mar 28 23:06:28 2023 +0800

    Check the existence of f16_model_path_base in quantize.py (#574)
    
    Co-authored-by: Jincheng Miao <jincheng.miao@gmail.com>

quantize.py

commit a6bdc47cba23713a22ade47dd65b6afeb8009ff4
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Tue Mar 28 16:26:55 2023 +0200

    Fix usage of F16C intrinsics in AVX code (#563)
    
    * Fix usage of F16C intrinsics in AVX code when F16C is not defined

ggml.c

commit 7b8dbcb78b2f65c4676e41da215800d65846edd0
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 28 17:09:55 2023 +0300

    main.cpp fixes, refactoring (#571)
    
    - main: entering empty line passes back control without new input in interactive/instruct modes
    - instruct mode: keep prompt fix
    - instruct mode: duplicate instruct prompt fix
    - refactor: move common console code from main->common

examples/common.cpp
examples/common.h
examples/main/main.cpp

commit 4b8efff0e3945090379aa2f897ff125c8f9cdbae
Author: RJ Adriaansen <adriaansen@eshcc.eur.nl>
Date:   Tue Mar 28 08:11:09 2023 +0200

    Add embedding example to Makefile (#540)

Makefile

commit 7e5395575a3360598f2565c73c8a2ec0c0abbdb8
Author: Marco Matthies <71844+marcom@users.noreply.github.com>
Date:   Mon Mar 27 06:55:26 2023 +0200

    Fix missing ggml link in cmake for examples/* on w64-mingw32 (#542)

examples/embedding/CMakeLists.txt
examples/main/CMakeLists.txt
examples/perplexity/CMakeLists.txt
examples/quantize/CMakeLists.txt

commit 34c1072e497eb92d81ee7c0e12aa6741496a41c6
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Sun Mar 26 17:48:40 2023 +0200

    ci: add debug build to sanitizer build matrix (#527)

.github/workflows/build.yml

commit 939ad2d3a56815f480b6fd5ea432a7ee576a7e6b
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Mar 26 15:34:02 2023 +0000

    Fix undefined variables in debug build, remove unused variables (#531)

ggml.c

commit 8c2ec5e21d580c99e257c3cfddcf21fa53229aa4
Author: Juan Calderon-Perez <835733+gaby@users.noreply.github.com>
Date:   Sun Mar 26 10:48:42 2023 -0400

    Add support for linux/arm64 platform during Docker Builds (#514)
    
    * Add support for linux/arm64 platform
    
    * Add platform to versioned builds

.github/workflows/docker.yml

commit b391579db92f095666be1d979899b54ae0981573
Author: Stephan Walter <stephan@walter.name>
Date:   Sun Mar 26 13:14:01 2023 +0000

    Update README and comments for standalone perplexity tool (#525)

README.md
examples/perplexity/perplexity.cpp

commit 7a87d31f4f0c37bbb2ea695929fa4fe3ad579cda
Author: anzz1 <anzz1@live.com>
Date:   Sun Mar 26 16:06:10 2023 +0300

    [main] fix infinite generation (-n == -1) (#523)

examples/main/main.cpp

commit 348d6926ee31d4476f9b90e1a627b0925a70f847
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 26 10:20:49 2023 +0300

    Add logo to README.md

README.md

commit 33e35b8fe8f09adcac0632e9cece62e1dd629f7d
Author: Harald Fernengel <harald.fernengel@here.com>
Date:   Sun Mar 26 07:25:46 2023 +0200

    Exit from interactive mode if input stream is bad (#491)
    
    Allow exiting the interactive prompt also with CTRL-D on Unix and CTRL-Z
    on Windows.

examples/main/main.cpp

commit 19726169b379bebc96189673a19b89ab1d307659
Author: anzz1 <anzz1@live.com>
Date:   Sun Mar 26 00:13:28 2023 +0200

    CI: Run other sanitizer builds even if one fails (#511)
    
    applies only to sanitizer builds so they wont be cancelled

.github/workflows/build.yml

commit f732695cd57fb41e3a1be625cec4edf5be45b40a
Author: jp-x-g <jpxg-dev@protonmail.com>
Date:   Sat Mar 25 14:53:55 2023 -0700

    Clarify console output in convert-pth-to-ggml.py (#512)
    
    "Processing part 1 of 3" instead of "Processing part 0"

convert-pth-to-ggml.py

commit 2f7bf7dd7cd7299874d582f7f34834418abf4057
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 25 23:38:11 2023 +0200

    CMake / CI additions (#497)
    
    * CMake: Add AVX512 option
    
    * CI: Add AVX/AVX512 builds (Windows)
    (AVX512 tests can only be run when the worker happens to support it, building works anyway)
    
    * CMake: Fix sanitizer linkage ( merged #468 )
    
    * CI: Add sanitizer builds (Ubuntu)
    
    * CI: Fix release tagging
    (change @zendesk/action-create-release to @anzz1/action-create-release until upstream PR Added commitish as input zendesk/action-create-release#32 is merged)

.github/workflows/build.yml
CMakeLists.txt

commit 34ab5268432fd287caa68d60bdd8aef411def3fa
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 25 22:29:22 2023 +0200

    (Windows) Set console to UTF-8 on init (#420)
    
    Sets console codepage to 65001 (CP_UTF8) on start for both input and output, should fix problems with UTF-8 characters.

examples/main/main.cpp

commit c2b25b6912662d2637d9c6e6df3a5de931e0d7ce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 21:53:39 2023 +0200

    Fix colors enabling on WIN32

examples/main/main.cpp

commit 79b2b266db6b198b5af450982c3cd61120fac951
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 21:51:41 2023 +0200

    If n_predict == -1, generate forever

examples/chat.sh
examples/common.cpp
examples/main/main.cpp

commit e2d490dafd860eaaaf9aa8008ab790527d556daf
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 21:36:22 2023 +0200

    Inifinite generation via context swapping (#71)

examples/chat.sh
examples/common.cpp
examples/common.h
examples/main/main.cpp

commit 03f7e335604b3d68f74995aa2ccb4955833ee423
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 20:51:14 2023 +0200

    Cleanup STL headers + fix embedding examples + minor stuff

examples/embedding/embedding.cpp
examples/perplexity/perplexity.cpp
llama.cpp
llama.h

commit 55ad42af845127bd0eb0c1f36f327ecec83f4bca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 20:36:52 2023 +0200

    Move chat scripts into "./examples"

README.md
examples/alpaca.sh
examples/chat-13B.sh
examples/chat.sh

commit 459e93cce07cab9052c06b5bf360819893442e1e
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sat Mar 25 19:31:48 2023 +0100

    Add AVX2 implementation of dequantize_row_q4_1 (#505)

ggml.c

commit a316a425d04027453dc0fd45f003b647c12f66f9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 20:26:40 2023 +0200

    Overhaul the examples structure
    
    - main -> examples
    - utils -> examples (renamed to "common")
    - quantize -> examples
    - separate tools for "perplexity" and "embedding"
    
    Hope I didn't break something !

.gitignore
CMakeLists.txt
Makefile
examples/CMakeLists.txt
examples/common.cpp
examples/common.h
examples/embedding/CMakeLists.txt
examples/embedding/README.md
examples/embedding/embedding.cpp
examples/main/CMakeLists.txt
examples/main/README.md
examples/main/main.cpp
examples/perplexity/CMakeLists.txt
examples/perplexity/README.md
examples/perplexity/perplexity.cpp
examples/quantize/CMakeLists.txt
examples/quantize/README.md
examples/quantize/quantize.cpp
ggml.c
tests/CMakeLists.txt
tests/test-tokenizer-0.cpp

commit ecbe466a364876927994e2f1ec14f4d82301d201
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 19:47:21 2023 +0200

    Retire the ggml_mul_mat() branch for transposed src0 (#500)
    
    * Retire the ggml_mul_mat() for transposed src0
    
    - It can always be made contiguous with ggml_cpy()
    - The code is now simplified
    - The results are deterministic in respect to num threads
    
    * SIMD-ify dequantize_row_q4_0() for ARM_NEON (#502)
    
    * Attempt to SIMD-ify dequantize_row_q4_0() for ARM_NEON
    
    * Fix dequantization - forgot to interleave the quants

ggml.c

commit 502a400192013d3e95ed87b777e8fa3bec45713c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 17:16:50 2023 +0200

    Disable prompt verbosity by default and add option to enable (#480)

main.cpp
utils.cpp
utils.h

commit 09aecbf6283bbce9449e2d96000073145aaaf5fc
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sat Mar 25 16:06:49 2023 +0100

    Add AVX2 implementation of dequantize_row_q4_0 (#467)

ggml.c

commit 4640eff23d341a0273587800e17ff4a378132d60
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 17:03:10 2023 +0200

    Don't interefe with BLAS for large prompts by running only 1 thread

llama.cpp

commit ab77d7631211b299cb734bea6ad1f74324154150
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 16:47:59 2023 +0200

    Add longer DAN prompt for testing big batch numbers

prompts/dan.txt

commit 29b7baab670ae8b76ac0da21c2ded69ff18971ee
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sat Mar 25 15:34:23 2023 +0100

    Add timings for the prompt evaluation (#478)

llama.cpp

commit 4a7129acd2e939b92d70dd568c746f2fa078232c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 16:30:32 2023 +0200

    Remove obsolete information from README

README.md

commit 6b6dbc8910c6d53f4d96c46c8fcec70e2cd435d8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 16:22:05 2023 +0200

    Remove obsolete assert and fix compiler warning

ggml.c
main.cpp

commit 2a2e63ce0503d9bf3e55283e40a052c78c1cc3a8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 25 16:09:54 2023 +0200

    Fix nasty bug in ggml_compute_forward_mul_mat_f32() and reenable BLAS

ggml.c
llama.cpp

commit e899bf54b291e8c84173a0e534a2c262f3f63229
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 25 14:42:09 2023 +0200

    bounds checking for input prefix (#492)

utils.cpp

commit fbd4d38c647f82b2598291ea9b8d0c09ac1ffb8c
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 25 14:03:19 2023 +0200

    feat: '--in-prefix STRING' option (#426)
    
    Prefix user inputs with a string

main.cpp
utils.cpp
utils.h

commit 58e6c9f36f97d0a3e287b97256dc5f6b0e9fb5ae
Author: Jed Fox <git@jedfox.com>
Date:   Sat Mar 25 01:26:28 2023 -0400

    Add support for file load progress reporting callbacks (#434)
    
    * File load progress reporting
    
    * Move llama_progress_handler into llama_context_params
    
    * Renames
    
    * Use seekg to find file size instead
    
    * More correct load progress
    
    * Call progress callback more frequently
    
    * Fix typo

llama.cpp
llama.h

commit 36d07532ef7ccf0bdc12e050472f359a6794957f
Author: Doomsdayrs <38189170+Doomsdayrs@users.noreply.github.com>
Date:   Sat Mar 25 01:21:24 2023 -0400

    Add missing struct annotation (#483)
    
    `llama_sample_top_p_top_k` was missing the struct annotation on line 126.
    
    This causes a compiler issue when being parsed by the Kotlin C interop generator.
    
    This commit fixes the above issue by adding the struct annotation.

llama.h

commit 6f1ee4b640912211a4b07965c585d327e32e734d
Author: Chris Kuehl <ckuehl@ckuehl.me>
Date:   Fri Mar 24 23:38:14 2023 -0500

    Fix crash for 65B model with pre-allocated memory (#485)

llama.cpp

commit 8520fc310eab87f2c4612f2a00d4adbd44a20d0d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 23:47:06 2023 +0200

    Disable BLAS altogether - the bug is not just for qunatized mat mul

ggml.c

commit b3f460e94139cb24b0af81cc8bc10eb86269d704
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 23:39:17 2023 +0200

    Disable BLAS branch in mul_mat - seems there is a bug

ggml.c

commit 04c6f5ed6fafd63601fa06757877ed5ccf9d5991
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 23:17:58 2023 +0200

    Immediately start processing the prompt before user input has been provided (#476)

alpaca.sh
chat.sh
examples/chatLLaMa
main.cpp

commit 7a9b6c3a8bdc1cb75fefc826dfaa7331eb63695d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 23:17:37 2023 +0200

    Reduce memory usage and allocate enough memory for largest context (#473)
    
    * Reduce memory usage and allocate enough memory for large contexts
    
    * Simpler scratch buffer usage
    
    * Reenable BLAS for quantized mul_mat
    
    * Fix number of layers in 30B and 65B
    
    * Fix KV cache size for F32

ggml.c
llama.cpp
main.cpp
utils.cpp
utils.h

commit 31572d966531f7d768eb773322016ab78eb6e835
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 18:23:56 2023 +0200

    Temporary bump the memory buffer size - hopefully fix issues from 483bab2e

llama.cpp

commit f4f5362edb01b05c383b23f36d7b3489c77061b5
Author: Gary Mulder <gjmulder@gmail.com>
Date:   Fri Mar 24 15:23:09 2023 +0000

    Update README.md (#444)
    
    Added explicit **bolded** instructions clarifying that people need to request access to models from Facebook and never through through this repo.

README.md

commit 863f65e2e32dc1e6d23c96a4811bf382d6b2a548
Author: rabidcopy <rabidcopy@yahoo.com>
Date:   Fri Mar 24 10:22:39 2023 -0500

    fix instruct mode (#445)
    
    changes to EOS behavior in interactive and reverse prompt handling broke instruct mode by erroneously injecting instruct mode's reverse prompt and an extra newline.

main.cpp

commit afd220d9c665e4c19107120ace2f0cb742e28aa1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 17:21:01 2023 +0200

    Properly free llama_context on failure

llama.cpp

commit 481044d50cfe8eaa6cd0c1a1b445680e4b0b3ebc
Author: Cameron Kaiser <classilla@users.noreply.github.com>
Date:   Fri Mar 24 08:19:26 2023 -0700

    additional optimizations for POWER9 (#454)

Makefile
ggml.c

commit 563cdc391dde140f1084d1012234e8e6f57f881f
Author: comex <comexk@gmail.com>
Date:   Fri Mar 24 08:19:05 2023 -0700

    Support calling mlock() on loaded model data on Linux and macOS (#453)
    
    * Support calling mlock() on loaded model data on Linux and macOS
    
    This is enabled by a new --mlock command line option.
    
    Using mlock() disables swapping and memory compression for the model
    data.  Doing so can be useful on systems where the model takes up a
    large fraction of system RAM.  In my experience, macOS is quite eager to
    start compressing llama.cpp's memory, which then makes it halt for a few
    seconds while it decompresses, even with a model that uses "only" 25GB
    out of 32GB.
    
    Of course, this comes at the cost of forcing the system to swap or
    compress other processes' memory instead, so it needs to be used with
    care and shouldn't be enabled by default.
    
    In theory it should be possible to support this on Windows as well using
    VirtualLock(), but I'm not much of a Windows user.
    
    * Update llama.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c
ggml.h
llama.cpp
llama.h
main.cpp
utils.cpp
utils.h

commit 8d4a855c241ecb0f3ddc03447fe56002ebf27a37
Author: Luciano <lucianostrika44@gmail.com>
Date:   Fri Mar 24 08:05:13 2023 -0700

    Add embedding mode with arg flag. Currently working (#282)
    
    * working but ugly
    
    * add arg flag, not working on embedding mode
    
    * typo
    
    * Working! Thanks to @nullhook
    
    * make params argument instead of hardcoded boolean. remove useless time check
    
    * start doing the instructions but not finished. This probably doesnt compile
    
    * Embeddings extraction support
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

llama.cpp
llama.h
main.cpp
utils.cpp
utils.h

commit b6b268d4415fd3b3e53f22b6619b724d4928f713
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 09:13:35 2023 +0200

    Add link to Roadmap discussion

README.md

commit 3cd8dde0d1357b7f11bdd25c45d5bf5e97e284a0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 06:22:28 2023 +0200

    Revert "Fix memory allocation issues and seg faults"
    
    This reverts commit 4870e455b3653f7d7769fa5772b2c90ffad088df.
    
    Will provide the correct fix later

llama.cpp

commit 4870e455b3653f7d7769fa5772b2c90ffad088df
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 24 00:11:53 2023 +0200

    Fix memory allocation issues and seg faults

llama.cpp

commit 483bab2e3d4a868fe679d8bb32827d2a4df214dc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 23:22:01 2023 +0200

    Avoid the transposed X branch in the Z = X * Y matrix multiplication (#439)
    
    Should make results reproducible for different number of threads and batch sizes

llama.cpp

commit 404e1da38ec8025707031a8027da14dc1590f952
Author: Jed Fox <git@jedfox.com>
Date:   Thu Mar 23 16:42:52 2023 -0400

    Fix quantize script not finding models in parent directory (#428)

quantize.py

commit 4cc053b6d5e9df7ac21fa06b7208a70c156d4d7a
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 22:39:44 2023 +0200

    Remove oboslete command from Docker script

.devops/tools.sh

commit 0ba5a3a9a5efedb1aeecbbc70a4e9825542472d5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 22:32:02 2023 +0200

    Obsolete

download-pth.py

commit 2e17dfd80a473099dacc0f41c9146d233c6a5972
Author: rabidcopy <rabidcopy@yahoo.com>
Date:   Thu Mar 23 15:22:47 2023 -0500

    Replace EOS with newline to prevent context/memory being flushed by EOS in interactive mode (#333)
    
    * Improve interactive mode's coherence after EOS
    
    Aims to improve coherence and ability to resume the interactive session when the user is given input back after an end of text token is reached.
    Not sure what token 13 is or why it seems to help. See conversation for examples.
    
    * Make newline token a constant
    
    * dynamically determine newline token
    
    * relocate previous newline token const
    
    * cleanup whitespace
    
    * print a new line on end of text in interactive
    
    this may need to be looked into further when not using a reverse prompt
    
    * only print manual newline with reverse prompt
    
    fix formatting of reverse prompts so they don't end up at the end of the current line while not introducing unnecessary new lines otherwise
    
    * alternate approach to replace end of text tokens
    
    * Inject the reverse prompt again after eos in interactive mode
    
    * tokenize reverse prompt when needed
    
    makes this PR compatible with https://github.com/ggerganov/llama.cpp/pull/330
    
    * tokenize and inject only first reverse prompt
    
    thanks to tjohnman
    
    * tokenize first reverse prompt once
    
    * add newline token
    
    * add newline token
    
    * tokenize/inject reverse prompt for refactor
    
    this doesn't seem right though
    
    * tokenize nothing for antiprompt if no reverse
    
    * Update main.cpp
    
    * Update main.cpp
    
    * tokenize and inject reverse prompt as needed
    
    this doesn't seem to work if the reverse prompt is tokenized outside earlier on
    
    * not needed
    
    * remove newline token
    
    * remove newline token
    
    * tokenize newline token
    
    * add space to comment
    
    * Update main.cpp
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    ---------
    
    Co-authored-by: Slaren <2141330+slaren@users.noreply.github.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp

commit 20a1a4e09c522a80e2a0db51643d25fa38326065
Author: Timmy Knight <r2d2fish@gmail.com>
Date:   Thu Mar 23 10:18:13 2023 -1000

    Fix GPTQ converter (#423)
    
    * Fix GPTQ converter
    
    * Fix comment
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-gptq-to-ggml.py

commit ad072fc5ad6f6905a7224ff6ea07c0644aa075b1
Author: nusu-github <29514220+nusu-github@users.noreply.github.com>
Date:   Fri Mar 24 05:16:48 2023 +0900

    Generate library with CMake (#430)
    
    * Generate library with CMake
    
    BUILD_SHARED_LIBS to allow llama library to be generated.
    
    * Turn ON PIC when BUILD_SHARED_LIBS is ON

CMakeLists.txt

commit ea10d3ded2994106596ddf8e4ed02741b3e053e6
Author: anzz1 <anzz1@live.com>
Date:   Thu Mar 23 19:54:28 2023 +0200

    Command line args bounds checking (#424)
    
    * command line args bounds checking
    
    * unknown and invalid param exit codes 0 -> 1

utils.cpp

commit a18c19259a3cb9dec332d613e8f15704f678a468
Author: Ben Siraphob <bensiraphob@gmail.com>
Date:   Wed Mar 22 00:37:02 2023 -0500

    Fix Nix build

flake.nix

commit a50e39c6fe36be3de0941b3c05aaf9c37912fd47
Author: Stephan Walter <stephan@walter.name>
Date:   Thu Mar 23 14:15:48 2023 +0000

    Revert "Delete SHA256SUMS for now" (#429)
    
    * Revert "Delete SHA256SUMS for now (#416)"
    
    This reverts commit 8eea5ae0e5f31238a97c79ea9103c27647380e37.
    
    * Remove ggml files until they can be verified
    * Remove alpaca json
    * Add also model/tokenizer.model to SHA256SUMS + update README
    
    ---------
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

README.md
SHA256SUMS

commit a140219e81cfb80356438112cd2290d701b282bb
Author: Kerfuffle <44031344+KerfuffleV2@users.noreply.github.com>
Date:   Thu Mar 23 05:41:32 2023 -0600

    Fix Makefile echo escape codes (by removing them). (#418)

Makefile

commit 8a3e5ef801339e57b9b0449220e9ffb11a6648e2
Author: Gary Mulder <gjmulder@gmail.com>
Date:   Thu Mar 23 11:30:40 2023 +0000

    Move model section from issue template to README.md (#421)
    
    * Update custom.md
    
    * Removed Model section as it is better placed in README.md
    
    * Updates to README.md model section
    
    * Inserted text that was removed from  issue template about obtaining models from FB and links to papers describing the various models
    
    * Removed IPF down links for the Alpaca 7B models as these look to be in the old data format and probably shouldn't be directly linked to, anyway
    
    * Updated the perplexity section to point at Perplexity scores #406 discussion

.github/ISSUE_TEMPLATE/custom.md
README.md

commit 8eea5ae0e5f31238a97c79ea9103c27647380e37
Author: anzz1 <anzz1@live.com>
Date:   Thu Mar 23 12:26:19 2023 +0200

    Delete SHA256SUMS for now (#416)
    
    Delete this for now to avoid confusion since it contains some wrong checksums from the old tokenizer format
    Re-add after #374 is resolved

SHA256SUMS

commit 93208cfb929c2323e5d2ac6bf354e278040e70ed
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 10:46:58 2023 +0200

    Adjust repetition penalty ..

README.md

commit 03ace14cfd68a1289ac3b76563534c8ee72a2e53
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 23 09:48:51 2023 +0200

    Add link to recent podcast about whisper.cpp and llama.cpp

README.md

commit e4412b45e395981068d2850d3fa04cc16c77d70d
Author: anzz1 <anzz1@live.com>
Date:   Thu Mar 23 04:20:34 2023 +0200

    CI: CMake: Separate build and test steps (#376)
    
    * CI: Separate Build and Test steps (CMake)
    
    * CI: Make sure build passes before running tests (CMake)
    
    * CI: Standardise step id names

.github/workflows/build.yml

commit f7dc43bc0d759732815856183246f167111587ad
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Thu Mar 23 01:30:23 2023 +0100

    Fix instruct mode broken by PR #354 (#409)
    
    Co-authored-by: Johnman <tjohnman@github>

main.cpp

commit ee8a7887865a893be208e0a92d6d94d2cb66a789
Author: Gary Mulder <gjmulder@gmail.com>
Date:   Wed Mar 22 19:06:18 2023 +0000

    Update issue template so people will use it (#404)

.github/ISSUE_TEMPLATE/custom.md

commit 69c92298a9e36dc2363b3bf50452976ce49487b3
Author: Stephan Walter <stephan@walter.name>
Date:   Wed Mar 22 17:29:06 2023 +0000

    Deduplicate q4 quantization functions (#383)
    
    * Deduplicate q4 quantization functions
    
    * Use const; add basic test
    
    * Re-enable quantization test
    
    * Disable AVX2 flags in CI
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.github/workflows/build.yml
ggml.c
ggml.h
tests/CMakeLists.txt
tests/test-quantize.c

commit 97940520e8fd49c56bb29b71cc350190b723513f
Author: Valentyn Bezshapkin <61702053+valentynbez@users.noreply.github.com>
Date:   Wed Mar 22 18:20:25 2023 +0100

    fix: add POSIX functionality for Linux compilation (#51)
    
    * fix: add POSIX functionality for Linux compilation
    
    * fix: older standard for compatibility

ggml.c

commit 305ba6f0e6daa3796aad9dd18053a1945dd4cc58
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Wed Mar 22 18:16:35 2023 +0100

    Don't force immediate interactive without `-i` (#354)
    
    * Don't force immediate interactive without -i
    
    Sometimes we might want to use a reverse prompt but we want to let the
    model generate tokens right after the initial prompt. So we don't force
    user input mode if the -i flag wasn't specified and instead let it run
    until we encounter the reverse prompt.
    
    This gives use some more flexibility, since it doesn't force the user to
    enter a newline if they want to let the model generate text right after
    the initial prompt and only be asked for input if the reverse prompt is
    encountered.
    
    The `--interactive-first` flag is reintroduced to force the old
    behavior. `-r` behaves like `-i` plus introduces a reverse prompt (it
    can be specified more than once).
    
    * Update help output.
    
    ---------
    
    Co-authored-by: Johnman <tjohnman@github>

main.cpp
utils.cpp
utils.h

commit 4122dffff958cd137175b58f1f27c0913528d7ba
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Wed Mar 22 17:37:10 2023 +0100

    cmake: make llama an actual library (#392)

CMakeLists.txt

commit 56e659a0b271436e24813a801640d015e7b05328
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Wed Mar 22 17:09:38 2023 +0100

    fix perplexity after c-api refactor (#390)
    
    * preallocate a buffer of fitting size for tokenization (utils.cpp)
    
    * don't create a new std::string (especially here, where it's usually large)

main.cpp
utils.cpp

commit 40ea807a972ec7b5a426f034ebfa593b5e7a06ed
Author: Gary Linscott <glinscott@gmail.com>
Date:   Wed Mar 22 08:53:54 2023 -0700

    Add details on perplexity to README.md (#395)

README.md

commit d5850c53ca179b9674b98f35d359763416a3cc11
Author: Yusuf Kağan Hanoğlu <hanoglu@yahoo.com>
Date:   Wed Mar 22 11:55:45 2023 +0300

    Add missing header for memcpy (#386)
    
    fixed: memcpy is not defined

llama.cpp

commit ae44e23ee36c02da0e37ab508a4b473ace724f8e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 22 07:47:15 2023 +0200

    When seed <= 0 - use the clock to generate one

main.cpp
utils.cpp

commit 928480ef5b7b03d7a07e98286aebe3d8b24457d9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 22 07:45:00 2023 +0200

    Init llama_context_params properly from CLI (#370)

llama.cpp
main.cpp

commit 56817b1f882b1894daa4051d0de0bf9a0926d315
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 22 07:34:02 2023 +0200

    Remove temporary notice and update hot topics

README.md

commit f5a77a629bd0f37ae1696747633ab42a5530ec15
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Wed Mar 22 07:32:36 2023 +0200

    Introduce C-style API (#370)
    
    * Major refactoring - introduce C-style API
    
    * Clean up
    
    * Add <cassert>
    
    * Add <iterator>
    
    * Add <algorithm> ....
    
    * Fix timing reporting and accumulation
    
    * Measure eval time only for single-token calls
    
    * Change llama_tokenize return meaning

CMakeLists.txt
Makefile
convert-pth-to-ggml.py
ggml.c
ggml.h
llama.cpp
llama.h
main.cpp
models/ggml-vocab.bin
quantize.cpp
tests/CMakeLists.txt
tests/test-tokenizer-0.cpp
utils.cpp
utils.h

commit da0e9fe90ccf6e73597eb19dd0cfc0a28363fb3b
Author: Gary Mulder <gjmulder@gmail.com>
Date:   Mon Mar 20 20:14:06 2023 +0000

    Add SHA256SUMS file and instructions to README how to obtain and verify the downloads
    
    Hashes created using:
    
    sha256sum models/*B/*.pth models/*[7136]B/ggml-model-f16.bin* models/*[7136]B/ggml-model-q4_0.bin* > SHA256SUMS

README.md
SHA256SUMS

commit e6c9e0986c79ba1cc8848879b2fcce979f9b4672
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 21 23:49:24 2023 +0200

    Fix bin dir for win ci

.github/workflows/build.yml

commit 01a297b09932e29f3319d6588977c32a926c7907
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Tue Mar 21 22:34:25 2023 +0100

    specify build type for ctest on windows (#371)

.github/workflows/build.yml

commit 3366853e41fcc818222a0271c76b6106179106fb
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 22:57:35 2023 +0200

    Add notice about pending change

README.md

commit 3f9c6135e45ae3f520b1e17197004cc60c9ca45b
Author: Mathieu Nayrolles <MathieuNls@users.noreply.github.com>
Date:   Tue Mar 21 16:52:27 2023 -0400

    fix typo in chatLLaMa (#368)
    
    The prompt contains a typo where 'alound' is used instead of 'aloud'.

examples/chatLLaMa

commit 0f6135270839f0715843c4d480c63ae150def419
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 19:47:27 2023 +0200

    Update issue templates

.github/ISSUE_TEMPLATE/custom.md

commit 353ec251a42491f5192c48561da4b444ef67f23c
Author: Fabio R. Sluzala <Fabio3rs@users.noreply.github.com>
Date:   Tue Mar 21 14:21:50 2023 -0300

    We could use std::unordered_map over std::map (#305)
    
    * Improve performance by changing std::map to std::unordered_map and std::map<id, token> id_to_token; to std::vector<token> id_to_token;
    
    * fix last commit on gpt_vocab_init add vocab.id_to_token.resize(vocab.token_to_id.size());
    
    * Removed include <map>
    
    * Nest struct token score inside gpt_vocab
    
    * renamed token to tok

main.cpp
quantize.cpp
utils.cpp
utils.h

commit 89d5d90f3b6d25f134da7a8e252c3432bffcf674
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Tue Mar 21 18:11:01 2023 +0100

    Fix color codes emitting mid-UTF8 code. (#312)

main.cpp

commit 16ffc013c62f22bdaa3cdc022d7a13fd952d73fc
Author: comex <comexk@gmail.com>
Date:   Tue Mar 21 09:42:25 2023 -0700

    Importer for GPTQ quantized LLaMA models (#301)
    
    * [WIP, broken] Importer for GPTQ quantized LLaMA models
    
    Based on: https://github.com/qwopqwop200/GPTQ-for-LLaMa
    
    Current status: Something is busted.  The output starts out decent, but
    quickly degrades into gibberish.  This doesn't happen with either the
    original GPTQ-for-LLaMa using the same weights, or llama.cpp when using
    weights quantized by its own quantizer.  Is there a bug in the
    conversion script that somehow only comes into play with a large context
    size?
    
    I did notice one potential issue.  It's clearly not the main cause of
    the gibberish, since it doesn't happen when using q4_1 weights quantized
    by llama.cpp itself, but it seems concerning.  When doing a matrix
    multiplication of f16 * f32 => f32 or q4_1 * f32 => f32, at least when
    the multiplication is not done with BLAS, the intermediate results are
    stored in the smaller format rather than f32.  This seems like an
    unnecessary waste of precision, especially in the q4_1 case.
    
    I was originally hoping to validate the results by matching the Python
    implementation's output exactly, but precision and non-associativity
    issues make this very difficult, including when performing matrix
    multiplications and, especially, computing norms.
    
    Anyway, design details:
    
    The models being imported store per-layer weights in essentially q4_1
    format, although the addend and scale are shared across an entire row
    rather than every group of 32 weights.  This script duplicates the
    addend and scale to match ggml's expectations, at the cost of wasting
    some memory.
    
    However, there are two differences which I accommodated changing the
    output format (and adding corresponding support to main.cpp) rather than
    having the script match the existing one:
    
    - The tok_embeddings and output weights (i.e. the weights that aren't
      per-layer) are f16 instead of q4_1.  They could be converted to q4_1,
      and the impact of the loss of precision would probably be low, but
      this would rule out exactly matching the Python implementation's
      output for validation.
    
    - There is no sharding, since the input doesn't have it, and for a
      CPU-only implementation it seems more useful to avoid having to deal
      with multiple files.
    
    The new format is differentiated from existing q4_1 format by changing
    the 'f16' header flag to a new value, 4.  That said, I think a cleaner
    approach would be to change main.cpp to support loading each tensor with
    an arbitrary sharding configuration and type rather than hardcoding
    specific combinations of types.  So far I've wasted too much time
    debugging to try implementing this...
    
    * Add missing permutation.  Now it works.
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-gptq-to-ggml.py
main.cpp

commit 486ae645fd3eda8b9d7413d5ff34fb65a3e337fb
Author: Gary Linscott <glinscott@gmail.com>
Date:   Tue Mar 21 09:27:42 2023 -0700

    Compute perplexity over prompt (#270)
    
    * Compute perplexity over prompt
    
    * More accurate perplexity calculation - over all logits in the context window (so 512x more tokens!)
    
    * Output all perplexitiies
    
    * Add timing/ETA

main.cpp
utils.cpp
utils.h

commit 3ab3e6582f7320c2b6568c892fdfc8215caf7e6c
Author: Jean-Christophe Hoelt <hoelt@fovea.cc>
Date:   Tue Mar 21 18:23:15 2023 +0200

    Add chatLLaMa script (#198)
    
    * Add chatLLaMa script
    
    * Fix shellcheck errors and do some cleanup
    
    * Move chatLLaMa script to `examples` directory
    
    * Reduce chatLLaMa context size to 2048
    
    Ref d7def1a7524f712e5ebb7cd02bab0f13aa56a7f9
    
    * Include n_predict to 2048 in examples/chatLLaMa

examples/chatLLaMa

commit f157088cb75f23208abc92b473a132ef3f7a7f15
Author: Alex von Gluck IV <kallisti5@unixzen.com>
Date:   Tue Mar 21 11:21:06 2023 -0500

    makefile: Fix CPU feature detection on Haiku (#218)

Makefile

commit c86ba036e613d46815501a4c6775117c9fc7afce
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 21 18:14:46 2023 +0200

    Enable ANSI colors on Windows 10+ (#311)
    
    * Enable ANSI colors on Windows 10+
    
    On older versions function will silently fail without any ill effects
    
    * Do not call SetConsoleMode if the mode is already set
    
    * Update main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp

commit 1daf4dd71235dbbf537738e7ad53daad8d97586f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 18:10:32 2023 +0200

    Minor style changes

README.md

commit dc6a845b8573cd7d06c6b295241d26f311602a1f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 18:09:37 2023 +0200

    Add chat.sh script

README.md
chat.sh

commit 6a612959e1b6c37b68b6b141329751a2902b1030
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Tue Mar 21 17:05:06 2023 +0100

    Check for reverse prompt by characters instead of tokens (#292) (#330)
    
    * Check for reverse prompt by characters instead of tokens (#292)
    
    * Update main.cpp
    
    Wording.
    
    * Cleanup.
    
    * Remove unnecessary use of std::stringstream.
    
    ---------
    
    Co-authored-by: Johnman <tjohnman@github>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

commit d5f56a5e5a0069329a81f96460221e7afb1daddc
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Tue Mar 21 17:04:43 2023 +0100

    Check for reverse prompt by characters instead of tokens (#292) (#330)
    
    * Check for reverse prompt by characters instead of tokens (#292)
    
    * Update main.cpp
    
    Wording.
    
    * Cleanup.
    
    * Remove unnecessary use of std::stringstream.
    
    ---------
    
    Co-authored-by: Johnman <tjohnman@github>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp

commit 3bfa3b43b7319b71853bfc7d3cf4e9767c24bbc8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 17:59:16 2023 +0200

    Fix convert script, warnings alpaca instructions, default params

README.md
alpaca.sh
convert-pth-to-ggml.py
main.cpp

commit 715d292ee0e34d27f27af43d7feaad1f1344981d
Author: Kevin Lo <kevlo@kevlo.org>
Date:   Tue Mar 21 09:50:09 2023 -0600

    Add OpenBSD support (#314)

Makefile
ggml.c
utils.cpp

commit c98ae02668a25916954b1653e25a5a35ca048d63
Author: Mack Straight <eiz@users.noreply.github.com>
Date:   Tue Mar 21 08:49:43 2023 -0700

    fix typo in comment (#318)

convert-pth-to-ggml.py

commit c3b2306b18a087799acc431e485b8a2e3162cd52
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Tue Mar 21 23:44:11 2023 +0800

    Makefile: slightly cleanup for Mac Intel; echo instead of run ./main -h (#335)

Makefile

commit 975d2cebf97ce888fa0aeee6f5ac774d7135891f
Author: anzz1 <anzz1@live.com>
Date:   Tue Mar 21 17:42:43 2023 +0200

    cmdline option for custom amount of model parts (--n_parts N) (#348)
    
    * cmdline option for custom amount of model parts (--n_parts N)
    
    * Update main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp
utils.cpp
utils.h

commit e0ffc861fae5ac8b40ce973f822d03db02929d36
Author: Kevin Kwok <antimatter15@gmail.com>
Date:   Tue Mar 21 08:34:49 2023 -0700

    Update IPFS links to quantized alpaca with new tokenizer format (#352)

README.md

commit 8f644a0a859938c787d329d27f98e03c58d7df27
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 17:32:14 2023 +0200

    Change default repeat_penalty to 1.0
    
    I feel this penalty is not really helping.
    Especially for the example from the README it makes results pretty bad

utils.h

commit eb34620aeceaf9d9df7fcb19acc17ad41b9f60f8
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 21 17:29:41 2023 +0200

    Add tokenizer test + revert to C++11 (#355)
    
    * Add test-tokenizer-0 to do a few tokenizations - feel free to expand
    * Added option to convert-pth-to-ggml.py script to dump just the vocabulary
    * Added ./models/ggml-vocab.bin containing just LLaMA vocab data (used for tests)
    * Added utility to load vocabulary file from previous point (temporary implementation)
    * Avoid using std::string_view and drop back to C++11 (hope I didn't break something)
    * Rename gpt_vocab -> llama_vocab
    * All CMake binaries go into ./bin/ now

.github/workflows/build.yml
CMakeLists.txt
Makefile
convert-pth-to-ggml.py
main.cpp
models/ggml-vocab.bin
quantize.cpp
tests/CMakeLists.txt
tests/test-tokenizer-0.cpp
utils.cpp
utils.h

commit 2e664f1ff413995506c9a54f3a8d5b8c64e37a91
Author: Casey Primozic <casey@cprimozic.net>
Date:   Tue Mar 21 07:35:42 2023 -0700

    Add initial AVX512 support for dot product on Linux (#320)
    
     * Update Makefile to detect AVX512 support and add compiler flags if it's available
     * Based on existing AVX2 implementation, dot product on one 32-value block of 4-bit quantized ints at a time
     * Perform 8 bit -> 16 bit sign extension and multiply+add on 32 values at time instead of 16
     * Use built-in AVX512 horizontal reduce add to get sum at the end
     * Manual unrolling on inner dot product loop to reduce loop counter overhead

Makefile
ggml.c

commit 8cf9f34eddc124d4ab28f4d2fe8e99d574510bde
Author: nusu-github <29514220+nusu-github@users.noreply.github.com>
Date:   Tue Mar 21 09:37:16 2023 +0900

    Adding missing features of CMakeLists.txt & Refactoring (#131)
    
    * Functionality addition CMakeLists.txt
    
    Refactoring:
    1. Simplify more options that are negation of negation.
    LLAMA_NO_ACCELERATE -> LLAMA_ACCELERATE
    2. Changed to an optional expression instead of forcing to enable AVX2 in MSVC.
    3. Make CMAKE_CXX_STANDARD, which is different from Makefile, the same.
    4. Use add_compile_options instead of adding options to CMAKE_C_FLAGS.
    5. Make utils use target_link_libraries instead of directly referencing code.
    
    Added features:
    1. Added some options.
    LLAMA_STATIC_LINK,LLAMA_NATIVE,LLAMA_LTO,LLAMA_GPROF,LLAMA_OPENBLAS
    
    * Fix Accelerate link in CMake
    
    * Windows build Fix
    
    * C++11 to C++17
    
    * Reflects C/C++ standard individually
    
    * Change the version to 3.12
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

CMakeLists.txt

commit bd4b46d6ba504b99c936f43fc014529adffb6048
Author: Ben Siraphob <bensiraphob@gmail.com>
Date:   Mon Mar 20 16:44:30 2023 -0500

    Nix flake: set meta.mainProgram to llama

flake.nix

commit 6b6d5b5024faaf82019d08cde5e8a9d69c6ca316
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Tue Mar 21 03:33:10 2023 +0800

    Fixed tokenizer.model not found error when model dir is symlink (#325)

convert-pth-to-ggml.py

commit a791a68b613b162c88a83f5f0225223bc167c762
Author: Mack Straight <eiz@users.noreply.github.com>
Date:   Mon Mar 20 12:26:01 2023 -0700

    move file magic/version to header, print expected version (#319)

main.cpp
quantize.cpp
utils.h

commit 0f1b21cb90ac6b84a9af70cafb8e13b5389e3b32
Author: Bernat Vadell <hounter.caza@gmail.com>
Date:   Mon Mar 20 18:05:20 2023 +0100

    Docker - Fix publish docker image in GitHub Registry (#235)
    
    * fix publish permission
    
    * try to fix docker pipeline using as password github_token & username repository_owner

.github/workflows/docker.yml

commit 074bea2eb1f1349a0118239c4152914aecaa1be4
Author: Mack Straight <eiz@users.noreply.github.com>
Date:   Mon Mar 20 03:17:23 2023 -0700

    sentencepiece bpe compatible tokenizer (#252)
    
    * potential out of bounds read
    
    * fix quantize
    
    * style
    
    * Update convert-pth-to-ggml.py
    
    * mild cleanup
    
    * don't need the space-prefixing here rn since main.cpp already does it
    
    * new file magic + version header field
    
    * readme notice
    
    * missing newlines
    
    Co-authored-by: slaren <2141330+slaren@users.noreply.github.com>

Makefile
README.md
convert-pth-to-ggml.py
main.cpp
quantize.cpp
utils.cpp
utils.h

commit 5cb63e2493c49bc2c3b9b355696e8dc26cdd0380
Author: Stephan Walter <stephan@walter.name>
Date:   Mon Mar 20 08:24:11 2023 +0000

    Add tqdm to Python requirements (#293)
    
    * Add tqdm to Python requirements
    * Remove torchvision torchaudio, add requests

.devops/full.Dockerfile

commit da5303c1ea68aa19db829c634f1e10d08d409680
Author: cocktailpeanut <121128867+cocktailpeanut@users.noreply.github.com>
Date:   Sun Mar 19 17:44:20 2023 -0400

    bugfix: default should not be interactive (#304)

main.cpp

commit 4545539d718cf88f4c3a76669b8ac2e26cd8a1e5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 21:58:51 2023 +0200

    Rename script

alpaca.sh

commit edeba283665591f2f726024a92efe4b0b40434b3
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 21:57:28 2023 +0200

    Add temporary helper script for Alpaca chat

chat.sh

commit 5c19c70ba631a8f5d54feb6634e0eea178911a84
Author: Rickey Bowers Jr <bitRAKE@gmail.com>
Date:   Sun Mar 19 13:44:30 2023 -0600

    fix coloring of last `n_batch` of prompt, and refactor line input (#221)
    
    * fix coloring of last `n_batch` of prompt, and refactor line input
    * forgot the newline that needs to be sent to the model
    * (per #283) try to force flush of color reset in SIGINT handler

main.cpp

commit 24568371ae0d7caf85164abe4753f36a7dba0288
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Sun Mar 19 20:33:06 2023 +0100

    Support for multiple reverse prompts. (#299)
    
    Co-authored-by: Johnman <>
    Co-authored-by: Johnman <tjohnman@github>

main.cpp
utils.cpp
utils.h

commit 7392f1cd2cef4dfed41f4db7c4160ab86c0dfcd9
Author: Suaj Carrot <72162667+SuajCarrot@users.noreply.github.com>
Date:   Sun Mar 19 12:38:44 2023 -0600

    Improved quantize script (#222)
    
    * Improved quantize script
    
    I improved the quantize script by adding error handling and allowing to select many models for quantization at once in the command line. I also converted it to Python for generalization as well as extensibility.
    
    * Fixes and improvements based on Matt's observations
    
    Fixed and improved many things in the script based on the reviews made by @mattsta. The parallelization suggestion is still to be revised, but code for it was still added (commented).
    
    * Small fixes to the previous commit
    
    * Corrected to use the original glob pattern
    
    The original Bash script uses a glob pattern to match files that have endings such as ...bin.0, ...bin.1, etc. That has been translated correctly to Python now.
    
    * Added support for Windows and updated README to use this script
    
    New code to set the name of the quantize script binary depending on the platform has been added (quantize.exe if working on Windows) and the README.md file has been updated to use this script instead of the Bash one.
    
    * Fixed a typo and removed shell=True in the subprocess.run call
    
    Fixed a typo regarding the new filenames of the quantized models and removed the shell=True parameter in the subprocess.run call as it was conflicting with the list of parameters.
    
    * Corrected previous commit
    
    * Small tweak: changed the name of the program in argparse
    
    This was making the automatic help message to be suggesting the program's usage as being literally "$ Quantization Script [arguments]". It should now be something like "$ python3 quantize.py [arguments]".

README.md
quantize.py
quantize.sh

commit ad5fd5b60cfdfbfb22b0f2bc9e9f6c9692768f8d
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Sun Mar 19 19:36:19 2023 +0100

    Make prompt randomization optional. (#300)
    
    Co-authored-by: Johnman <>

main.cpp
utils.cpp
utils.h

commit 368d0c8a9ebae16a20e1c8971b21ee888bdefad5
Author: tjohnman <tjohnman@users.noreply.github.com>
Date:   Sun Mar 19 19:31:17 2023 +0100

    Respect the maximum number of tokens in interactive. (#298)
    
    Co-authored-by: Johnman <johnman@github>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp

commit 50fae10d0339f2bd639f69dd679c0201d939a265
Author: slaren <2141330+slaren@users.noreply.github.com>
Date:   Sun Mar 19 19:22:48 2023 +0100

    Add --ignore-eos parameter (#181)
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp
utils.cpp
utils.h

commit 084e2f0ec081c929343d44b09df07ae87cd1ed32
Author: Qingyou Meng <meng.qingyou@gmail.com>
Date:   Mon Mar 20 02:10:00 2023 +0800

    interactive mode: print '\n' in sigint_handler, this flush stdout thus ensure color reset. (#283)

main.cpp

commit 0b366e735729327476ec31da02de3c9c9771ddfb
Author: Erik Scholz <Green-Sky@users.noreply.github.com>
Date:   Sun Mar 19 18:57:00 2023 +0100

    Command line switch to use F16 for memory_k and memory_v (refactor of #154) (#294)
    
    * Use F16 for memory_k and memory_v
    
    * add command line switch to use f16 instead of f32 for memory k+v
    
    ---------
    
    Co-authored-by: Ty Everett <ty@tyweb.us>

main.cpp
utils.cpp
utils.h

commit 160bfb217da5038ccbd74438f9f16a16012d7866
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 19:51:55 2023 +0200

    Update hot topics to mention Alpaca support

README.md

commit c494ed5b94b429d3d73721235e78c9f5fa6e5652
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 19:46:32 2023 +0200

    Fix off-by-one bug (#115)

main.cpp

commit c1c7026b470ced0b8a6c67e968c04bb47864def1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 19:33:18 2023 +0200

    Fix python stuff (#109)

convert-pth-to-ggml.py

commit 467b149761cd63248b00d6ffb204d50a4cbb451a
Author: qunash <anzoria@gmail.com>
Date:   Sun Mar 19 20:17:39 2023 +0300

    Refactoring `convert-pth-to-ggml.py`: more concise and readable (#109)
    
    * Refactor get_n_parts function to simplify code and improve readability
    
    * Use f-strings instead of concatenation
    
    * Refactoring: more concise and readable
    
    * modularize
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

convert-pth-to-ggml.py

commit 70f01cb8632f73b5cf70428608b89cd3c0775d23
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 19:04:44 2023 +0200

    Drop trailing new line from file prompts (#80)

main.cpp
utils.cpp

commit a4e63b73dfa1894387926cc8072b5f36deebf0a5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 18:49:50 2023 +0200

    Add instruction for using Alpaca (#240)

README.md

commit 9e1707218a24ff758c7b623594f8c0ce5e12eb6c
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 18:37:02 2023 +0200

    Add "--instruct" argument for usage with Alpaca (#240)
    
    Also start adding prompts in "./prompts"

main.cpp
prompts/alpaca.txt
prompts/chat-with-bob.txt
utils.cpp
utils.h

commit 22213a17b56336bbea384a572a9484ce208c0333
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 19 17:30:00 2023 +0200

    Change RMSNorm eps to 1e-6 (#173)
    
    I think this is what is used in the Python code

ggml.c

commit d7def1a7524f712e5ebb7cd02bab0f13aa56a7f9
Author: Ronsor <ronsor@ronsor.pw>
Date:   Sat Mar 18 17:10:47 2023 -0700

    Warn user if a context size greater than 2048 tokens is specified (#274)
    
    LLaMA doesn't support more than 2048 token context sizes, and going above that produces terrible results.

main.cpp

commit 6f61c18ec9a30416e21ed5abfb1321bdb14979be
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sat Mar 18 22:39:46 2023 +0100

    Fix typo in readme

README.md

commit 1e5a6d088d0f3a967c6e86298a756daec9e8df12
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sat Mar 18 22:20:04 2023 +0100

    Add note about Python 3.11 to readme

README.md

commit 554b54152145c30618bac171efb712cf4a7d1e96
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sat Mar 18 21:58:46 2023 +0100

    Add memory/disk requirements to readme

README.md

commit d3f202d57b694376cef6f381a6b6901825c3f6d9
Author: Alex Nguyen <tiendung@users.noreply.github.com>
Date:   Sat Mar 18 20:51:49 2023 +0700

    Remove unused code since n_vocab is model.hparams.n_vocab (#262)

main.cpp

commit e03e359730c127f888fcf00e93375771bc0a3500
Author: Justin Suess <justin.suess@westpoint.edu>
Date:   Sat Mar 18 07:44:09 2023 -0400

    fixed warning with std::ignore about unused function result (#151)
    
    fixed warning with std::ignore about unused function result

main.cpp

commit a81d0c2a171a4446e6a21a3ec74a0c0768d71184
Author: Gary Linscott <glinscott@gmail.com>
Date:   Sat Mar 18 04:17:19 2023 -0700

    Fix n^2 loop in tokenization (#254)
    
    This causes long prompts to parse very slowly.

utils.cpp

commit b2de7f18dfbb93463eeb5b4392117bbe82d5bd1b
Author: anzz1 <anzz1@live.com>
Date:   Sat Mar 18 09:27:12 2023 +0200

    CI Improvements (#230)
    
    * CI Improvements
    
    Manual build feature, autoreleases for Windows
    
    * better CI naming convention
    
    use branch name in releases and tags

.github/workflows/build.yml

commit a29274789309029fd88a9465e6d0832d4632272b
Author: Niklas Korz <niklas@niklaskorz.de>
Date:   Fri Mar 17 23:03:48 2023 +0100

    Nix flake (#40)
    
    * Nix flake
    
    * Nix: only add Accelerate framework on macOS
    
    * Nix: development shel, direnv and compatibility
    
    * Nix: use python packages supplied by withPackages
    
    * Nix: remove channel compatibility
    
    * Nix: fix ARM neon dotproduct on macOS
    
    ---------
    
    Co-authored-by: Pavol Rusnak <pavol@rusnak.io>

.gitignore
flake.lock
flake.nix

commit c9f670a17755311aa28c411f5c7f3c8c05434770
Author: thement <40525767+thement@users.noreply.github.com>
Date:   Fri Mar 17 21:05:58 2023 +0100

    Implement non-greedy tokenizer that tries to maximize token lengths (#242)
    
    * Implement non-greedy tokenizer that tries to maximize token lengths
    
    * Insert single space in front of the prompt
    
    - this is to match original llama tokenizer behavior
    
    ---------
    
    Co-authored-by: Jakub Horak <jakub.horak@ibawizard.net>

main.cpp
utils.cpp

commit 4f546091102a418ffdc6230f872ac56e5cedb835
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 17 21:46:46 2023 +0200

    Default to 4 threads (#243)

utils.h

commit e81b9c81c101f64531ef0fa1ee6b77d562635652
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 17 20:30:04 2023 +0200

    Update Contributing section

README.md

commit 367946c668757532deed929e1d78673c6ac6bcb8
Author: Stephan Walter <stephan@walter.name>
Date:   Fri Mar 17 17:47:35 2023 +0000

    Don't tell users to use a bad number of threads (#243)
    
    The readme tells people to use the command line option "-t 8", causing 8
    threads to be started. On systems with fewer than 8 cores, this causes a
    significant slowdown. Remove the option from the example command lines
    and use /proc/cpuinfo on Linux to determine a sensible default.

.devops/tools.sh
README.md
ggml.c
utils.cpp
utils.h

commit 6b0df5ccf360fe5c015f6607f0375bfc6849005e
Author: mmyjona <jonathan.gonse@gmail.com>
Date:   Sat Mar 18 00:38:24 2023 +0800

    add ptread link to fix cmake build under linux (#114)
    
    * add ptread link to fix cmake build under linux
    
    * add cmake to linux and macos platform
    
    * separate make and cmake workflow
    
    ---------
    
    Co-authored-by: Sebastián A <sebastian.aedo29@gmail.com>

.github/workflows/build.yml
CMakeLists.txt

commit 2af23d30434a677c6416812eea52ccc0af65119c
Author: Bernat Vadell <hounter.caza@gmail.com>
Date:   Fri Mar 17 10:47:06 2023 +0100

    🚀 Dockerize llamacpp (#132)
    
    * feat: dockerize llamacpp
    
    * feat: split build & runtime stages
    
    * split dockerfile into main & tools
    
    * add quantize into tool docker image
    
    * Update .devops/tools.sh
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>
    
    * add docker action pipeline
    
    * change CI to publish at github docker registry
    
    * fix name runs-on macOS-latest is macos-latest (lowercase)
    
    * include docker versioned images
    
    * fix github action docker
    
    * fix docker.yml
    
    * feat: include all-in-one command tool & update readme.md
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

.devops/full.Dockerfile
.devops/main.Dockerfile
.devops/tools.sh
.dockerignore
.github/workflows/build.yml
.github/workflows/docker.yml
README.md
convert-pth-to-ggml.py
download-pth.py

commit 904d2a8d6acd667c9633138d45a361d40fbf76d0
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Fri Mar 17 05:48:39 2023 +0100

    Q4_1 quantization (#193)
    
    * Add AVX2 version of ggml_vec_dot_q4_1
    
    * Small optimisations to q4_1 dot product (@Const-me)
    
    * Rearrange Q4_1 quantization to work for multipart models. (Fix #152)
    
    * Fix ggml_vec_mad_q4_1 too
    
    * Fix non-vectorised q4_1 vec mul

ggml.c
utils.cpp

commit 721311070e31464ac12bef9a4444093eb3eaebf7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 16 15:00:09 2023 +0200

    Update README.md

README.md

commit ac15de789547e5a6e93df552e787379b3a23ef26
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 16 08:55:13 2023 +0200

    Expand "Contributing" section

README.md

commit 273abc47ff9dd899b3c4f58acd19d4649e90d6b4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Thu Mar 16 07:12:12 2023 +0200

    Update hot topics - RMSnorm

README.md

commit 9b4a15b17d8395eb075379b140fcd0b0283f4ef6
Author: Nebula <infinitewormhole@gmail.com>
Date:   Wed Mar 15 19:29:25 2023 -0400

    Fix RMS norm in GGML (#191)

ggml.c

commit 6eac39ba953acaeec396cea2969dbf413907e2ec
Author: hoangmit <hoangmit@users.noreply.github.com>
Date:   Wed Mar 15 18:41:38 2023 -0400

    Add RMS norm and use it (#187)
    
    * add ggml_rms_norm
    
    * update op num

ggml.c
ggml.h
main.cpp

commit 27944c4206a49bbe003021a2610bacaa3044e619
Author: moritzbrantner <31051084+moritzbrantner@users.noreply.github.com>
Date:   Wed Mar 15 21:35:25 2023 +0100

    fixed typo (#178)

README.md

commit 2d15d6c9a959749f954d4fbbf44d711e19c5bdff
Author: Rickey Bowers Jr <bitRAKE@gmail.com>
Date:   Wed Mar 15 13:56:24 2023 -0600

    add SIGINT support for _WIN32 environments (#120)
    
    * add SIGINT support for _WIN32 environments
    
    * perhaps more consistent

main.cpp

commit 2d64715ad475f192a4004a52d134c67ccb6f44ad
Author: Justin Suess <justin.suess@westpoint.edu>
Date:   Wed Mar 15 15:42:40 2023 -0400

    added ctx_size parameter (#148)
    
    * added ctx_size parameter
    
    * added it in more places
    
    * Apply suggestions from code review
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp
utils.cpp
utils.h

commit 16b2c61a22f828ea77d9f084ca871c63bc5cc283
Author: Justin Suess <justin.suess@westpoint.edu>
Date:   Wed Mar 15 15:39:38 2023 -0400

    fixed color reset on exit (#149)
    
    * fixed color reset on exit
    
    * added sigint handler for ansi_color_reset
    
    * Update main.cpp
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp

commit 977295c700a2952c18400026d57467077dcd1a20
Author: Musab Gultekin <musabgultekin@users.noreply.github.com>
Date:   Wed Mar 15 22:39:06 2023 +0300

    Fix potential licensing issue (#126)
    
    * Update README.md
    
    * Update README.md
    
    remove facebook

README.md

commit 956dfda8ad8cea7961e22e0384bbc315bf79aed2
Author: Ronsor <ronsor@ronsor.pw>
Date:   Wed Mar 15 12:37:50 2023 -0700

    Use `tokenizer.vocab_size()` instead of hardcoding 32000 in convert-pth-to-ggml.py (#142)
    
    There are ways that special tokens or other new tokens could be added to the tokenizer; therefore it's probably best not to assume the vocabulary is only 32000 tokens.

convert-pth-to-ggml.py

commit 113e685d18ac4edb20f647fd34b000941556f6a6
Author: hoangmit <hoangmit@users.noreply.github.com>
Date:   Wed Mar 15 15:05:14 2023 -0400

    inline -> static inline for "bytesFromNibbles" (#161)
    
    Without "static" prefix, it fails to compile in clang

ggml.c

commit 47857e564c218a2c38346d0cdd94314632878fcb
Author: Ronsor <ronsor@ronsor.pw>
Date:   Tue Mar 14 12:34:37 2023 -0700

    Don't use vdotq_s32 if it's not available (#139)
    
    * Don't use vdotq_s32 if it's not available
    
    `dotprod` extensions aren't available on some ARM CPUs (e.g. Raspberry Pi 4), so check for them and only use them if they're available.
    
    Reintroduces the code removed in 84d9015 if `__ARM_FEATURE_DOTPROD` isn't defined.
    
    * Update ggml.c
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

ggml.c

commit 60f819a2b10475055a36415bc489e5b55df2d052
Author: Radoslav Gerganov <rgerganov@gmail.com>
Date:   Tue Mar 14 15:30:08 2023 +0200

    Add section to README on how to run the project on Android (#130)

README.md
models/.gitignore

commit 97ab2b257897bfe7e2ae72876a3e50ed41b8c7ce
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Tue Mar 14 09:43:52 2023 +0200

    Add Misc section + update hot topics + minor fixes

README.md

commit 2f700a27381e558a4eb5a3f8fd56757f4c7a417c
Author: Sebastián A <sebastian.aedo29@gmail.com>
Date:   Mon Mar 13 17:29:10 2023 -0300

    Add windows to the CI (#98)

.github/workflows/build.yml

commit c09a9cfb06c87d114615c105adda91b0e6273b69
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 21:22:15 2023 +0200

    CMake build in Release by default (#75)

CMakeLists.txt

commit 7ec903d3c162417c11463f14ad5b773a918fb7f1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 19:21:51 2023 +0200

    Update contribution section, hot topics, limitations, etc.

README.md

commit 4497ad819c0010a8b19ffeaf8c0428eb7558d3e0
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 19:15:08 2023 +0200

    Print system information

main.cpp

commit ed6849cc07a8973e5d31947b9df2df2da975ac96
Author: Sebastián A <sebastian.aedo29@gmail.com>
Date:   Mon Mar 13 14:12:33 2023 -0300

    Initial support for CMake (#75)

CMakeLists.txt

commit 41be0a3b3d76ee4f254dc81b42bd8ed26ee324e7
Author: Thomas Klausner <wiz@gatalith.at>
Date:   Mon Mar 13 17:40:54 2023 +0100

    Add NetBSD support. (#90)

Makefile
ggml.c
utils.cpp

commit 671d5cac15241b495006f56482bf2d6967dca91f
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Mon Mar 13 17:39:56 2023 +0100

    Use fprintf for diagnostic output (#48)
    
    keep printf only for printing model output
    
    one can now use ./main ... 2>dev/null to suppress any diagnostic output

main.cpp

commit 84d9015c4a91ab586ba65d5bd31a8482baf46ba1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 18:36:44 2023 +0200

    Use vdotq_s32 to improve performance (#67)
    
    * 10% performance boost on ARM
    
    * Back to original change

ggml.c

commit 63fd76fbb06f9b723ca11505352387a3148b1814
Author: uint256_t <konndennsa@gmail.com>
Date:   Tue Mar 14 01:33:43 2023 +0900

    Reduce model loading time (#43)
    
    * Use buffering
    
    * Use vector
    
    * Minor
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp

commit 2a20f48efad692a8c2744f10c673bbdbe0c751b7
Author: Val Kharitonov <mail@kharvd.com>
Date:   Mon Mar 13 12:24:18 2023 -0400

    Fix UTF-8 handling (including colors) (#79)

convert-pth-to-ggml.py
main.cpp

commit d1f224712d78ab2cbb78777acfeb6739f660eb96
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Mon Mar 13 17:15:20 2023 +0100

    Add quantize script for batch quantization (#92)
    
    * Add quantize script for batch quantization
    
    * Indentation
    
    * README for new quantize.sh
    
    * Fix script name
    
    * Fix file list on Mac OS
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

README.md
quantize.sh

commit 1808ee0500ea674b4bc2911acd0489ee5cbcef87
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 09:42:26 2023 +0200

    Add initial contribution guidelines

README.md

commit a169bb889cfe7b77a798f04fbc573e67ccb4316a
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Mon Mar 13 04:08:01 2023 +0100

    Gate signal support on being on a unixoid system. (#74)

main.cpp

commit 460c48254098b28d422382a2bbff6a0b3d7f7e17
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Mon Mar 13 00:35:51 2023 +0100

    Fix token count accounting

main.cpp

commit c80e2a8f2adeda202cbffe76ef800f134e51f03f
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 01:28:08 2023 +0200

    Revert "10% performance boost on ARM"
    
    This reverts commit 113a9e83ebc0f788f861394437087bf3ca0e019b.
    
    There are some reports for illegal instruction.
    Moved this stuff to vdotq_s32 branch until resolve

ggml.c

commit 54a0e66ea0ed3248e6c95a070a2da0bf5c6d4817
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 01:21:03 2023 +0200

    Check for vdotq_s32 availability

ggml.c

commit 543c57e991a23121c666561c2837faa09c4a78ca
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 01:05:24 2023 +0200

    Ammend to previous commit - forgot to update non-QRDMX branch

ggml.c

commit 113a9e83ebc0f788f861394437087bf3ca0e019b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Mon Mar 13 00:56:10 2023 +0200

    10% performance boost on ARM

ggml.c

commit 404fac0d623c9eea74ad7a9347da69e33f10984e
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Sun Mar 12 23:07:34 2023 +0100

    Fix color getting reset before prompt output done (#65)
    
    (cherry picked from commit 7eb2987619feee04c40eff69b604017d09919cb6)

main.cpp

commit 1a0a74300f35ad4868715d684d0bc0effdaa9d31
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 23:39:01 2023 +0200

    Update README.md

README.md

commit 96ea727f4780620b60c1897b538654931411a3fd
Author: Matvey Soloviev <blackhole89@gmail.com>
Date:   Sun Mar 12 22:13:28 2023 +0100

    Add interactive mode (#61)
    
    * Initial work on interactive mode.
    
    * Improve interactive mode. Make rev. prompt optional.
    
    * Update README to explain interactive mode.
    
    * Fix OS X build

README.md
main.cpp
utils.cpp
utils.h

commit 966195483549e201cff062096a848d9e9833a1a6
Author: Marc Köhlbrugge <subscriptions@marckohlbrugge.com>
Date:   Mon Mar 13 03:30:08 2023 +0700

    Fix typo in README (#45)

README.md

commit f385f8dee83d1baf59896b2eb09f1524dc9cde45
Author: Ben Garney <bengarney@users.noreply.github.com>
Date:   Sun Mar 12 13:28:36 2023 -0700

    Allow using prompt files (#59)

utils.cpp

commit 02f0c6fe7f9b7be24c7d339aed016e54a92388ea
Author: beiller <beiller@gmail.com>
Date:   Sun Mar 12 16:23:15 2023 -0400

    Add back top_k (#56)
    
    * Add back top_k
    
    * Update utils.cpp
    
    * Update utils.h
    
    ---------
    
    Co-authored-by: Bill Hamilton <bill.hamilton@shopify.com>
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp
utils.cpp
utils.h

commit eb062bb012c4e131818dd757a6d3a757fdee3961
Author: Sebastián A <sebastian.aedo29@gmail.com>
Date:   Sun Mar 12 17:15:00 2023 -0300

    Windows fixes (#31)
    
    * Apply fixes suggested to build on windows
    
    Issue: https://github.com/ggerganov/llama.cpp/issues/22
    
    * Remove unsupported VLAs
    
    * MSVC: Remove features that are only available on MSVC C++20.
    
    * Fix zero initialization of the other fields.
    
    * Change the use of vector for stack allocations.

ggml.c
main.cpp
quantize.cpp
utils.cpp

commit 7027a97837c351e0a7bc48db2027af368de382db
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 22:09:26 2023 +0200

    Update README.md

README.md

commit 2d555e5b42922cda6dfc0c3ff54df7b1ee4d0ff4
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 22:08:24 2023 +0200

    Add CI (#60)

.github/workflows/build.yml

commit 7c9e54e55e4106f84688245fb15207f6df917e12
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 20:59:01 2023 +0200

    Revert "weights_only" arg - this causing more trouble than help

README.md
convert-pth-to-ggml.py

commit b9bd1d014113b7498f04ad4d28e6021d5f4cddad
Author: Oleksandr Nikitin <oleksandr@tvori.info>
Date:   Sun Mar 12 14:16:33 2023 +0200

    python/pytorch compat notes (#44)

README.md
convert-pth-to-ggml.py

commit 129c7d1ea886e52ac1b87ff6184310bab3158806
Author: beiller <beiller@gmail.com>
Date:   Sun Mar 12 05:27:42 2023 -0400

    Add repetition penalty (#20)
    
    * Adding repeat penalization
    
    * Update utils.h
    
    * Update utils.cpp
    
    * Numeric fix
    
    Should probably still scale by temp even if penalized
    
    * Update comments, more proper application
    
    I see that numbers can go negative so a fix from a referenced commit
    
    * Minor formatting
    
    ---------
    
    Co-authored-by: Georgi Gerganov <ggerganov@gmail.com>

main.cpp
utils.cpp
utils.h

commit 702fddf5c5c3c1377e169ba9ecdfed4cb16c268b
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 09:03:25 2023 +0200

    Clarify meaning of hacking

README.md

commit 7d86e25bf648eb369a3a8388bf239b6b19f7a789
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 08:41:54 2023 +0200

    README: add "Supported platforms" + update hot topics

README.md

commit a93120236f99e13d77e4b278e47ffcaad4a899e4
Author: deepdiffuser <112834445+deepdiffuser@users.noreply.github.com>
Date:   Sat Mar 11 22:36:35 2023 -0800

    use weights_only in conversion script (#32)
    
    this restricts malicious weights from executing arbitrary code by restricting the unpickler to only loading tensors, primitive types, and dictionaries

convert-pth-to-ggml.py

commit 6a9a67f0bee5eed67cf8bf03f74f77619da40d3f
Author: Pavol Rusnak <pavol@rusnak.io>
Date:   Sun Mar 12 07:36:03 2023 +0100

    Add LICENSE (#21)

LICENSE

commit da1a4ff01f42d058cfa59806dd5679c0fe5a8604
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sun Mar 12 01:26:32 2023 +0200

    Update README.md

README.md

commit 6b2cb6302ffaf8264e33af1dc52e3ea54003e690
Author: Juraj Bednar <juraj@bednar.io>
Date:   Sat Mar 11 18:32:20 2023 +0100

    Fix a typo in model name (#16)

README.md

commit 4235e3d5b3f4a0e6844f6291322ebb42181345c9
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 18:10:18 2023 +0200

    Update README.md

README.md

commit f1eaff4721153a5a5094fd1bd8cbdae7a3c079cc
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 17:58:18 2023 +0200

    Add AVX2 support for x86 architectures thanks to @Const-me !

README.md
ggml.c

commit a9e58529ea507ac15cd2df4c39d1b9613d6acb6e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 17:40:14 2023 +0200

    Fix un-initialized FP16 tables on x86 (#15, #2)

quantize.cpp

commit 7d9ed7b25fe17db3fc8848b5116d14682864ce8e
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 12:44:21 2023 +0200

    Bump memory buffer

main.cpp

commit 0c6803321c818f3f2da4a0693d20128b0f79ad28
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 12:31:21 2023 +0200

    Update README.md

README.md

commit f60fa9e50afce35e7ebe1fedf34d4a9327353927
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 12:26:46 2023 +0200

    .gitignore models/

.gitignore

commit 7211862c943273fc8ce4b7fdf4c04f9821b7b591
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 12:26:16 2023 +0200

    Update Makefile var + add comment

Makefile
README.md

commit a5c5ae2f545d0d3338f5b2a7840457e35d5bccc1
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 11:34:25 2023 +0200

    Update README.md

README.md

commit ea977e85ecda7b983f0e7b1db20b509998ddc889
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 11:34:11 2023 +0200

    Update README.md

README.md

commit 007a8f6f459c6eb56678fdee4c09219ddb85b640
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 10:47:09 2023 +0200

    Support all LLaMA models + change Q4_0 quantization storage

README.md
convert-pth-to-ggml.py
ggml.c
main.cpp
utils.cpp

commit 5f2f970d51a04b783799bc92fd1d006408269f26
Author: Simon Willison <swillison@gmail.com>
Date:   Fri Mar 10 21:47:26 2023 -0800

    Include Python dependencies in README (#6)

README.md

commit 73c6ed5e8784a20f89d51b1703a09bc690c68227
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 01:30:47 2023 +0200

    Update README.md

README.md

commit 01eeed8fb1437978603a8523c0b8ea2f6280f5d7
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 01:22:58 2023 +0200

    Update README.md

README.md

commit 6da2df34ee40301d9ecb126968ec4c0c6195f26d
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 01:18:10 2023 +0200

    Update README.md

README.md

commit 9dcf4dba459ba6482a7a5aced22645a387ec6991
Author: Jean-Michaël Celerier <jeanmichael.celerier+github@gmail.com>
Date:   Fri Mar 10 18:04:06 2023 -0500

    Add missing headers for memcpy and assert (#3)

utils.cpp

commit 920a7fe2d94cc7e4fed0e88db830b674c91865c5
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 00:55:22 2023 +0200

    Update README.md

README.md

commit 3a57ee59de53c2a9d2c3a2c643b609ce07a58a16
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 00:51:46 2023 +0200

    Update README.md

README.md

commit b85028522d6e924473159ba0da3543fc174d2ded
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Sat Mar 11 00:09:19 2023 +0200

    Update README.md

README.md

commit 8a01f565ff78cc6c0c5a9fa402787a2f179f2d78
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 23:53:11 2023 +0200

    Update README.md

README.md

commit 70bc0b8b15b98dca23b28f0c8f5e34b27e424cda
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 23:46:39 2023 +0200

    Fix a bug in the rope calculation

convert-pth-to-ggml.py
main.cpp
utils.cpp
utils.h

commit 18ebda34d67c05f4f5584a9209e7efb949f5fd56
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 21:52:27 2023 +0200

    Update README.md

README.md

commit 319cdb3e1ffe263cf5b08249c9559e011396c1de
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 21:50:46 2023 +0200

    Final touches

README.md
main.cpp
models/.gitignore
utils.cpp
utils.h

commit 775328064e69db1ebd7e19ccb59d2a7fa6142470
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 21:47:46 2023 +0200

    Create README.md

README.md

commit 26c084662903ddaca19bef982831bfb0856e8257
Author: Georgi Gerganov <ggerganov@gmail.com>
Date:   Fri Mar 10 20:40:58 2023 +0200

    Initial release

.gitignore
Makefile
convert-pth-to-ggml.py
ggml.c
ggml.h
main.cpp
quantize.cpp
utils.cpp
utils.h
