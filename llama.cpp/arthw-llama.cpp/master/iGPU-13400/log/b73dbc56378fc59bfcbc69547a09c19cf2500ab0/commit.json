{"author": "Molly Sophia <mollysophia379@gmail.com>", "date": "2025-01-10 09:58:08", "title": "llama: add support for QRWKV6 model architecture", "pr_id": "11001", "files": ["README.md", "convert_hf_to_gguf.py", "ggml/include/ggml.h", "ggml/src/ggml-cpu/ggml-cpu.c", "ggml/src/ggml-cuda/ggml-cuda.cu", "ggml/src/ggml-cuda/gla.cu", "ggml/src/ggml-cuda/gla.cuh", "ggml/src/ggml-cuda/wkv6.cu", "ggml/src/ggml-sycl/wkv6.cpp", "ggml/src/ggml-vulkan/ggml-vulkan.cpp", "ggml/src/ggml.c", "gguf-py/gguf/constants.py", "gguf-py/gguf/gguf_writer.py", "gguf-py/gguf/tensor_mapping.py", "src/llama-arch.cpp", "src/llama-arch.h", "src/llama-hparams.cpp", "src/llama-hparams.h", "src/llama-model.cpp", "src/llama-model.h", "src/llama-quant.cpp", "src/llama.cpp", "tests/test-backend-ops.cpp"]}