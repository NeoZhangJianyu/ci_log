{"author": "slaren <slarengh@gmail.com>", "date": "2024-03-18 11:03:04", "title": "backend : offload large batches to GPU", "pr_id": "6083", "files": ["examples/imatrix/imatrix.cpp", "examples/llama-bench/llama-bench.cpp", "ggml-alloc.c", "ggml-backend-impl.h", "ggml-backend.c", "ggml-backend.h", "ggml-cuda.cu", "ggml-cuda.h", "ggml-kompute.cpp", "ggml-metal.m", "ggml-sycl.cpp", "ggml-vulkan.cpp", "ggml.c", "llama.cpp"]}