+ gg_run_ctest_release
+ cd /home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp
+ tee /home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp/tmp/ci_log/40643edb86eb10b471b0f57d4f3f7eb0e06a0df7/tmp/results/ctest_release.log
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp/tmp/ci_log/40643edb86eb10b471b0f57d4f3f7eb0e06a0df7/tmp/results/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_SYCL=1 DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DLLAMA_SYCL_F16=ON -DLLAMA_CURL=OFF ..
CMake Warning:
  Ignoring extra path from command line:

   "/home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp/build-ci-release/DCMAKE_C_COMPILER=icx"


-- The C compiler identification is GNU 13.3.0
-- The CXX compiler identification is IntelLLVM 2025.0.4
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /opt/intel/oneapi/compiler/2025.0/bin/icpx - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.43.0") 
CMake Warning at CMakeLists.txt:116 (message):
  LLAMA_SYCL is deprecated and will be removed in the future.

  Use GGML_SYCL instead

Call Stack (most recent call first):
  CMakeLists.txt:128 (llama_option_depr)


CMake Warning at CMakeLists.txt:116 (message):
  LLAMA_SYCL_F16 is deprecated and will be removed in the future.

  Use GGML_SYCL_F16 instead

Call Stack (most recent call first):
  CMakeLists.txt:129 (llama_option_depr)


-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- GGML_SYSTEM_ARCH: x86
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fiopenmp (found version "5.1") 
-- Found OpenMP: TRUE (found version "4.5")  
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- GGML_SYCL_TARGET=INTEL
-- Performing Test SUPPORTS_SYCL
-- Performing Test SUPPORTS_SYCL - Success
-- Using oneAPI Release SYCL compiler (icpx).
-- SYCL found
-- Found IntelSYCL: /opt/intel/oneapi/compiler/2025.0/include (found version "202001") 
-- Found oneDNN: /opt/intel/oneapi/dnnl/2025.0/lib/libdnnl.so.3.6
-- MKL_VERSION: 2025.0.1
-- MKL_ROOT: /opt/intel/oneapi/mkl/2025.0
-- MKL_ARCH: intel64
-- MKL_SYCL_LINK: None, set to ` dynamic` by default
-- MKL_LINK: None, set to ` dynamic` by default
-- MKL_SYCL_INTERFACE_FULL: None, set to ` intel_ilp64` by default
-- MKL_INTERFACE_FULL: None, set to ` intel_ilp64` by default
-- MKL_SYCL_THREADING: None, set to ` tbb_thread` by default
-- MKL_THREADING: None, set to ` intel_thread` by default
-- MKL_MPI: None, set to ` intelmpi` by default
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_scalapack_ilp64.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_cdft_core.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_intel_ilp64.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_intel_thread.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_core.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_blacs_intelmpi_ilp64.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_sycl_blas.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_sycl_lapack.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_sycl_dft.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_sycl_sparse.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_sycl_data_fitting.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_sycl_rng.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_sycl_stats.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_sycl_vm.so
-- Found /opt/intel/oneapi/mkl/2025.0/lib/libmkl_tbb_thread.so
-- Found /opt/intel/oneapi/compiler/2025.0/lib/libiomp5.so
-- Including SYCL backend
-- Configuring done (1.6s)
-- Generating done (0.1s)
-- Build files have been written to: /home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp/build-ci-release

real	0m1.754s
user	0m1.157s
sys	0m0.588s
+ tee -a /home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp/tmp/ci_log/40643edb86eb10b471b0f57d4f3f7eb0e06a0df7/tmp/results/ctest_release-make.log
+ make -j
[  1%] Building CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o
[  1%] Building CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o
[  2%] Building CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target build_info
[  6%] Linking CXX executable ../../bin/llama-minicpmv-cli
[  7%] Linking CXX executable ../../bin/llama-llava-cli
[  7%] Linking CXX executable ../../bin/llama-gemma3-cli
[  7%] Built target sha256
[  7%] Built target sha1
[  7%] Built target llama-llava-cli
[  7%] Built target llama-gemma3-cli
[  7%] Built target llama-minicpmv-cli
[  7%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[  7%] Built target llama-qwen2vl-cli
[  7%] Built target xxhash
[  7%] Linking CXX shared library ../../bin/libggml-base.so
[  7%] Built target ggml-base
[  7%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/ggml-sycl.cpp.o
[  7%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/binbcast.cpp.o
[  8%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/common.cpp.o
[  8%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/conv.cpp.o
[ 10%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/convert.cpp.o
[  9%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/dmmv.cpp.o
[ 10%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/cpy.cpp.o
[ 10%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/concat.cpp.o
[ 10%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/element_wise.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/getrows.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o
[ 12%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/im2col.cpp.o
[ 13%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/gla.cpp.o
[ 14%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/mmvq.cpp.o
[ 14%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/mmq.cpp.o
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 15%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o
[ 17%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/norm.cpp.o
[ 16%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/outprod.cpp.o
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o
[ 17%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/softmax.cpp.o
[ 18%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/rope.cpp.o
[ 18%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o
[ 19%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/sycl_hw.cpp.o
[ 18%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/wkv.cpp.o
[ 20%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o
[ 20%] Building CXX object ggml/src/ggml-sycl/CMakeFiles/ggml-sycl.dir/tsembd.cpp.o
[ 20%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 21%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o
[ 21%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o
[ 22%] Linking CXX shared library ../../bin/libggml-cpu.so
[ 22%] Built target ggml-cpu
[ 22%] Linking CXX shared library ../../../bin/libggml-sycl.so
[ 22%] Built target ggml-sycl
[ 23%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 23%] Linking CXX shared library ../../bin/libggml.so
[ 23%] Built target ggml
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 24%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 24%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o
[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 27%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 27%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 27%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o
[ 28%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-recurrent.cpp.o
[ 29%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 30%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 30%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o
[ 30%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 30%] Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o
[ 30%] Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o
[ 31%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 31%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 32%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 32%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 31%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 32%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-gguf
/home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp/src/llama-vocab.cpp:2576:46: error: no member named 'numeric_limits' in namespace 'std'
 2576 |         if (size >= static_cast<size_t>(std::numeric_limits<int32_t>::max())) {
      |                                         ~~~~~^
/home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp/src/llama-vocab.cpp:2576:61: error: unexpected type name 'int32_t': expected expression
 2576 |         if (size >= static_cast<size_t>(std::numeric_limits<int32_t>::max())) {
      |                                                             ^
/home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp/src/llama-vocab.cpp:2576:71: error: no member named 'max' in the global namespace
 2576 |         if (size >= static_cast<size_t>(std::numeric_limits<int32_t>::max())) {
      |                                                                     ~~^
3 errors generated.
make[2]: *** [src/CMakeFiles/llama.dir/build.make:384: src/CMakeFiles/llama.dir/llama-vocab.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
[ 33%] Built target llama-gguf
[ 34%] Linking CXX executable ../../bin/llama-gguf-hash
[ 34%] Built target llama-gguf-hash
make[1]: *** [CMakeFiles/Makefile2:1824: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	1m1.864s
user	6m23.262s
sys	0m15.856s
+ cur=2
+ echo 2
+ set +x
cat: /home/zjy/ws/jenkins/workspace/ci-arc770/ggerganov-llama.cpp/tmp/ci_log/40643edb86eb10b471b0f57d4f3f7eb0e06a0df7/tmp/results/ctest_release-ctest.log: No such file or directory
