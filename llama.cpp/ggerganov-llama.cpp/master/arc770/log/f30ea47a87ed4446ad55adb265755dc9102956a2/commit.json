{"author": "slaren <slarengh@gmail.com>", "date": "2024-03-13 18:54:21", "title": "llama : add pipeline parallelism support", "pr_id": "6017", "files": ["CMakeLists.txt", "Makefile", "common/common.cpp", "common/common.h", "examples/batched-bench/batched-bench.cpp", "examples/embedding/embedding.cpp", "examples/llama-bench/llama-bench.cpp", "examples/llama.swiftui/llama.cpp.swift/LibLlama.swift", "examples/perplexity/perplexity.cpp", "examples/server/server.cpp", "examples/server/tests/features/embeddings.feature", "examples/server/tests/features/steps/steps.py", "ggml-alloc.c", "ggml-alloc.h", "ggml-backend-impl.h", "ggml-backend.c", "ggml-backend.h", "ggml-cuda.cu", "ggml-kompute.cpp", "ggml-metal.m", "ggml-sycl.cpp", "ggml-vulkan.cpp", "ggml.c", "llama.cpp", "llama.h"]}