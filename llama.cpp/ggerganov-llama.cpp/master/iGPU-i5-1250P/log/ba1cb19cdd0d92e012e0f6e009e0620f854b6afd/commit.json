{"author": "HimariO <dsfhe49854@gmail.com>", "date": "2024-12-14 20:43:46", "title": "llama : add Qwen2VL support + multimodal RoPE", "pr_id": "10361", "files": ["Makefile", "README.md", "convert_hf_to_gguf.py", "examples/llava/CMakeLists.txt", "examples/llava/clip.cpp", "examples/llava/clip.h", "examples/llava/llava.cpp", "examples/llava/qwen2_vl_surgery.py", "examples/llava/qwen2vl-cli.cpp", "ggml/include/ggml.h", "ggml/src/ggml-cann/ggml-cann.cpp", "ggml/src/ggml-cpu/ggml-cpu.c", "ggml/src/ggml-cuda/rope.cu", "ggml/src/ggml-kompute/ggml-kompute.cpp", "ggml/src/ggml-metal/ggml-metal.m", "ggml/src/ggml-sycl/ggml-sycl.cpp", "ggml/src/ggml-vulkan/ggml-vulkan.cpp", "ggml/src/ggml.c", "gguf-py/gguf/constants.py", "gguf-py/gguf/gguf_writer.py", "include/llama.h", "src/llama.cpp", "tests/test-backend-ops.cpp", "tests/test-rope.cpp"]}